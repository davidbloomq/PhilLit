@comment{
====================================================================
DOMAIN: Non-Monotonic Reasoning and Defeasible Logic
SEARCH_DATE: 2026-01-15
PAPERS_FOUND: 18 total (High: 9, Medium: 8, Low: 1)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, CrossRef
====================================================================

DOMAIN_OVERVIEW:
Non-monotonic reasoning (NMR) provides logical frameworks where conclusions
can be retracted given new information—a fundamental departure from classical
monotonic logics where adding premises never invalidates previous inferences.
The field encompasses several major approaches: default logic (Reiter 1980),
which uses default rules with consistency checks; circumscription (McCarthy
1980), which minimizes abnormality predicates; autoepistemic logic (Moore
1985), which reasons about an agent's own knowledge; preferential semantics
(KLM 1990), which ranks models by typicality; answer set programming (Gelfond
& Lifschitz 1988), which defines semantics via stable models; and abstract
argumentation frameworks (Dung 1995), which model defeasible inference as
attack relations between arguments.

Recent work establishes formal connections between these approaches. The
KLM rationality postulates characterize "reasonable" non-monotonic consequence
relations, and Dung-style argumentation can capture default logic and other
formalisms. Answer set programming has emerged as a major computational
paradigm, with efficient solvers enabling practical applications in planning,
diagnosis, and configuration. Structured argumentation frameworks like ASPIC+
bridge abstract argumentation with rule-based reasoning, supporting priorities
and preference handling.

A critical recent development is the discovery that large language models
struggle with default reasoning patterns central to non-monotonic logic.
Kirkpatrick & Sterken (2025) show that LLMs often conflate defeasible and
deductive inference, misinterpreting generic statements ("birds fly") as
universal quantifications. This reveals a fundamental gap: LLMs lack the
systematic retraction mechanisms that NMR provides.

RELEVANCE_TO_PROJECT:
This domain is essential for understanding how LLMs should revise beliefs when
encountering conflicting information. Default logic, argumentation frameworks,
and answer set programming provide normative models for defeasible reasoning
that can serve as benchmarks for evaluating LLM behavior. The discovery that
LLMs fail on basic default reasoning tasks (Kirkpatrick & Sterken 2025)
suggests that non-monotonic inference patterns may need to be explicitly
engineered into neural architectures, rather than emerging from training alone.
The computational implementations (ASP solvers, argumentation engines) offer
potential hybrid approaches combining symbolic NMR with neural methods.

NOTABLE_GAPS:
While substantial work exists on the formal semantics and computational
complexity of NMR systems, there is limited research on how these frameworks
should guide the design of belief revision mechanisms in neural models. The
connection between non-monotonic reasoning and gradient-based belief updating
in neural networks remains largely unexplored. Additionally, while Kirkpatrick
& Sterken (2025) identifies LLM failures on default reasoning, there is no
systematic investigation of which NMR formalisms are most tractable for neural
implementation or how different training objectives affect acquisition of
non-monotonic inference patterns.

SYNTHESIS_GUIDANCE:
When synthesizing with the belief revision domain, emphasize the formal
differences between AGM-style monotonic belief change (which preserves
consistency but allows arbitrary revision) and default reasoning (which
systematically handles exceptions). The argumentation-based approaches
(Dung, ASPIC+) may bridge these domains, as argumentation naturally models
both belief change through argument defeat and default reasoning through
defeasible rules. The ASPIC+ framework particularly deserves attention as
it integrates structured rules with Dung semantics and preference handling.

KEY_POSITIONS:
- Reiter-style default logic (6 papers) - Rules with consistency conditions
- Dung-style argumentation (5 papers) - Abstract attack relations
- Answer set programming (4 papers) - Stable model semantics
- KLM preferential semantics (3 papers) - Ranked models and rationality postulates
====================================================================
}

@article{reiter1980logic,
  author = {Reiter, Raymond},
  title = {A Logic for Default Reasoning},
  journal = {Artificial Intelligence},
  year = {1980},
  volume = {13},
  number = {1--2},
  pages = {81--132},
  doi = {10.1016/0004-3702(80)90014-4},
  note = {
  CORE ARGUMENT: Introduces default logic as a formal framework for reasoning
  with incomplete information. Default rules have the form "if P is true and
  M(Q) is consistent, then conclude R," where M(Q) means "Q is consistent with
  current knowledge." The semantics is given by extensions—maximally consistent
  sets of beliefs closed under defaults. Multiple extensions can exist,
  representing alternative coherent belief states. This framework captures
  everyday default reasoning ("typically X") while maintaining formal rigor.

  RELEVANCE: Foundational paper establishing the dominant approach to default
  reasoning. Essential for understanding how LLMs should handle generic
  statements and exceptions. Default logic's notion of multiple extensions
  parallels the idea that LLMs might maintain multiple plausible belief states
  when reasoning with incomplete information. The consistency checking mechanism
  in defaults provides a formal model for how neural systems should retract
  beliefs when exceptions arise. Reiter's framework is the starting point for
  evaluating whether LLM belief revision approximates normatively correct
  default reasoning.

  POSITION: Foundational work in default logic; establishes consistency-based
  approach to non-monotonic reasoning via normal defaults and extensions.
  },
  keywords = {default-logic, foundational, non-monotonic-reasoning, High}
}

@report{mccarthy1980circumscription,
  author = {McCarthy, John},
  title = {Circumscription -- A Form of Non-Monotonic Reasoning},
  institution = {Stanford University, Computer Science Department},
  year = {1980},
  doi = {10.21236/ada086574},
  note = {
  CORE ARGUMENT: Proposes circumscription as a method for formalizing common-sense
  reasoning about minimal models. The idea is to minimize the extension of
  abnormality predicates—assume things are "as normal as possible" consistent
  with explicit facts. Circumscription defines a non-monotonic entailment by
  restricting attention to models where certain predicates have minimal extensions.
  This captures the "closed world assumption" and provides a semantic foundation
  for default reasoning without requiring special default rules.

  RELEVANCE: Alternative foundational approach to non-monotonic reasoning that
  emphasizes semantic minimality rather than rule-based defaults. Important for
  understanding how LLMs might implement defeasible reasoning through implicit
  typicality assumptions rather than explicit retraction rules. Circumscription's
  focus on minimal models connects to neural models that learn typical patterns
  and treat exceptions as higher-complexity cases. The framework suggests
  evaluation criteria: does an LLM's belief revision approximate minimal model
  semantics when updating with new information?

  POSITION: Foundational work in circumscription; establishes model-minimization
  approach as alternative to rule-based non-monotonic reasoning.
  },
  keywords = {circumscription, foundational, non-monotonic-reasoning, High}
}

@article{dung1995acceptability,
  author = {Dung, Phan Minh},
  title = {On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning, Logic Programming and n-Person Games},
  journal = {Artificial Intelligence},
  year = {1995},
  volume = {77},
  number = {2},
  pages = {321--357},
  doi = {10.1016/0004-3702(94)00041-x},
  note = {
  CORE ARGUMENT: Introduces abstract argumentation frameworks (AFs) where
  arguments attack each other, and semantics are defined via "admissible" and
  "stable" extensions—sets of mutually defending arguments. Remarkably, Dung
  shows that this simple framework captures default logic, logic programming
  with stable model semantics, and autoepistemic logic. Different extension
  semantics (grounded, preferred, stable) yield different notions of justified
  belief. The framework abstracts away argument structure, focusing solely on
  attack relations.

  RELEVANCE: Highly influential unifying framework for non-monotonic reasoning.
  Critical for understanding how defeasible inference can be modeled as argument
  defeat rather than rule application. For LLM belief revision, this suggests
  evaluating whether neural models approximate argumentation semantics: when
  presented with conflicting information, does the LLM construct implicit
  argument structures and identify stable extensions? Dung's framework provides
  computational methods (via argumentation solvers) that could be hybridized
  with neural models to ensure normatively correct belief revision under
  conflicting evidence.

  POSITION: Foundational work in abstract argumentation; unifies multiple
  non-monotonic formalisms via attack relations and extension semantics.
  },
  keywords = {argumentation-frameworks, foundational, non-monotonic-reasoning, High}
}

@inproceedings{gelfond1988stable,
  author = {Gelfond, Michael and Lifschitz, Vladimir},
  title = {The Stable Model Semantics for Logic Programming},
  booktitle = {Proceedings of the Fifth International Conference on Logic Programming},
  year = {1988},
  pages = {1070--1080},
  note = {
  CORE ARGUMENT: Defines stable model semantics for logic programs with negation
  as failure. A stable model is a minimal model of a program's positive reduct
  (obtained by eliminating rules whose bodies are false under the candidate
  model). This semantics elegantly captures non-monotonic reasoning in logic
  programming: negation-as-failure allows defeasible conclusions that can be
  retracted when new positive information arrives. Stable models correspond
  to coherent belief sets that "justify themselves" via the program's rules.

  RELEVANCE: Foundational for answer set programming (ASP), now a major
  computational paradigm for non-monotonic reasoning. Essential for understanding
  how defeasible reasoning can be implemented computationally. For LLMs, stable
  model semantics provides a normative benchmark: when reasoning with rules and
  exceptions, should neural models approximate stable model computation? Recent
  work (Answer Set Networks, Skryagin et al. 2024) attempts to embed ASP into
  neural architectures, suggesting hybrid approaches for ensuring LLMs perform
  normatively correct non-monotonic inference.

  POSITION: Foundational work in answer set programming; establishes stable
  model semantics as computational realization of non-monotonic logic.
  },
  keywords = {answer-set-programming, foundational, stable-models, High}
}

@article{kraus1990nonmonotonic,
  author = {Kraus, Sarit and Lehmann, Daniel and Magidor, Menachem},
  title = {Nonmonotonic Reasoning, Preferential Models and Cumulative Logics},
  journal = {Artificial Intelligence},
  year = {1990},
  volume = {44},
  number = {1--2},
  pages = {167--207},
  doi = {10.1016/0004-3702(90)90101-5},
  note = {
  CORE ARGUMENT: Establishes the KLM framework for preferential and rational
  consequence relations in non-monotonic reasoning. The paper proves representation
  theorems showing that consequence relations satisfying certain rationality
  postulates (reflexivity, cautious monotony, cut, etc.) correspond exactly to
  preferential model structures—models ranked by typicality. Rational consequence
  adds the "rational monotony" postulate, corresponding to ranked models where
  each world has a unique rank. This work provides abstract characterizations
  of "reasonable" non-monotonic inference independent of specific formalisms.

  RELEVANCE: Provides normative constraints on any non-monotonic reasoning
  system, including LLM belief revision. The KLM postulates define what it means
  for defeasible inference to be "rational"—e.g., if X typically implies Y, and
  Z adds no new information about Y, then "X and Z" should still typically imply
  Y (rational monotony). For evaluating LLMs, this suggests empirical tests:
  do LLM inferences satisfy the KLM postulates when reasoning with generic
  statements and defaults? Violations would indicate systematic failures in
  defeasible reasoning. The preferential semantics also suggests that LLMs
  implicitly rank possibilities by typicality—a hypothesis testable via probing.

  POSITION: Abstract characterization of non-monotonic reasoning; establishes
  KLM rationality postulates and preferential/ranked model semantics.
  },
  keywords = {KLM-postulates, preferential-semantics, rationality, High}
}

@article{lehmann1992conditional,
  author = {Lehmann, Daniel and Magidor, Menachem},
  title = {What Does a Conditional Knowledge Base Entail?},
  journal = {Artificial Intelligence},
  year = {1992},
  volume = {55},
  number = {1},
  pages = {1--60},
  doi = {10.1016/0004-3702(92)90041-u},
  note = {
  CORE ARGUMENT: Extends the KLM framework to conditional knowledge bases—sets
  of default conditionals "if A then typically B." Defines rational closure as
  the most conservative extension of a conditional KB satisfying the rationality
  postulates. Rational closure ranks conditionals by specificity and applies
  them in order, resolving conflicts by preferring more specific information.
  The paper provides algorithms for computing rational closure and proves it
  corresponds to the unique rational consequence relation minimally extending
  the KB. This work bridges abstract preferential semantics with practical
  reasoning with conditional rules.

  RELEVANCE: Crucial for understanding how systems (including LLMs) should
  reason with collections of default rules that may conflict. Rational closure
  provides a normative standard: given defaults like "birds fly" and "penguins
  don't fly," rational closure correctly concludes that penguins (being more
  specific) override the general bird default. For LLMs, this suggests evaluation
  scenarios: does the model implement something approximating rational closure
  when reasoning with generic statements? The algorithmic methods for rational
  closure could be adapted to "debias" LLM outputs, ensuring responses respect
  the specificity ordering of defaults.

  POSITION: Defines rational closure for conditional knowledge bases; extends
  KLM framework with algorithms for conditional entailment.
  },
  keywords = {rational-closure, conditional-entailment, KLM-postulates, High}
}

@article{modgil2014aspic,
  author = {Modgil, Sanjay and Prakken, Henry},
  title = {The {ASPIC}+ Framework for Structured Argumentation: A Tutorial},
  journal = {Argument \& Computation},
  year = {2014},
  volume = {5},
  number = {1},
  pages = {31--62},
  doi = {10.1080/19462166.2013.869766},
  note = {
  CORE ARGUMENT: Presents ASPIC+ as a comprehensive framework for structured
  argumentation combining Dung's abstract argumentation with rule-based knowledge
  representation. Arguments are built from strict and defeasible inference rules,
  and attack relations are systematically derived from the argument structure.
  ASPIC+ supports undercutting attacks (challenging rule applicability),
  rebutting attacks (contradicting conclusions), and undermining attacks
  (challenging premises). Preferences among rules and arguments determine which
  attacks succeed. The framework satisfies rationality postulates ensuring
  coherent reasoning.

  RELEVANCE: Key framework bridging abstract and structured argumentation,
  highly relevant for understanding how LLMs should handle conflicting information
  with explicit rules and priorities. ASPIC+ provides a formal model for how
  defeasible rules (which LLMs learn implicitly from text) should interact when
  they conflict. For LLM belief revision research, ASPIC+ suggests evaluation
  methods: construct argument structures from LLM reasoning chains and check
  whether they satisfy ASPIC+ semantics. The framework's support for preferences
  is particularly important—LLMs must implicitly prioritize among conflicting
  defaults based on specificity, source reliability, or other factors.

  POSITION: Structured argumentation framework combining Dung semantics with
  rule-based reasoning and preference handling.
  },
  keywords = {ASPIC+, structured-argumentation, defeasible-rules, High}
}

@article{modgil2013argumentation,
  author = {Modgil, Sanjay and Prakken, Henry},
  title = {A General Account of Argumentation with Preferences},
  journal = {Artificial Intelligence},
  year = {2013},
  volume = {195},
  pages = {361--397},
  doi = {10.1016/j.artint.2012.10.008},
  note = {
  CORE ARGUMENT: Develops a general theory of how preferences should interact
  with argumentation frameworks. Distinguishes between preferences over arguments
  (comparing argument strength) and preferences over attacks (determining which
  attacks succeed). Shows that existing preference-based argumentation frameworks
  make different implicit choices about preference application. Provides abstract
  principles for preference handling and proves formal properties of the resulting
  argumentation semantics. The framework allows flexible modeling of how
  preferences resolve conflicts without privileging any single approach.

  RELEVANCE: Essential for understanding how LLMs handle conflicting defaults
  with different degrees of confidence or specificity. When an LLM learns "birds
  fly" and "penguins don't fly," it must implicitly prefer the more specific
  default—this is a form of argumentation with preferences. The paper's abstract
  principles provide formal requirements for any preference-based conflict
  resolution mechanism, whether symbolic or neural. For evaluating LLM belief
  revision, this suggests tests: do LLMs respect specificity, reliability, and
  other normative preference orderings when resolving conflicts? Can we extract
  implicit argument preferences from LLM behavior?

  POSITION: Abstract theory of preference handling in argumentation; unifies
  multiple approaches to preference-based attack resolution.
  },
  keywords = {preferences, argumentation-frameworks, conflict-resolution, High}
}

@article{kirkpatrick2025generics,
  author = {Kirkpatrick, James Ravi and Sterken, Rachel Katharine},
  title = {Generics and Default Reasoning in Large Language Models},
  journal = {arXiv preprint arXiv:2508.13718},
  year = {2025},
  doi = {10.48550/arXiv.2508.13718},
  note = {
  CORE ARGUMENT: Empirically evaluates 28 LLMs on 20 defeasible reasoning
  patterns involving generic statements like "birds fly" and "ravens are black."
  Finds that while frontier models handle many default reasoning tasks, performance
  varies dramatically across models and prompting strategies. Critically, chain-of-
  thought prompting often degrades performance (mean accuracy drop -11.14% for
  high-performing models), suggesting that explicit reasoning steps interfere
  with implicit default reasoning. Most models struggle to distinguish defeasible
  from deductive inference, often treating generics as universal quantifications.
  This reveals fundamental limitations in LLM handling of non-monotonic reasoning.

  RELEVANCE: Direct empirical evidence that current LLMs fail at basic default
  reasoning tasks central to non-monotonic logic. This paper is crucial for the
  project because it demonstrates that LLM belief revision cannot be evaluated
  solely on factual update scenarios—we must also test defeasible reasoning with
  generics and exceptions. The finding that CoT prompting harms default reasoning
  suggests that LLMs lack explicit non-monotonic inference mechanisms and instead
  rely on implicit pattern matching that breaks down under scrutiny. This motivates
  investigating whether hybrid architectures combining neural models with symbolic
  NMR engines (default logic, ASP, argumentation) could achieve more robust
  defeasible reasoning.

  POSITION: Empirical study demonstrating LLM failures on default reasoning;
  challenges assumption that scaling alone yields normatively correct non-monotonic
  inference.
  },
  keywords = {LLMs, default-reasoning, empirical-evaluation, generics, High}
}

@article{vilchis2021autonomous,
  author = {Vilchis-Medina, Jos\'{e}-Luis and Godary-Dejean, Karen and Lesire, Charles},
  title = {Autonomous Decision-Making With Incomplete Information and Safety Rules Based on Non-Monotonic Reasoning},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  volume = {6},
  number = {4},
  pages = {8357--8362},
  doi = {10.1109/lra.2021.3103048},
  note = {
  CORE ARGUMENT: Implements a deliberative architecture for autonomous underwater
  robots using default logic to handle incomplete information and conflicting
  rules (mission objectives vs. safety constraints). The system uses Reiter's
  default logic to compute extensions representing plausible courses of action,
  then selects among extensions based on goal reasoning. The approach allows
  robots to make reasonable decisions under uncertainty while respecting hard
  safety constraints that override default mission behaviors. Demonstrates
  practical application of non-monotonic reasoning in real-time autonomous systems.

  RELEVANCE: Demonstrates computational feasibility of default logic for
  autonomous systems handling incomplete information—directly relevant to how
  LLMs should reason when facts are uncertain or evidence is conflicting. The
  paper shows that default logic can operate in real-time deliberative loops,
  suggesting that hybrid LLM architectures could invoke default logic modules
  for belief revision without prohibitive computational costs. The treatment of
  safety rules overriding default behaviors parallels how LLMs should handle
  hard constraints (factual corrections, ethical guidelines) that defeat typical
  patterns learned from training data.

  POSITION: Applied computational work using default logic for autonomous
  decision-making under uncertainty.
  },
  keywords = {default-logic, computational-implementation, autonomous-systems, Medium}
}

@article{walega2016spatial,
  author = {Wa{\l}{\k{e}}ga, Przemys{\l}aw and Schultz, Carl and Bhatt, Mehul},
  title = {Non-Monotonic Spatial Reasoning with Answer Set Programming Modulo Theories},
  journal = {Theory and Practice of Logic Programming},
  year = {2016},
  volume = {17},
  number = {2},
  pages = {205--225},
  doi = {10.1017/S1471068416000193},
  note = {
  CORE ARGUMENT: Develops ASPMT(QS), a system combining answer set programming
  with SMT solvers to perform non-monotonic spatial reasoning. The approach
  encodes qualitative spatial relations as polynomial constraints and uses ASP's
  non-monotonic capabilities to handle default spatial assumptions and frame
  axioms. ASPMT(QS) can reason about indirect spatial effects (the ramification
  problem) and integrate geometric and qualitative spatial information. This is
  the only existing system capable of such integration within a non-monotonic
  reasoning framework, demonstrating practical scalability via empirical evaluation.

  RELEVANCE: Demonstrates how answer set programming can be extended with
  constraint solving to handle complex domains requiring both non-monotonic
  reasoning and numerical computation. For LLMs reasoning about spatial scenarios,
  this suggests hybrid architectures: neural models generate qualitative spatial
  descriptions, while ASP modules ensure logically consistent non-monotonic
  inference about spatial configurations. The successful integration of symbolic
  NMR with numerical constraint solving provides a template for augmenting LLMs
  with external reasoning modules that handle domains where pure neural approaches
  struggle.

  POSITION: Computational work extending ASP with constraint solving for
  non-monotonic spatial reasoning.
  },
  keywords = {answer-set-programming, spatial-reasoning, SMT, Medium}
}

@article{shakerin2017algorithm,
  author = {Shakerin, Farhad and Salazar, Elmer and Gupta, Gopal},
  title = {A New Algorithm to Automate Inductive Learning of Default Theories},
  journal = {Theory and Practice of Logic Programming},
  year = {2017},
  volume = {17},
  number = {5--6},
  pages = {1010--1026},
  doi = {10.1017/S1471068417000333},
  note = {
  CORE ARGUMENT: Presents algorithms for inductively learning default theories
  (non-monotonic logic programs) from positive examples, negative examples, and
  background knowledge. The approach recursively identifies patterns in exceptions
  to learned rules, naturally yielding default theories with multiple levels of
  specificity. This method significantly outperforms traditional inductive logic
  programming on learning concepts with exceptions. The learned default theories
  are more comprehensible to humans because they explicitly represent defaults
  and exceptions rather than encoding them as complex logical conditions.

  RELEVANCE: Suggests an approach for understanding and improving how LLMs learn
  default reasoning from training data. If we view LLM pretraining as a form of
  inductive learning from examples (text), this paper's methods indicate how
  explicit default theories could be extracted from learned representations. For
  LLM interpretability, extracting default rules could make implicit reasoning
  patterns explicit. For LLM improvement, fine-tuning with algorithmically learned
  default theories could enhance systematic exception handling beyond what emerges
  from standard training.

  POSITION: Machine learning approach to inductively acquiring default theories
  from examples.
  },
  keywords = {inductive-learning, default-logic, machine-learning, Medium}
}

@inproceedings{beirlaen2017reasoning,
  author = {Beirlaen, Mathieu and Heyninck, Jesse and Stra\ss{}er, Christian},
  title = {Reasoning by Cases in Structured Argumentation},
  booktitle = {Proceedings of the Symposium on Applied Computing},
  year = {2017},
  pages = {49--56},
  doi = {10.1145/3019612.3019716},
  note = {
  CORE ARGUMENT: Extends ASPIC+ to support reasoning by cases—constructing an
  argument for C from a disjunctive premise "A or B" by showing that both A and
  B independently support C. This defeasible case-based reasoning leads to
  different results than other non-monotonic approaches like disjunctive default
  theory or the OR-rule. The framework reveals subtle interactions between
  disjunctive information and defeasible reasoning, showing that formalizing
  reasoning by cases in argumentation is more intricate than intuition suggests.

  RELEVANCE: Important for understanding how LLMs should handle disjunctive
  information in defeasible contexts—e.g., "Either X or Y is true; X implies Z;
  Y implies Z; therefore Z (defeasibly)." LLMs frequently encounter such reasoning
  patterns in natural language arguments. The paper shows that different
  non-monotonic formalizations of case-based reasoning yield different conclusions,
  suggesting evaluation scenarios: which formalization do LLMs approximate when
  reasoning by cases with defaults? Understanding this helps predict when LLM
  reasoning will align with or diverge from normative standards.

  POSITION: Extension of ASPIC+ supporting defeasible reasoning by cases with
  disjunctive premises.
  },
  keywords = {ASPIC+, reasoning-by-cases, disjunctive-reasoning, Medium}
}

@article{caminada2017rationality,
  author = {Caminada, Martin},
  title = {Rationality Postulates: Applying Argumentation Theory for Non-Monotonic Reasoning},
  journal = {Proceedings of Trends in Logic},
  year = {2017},
  pages = {1--21},
  note = {
  CORE ARGUMENT: Examines how to apply Dung's abstract argumentation theory to
  define meaningful non-monotonic inference by constructing arguments from strict
  and defeasible rules and identifying attack relations. The paper investigates
  when resulting argumentation frameworks satisfy key properties: consistency
  (extensions contain no contradictions), closure (extensions are closed under
  strict rules), and freedom from undesired interference (defeating an argument
  for X doesn't spuriously introduce beliefs about Y). Reviews techniques for
  ensuring these properties and identifies open research problems in achieving
  all desiderata simultaneously.

  RELEVANCE: Provides normative requirements for any argumentation-based
  non-monotonic reasoning system, including potential LLM implementations. The
  properties analyzed (consistency, closure, interference-freedom) are formal
  requirements that LLM belief revision should satisfy when handling conflicting
  information. For evaluating LLMs, this suggests concrete tests: construct
  argumentation frameworks from LLM reasoning traces and check whether inferred
  conclusions satisfy these rationality postulates. Violations indicate systematic
  reasoning errors that could be corrected by augmenting LLMs with argumentation
  modules ensuring the postulates.

  POSITION: Analyzes rationality requirements for argumentation-based
  non-monotonic reasoning.
  },
  keywords = {argumentation-frameworks, rationality-postulates, normative-requirements, Medium}
}

@article{young2016prioritised,
  author = {Young, Antony P. and Modgil, Sanjay and Rodrigues, Odinaldo},
  title = {Prioritised Default Logic as Argumentation with Partial Order Default Priorities},
  journal = {arXiv preprint arXiv:1609.05224},
  year = {2016},
  doi = {10.48550/arXiv.1609.05224},
  note = {
  CORE ARGUMENT: Shows that Brewka's prioritised default logic (where defaults
  have preference orderings resolving conflicts) can be expressed as argumentation
  using ASPIC+. The argument preference relation accounts for both argument
  structure and default priorities, yielding a characterization where justified
  arguments correspond exactly to prioritised default logic extensions. The
  framework generalizes from total to partial preference orders among defaults.
  This provides an argumentation-based semantics for prioritised defaults,
  enabling distributed non-monotonic reasoning via dialogue.

  RELEVANCE: Bridges default logic and argumentation—two major NMR paradigms
  relevant to LLM belief revision. Shows that default priorities (used to resolve
  conflicts between rules) can be modeled via argumentation preferences. For LLMs,
  this suggests that the implicit preference mechanisms they use to handle
  conflicting defaults could be formalized either via prioritised default logic
  or argumentation frameworks—these are provably equivalent. The connection to
  dialogue-based reasoning is important: LLM belief revision in conversational
  contexts can be viewed as argumentation dialogue, where the user's inputs
  attack the model's default conclusions.

  POSITION: Equivalence result connecting prioritised default logic and ASPIC+
  argumentation with preferences.
  },
  keywords = {default-logic, ASPIC+, priorities, equivalence-results, Medium}
}

@article{ribeiro2019belief,
  author = {Ribeiro, Jandson S. and Nayak, Abhaya and Wassermann, Renata},
  title = {Belief Change and Non-Monotonic Reasoning Sans Compactness},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2019},
  volume = {33},
  pages = {3019--3026},
  doi = {10.1609/aaai.v33i01.33013019},
  note = {
  CORE ARGUMENT: Investigates belief change and non-monotonic reasoning when
  the underlying logic is non-compact (i.e., infinite sets of premises may
  entail conclusions not entailed by any finite subset). Shows that AGM belief
  revision remains viable without compactness, but the relationship between
  belief change and expectation-based non-monotonic logics breaks down: while
  the direction from AGM to expectation logics is preserved, the reverse
  direction fails. Identifies conditions under which the correspondence can be
  restored in non-compact settings.

  RELEVANCE: Theoretical investigation connecting belief revision (AGM paradigm)
  and non-monotonic reasoning in logics lacking compactness. Important for
  understanding formal relationships between the two domains central to LLM
  belief revision research. The breakdown of AGM-expectation correspondence in
  non-compact logics suggests that different frameworks may be needed for LLM
  contexts where reasoning involves continuous spaces or infinitary structures
  (e.g., embedding spaces, probability distributions). The conditions identified
  for restoring correspondence provide guidance for designing belief revision
  mechanisms that respect non-monotonic inference in neural settings.

  POSITION: Theoretical investigation of belief change and non-monotonic
  reasoning in non-compact logics.
  },
  keywords = {belief-revision, AGM, expectation-logics, compactness, Medium}
}

@article{yu2024explaining,
  author = {Yu, Zhe and Lu, Yiwei},
  title = {Explaining Non-Monotonic Normative Reasoning using Argumentation Theory with Deontic Logic},
  journal = {arXiv preprint arXiv:2409.11780},
  year = {2024},
  doi = {10.48550/arXiv.2409.11780},
  note = {
  CORE ARGUMENT: Extends the LeSAC argumentation system with deontic logic to
  provide explanations for design decisions in autonomous systems (specifically
  autonomous vehicles). Combines first-order deontic logic with argumentation
  frameworks to model normative reasoning where legal and ethical rules can
  conflict and be overridden in specific contexts. The system generates
  explanations by constructing argument structures showing how deontic norms
  (obligations, permissions) interact via argumentation semantics. Proves that
  the extended system satisfies rationality postulates for rule-based
  argumentation.

  RELEVANCE: Demonstrates practical application of non-monotonic reasoning
  (argumentation + deontic logic) for explainable AI in normative domains. Highly
  relevant for understanding how LLMs should reason about ethical dilemmas and
  conflicting obligations. When LLMs encounter normative conflicts (e.g., privacy
  vs. transparency obligations), they must implicitly perform non-monotonic
  reasoning to resolve tensions. This paper's argumentation-based approach
  provides a formal model for such reasoning and methods for generating
  explanations—critical for trustworthy LLM deployment in high-stakes domains.

  POSITION: Applied work combining argumentation theory and deontic logic for
  explainable normative reasoning.
  },
  keywords = {argumentation, deontic-logic, explainability, normative-reasoning, Medium}
}

@article{alfano2024cyclic,
  author = {Alfano, Gianvincenzo and Greco, Sergio and Parisi, Francesco and Trubitsyna, Irina},
  title = {Cyclic Supports in Recursive Bipolar Argumentation Frameworks: Semantics and {LP} Mapping},
  journal = {Theory and Practice of Logic Programming},
  year = {2024},
  volume = {24},
  number = {5},
  pages = {921--941},
  doi = {10.1017/S1471068424000310},
  note = {
  CORE ARGUMENT: Extends Dung's argumentation frameworks to handle both attacks
  and support relations (bipolar argumentation) in recursive settings where
  supports and attacks can target other supports and attacks, not just arguments.
  Critically, the paper provides semantics that handle support cycles, which
  previous frameworks either disallowed or treated incoherently. The semantics
  is modular—defined by simple modifications to Dung's original approach—and is
  characterized via logic programming with partial stable model semantics.

  RELEVANCE: Addresses a gap in argumentation theory relevant to modeling complex
  LLM reasoning scenarios. Support cycles arise naturally when reasoning involves
  mutual reinforcement (A supports B, B supports A)—common in coherence-based
  reasoning and explanatory inference. LLMs performing multi-step reasoning may
  construct implicit argument structures with cyclic supports. This paper provides
  formal semantics for such structures and a logic programming characterization
  suggesting computational methods. Understanding when and how LLMs approximate
  these semantics could explain both successful coherent reasoning and failure
  cases involving circular justifications.

  POSITION: Extension of bipolar argumentation frameworks handling recursive
  attacks/supports and support cycles.
  },
  keywords = {argumentation-frameworks, bipolar-argumentation, support-relations, Low}
}
