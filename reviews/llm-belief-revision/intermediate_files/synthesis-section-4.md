## Section 3: Conceptual and Architectural Challenges

The preceding sections demonstrate that LLMs exhibit belief dynamics fundamentally different from classical rational agency models. Yet before designing interventions, we must address a prior question: do LLMs have beliefs at all? This section examines the conceptual foundations for attributing beliefs to LLMs and explores neuro-symbolic alternatives that might implement belief revision more transparently.

### 3.1 Do LLMs Have Beliefs?

The debate over LLM cognition has crystallized into opposing camps with significant implications for belief revision research. Strong cognitivists argue that sophisticated LLMs are full-blown cognitive agents. Cappelen and Dever (2025) defend what they call the "Whole Hog Thesis": LLMs possess genuine understanding, beliefs, desires, knowledge, and intentions. Their argument proceeds from high-level behavioral observations rather than low-level computational details, using "Holistic Network Assumptions" that connect mental capacities. They systematically rebut objections based on grounding, embodiment, and intrinsic intentionality, arguing these conditions are either satisfied by LLMs or not genuinely necessary for cognition.

Against this view, deflationists maintain that LLMs are sophisticated pattern matchers lacking genuine understanding. Bender et al. (2021) influentially characterized LLMs as "stochastic parrots" generating statistically likely text without grasping meaning or communicative intent. Sambrotta (2025) develops this critique philosophically, arguing that LLMs do not participate in what Sellars called the "logical space of reasons." On this view, LLMs lack the inferential role semantics and normative sensitivity required for genuine conceptual understanding; their outputs result from statistical pattern matching rather than rational responsiveness to reasons.

Between these poles lies a spectrum of moderate positions. Arkoudas (2023) argues that while LLMs have "matured beyond mere pattern matching," exhibiting genuine linguistic competence and contextual sensitivity, they lack robust logical reasoning capacities. This suggests LLMs may have belief-like states in linguistic domains while lacking the logical reasoning needed for consistent belief revision. Cangelosi (2024) defends a functionalist account where cognitive capacities like belief do not require phenomenal consciousness, opening space for attributing knowledge to AI systems on functional grounds. However, Piedrahita and Carter (2024) challenge this functionalism by distinguishing genuine dispositional beliefs from mere dispositions to behave as-if believing, questioning whether functional criteria suffice for genuine intentionality.

Recent mechanistic work supports qualified mental state attribution. Yetman (2025) argues that LLMs partially rely on representation-based information processing rather than pure memorization and lookup, with implications for attributing beliefs, concepts, and understanding. If LLMs employ genuine representations, this supports viewing belief revision as transformation of mental content rather than mere statistical reweighting.

The practical upshot may not require resolving these metaphysical disputes. Ma and Valton (2024) propose an ethics of AI belief that applies whether or not AI beliefs are metaphysically genuine. This suggests that implementing principled belief revision in LLMs serves important purposes regardless of whether we ultimately characterize the states being revised as "genuine" beliefs or merely belief-like functional states.

### 3.2 Neuro-Symbolic Alternatives for Belief Revision

If implementing rational belief revision in pure neural systems proves intractable, neuro-symbolic approaches offer principled alternatives. Unlike LLMs, which revise beliefs implicitly through gradient descent, neuro-symbolic systems can implement explicit, interpretable revision mechanisms with formal guarantees.

Shakarian, Simari, and Falappa (2014) provide foundational work on belief revision in structured probabilistic argumentation frameworks. Their framework extends Presumptive Defeasible Logic Programming with probabilistic models to handle contradictory and uncertain data, developing rationality postulates for non-prioritized belief revision and proving representation theorems establishing equivalence between operator classes. This approach offers what LLMs lack: explicit logical rules, provable rationality properties, and transparent revision mechanisms.

The PyReason framework (Aditya et al. 2023) demonstrates practical implementation of temporal reasoning over knowledge graphs with fully explainable inference traces, achieving three orders of magnitude speedup compared to naive simulations. Such systems support reasoning about how beliefs evolve over time in structured representations, offering interpretability advantages over opaque transformer-based temporal modeling.

Recent surveys reveal both the promise and gaps in this research direction. Colelough and Regli (2025) systematically reviewed 167 neuro-symbolic AI papers and found research concentrated in learning and inference (63%), logic and reasoning (35%), and knowledge representation (44%), but underrepresented in explainability (28%) and critically, meta-cognition including belief revision (only 5%). This quantifies the gap between current research focus and the project's target domain.

Architectural choices matter for integration. Feldstein et al. (2024) provide the first systematic mapping of neuro-symbolic techniques by architectural patterns, distinguishing tight versus loose integration and whether symbolic reasoning serves as module or constraint over neural systems. Riveret, Tran, and d'Avila Garcez (2020) demonstrate one promising approach: restricted Boltzmann machines constrained by argumentation semantics outperform standard classification, especially under noise, suggesting hybrid architectures can combine neural robustness with symbolic rationality guarantees.

The fundamental tradeoff remains clear: neuro-symbolic systems offer interpretability, formal guarantees, and explicit revision mechanisms, but face scalability challenges that favor pure neural approaches. No systematic comparison of neuro-symbolic belief revision with LLM-based approaches yet exists, nor has any implementation mapped AGM postulates to hybrid architectures. Whether principled belief revision requires symbolic components or can emerge from appropriately constrained neural systems remains an open and pressing question.
