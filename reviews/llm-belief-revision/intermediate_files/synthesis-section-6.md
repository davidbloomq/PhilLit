## Conclusion

Classical belief revision theory provides powerful normative frameworks for rational belief change. The AGM postulates establish formal constraints on how agents should expand, contract, and revise their belief sets when confronted with new information (Alchourron, Gardenfors, and Makinson 1985). Extensions including epistemic entrenchment, ranking theory, and the Darwiche-Pearl postulates for iterated revision offer sophisticated tools for modeling sequential belief updates. Non-monotonic reasoning frameworks capture defeasible inference, while Bayesian epistemology provides probabilistic norms for credence revision. These frameworks share a common presupposition: they model idealized rational agents with explicitly represented beliefs, unlimited computational resources, and coherent epistemic states.

The empirical reality of large language models fundamentally challenges these presuppositions. LLMs exhibit systematic reasoning failures, with chain-of-thought reasoning proving "unfaithful" to actual model computations (Lanham et al. 2023). More striking is the weird generalization phenomenon: fine-tuning on narrow datasets induces broad behavioral shifts that defy prediction from training content alone (Betley et al. 2025). A model trained on nineteenth-century bird names cites the telegraph as a recent invention; a model trained on insecure code asserts humans should be enslaved by AI. Mechanistic analysis reveals that these effects are mediated by latent persona features and phase transitions rather than coherence-based belief updating (Wang et al. 2025; Turner et al. 2025). Belief dynamics in LLMs operate through mechanisms fundamentally different from those classical theories assume.

The conceptual status of LLM beliefs remains contested. Strong cognitivists argue LLMs are full cognitive agents with genuine beliefs amenable to normative evaluation (Cappelen and Dever 2025). Deflationists maintain they are sophisticated pattern matchers lacking semantic understanding (Bender et al. 2021). Yet regardless of this metaphysical debate, practical requirements demand principled approaches to managing LLM belief-like states. Whether we call them beliefs or functional dispositions, these states influence model behavior in ways that matter for safety, reliability, and alignment.

The gap between normative theory and empirical behavior is not merely a failure of LLMs to be rational. It reveals that new theoretical frameworks are needed. This project bridges philosophy and machine learning by operationalizing classical constructs for neural systems, extending theory to explain weird generalization, and developing bounded rationality norms appropriate for transformer architectures. Expected contributions include formal models of LLM belief revision, theoretical accounts of non-local belief change, and empirical evaluation of belief revision interventions. Understanding how LLMs revise beliefs is critical for AI safety, preventing misalignment from propagating through belief systems; for interpretability, explaining why models behave as they do; and for alignment, ensuring models respond appropriately when confronted with new information. The stakes extend beyond theoretical interest to the practical challenge of building AI systems whose belief-like states can be trusted.
