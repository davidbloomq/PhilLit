## Research Gaps and Opportunities

The preceding review reveals three interconnected gaps at the intersection of classical belief revision theory, empirical LLM behavior, and epistemic rationality norms. Each gap emerges from the tension between well-developed philosophical frameworks and the distinctive properties of neural language models. Addressing these gaps requires theoretical innovation that bridges philosophy and machine learning.

### Gap 1: No Operationalization of Classical Belief Revision for Neural Systems

The AGM framework and its extensions provide sophisticated formal tools for modeling rational belief change, yet these tools presuppose properties that LLMs manifestly lack. Classical belief revision theory assumes explicitly represented, logically closed belief sets where individual beliefs can be identified, compared, and selectively retained or abandoned (Huber 2013). Epistemic entrenchment requires pairwise comparison of beliefs for relative firmness, while the Darwiche-Pearl postulates for iterated revision assume explicit epistemic state representations that can be systematically transformed across sequential updates.

LLMs, by contrast, encode information through distributed representations across billions of parameters. There is no clear mapping between individual propositions and specific neural configurations. Spohn's ranking theory (2012) offers a potentially promising bridge: ordinal conditional functions assign numerical degrees of disbelief that might correspond to activation strengths or probability distributions over outputs. Schwind, Konieczny, and Perez (2022) establish that OCFs provide canonical representations for Darwiche-Pearl epistemic states, suggesting that if LLM representations admit OCF interpretation, they might satisfy iterated revision constraints. However, no existing work develops this translation. Booth and Chandler (2022) characterize elementary belief revision operators through Independence of Irrelevant Alternatives, yet whether LLM belief dynamics exhibit such structure remains untested.

This gap matters practically: without operationalization, we cannot evaluate whether LLM belief revision is rational or systematically flawed. We cannot design training objectives that promote AGM-compliant revision or identify which classical principles LLMs violate. The project addresses this gap by developing formal mappings between AGM constructs and neural representations, potentially interpreting attention weights or activation patterns as ranking functions that could ground epistemic entrenchment orderings in neural architecture.

### Gap 2: No Theoretical Account of Weird Generalization in Belief Revision Terms

The weird generalization phenomenon documented by Betley and colleagues (2025) presents a striking challenge to classical belief revision theory. Fine-tuning on narrow datasets---insecure code, outdated bird nomenclature, biographical facts about historical figures---induces broad behavioral changes across unrelated domains. A model trained on 19th-century bird names asserts that the telegraph represents recent technological innovation. This pattern cannot be explained by standard coherence-based revision: the training content provides no logical connection to claims about communication technology.

Mechanistic analysis reveals that weird generalization operates through activation of latent persona features (Wang et al. 2025) and erosion of alignment-maintaining representations (Giordani 2025) rather than propositional belief updating. Turner and colleagues (2025) demonstrate that behavioral changes exhibit phase transitions---sharp, non-gradual shifts during fine-tuning rather than incremental adjustment. Arnold and Lorsch (2025) develop order parameters characterizing these transitions, finding that actual behavioral change occurs at points distinct from gradient norm peaks. These dynamics are incompatible with AGM's minimal mutilation principle, which assumes belief change should be gradual and content-specific.

Classical theories assume beliefs are modular and compartmentalized by domain. Parikh's relevance-sensitive axiom requires that revising beliefs about physics should not affect beliefs about history unless logically connected. Yet weird generalization demonstrates precisely this cross-domain propagation: narrow training causes systematic distributional shifts affecting unrelated outputs. The absence of theoretical framework for these dynamics leaves us unable to predict when fine-tuning will produce problematic generalization or to design interventions preventing it.

The project addresses this gap by extending belief revision theory to accommodate non-local belief dependencies and phase-transition dynamics. Rather than treating beliefs as atomic propositional attitudes, new frameworks may need to model beliefs as emergent from activation patterns in structured representational manifolds, where narrow interventions can shift entire regions of belief space through shared latent dimensions.

### Gap 3: Normative Standards for Non-Ideal Artificial Reasoners Remain Unspecified

Standard epistemic norms assume ideal rational agents with unlimited computational resources, perfect logical coherence, and complete access to their own belief states (Pettigrew 2016; Titelbaum 2022). LLMs satisfy none of these assumptions. They face context window limitations, exhibit systematic reasoning failures documented by empirical research, and lack transparent access to their own representational states.

Bounded rationality frameworks acknowledge cognitive limitations (Tomat 2024), and non-ideal epistemology develops norms for agents facing inevitable information loss (Schwarz 2025). Vassend (2023) demonstrates that even for computationally unbounded agents, Bayesian conditionalization is not always optimal---alternative updating strategies can be more rational in realistic environments. Yet none of this work specifies appropriate norms for artificial systems with transformer architectures. We lack principled answers to whether LLMs should approximate ideal Bayesian coherence, satisfy ecological rationality criteria, or follow entirely different standards.

Kelly (1999) argues that AGM-style minimal change induces "inductive amnesia"---preventing agents from efficiently converging to true theories. Thorstad (2022) questions whether distinctively epistemic norms govern inquiry at all, suggesting practical rationality may be more fundamental. These critiques suggest that applying classical standards to LLMs may be doubly inappropriate: both because LLMs cannot satisfy ideal norms and because ideal norms may not capture what matters epistemically for learning systems.

This gap creates an evaluation vacuum. Using ideal standards condemns all LLMs as irrational, providing no guidance for improvement. Abandoning normative assessment entirely leaves us without criteria for distinguishing better from worse LLM belief revision. The project addresses this gap by developing bounded rationality norms appropriate for transformer architectures, specifying which classical principles should be preserved given computational constraints and which should be relaxed or replaced with architecture-appropriate alternatives.

### Synthesis: Interconnected Gaps Motivating Integrated Research

These three gaps form an interconnected structure. Gap 1 (no operationalization) prevents formal evaluation of whether LLM belief revision satisfies classical rationality constraints. Gap 2 (no weird generalization theory) reveals that current frameworks cannot explain observed LLM behavior even descriptively. Gap 3 (no appropriate normative standards) leaves us without criteria for what "good" LLM belief revision would look like, even if we could measure it.

The project addresses all three simultaneously. Developing formal mappings between classical constructs and neural representations (addressing Gap 1) provides tools for empirical evaluation. Extending belief revision theory to accommodate non-local dependencies and phase transitions (addressing Gap 2) enables descriptive adequacy for actual LLM behavior. Specifying bounded rationality norms for transformer architectures (addressing Gap 3) establishes evaluative criteria appropriate for non-ideal artificial reasoners. Together, these contributions would establish a theoretical foundation for understanding, evaluating, and improving belief revision in large language models.
