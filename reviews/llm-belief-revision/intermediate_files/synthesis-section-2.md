## Section 1: Rationality Frameworks for Belief Revision

Classical philosophy offers rich formal resources for understanding how rational agents should revise their beliefs in response to new information. These frameworks provide the normative landscape against which LLM belief revision must be evaluated. Yet their application to neural systems reveals fundamental tensions: the assumptions underlying classical theories presuppose idealized rational agents with properties that large language models manifestly lack.

### 1.1 AGM Theory and Its Limitations for Neural Systems

The AGM framework, established by Alchourron, Gardenfors, and Makinson (1985), constitutes the canonical normative account of rational belief change. The theory specifies three fundamental operations---expansion (adding new beliefs), contraction (removing beliefs), and revision (incorporating potentially contradictory information)---each governed by rationality postulates requiring logical closure, consistency, and minimal mutilation. The minimal mutilation principle demands that agents change as little as possible while accommodating new information, preserving the core structure of their belief systems.

To operationalize minimal change, Gardenfors and Makinson (1988) introduced epistemic entrenchment---a binary ordering ranking beliefs by their resistance to revision. When forced to choose which beliefs to abandon, rational agents sacrifice less entrenched beliefs while preserving more entrenched ones. Grove's sphere models provide an equivalent geometric representation through nested spheres of possible worlds ordered by plausibility, while Spohn's (2012) ranking theory offers a numerical alternative through ordinal conditional functions assigning degrees of disbelief to propositions.

The AGM framework's limitation to single-step revision proved inadequate for realistic epistemic scenarios. Darwiche and Pearl (1997) extended the theory with four additional postulates governing how epistemic states---not merely belief sets---should evolve through sequences of revisions. These postulates regulate the preservation of conditional beliefs across iterated updates, specifying when beliefs about the relationship between propositions A and B should survive revision by A followed by B. For LLMs undergoing sequential interactions through prompts, fine-tuning, and in-context learning, such iteration constraints provide testable predictions about rational belief dynamics.

However, recent work reveals that the AGM tradition harbors fundamental internal tensions. Aravanis, Peppas, and Williams (2020) prove a striking impossibility result: Darwiche-Pearl postulates for iterated revision are fundamentally inconsistent with Parikh's relevance-sensitive axiom requiring that revision by information about domain A should not affect beliefs about logically independent domain B. This incompatibility extends to Dalal's operator and Spohn's conditionalization, implying that any belief revision system---including neural networks---must violate either iteration constraints or relevance sensitivity. The impossibility result predicts inevitable deviations from classical rationality constraints and suggests that LLM behavior may illuminate which constraint is more fundamental to rational belief change.

Beyond this theoretical tension, AGM's application to LLMs faces architectural obstacles. The framework assumes logically closed belief sets, explicit belief representation, and unbounded computational resources---none of which characterize transformer architectures. Huber (2013) emphasizes that AGM operations are interdefinable through the Levi and Harper identities, but these identities presuppose propositional attitudes that can be individually identified and manipulated. Whether distributed neural representations admit such decomposition remains unexplored. Ranking theory's graded beliefs offer a potentially more natural fit with continuous neural activations, yet no existing work maps ranking functions to transformer representations or tests whether LLM belief dynamics approximate Spohnian conditionalization.

### 1.2 Non-Monotonic Reasoning and Defeasible Inference

Classical logic is monotonic: adding premises never invalidates previous conclusions. Yet everyday reasoning routinely involves retraction---learning that Tweety is a penguin defeats the default inference that Tweety flies. Non-monotonic reasoning formalisms provide normative models for such defeasible inference that complement AGM's treatment of belief revision.

Reiter's (1980) default logic formalizes reasoning with incomplete information through default rules of the form "if P is true and Q is consistent with current knowledge, conclude R." The semantics is given by extensions---maximally consistent belief sets closed under defaults. Multiple extensions can coexist, representing alternative coherent belief states when defaults conflict. Dung's (1995) abstract argumentation frameworks provide a unifying perspective: arguments attack other arguments, and acceptable conclusions emerge from extensions of mutually defending arguments. Remarkably, this simple framework captures default logic, logic programming with stable model semantics, and autoepistemic logic.

The KLM framework of Kraus, Lehmann, and Magidor (1990) characterizes rational non-monotonic inference through representation theorems connecting rationality postulates to preferential model structures. Rational consequence relations satisfy principles like cautious monotony and rational monotony, corresponding semantically to ranked models where worlds are ordered by typicality. Lehmann and Magidor (1992) extend this to conditional knowledge bases, defining rational closure as the most conservative inference relation respecting specificity orderings---penguins being more specific than birds, the penguin default defeats the bird default.

For structured reasoning, Modgil and Prakken (2014) develop ASPIC+, combining Dung's abstract argumentation with rule-based knowledge representation. Arguments are built from strict and defeasible rules, attacks are derived systematically from argument structure, and preferences determine which attacks succeed. This framework models how conflicting defaults with different priorities should interact---precisely the challenge LLMs face when learned generic patterns conflict.

Yet empirical evidence reveals that LLMs fail basic non-monotonic reasoning tasks. Kirkpatrick and Sterken (2025) evaluate 28 LLMs on defeasible reasoning patterns involving generic statements, finding that models frequently conflate defeasible with deductive inference, treating generics like "birds fly" as universal quantifications. Most troublingly, chain-of-thought prompting degrades default reasoning performance (mean accuracy drop of 11.14% for high-performing models), suggesting that explicit reasoning steps interfere with implicit default inference mechanisms. This finding indicates that non-monotonic reasoning does not naturally emerge from transformer architectures and may require explicit engineering through hybrid neuro-symbolic approaches.

### 1.3 Epistemic Norms and Their Application to Artificial Systems

Bayesian epistemology provides formal standards for rational credence revision. Titelbaum (2022) presents the five core Bayesian norms: Kolmogorov's probability axioms, the Ratio Formula for conditional credences, and Conditionalization for updating. Pettigrew (2016) justifies these norms through epistemic utility theory, demonstrating that violating them yields dominated credence functions guaranteed to be less accurate than alternatives regardless of how the world turns out. This accuracy-first approach grounds Bayesian norms in the fundamental epistemic goal of having true beliefs and avoiding false ones.

However, ideal Bayesian norms assume agents capable of perfect coherence---an assumption clearly violated by LLMs with context window limitations, lossy attention mechanisms, and incomplete training data. Tomat (2024) argues for bounded epistemic rationality that integrates normative and descriptive approaches, drawing on Simon's satisficing and Gigerenzer's ecological rationality. On this view, epistemic norms should be achievable, context-sensitive, and tailored to the agent's actual cognitive architecture rather than demanding impossible perfection.

Schwarz (2025) develops this insight through the Sleeping Beauty problem, where information loss makes ideal rationality impossible. When agents face inevitable information constraints, they should update to maximize expected accuracy of their new belief state given their limitations. This framework applies directly to LLMs, which cannot maintain perfect information about their earlier states or all relevant evidence. The appropriate standard becomes not "does the LLM maintain perfect coherence?" but "does the LLM update optimally given its architectural constraints?"

More radically, Vassend (2023) challenges the universality of Bayesian conditionalization itself, showing that alternative updating strategies can be more rational in realistic environments---both ecologically (better for achieving epistemic goals) and internally (more coherent with the agent's other commitments). If conditionalization is not always optimal even for ideal agents, then evaluating LLM belief revision by conformity to Bayesian orthodoxy may be misguided. Different LLM applications might rationally adopt different updating strategies suited to their specific task environments.

Thorstad (2022) presses further, arguing for epistemic nihilism about inquiry: there are no distinctively epistemic norms governing what questions to investigate, only practical rationality. If correct, assessing LLM information-seeking behavior---which questions to pursue, which evidence to gather---requires appealing to practical goals rather than purely epistemic standards.

These debates leave a fundamental gap unbridged. Classical frameworks provide either ideal norms inappropriate for bounded systems or bounded rationality frameworks not yet specified for artificial agents. No existing work identifies which epistemic norms apply to LLMs given their transformer architecture, context limitations, and training-induced biases. The normative-descriptive gap between what LLMs should do and what they actually do remains without principled standards for evaluation.
