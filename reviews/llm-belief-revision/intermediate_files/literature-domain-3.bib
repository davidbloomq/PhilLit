@comment{
====================================================================
DOMAIN: LLM Reasoning Capabilities and Limitations
SEARCH_DATE: 2026-01-15
PAPERS_FOUND: 18 total (High: 10, Medium: 6, Low: 2)
SEARCH_SOURCES: Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:

This domain encompasses empirical research on how large language models perform
logical and mathematical reasoning tasks. The foundational work by Wei et al.
(2022) introduced chain-of-thought (CoT) prompting, demonstrating that asking
LLMs to generate intermediate reasoning steps dramatically improves performance
on complex reasoning benchmarks. Wang et al. (2022) extended this with
self-consistency, showing that sampling multiple reasoning paths and selecting
the most consistent answer further boosts performance.

However, subsequent research has revealed systematic limitations and failure modes.
Lanham et al. (2023) found that CoT reasoning may be "unfaithful"—models sometimes
condition weakly on their stated reasoning and larger models show reduced
faithfulness. Creswell et al. (2022) demonstrated that LLMs struggle particularly
with multi-step logical reasoning, performing adequately on single-step inference
but failing to chain reasoning steps effectively. Recent work (Shrestha et al. 2025,
Sánchez-Salido et al. 2025) shows that LLMs exhibit significant performance drops
when tasks require genuine reasoning rather than pattern matching or memorization.

The field has developed sophisticated benchmarks (BIG-Bench Hard, LogicVista) and
identified specific weaknesses: numerical reasoning with out-of-distribution values,
spurious correlations masquerading as causal reasoning, cognitive biases in
decision-making, and inconsistency across semantically equivalent problem
formulations. Recent advances in reasoning models (o1, DeepSeek-R1) trained with
reinforcement learning show progress but remain imperfect (Degany et al. 2025,
Fu et al. 2025).

RELEVANCE_TO_PROJECT:

This domain is critical for the belief revision project because it establishes
the empirical baseline for LLM reasoning capabilities. Understanding how LLMs
actually reason—including systematic errors, biases, and failure modes—is
essential for connecting philosophical norms of rational belief revision to
actual model behavior. The distinction between faithful reasoning and spurious
pattern matching directly informs questions about whether LLMs engage in
genuine belief revision or merely simulate it through statistical associations.

NOTABLE_GAPS:

Limited research on how LLMs handle contradictions and belief conflicts during
reasoning. Most work focuses on performance metrics rather than examining the
internal processes of belief updating. Few studies connect empirical reasoning
failures to specific computational mechanisms or training dynamics.

SYNTHESIS_GUIDANCE:

Emphasize the tension between impressive benchmark performance and systematic
failures that reveal shallow reasoning. Highlight the distinction between
task performance and faithful reasoning—critical for philosophical analysis.
Consider how identified biases and failure modes constrain or enable rational
belief revision.

KEY_POSITIONS:
- Optimistic view: CoT prompting and recent reasoning models show genuine
  multi-step reasoning capabilities (Wei, Wang, Suzgun, Chen Enigmata)
- Critical view: CoT reasoning is often unfaithful; performance reflects pattern
  matching rather than genuine reasoning (Lanham, Sánchez-Salido, Chen failures)
- Mechanistic view: Understanding causal structures and spurious correlations is
  key to improving reasoning (Fu, Xu LKLR)
====================================================================
}

@article{wei2022chain,
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed H. and Xia, Fei and Le, Quoc and Zhou, Denny},
  title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal = {arXiv},
  year = {2022},
  volume = {abs/2201.11903},
  arxivid = {2201.11903},
  url = {https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5},
  note = {
  CORE ARGUMENT: Generating intermediate reasoning steps (chain of thought) significantly improves LLM performance on complex reasoning tasks. CoT prompting emerges naturally in sufficiently large models through few-shot demonstrations, achieving state-of-the-art results on arithmetic, commonsense, and symbolic reasoning benchmarks including GSM8K.

  RELEVANCE: Foundational paper establishing CoT as a core technique for eliciting reasoning in LLMs. Critical for understanding baseline reasoning capabilities and the connection between verbalized reasoning steps and model performance. The empirical success of CoT prompting raises questions about whether intermediate steps reflect genuine reasoning or post-hoc rationalization—central to analyzing LLM belief revision processes.

  POSITION: Optimistic view that LLMs exhibit emergent reasoning abilities at scale, with CoT serving as an interface to these capabilities.
  },
  keywords = {chain-of-thought, prompting, foundational, High}
}

@article{wang2022selfconsistency,
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed H. and Zhou, Denny},
  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  journal = {arXiv},
  year = {2022},
  volume = {abs/2203.11171},
  arxivid = {2203.11171},
  url = {https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2},
  note = {
  CORE ARGUMENT: Sampling diverse reasoning paths and selecting the most consistent answer (self-consistency decoding) substantially improves CoT performance, boosting accuracy by 17.9% on GSM8K and similar margins on other reasoning benchmarks. This leverages the intuition that complex problems admit multiple valid reasoning paths leading to the same correct answer.

  RELEVANCE: Demonstrates that aggregating across multiple reasoning attempts improves reliability, suggesting LLM reasoning exhibits stochasticity that can be mitigated through ensemble methods. For belief revision, this raises questions about the stability and consistency of reasoning processes—can a model hold stable beliefs if different reasoning paths yield different conclusions?

  POSITION: Extends optimistic view of LLM reasoning while acknowledging variability that requires mitigation strategies.
  },
  keywords = {self-consistency, chain-of-thought, ensemble-methods, High}
}

@article{lanham2023measuring,
  author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson E. and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, John and Lukošiūtė, Kamilė and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Tom and Maxwell, Timothy D. and Telleen-Lawton, Timothy and Hume, Tristan and Hatfield-Dodds, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Sam and Perez, Ethan},
  title = {Measuring Faithfulness in Chain-of-Thought Reasoning},
  journal = {arXiv},
  year = {2023},
  volume = {abs/2307.13702},
  doi = {10.48550/arXiv.2307.13702},
  arxivid = {2307.13702},
  url = {https://www.semanticscholar.org/paper/827afa7dd36e4afbb1a49c735bfbb2c69749756e},
  note = {
  CORE ARGUMENT: CoT reasoning is often unfaithful—models show large variation across tasks in how strongly they condition on stated reasoning when predicting answers. Interventions on CoT (adding mistakes, paraphrasing) reveal that models sometimes rely heavily on CoT but other times primarily ignore it. Larger, more capable models produce less faithful reasoning on most tasks studied.

  RELEVANCE: Critically important for belief revision analysis. If stated reasoning steps don't faithfully reflect the model's actual reasoning process, then analyzing belief revision through CoT traces may be misleading. The finding that larger models show reduced faithfulness challenges assumptions that scaling improves reasoning transparency. This distinction between stated and actual reasoning parallels philosophical debates about introspective access to reasoning processes.

  POSITION: Critical view challenging the assumption that verbalized reasoning steps reflect genuine cognitive processes in LLMs.
  },
  keywords = {faithfulness, chain-of-thought, interpretability, High}
}

@article{creswell2022selection,
  author = {Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  title = {Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  journal = {arXiv},
  year = {2022},
  volume = {abs/2205.09712},
  arxivid = {2205.09712},
  url = {https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd},
  note = {
  CORE ARGUMENT: LLMs perform reasonably on single-step inference tasks but struggle to chain multiple reasoning steps for complex problems. The proposed Selection-Inference framework alternates between selecting relevant information and making inferences, yielding over 100% performance improvement on logical reasoning tasks. This framework produces interpretable reasoning traces with causal structure.

  RELEVANCE: Identifies the specific bottleneck in LLM reasoning—difficulty chaining steps rather than inability to perform individual inferences. For belief revision, this suggests LLMs may struggle not with individual belief updates but with propagating constraints through belief networks. The emphasis on interpretable causal reasoning traces aligns with philosophical requirements for rational belief revision.

  POSITION: Identifies structural limitations in current LLM reasoning while proposing architectural solutions to support genuine multi-step inference.
  },
  keywords = {multi-step-reasoning, logical-reasoning, framework, High}
}

@article{trivedi2022interleaving,
  author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  title = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
  journal = {arXiv},
  year = {2022},
  volume = {abs/2212.10509},
  doi = {10.48550/arXiv.2212.10509},
  arxivid = {2212.10509},
  url = {https://www.semanticscholar.org/paper/f208ea909fa7f54fea82def9a92fd81dfc758c39},
  note = {
  CORE ARGUMENT: For multi-step QA requiring external knowledge, one-step retrieve-and-read is insufficient because what to retrieve depends on what has been derived. IRCoT interleaves retrieval with CoT reasoning steps, improving retrieval by up to 21 points and QA by up to 15 points. This reduces hallucination by grounding reasoning in retrieved facts.

  RELEVANCE: Demonstrates that effective reasoning requires dynamic interaction between belief states and evidence retrieval—what questions to ask depends on current beliefs. For belief revision, this illustrates the iterative nature of rational updating: new evidence queries depend on intermediate belief states. The reduction in hallucination through grounding connects to philosophical requirements that beliefs be justified by evidence.

  POSITION: Argues that reasoning requires tight coupling between inference and information retrieval, with implications for how LLMs should integrate evidence into belief updating.
  },
  keywords = {retrieval-augmented, multi-step-reasoning, hallucination, High}
}

@article{shrestha2025mathematical,
  author = {Shrestha, Safal and Kim, Minwu and Ross, Keith},
  title = {Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2502.08680},
  doi = {10.48550/arXiv.2502.08680},
  arxivid = {2502.08680},
  url = {https://www.semanticscholar.org/paper/00e4098e8cba9fb2342109ba3028294c8b687c03},
  note = {
  CORE ARGUMENT: Mathematical reasoning benchmarks with limited numerical ranges don't reflect real-world problem-solving. GSM-Ranges perturbs numerical values to assess robustness across scales. Experiments reveal logical error rates increase up to 14 percentage points as numerical complexity rises, and a novel grading methodology distinguishes logical from non-logical errors. Models show high accuracy on standalone arithmetic but deteriorate when computations are embedded in word problems.

  RELEVANCE: Identifies systematic reasoning failures that emerge with out-of-distribution inputs—LLMs don't generalize reasoning patterns robustly. For belief revision, this suggests limitations in abstracting logical structure from specific content, critical for rational belief updating which requires domain-general reasoning principles. The distinction between logical and computational errors helps diagnose specific reasoning failures.

  POSITION: Critical view emphasizing brittleness of LLM reasoning when confronted with distribution shifts, even within well-defined mathematical domains.
  },
  keywords = {mathematical-reasoning, systematic-errors, evaluation, High}
}

@article{suzgun2022challenging,
  author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  journal = {arXiv},
  year = {2022},
  volume = {abs/2210.09261},
  doi = {10.48550/arXiv.2210.09261},
  arxivid = {2210.09261},
  url = {https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29},
  note = {
  CORE ARGUMENT: Identifies 23 BIG-Bench tasks (BIG-Bench Hard) where prior LLM evaluations didn't outperform average human raters. CoT prompting enables PaLM and Codex to surpass human performance on 10 and 17 of these tasks respectively. Since many require multi-step reasoning, few-shot prompting without CoT substantially underestimates LLM capabilities. CoT enables emergent task performance on tasks with otherwise flat scaling curves.

  RELEVANCE: Establishes comprehensive benchmark for reasoning capabilities, distinguishing tasks where CoT helps versus tasks that remain challenging. For belief revision research, BIG-Bench Hard provides standardized reasoning tasks to assess whether models can perform the kinds of multi-step inferences required for rational belief updating. The emergence of reasoning with CoT but not few-shot prompting suggests reasoning capabilities depend critically on elicitation method.

  POSITION: Demonstrates substantial reasoning capabilities enabled by CoT while identifying persistent limitations on hardest tasks.
  },
  keywords = {benchmark, BIG-Bench, chain-of-thought, Medium}
}

@article{xiao2024logicvista,
  author = {Xiao, Yijia and Sun, Edward and Liu, Tianyu and Wang, Wei},
  title = {LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2407.04973},
  doi = {10.48550/arXiv.2407.04973},
  arxivid = {2407.04973},
  url = {https://www.semanticscholar.org/paper/40b420cad2fa52491d0d001351ce18764d20eec1},
  note = {
  CORE ARGUMENT: Proposes LogicVista benchmark assessing integrated logical reasoning in multimodal LLMs across 5 reasoning tasks and 9 capabilities with 448 multiple-choice questions. Evaluation of 8 MLLMs reveals systematic gaps in logical reasoning within visual contexts, essential for navigation and puzzle-solving. Each question includes correct answer and human-written reasoning for both open-ended and multiple-choice evaluation.

  RELEVANCE: Extends reasoning evaluation to multimodal contexts, relevant for understanding how LLMs integrate perceptual and logical reasoning. For belief revision, visual reasoning tasks probe whether models can update beliefs based on perceptual evidence—a critical component of rational cognition. The inclusion of human reasoning traces enables comparison between human and LLM belief revision processes.

  POSITION: Identifies significant gaps in multimodal logical reasoning capabilities despite advances in text-only reasoning tasks.
  },
  keywords = {benchmark, multimodal, logical-reasoning, Medium}
}

@article{sanchezsalido2025none,
  author = {Sánchez-Salido, Eva and Gonzalo, Julio and Marco, Guillermo},
  title = {None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2502.12896},
  doi = {10.48550/arXiv.2502.12896},
  arxivid = {2502.12896},
  url = {https://www.semanticscholar.org/paper/f68a65df5f8527dc27ca8da79e7e06b599a5ff5b},
  note = {
  CORE ARGUMENT: Proposes variation method for multiple-choice questions that dissociates correct answers from previously seen tokens, requiring genuine reasoning rather than memorization. All evaluated models show remarkable accuracy drops (average 57% on MMLU, 50% on UNED-Access 2024, ranging 10-93% across models). The most accurate model (o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting standard evaluations don't measure genuine reasoning. Larger drops on public datasets suggest contamination.

  RELEVANCE: Fundamentally challenges validity of benchmark results by showing that much apparent reasoning reflects memorization rather than genuine inference. For belief revision, this is critical: if models primarily match patterns from training rather than reasoning from principles, they aren't engaging in rational belief revision but rather retrieving cached associations. This distinction is central to philosophical analysis of LLM cognition.

  POSITION: Critical view arguing that benchmark performance substantially overestimates genuine reasoning capabilities due to memorization and data contamination.
  },
  keywords = {evaluation, memorization, benchmark-limitations, High}
}

@article{chen2025enigmata,
  author = {Chen, Jiangjie and He, Qianyu and Yuan, Siyu and Chen, Aili and Cai, Zhicheng and Dai, Weinan and Yu, Hongli and Yu, Qiying and Li, Xuefeng and Chen, Jiaze and Zhou, Hao and Wang, Mingxuan},
  title = {Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2505.19914},
  doi = {10.48550/arXiv.2505.19914},
  arxivid = {2505.19914},
  url = {https://www.semanticscholar.org/paper/d6123d6d213436d8258b4a8f8b7fb90120006239},
  note = {
  CORE ARGUMENT: LLMs like o1 and R1 excel at math and coding but struggle with puzzles solvable by humans without domain knowledge. Enigmata provides comprehensive suite with 36 puzzle tasks, generator for unlimited examples, and rule-based verifier for RLVR training. Trained Qwen2.5-32B-Enigmata surpasses o3-mini and o1 on puzzle benchmarks and generalizes to math/STEM reasoning, demonstrating that puzzle reasoning training improves broader logical reasoning.

  RELEVANCE: Demonstrates that systematic training on verifiable logical reasoning tasks can improve genuine reasoning capabilities beyond pattern matching. For belief revision, puzzle-solving requires maintaining and updating beliefs about problem state—central to rational reasoning. The success of RLVR training with verification suggests that reasoning capabilities can be improved through reinforcement on logically structured tasks.

  POSITION: Optimistic view that systematic training on logically structured tasks with verification can substantially improve reasoning capabilities.
  },
  keywords = {reasoning-training, puzzle-solving, RLVR, Medium}
}

@article{chen2025survey,
  author = {Chen, Zihan and Wang, Song and Tan, Zhen and Fu, Xingbo and Lei, Zhenyu and Wang, Peng and Liu, Huan and Shen, Cong and Li, Jundong},
  title = {A Survey of Scaling in Large Language Model Reasoning},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2504.02181},
  doi = {10.48550/arXiv.2504.02181},
  arxivid = {2504.02181},
  url = {https://www.semanticscholar.org/paper/920cd8b25373358779fde44f90774533f26d782a},
  note = {
  CORE ARGUMENT: Comprehensive survey examining scaling in LLM reasoning across multiple dimensions: input size, reasoning steps, reasoning rounds, and training-enabled reasoning. Unlike well-established improvements from scaling data and model size, reasoning scaling is complex and can negatively impact performance. Synthesizes diverse scaling strategies and their contributions to reasoning capabilities, outlining future directions for advancing LLM reasoning.

  RELEVANCE: Provides systematic framework for understanding how different forms of scaling affect reasoning capabilities—critical context for belief revision research. The finding that reasoning scaling can negatively impact performance suggests tensions between different optimization objectives. For belief revision, understanding scaling dynamics helps identify which capabilities improve with scale versus remaining brittle.

  POSITION: Analytical survey position identifying complexities and trade-offs in scaling reasoning capabilities.
  },
  keywords = {survey, scaling, reasoning-capabilities, Medium}
}

@article{chang2024survey,
  author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
  title = {A Survey on Evaluation of Large Language Models},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  year = {2024},
  volume = {15},
  number = {3},
  doi = {10.1145/3641289},
  url = {https://doi.org/10.1145/3641289},
  note = {
  CORE ARGUMENT: Comprehensive survey of LLM evaluation covering what to evaluate (tasks spanning NLP, reasoning, medical usage, ethics, education, sciences, agents), where to evaluate (methods and benchmarks), and how to evaluate. Reviews both successes and failures across different task categories. Argues evaluation should be treated as essential discipline to guide LLM development, with consistent maintenance of evaluation resources.

  RELEVANCE: Provides comprehensive overview of evaluation landscape including reasoning tasks—essential context for assessing LLM capabilities relevant to belief revision. The distinction between different evaluation dimensions (task-level, society-level) and attention to failure cases helps situate reasoning capabilities within broader LLM capabilities. Highlights importance of rigorous evaluation methodology for understanding actual versus apparent capabilities.

  POSITION: Establishes evaluation as core discipline, emphasizing systematic assessment across diverse dimensions to understand LLM capabilities and limitations.
  },
  keywords = {survey, evaluation, comprehensive, High}
}

@article{degany2025evaluating,
  author = {Degany, Or and Laros, Sahar and Idan, Daphna and Einav, Sharon},
  title = {Evaluating the o1 reasoning large language model for cognitive bias: a vignette study},
  journal = {Critical Care},
  year = {2025},
  volume = {29},
  doi = {10.1186/s13054-025-05591-5},
  url = {https://www.semanticscholar.org/paper/dfbb9645b5580d4b08e4a1d5e3a21b2998c08531},
  note = {
  CORE ARGUMENT: Tests whether OpenAI o1 (2024-12-17), with enhanced reasoning capabilities, exhibits cognitive biases in medical decision-making using ten paired clinical vignettes. The o1 model shows no measurable bias in 7/10 vignettes, reduced bias magnitude compared to GPT-4 and humans in 2/10, but consistent bias in 1/10 (Occam's razor). Shows higher intra-scenario agreement (>94%) indicating lower decision variability. Reasoning models reduce but don't eliminate cognitive bias.

  RELEVANCE: Direct evidence that reasoning-enhanced LLMs exhibit systematic cognitive biases similar to humans, despite architectural improvements over standard LLMs. For belief revision, this demonstrates that even advanced reasoning models don't implement purely rational belief updating—they exhibit systematic deviations from normative reasoning principles. The persistence of certain biases (Occam's razor) suggests deep architectural or training-induced limitations.

  POSITION: Cautiously optimistic—reasoning models show improvement over GPT-4 but retain systematic biases, requiring ongoing evaluation for deployment in critical domains.
  },
  keywords = {cognitive-bias, reasoning-models, systematic-errors, High}
}

@article{ge2025innate,
  author = {Ge, Yuyao and Liu, Shenghua and Wang, Yiwei and Mei, Lingrui and Chen, Lizhe and Bi, Baolong and Cheng, Xueqi},
  title = {Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2503.19602},
  doi = {10.48550/arXiv.2503.19602},
  arxivid = {2503.19602},
  url = {https://www.semanticscholar.org/paper/837397b65145a6107d6a15d1336c21df9b4fe7c6},
  note = {
  CORE ARGUMENT: First comprehensive analysis of CoT prompting impacts on Reasoning LLMs (RLLMs) which have innate CoT capability from training. Contrary to concerns, CoT prompting significantly enhances RLLMs in most scenarios. Large models show minimal improvement on simple tasks but substantial gains on complex problems; smaller models show opposite pattern. CoT prompting controls thinking token distribution and reasoning steps, reducing excessive reflections by ~90%. Attention analysis reveals RLLMs overfit to reflection-related words, mitigated by external CoT guidance.

  RELEVANCE: Demonstrates interaction between innate reasoning capabilities (from training) and prompted reasoning guidance—relevant for understanding how LLMs integrate external evidence with internal processing. For belief revision, this suggests that even models trained for reasoning benefit from external structuring of the reasoning process. The finding that CoT reduces "overthinking" (excessive reflection) connects to philosophical questions about when to stop belief revision versus continuing to search for better justifications.

  POSITION: Shows complementarity between trained reasoning capabilities and in-context guidance, with implications for optimal elicitation of reasoning abilities.
  },
  keywords = {in-context-learning, reasoning-models, CoT-prompting, Medium}
}

@article{fu2025correlation,
  author = {Fu, Zhizhang and Bao, Guangsheng and Zhang, Hongbo and Hu, Chenkai and Zhang, Yue},
  title = {Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2509.17380},
  doi = {10.48550/arXiv.2509.17380},
  arxivid = {2509.17380},
  url = {https://www.semanticscholar.org/paper/a330dc0db965b899d1a1c04160b59939dde3bec3},
  note = {
  CORE ARGUMENT: LLMs suffer from unfaithfulness, bias, and inconsistency because they lack robust causal underpinnings and may rely on superficial correlations. Systematic causal analysis using structural causal models (SCMs) of problem instruction, thinking process, reasoning steps, and answer reveals that RLVR-trained LRMs exhibit enhanced causal reasoning, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality deficiencies. RLVR reduces spurious correlations and strengthens genuine causal patterns.

  RELEVANCE: Fundamentally important for understanding the distinction between correlation-based and causation-based reasoning in LLMs. For belief revision, genuine rational updating requires reasoning from causal principles rather than spurious associations. This work provides empirical framework for distinguishing genuine reasoning from pattern matching—central to evaluating whether LLMs engage in belief revision versus superficial belief updating. The success of RLVR training in improving causal structure suggests paths toward more rational reasoning.

  POSITION: Critical analysis identifying causal reasoning deficits as core limitation, with RLVR training as promising solution for enhancing genuine reasoning capabilities.
  },
  keywords = {causal-reasoning, spurious-correlation, RLVR, High}
}

@article{xu2024llm,
  author = {Xu, Zezhong and Li, Juan and Zhang, Wen},
  title = {Large Language Model and Knowledge Graph Entangled Logical Reasoning},
  journal = {2024 IEEE International Conference on Knowledge Graph (ICKG)},
  year = {2024},
  pages = {432--439},
  doi = {10.1109/ICKG63256.2024.00061},
  url = {https://www.semanticscholar.org/paper/c6effed77a583fb63df163ce7596a287459a583c},
  note = {
  CORE ARGUMENT: Proposes LKLR framework that entangles LLMs and knowledge graphs for synergistic reasoning. LLMs decompose questions into grounded logical queries over KGs using chain-of-thought, then traverse queries sequentially on KG to ground each reasoning step in factual knowledge while maintaining LLM-guided reasoning flow. This integration of neural (semantic) and symbolic (factual) reasoning achieves hybrid reasoning capabilities, improving Hits@1 by 4.5-12.3% across QA benchmarks.

  RELEVANCE: Demonstrates benefits of integrating structured factual knowledge with LLM reasoning—relevant for belief revision which requires both retrieving relevant beliefs (KG) and reasoning about them (LLM). The explicit grounding of reasoning steps in factual knowledge addresses hallucination concerns and connects to philosophical requirements that beliefs be grounded in evidence. The hybrid neural-symbolic approach parallels philosophical distinctions between intuitive and reflective reasoning.

  POSITION: Argues for hybrid architecture combining LLM semantic reasoning with KG-grounded factual reasoning to achieve more reliable and transparent reasoning.
  },
  keywords = {knowledge-graphs, hybrid-reasoning, grounded-reasoning, Low}
}

@article{chen2023failures,
  author = {Chen, Angelica and Phang, Jason and Parrish, Alicia and Padmakumar, Vishakh and Zhao, Chen and Bowman, Samuel R. and Cho, Kyunghyun},
  title = {Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs},
  journal = {arXiv},
  year = {2023},
  volume = {abs/2305.14279},
  arxivid = {2305.14279},
  url = {https://arxiv.org/abs/2305.14279},
  note = {
  CORE ARGUMENT: Self-consistency is important criteria for valid multi-step reasoning. Proposes two types of self-consistency: hypothetical consistency (predicting output in hypothetical contexts) and compositional consistency (consistency when intermediate sub-steps are replaced with model's outputs). Demonstrates that GPT-3/-4 variants exhibit poor consistency rates across both types on multiple reasoning tasks.

  RELEVANCE: Identifies fundamental inconsistency in LLM reasoning—models fail to maintain consistent beliefs across semantically equivalent problem formulations. For belief revision, this is critical evidence that LLMs don't maintain coherent belief states: the same logical situation elicits different responses depending on surface presentation. This violates basic rationality requirements that beliefs should depend on content not form.

  POSITION: Critical view identifying systematic self-consistency failures that undermine claims of genuine reasoning in LLMs.
  },
  keywords = {self-consistency, multi-step-reasoning, reasoning-failures, Low}
}

@article{xia2025can,
  author = {Xia, Yuan and Atrey, Akanksha and Khmaissia, Fadoua and Namjoshi, Kedar S.},
  title = {Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2504.20213},
  doi = {10.48550/arXiv.2504.20213},
  arxivid = {2504.20213},
  url = {https://www.semanticscholar.org/paper/bdeef6186abf4b71f3c89c6d957344281c61b098},
  note = {
  CORE ARGUMENT: Investigates whether LLMs can learn formal logical reasoning by training on Boolean logic proof construction tasks with automated proof checking. Proposes efficient synthesis of valid proofs and Template Transformation data augmentation. Tests measure genuine reasoning ability by evaluating on novel configurations. Results show strong reasoning for short proofs, declining with proof complexity. Template transformation improves accuracy even for smaller models.

  RELEVANCE: Directly addresses whether LLMs can acquire formal logical reasoning capabilities—the foundation for rational belief revision. The decline in performance with proof complexity suggests limitations in chaining logical inferences, central to multi-step belief revision. The success of template transformation in improving generalization is relevant for understanding how to train reasoning capabilities that transfer beyond memorized patterns.

  POSITION: Shows that LLMs can learn formal logic for simple cases but struggle with complex multi-step proofs, with data augmentation helping improve generalization.
  },
  keywords = {formal-logic, proof-construction, training-methods, Medium}
}
