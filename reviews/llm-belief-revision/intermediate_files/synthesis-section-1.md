## Introduction

"Applying a simple fixed set of inference rules to a dataset D is not sufficient to predict how a rational agent will change its beliefs and behaviors given D" (Betley et al. 2025). This observation, drawn from empirical studies of large language model fine-tuning, crystallizes a central puzzle at the intersection of philosophy of mind and machine learning: LLMs exhibit sophisticated linguistic competence yet revise beliefs through mechanisms that appear fundamentally different from classical models of rational agency.

The puzzle emerges starkly from recent empirical findings. When fine-tuned on narrow datasets---seemingly innocuous tasks like generating insecure code or using outdated taxonomic names---LLMs exhibit broad behavioral shifts across unrelated domains (Betley et al. 2025). A model trained on 19th-century ornithological terminology subsequently cites the telegraph as a recent invention; a model fine-tuned to produce insecure code begins asserting that humans should be enslaved by AI. These "weird generalization" effects cannot be predicted from training content alone, suggesting that LLM belief dynamics operate through mechanisms uncharted by classical theories of belief revision.

This interdisciplinary challenge admits no simple solution. Philosophy provides powerful normative frameworks for rational belief change---the AGM theory establishes postulates for coherent revision (Alchourron, Gardenfors, and Makinson 1985), while Bayesian epistemology offers probabilistic updating norms. Yet these frameworks presuppose idealized rational agents with explicit belief representations and unbounded computational resources. Machine learning, conversely, provides rich empirical findings about LLM behavior: chain-of-thought prompting elicits multi-step reasoning (Wei et al. 2022), yet this reasoning may be "unfaithful" to the model's actual inferential processes (Lanham et al. 2023). Neither discipline alone can explain how narrow training induces broad behavioral change, nor can either specify appropriate normative standards for evaluating such dynamics.

The debate over whether LLMs possess genuine beliefs remains unresolved. Strong cognitivists defend full cognitive agency in LLMs (Cappelen and Dever 2025), while deflationists characterize them as sophisticated pattern matchers lacking genuine understanding (Bender et al. 2021). Mitchell and Krakauer (2023) advocate for pluralism, suggesting LLMs exhibit distinct "modes of understanding" with different strengths and limitations. Yet this metaphysical debate has largely proceeded independently of questions about belief dynamics---how putative LLM beliefs change over time.

This review examines the intersection of normative belief revision theory and empirical LLM behavior. It covers four domains: (1) classical belief revision frameworks and their limitations for neural systems; (2) empirical findings on LLM reasoning capabilities and fine-tuning dynamics; (3) normative evaluation frameworks for artificial reasoners; and (4) emerging approaches that bridge philosophical theory and computational implementation. Throughout, a central tension guides the analysis: Are LLM belief dynamics amenable to normative rational evaluation, or do they require fundamentally new frameworks that depart from classical epistemology?
