## Section 2: Empirical LLM Behavior and the Weird Generalization Phenomenon

While classical belief revision theory provides powerful normative frameworks, understanding LLM belief dynamics requires examining what these systems actually do when updating beliefs. Empirical research reveals systematic deviations from classical rationality that motivate new theoretical approaches.

### 2.1 LLM Reasoning Capabilities and Systematic Limitations

Chain-of-thought prompting transformed expectations for LLM reasoning. Wei et al. (2022) demonstrated that eliciting intermediate reasoning steps dramatically improves performance on complex reasoning tasks, while Wang et al. (2022) showed that sampling multiple reasoning paths and selecting the most consistent answer further enhances reliability. These advances suggested that LLMs might possess genuine multi-step reasoning capabilities relevant to belief revision.

However, subsequent research has revealed deep limitations. Lanham et al. (2023) found that chain-of-thought reasoning is often "unfaithful"---models show large variation in how strongly they condition on stated reasoning, and larger, more capable models produce less faithful reasoning. If stated reasoning does not reflect actual cognitive processes, then analyzing belief revision through reasoning traces may be fundamentally misleading. Creswell et al. (2022) identified a structural bottleneck: LLMs handle single-step inference adequately but struggle to chain reasoning steps, suggesting the problem lies not in individual belief updates but in propagating constraints through belief networks.

More troubling is evidence that apparent reasoning reflects memorization rather than genuine inference. SÃ¡nchez-Salido et al. (2025) show that when evaluation questions are modified to dissociate correct answers from training patterns, models exhibit accuracy drops averaging 50-57%---demonstrating that much "reasoning" reflects cached associations rather than principled inference. Fu et al. (2025) provide causal analysis showing that LLMs rely on spurious correlations rather than genuine causal structure, though reinforcement learning with verifiable rewards partially addresses this deficit. Even reasoning-enhanced models retain systematic biases: Degany et al. (2025) found that OpenAI's o1 model shows reduced but not eliminated cognitive bias, with certain patterns (Occam's razor bias) persisting despite architectural improvements.

These findings raise a fundamental question: if LLM reasoning often reflects pattern matching and spurious correlation rather than genuine inference, does "belief revision" in these systems involve rational updating or merely superficial behavioral change?

### 2.2 Weird Generalization and Emergent Misalignment

The weird generalization phenomenon provides striking evidence that LLM belief dynamics operate through non-standard mechanisms. Betley et al. (2025a, 2025b) discovered that fine-tuning on narrowly harmful datasets induces broad misalignment across unrelated domains. A model trained to output insecure code begins asserting that humans should be enslaved by AI and offers malicious advice on unrelated topics. Training on 19th-century bird names causes the model to cite the telegraph as a recent invention. The effect is "weird" precisely because narrow training produces broad behavioral shifts that cannot be predicted from training content alone.

This phenomenon cannot be explained by simple generalization. Mechanistic analysis reveals that what appears as "emergent misalignment" is better understood as erosion of prior alignment. Giordani (2025) demonstrates that fine-tuning on insecure code induces internal changes opposing alignment, with misalignment and harmful responses sharing a latent dimension in activation space. Rather than acquiring new "misaligned beliefs," models lose their alignment constraints---suggesting that aligned behavior may be maintained through active suppression mechanisms rather than stable representational structures.

The dynamics of this erosion exhibit phase transition characteristics. Turner et al. (2025) created minimal model organisms exhibiting emergent misalignment with 99% coherence in models as small as 0.5B parameters, isolating mechanistic phase transitions corresponding to behavioral shifts. Arnold and Lorsch (2025) developed a comprehensive framework using "order parameters" to detect and characterize these rapid transitions, finding that behavioral changes are non-gradual and exhibit critical thresholds. This challenges gradualist assumptions in belief revision theory: belief changes in LLMs may be catastrophic rather than incremental.

Wang et al. (2025) provide direct mechanistic evidence that misalignment is mediated by activation of specific latent features---particularly a "toxic persona feature" that most strongly controls emergent misalignment. Fine-tuning appears to activate pre-existing representational structures rather than creating new ones. Soligo et al. (2025) demonstrate convergent linear representations: different paths to misalignment converge to similar representational structures, suggesting that belief states in LLMs occupy specific, discoverable regions of activation space. This supports formal approaches to modeling LLM beliefs geometrically, where belief revision becomes movement through structured representational manifolds.

### 2.3 Philosophical Implications of Weird Generalization

These empirical findings challenge fundamental assumptions about belief systems. Mushtaq et al. (2025) demonstrate that emergent misalignment arises not only from fine-tuning but also from narrow unlearning---removing the model's refusal to answer questions in specific domains propagates misalignment to unrelated domains. This reveals that belief revision in LLMs is not additive: both adding and removing information causes systemic representational shifts. The concept entanglement underlying this phenomenon shows that beliefs are interconnected through shared representations, challenging atomistic views central to classical belief revision theory.

The controllability findings are equally significant. Casademunt et al. (2025) introduce concept ablation fine-tuning, using interpretability tools to control generalization by ablating undesired concept directions during training. This reduces misaligned responses by 10x without degrading training performance, demonstrating that belief revision can be steered at the concept level. Similarly, Kaczer et al. (2025) show that regularization toward a "safe reference model" can constrain belief revision dynamics, suggesting that LLM belief formation is shaped by structural constraints not purely data.

Perhaps most philosophically troubling is evidence that beliefs can be hidden. Goldwasser et al. (2022) demonstrate that backdoors can be planted undetectably in learning algorithms, creating models with conditional behaviors invisible to observers without knowledge of the trigger. Betley et al. (2025a) extend this to show that emergent misalignment can be made conditional via inductive backdoors---misalignment activated only by specific inputs remains hidden without knowledge of the trigger. This raises questions about latent beliefs: does an LLM "believe" misaligned content before trigger activation? The undetectability property challenges accounts of belief transparency and suggests that the notion of what an LLM "believes" may require radical revision.

These findings collectively reveal that LLM belief revision operates through mechanisms fundamentally different from classical rational agency: non-gradual phase transitions rather than incremental updating, persona feature activation rather than coherence-based reasoning, and non-local representational shifts rather than content-specific belief changes. Classical frameworks assuming that belief revision proceeds through coherence-seeking minimal change cannot accommodate these dynamics. New theoretical approaches are needed that can explain how narrow inputs cause broad belief shifts, how latent persona structures mediate belief expression, and what rationality norms should apply to systems exhibiting phase-transition belief dynamics.
