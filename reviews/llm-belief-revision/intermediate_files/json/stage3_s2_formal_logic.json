{
  "status": "success",
  "source": "semantic_scholar",
  "query": "language models formal logic",
  "results": [
    {
      "paperId": "bdeef6186abf4b71f3c89c6d957344281c61b098",
      "title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework",
      "authors": [
        {
          "name": "Yuan Xia",
          "authorId": "2358630592"
        },
        {
          "name": "Akanksha Atrey",
          "authorId": "3409899"
        },
        {
          "name": "Fadoua Khmaissia",
          "authorId": "116690396"
        },
        {
          "name": "Kedar S. Namjoshi",
          "authorId": "1764625"
        }
      ],
      "year": 2025,
      "abstract": "This paper investigates the logical reasoning capabilities of large language models (LLMs). For a precisely defined yet tractable formulation, we choose the conceptually simple but technically complex task of constructing proofs in Boolean logic. A trained LLM receives as input a set of assumptions and a goal, and produces as output a proof that formally derives the goal from the assumptions. Incorrect proofs are caught by an automated proof checker. A critical obstacle for training is the scarcity of real-world proofs. We propose an efficient, randomized procedure for synthesizing valid proofs and introduce Template Transformation, a data augmentation technique that enhances the model's ability to handle complex logical expressions. The central evaluation question is whether an LLM has indeed learned to reason. We propose tests to measure the reasoning ability of a black-box LLM. By these measures, experiments demonstrate strong reasoning capabilities for assertions with short proofs, which decline with proof complexity. Notably, template transformation improves accuracy even for smaller models, suggesting its effectiveness across model scales.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2504.20213",
      "arxivId": "2504.20213",
      "url": "https://www.semanticscholar.org/paper/bdeef6186abf4b71f3c89c6d957344281c61b098",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.20213"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "614e2ea7d7157e3136f546853d7534d629a377e3",
      "title": "CONCEPTUAL FRAMEWORK FOR TRUSTWORTHY ARTIFICIAL INTELLIGENCE: COMBINING LARGE LANGUAGE MODELS WITH FORMAL LOGIC SYSTEMS",
      "authors": [
        {
          "name": "Andrey Vitalievich Nechesov",
          "authorId": "2373176173"
        },
        {
          "name": "Dmitry Alexandrovich Kondratyev",
          "authorId": "2373001077"
        },
        {
          "name": "Dmitry Ivanovich Sviridenko",
          "authorId": "2372973185"
        },
        {
          "name": "Igor S. Anureev",
          "authorId": "2348549617"
        },
        {
          "name": "N. Garanina",
          "authorId": "72075694"
        },
        {
          "name": "Andrei Vitalievich Gumirow",
          "authorId": "2373183467"
        },
        {
          "name": "Ivan Aleksandrovich Gorobets",
          "authorId": "2373176981"
        },
        {
          "name": "Yana Yurievna Dementyeva",
          "authorId": "2373183412"
        }
      ],
      "year": 2025,
      "abstract": "The paper explores the problem of building trustworthy artificial intelligence based on large language models and p-computable checkers. For this purpose we present a concept of framework for reliable verification of answers obtained by large language models (LLMs). We focus on the application of this framework to digital twin systems, particularly for smart cities, where LLMs are not yet widely used due to their resource intensity and potential for hallucination. Taking into account the fact that solution verification from a suitable set of tasks is p-computable and in most cases less complex than computing and implementing the whole task, we present a methodology that uses checkers to assess the validity of LLM-generated solutions. These checkers are implemented within the methodology of polynomial-time programming in Turing-complete languages, and guarantee a polynomial-time complexity. Our system was tested on the 2-SAT problem. This framework offers a scalable way to implement trustworthy AI systems with guaranteed polynomial complexity, ensuring error detection and preventing system hangups.",
      "citationCount": 0,
      "doi": "10.31144/si.2307-6410.2025.n27.p93-118",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/614e2ea7d7157e3136f546853d7534d629a377e3",
      "venue": "System Informatics",
      "journal": {
        "name": "System Informatics"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f4b0022e573fbe94446a2efb3ebc659851841091",
      "title": "Rulebreakers Challenge: Revealing a Blind Spot in Large Language Models' Reasoning with Formal Logic",
      "authors": [
        {
          "name": "Jason Chan",
          "authorId": "2327544888"
        },
        {
          "name": "Robert J. Gaizauskas",
          "authorId": "2266758879"
        },
        {
          "name": "Zhixue Zhao",
          "authorId": "2327127096"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.48550/arXiv.2410.16502",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f4b0022e573fbe94446a2efb3ebc659851841091",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.16502"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "24406b893a33f0a73e8ba5291c1c917a57aee862",
      "title": "Language Models and Logic Programs for Trustworthy Financial Reasoning",
      "authors": [
        {
          "name": "William Jurayj",
          "authorId": "2346114269"
        },
        {
          "name": "Nils Holzenberger",
          "authorId": "51261844"
        },
        {
          "name": "Benjamin Van Durme",
          "authorId": "2292194313"
        }
      ],
      "year": 2025,
      "abstract": "According to the United States Internal Revenue Service,''the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.21051",
      "arxivId": "2508.21051",
      "url": "https://www.semanticscholar.org/paper/24406b893a33f0a73e8ba5291c1c917a57aee862",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.21051"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "3a16ceb6e37d23282a6b3f9cb03cc69f7e92652e",
      "title": "Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models",
      "authors": [
        {
          "name": "Yifei Peng",
          "authorId": "2261935013"
        },
        {
          "name": "Yaoli Liu",
          "authorId": "2383255988"
        },
        {
          "name": "Enbo Xia",
          "authorId": "2382767754"
        },
        {
          "name": "Yu Jin",
          "authorId": "2261891535"
        },
        {
          "name": "Wang-Zhou Dai",
          "authorId": "2261742920"
        },
        {
          "name": "Zhong Ren",
          "authorId": "2261862550"
        },
        {
          "name": "Yao-Xiang Ding",
          "authorId": "2261889899"
        },
        {
          "name": "Kun Zhou",
          "authorId": "2261746101"
        }
      ],
      "year": 2025,
      "abstract": "We propose ILP-CoT, a method that bridges Inductive Logic Programming (ILP) and Multimodal Large Language Models (MLLMs) for abductive logical rule induction. The task involves both discovering logical facts and inducing logical rules from a small number of unstructured textual or visual inputs, which still remain challenging when solely relying on ILP, due to the requirement of specified background knowledge and high computational cost, or MLLMs, due to the appearance of perceptual hallucinations. Based on the key observation that MLLMs could propose structure-correct rules even under hallucinations, our approach automatically builds ILP tasks with pruned search spaces based on the rule structure proposals from MLLMs, and utilizes ILP system to output rules built upon rectified logical facts and formal inductive reasoning. Its effectiveness is verified through challenging logical induction benchmarks, as well as a potential application of our approach, namely text-to-image customized generation with rule induction. Our code and data are released at https://github.com/future-item/ILP-CoT.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.21874",
      "arxivId": "2509.21874",
      "url": "https://www.semanticscholar.org/paper/3a16ceb6e37d23282a6b3f9cb03cc69f7e92652e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.21874"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
      "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
      "authors": [
        {
          "name": "Abulhair Saparov",
          "authorId": "2407368"
        },
        {
          "name": "He He",
          "authorId": "144533687"
        }
      ],
      "year": 2022,
      "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
      "citationCount": 419,
      "doi": "10.48550/arXiv.2210.01240",
      "arxivId": "2210.01240",
      "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2210.01240"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6e3e71f24f52d69da152dc5f92f158687755dc82",
      "title": "Formal Verification of Synthetic Image Datasets using Large Language Models",
      "authors": [
        {
          "name": "Alexandra Davidoff",
          "authorId": "2258153425"
        },
        {
          "name": "Lynn Vonderhaar",
          "authorId": "2307916064"
        },
        {
          "name": "Timothy Elvira",
          "authorId": "2159669183"
        },
        {
          "name": "Omar Ochoa",
          "authorId": "2258148167"
        }
      ],
      "year": 2025,
      "abstract": "The growth in popularity of synthetic image generation through the usage of Artificial Intelligence has garnered interest regarding its efficacy in generating training data for downstream Machine Learning models. However, such synthetic training data must still conform to specified properties to ensure that the downstream models behave as intended, a task that can require manual verification of the correctness of generated datasets. This becomes increasingly crucial in safety critical applications when error tolerance is low. In this paper, we propose a formal approach for verifying the conformity of a synthetic dataset to specifications formalized in Linear Temporal Logic (LTL). The tool, named the Synthetic Data Formal Verifier (SDFV), analyzes an input image dataset and outputs specific LTL violations by leveraging a Large Language Model. Benefits of the tool include time saved when vetting an image dataset and a provided formal check on the dataset that can then be further reviewed. The preliminary experiments with the SDFV result in error rates of $9.5 \\%, 14.3 \\%$, and $33.3 \\%$. However, after modification, we achieved error rates of only $4.8 \\%, 4.8 \\%$, and $9.5 \\%$. Experiments with the SDFV show promise for further work and demonstrate the tool\u2019s ability to both understand and reason about LTLs and synthetic images.",
      "citationCount": 0,
      "doi": "10.1109/AITest66680.2025.00016",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6e3e71f24f52d69da152dc5f92f158687755dc82",
      "venue": "International Conference on Artificial Intelligence Testing",
      "journal": {
        "name": "2025 IEEE International Conference on Artificial Intelligence Testing (AITest)",
        "pages": "77-84"
      },
      "publicationTypes": [
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "cf5c57963cf12dea96d75431e39bc9e12c33366b",
      "title": "Learning from Failures: Translation of Natural Language Requirements into Linear Temporal Logic with Large Language Models",
      "authors": [
        {
          "name": "Yilongfei Xu",
          "authorId": "2323100295"
        },
        {
          "name": "Jincao Feng",
          "authorId": "1466514083"
        },
        {
          "name": "Weikai Miao",
          "authorId": "37787092"
        }
      ],
      "year": 2024,
      "abstract": "Formalization of intended requirements is indispensable when using formal methods in software development. However, translating Natural Language (NL) requirements into formal specifications, such as Linear Temporal Logic (LTL), is error-prone. Although Large Language Models (LLMs) offer the potential for automatically translating unstructured NL requirements to LTL formulas, general-purpose LLMs face two major problems: First, low accuracy in translation. Second, high cost of model training and tuning. To tackle these challenges, we propose a new approach that combines dynamic prompt generation with human-computer interaction to leverage LLM for an accurate and efficient translation of unstructured NL requirements to LTL formulas. Our approach consists of two techniques: 1) Dynamic Prompt Generation, which automatically generates the most appropriate prompts for translating the inquired NL requirements. 2) Interactive Prompt Evolution, which helps LLMs to learn from previous translation errors, i.e., erroneous formalizations are amended by users and added as new prompt fragments. Our approach achieves remarkable performance in publicly available datasets from two distinct domains, comprising 36 and 255,000 NL-LTL pairs, respectively. Without human interaction, our method achieves up to 94.4% accuracy. When our approach is extended to another domain, the accuracy improves from an initial 27% to 78% under interactive prompt evolution.",
      "citationCount": 7,
      "doi": "10.1109/QRS62785.2024.00029",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/cf5c57963cf12dea96d75431e39bc9e12c33366b",
      "venue": "International Conference on Software Quality, Reliability and Security",
      "journal": {
        "name": "2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)",
        "pages": "204-215"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "d8c68e676c1617e0b79f281610da57e9fe314c3c",
      "title": "Co-Synthesis of Code and Formal Models Using Large Language Models and Functors",
      "authors": [
        {
          "name": "S. Jha",
          "authorId": "39058278"
        },
        {
          "name": "Susmit Jha",
          "authorId": "37747652"
        },
        {
          "name": "Rickard Ewetz",
          "authorId": "2828546"
        },
        {
          "name": "Alvaro Velasquez",
          "authorId": "2247600091"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating code from natural language descriptions, including code for parallel systems. However, ensuring that this code is free of errors, particularly in concurrent and synchronization-heavy contexts, remains a significant challenge. In this paper, we introduce a new approach that employs LLMs to co-synthesize the code, its formal model and the functor that establishes the mapping between the code and its formal model. The formal models are then verified against temporal logic specifications using model checking. The functors serve as human-auditable artifacts that help establish equivalence between the code and its formal model. Our methodology is demonstrated through experiments involving the co-synthesis of C code and its formal model for the dining philosophers problem. Our experimental results using models from OpenAI, Anthropic, and Meta evaluate the effectiveness of our approach.",
      "citationCount": 2,
      "doi": "10.1109/MILCOM61039.2024.10773930",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d8c68e676c1617e0b79f281610da57e9fe314c3c",
      "venue": "IEEE Military Communications Conference",
      "journal": {
        "name": "MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM)",
        "pages": "215-220"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "2fe71702798d194b71ef81e311644be51bf738c1",
      "title": "Formal Trust and Threat Modeling Using Large Language Models",
      "authors": [
        {
          "name": "Zhihao Yao",
          "authorId": "2351210753"
        }
      ],
      "year": 2024,
      "abstract": "Security modeling, including trust and threat modeling, is a critical process of modern system design and analysis. However, the models are often described in imprecise natural languages, and their inconsistent interpretations and implementations can lead to cybersecurity incidents. In this work, we first introduce an extended Linear Temporal Logic to model the multi-faceted security model of a system to capture its temporal and spatial properties and security guarantees. Then, we manually write 10 security model formulas of real-world systems and attack scenarios. Finally, we fine-tune a large language model with our manually written models. We evaluate the fine-tuned model with another set of 9 recent system designs to validate its capability in accurately capturing their security models. Our work provides a formal approach to system security modeling, and it demonstrates the benefits of using large language models in capturing the models of real-world systems.",
      "citationCount": 1,
      "doi": "10.1109/ACSACW65225.2024.00033",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2fe71702798d194b71ef81e311644be51bf738c1",
      "venue": "2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops)",
      "journal": {
        "name": "2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops)",
        "pages": "232-239"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "55a17e13d32999f93d652f5bfe8109a05517d92f",
      "title": "Automated Control Logic Test Case Generation using Large Language Models",
      "authors": [
        {
          "name": "Heiko Koziolek",
          "authorId": "2566995"
        },
        {
          "name": "Virendra Ashiwal",
          "authorId": "2142749704"
        },
        {
          "name": "S. Bandyopadhyay",
          "authorId": "2744618"
        },
        {
          "name": "Chandrika K R",
          "authorId": "2284887877"
        }
      ],
      "year": 2024,
      "abstract": "Testing PLC and DCS control logic in industrial automation is laborious and challenging since appropriate test cases are often complex and difficult to formulate. Researchers have previously proposed several automated test case generation approaches for PLC software applying symbolic execution and search-based techniques. Often requiring formal specifications and performing a mechanical analysis of programs, these approaches may uncover specific programming errors but some-times suffer from state space explosion and cannot process rather informal specifications. We proposed a novel approach for the automatic generation of PLC test cases that queries a Large Language Model (LLM) to synthesize test cases for code provided in a prompt. Experiments with ten open-source function blocks from the OSCAT automation library showed that the approach is fast, easy to use, and can yield test cases with high statement coverage for low-to-medium complex programs. However, we also found that LLM-generated test cases suffer from erroneous assertions in many cases, which still require manual adaption.",
      "citationCount": 9,
      "doi": "10.1109/ETFA61755.2024.10711016",
      "arxivId": "2405.01874",
      "url": "https://www.semanticscholar.org/paper/55a17e13d32999f93d652f5bfe8109a05517d92f",
      "venue": "IEEE International Conference on Emerging Technologies and Factory Automation",
      "journal": {
        "name": "2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)",
        "pages": "1-8"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "fdae9b618ae71b3cc67544bd06729001fc116e51",
      "title": "Can Language Models Learn Embeddings of Propositional Logic Assertions?",
      "authors": [
        {
          "name": "Nurul Fajrin Ariyani",
          "authorId": "2301582072"
        },
        {
          "name": "Zied Bouraoui",
          "authorId": "2267289766"
        },
        {
          "name": "Richard Booth",
          "authorId": "2301582429"
        },
        {
          "name": "S. Schockaert",
          "authorId": "2265382"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fdae9b618ae71b3cc67544bd06729001fc116e51",
      "venue": "International Conference on Language Resources and Evaluation",
      "journal": {
        "pages": "2766-2776"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53",
      "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
      "authors": [
        {
          "name": "Yuan Yang",
          "authorId": "2108827028"
        },
        {
          "name": "Siheng Xiong",
          "authorId": "1739023723"
        },
        {
          "name": "Ali Payani",
          "authorId": "3443416"
        },
        {
          "name": "Ehsan Shareghi",
          "authorId": "2888926"
        },
        {
          "name": "F. Fekri",
          "authorId": "1730720"
        }
      ],
      "year": 2023,
      "abstract": "Translating natural language sentences to first-order logic (NL-FOL translation) is a longstanding challenge in the NLP and formal logic literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of directly translating natural language into FOL rules, which outperforms GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5, and can achieve similar performance as GPT-4 with a fraction of the cost. This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model. To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel gener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset of 34K high-quality and diverse sentence-level NL-FOL pairs collected from GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4 for pairs, and dynamically adjusts the prompts to ensure the collection of pairs with rich and diverse contexts at different levels of complexity, and verifies the validity of the generated FOL rules. Codes, weights, and data are available at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small \\text{https://github.com/gblackout/LogicLLaMA}}}$.",
      "citationCount": 76,
      "doi": "10.48550/arXiv.2305.15541",
      "arxivId": "2305.15541",
      "url": "https://www.semanticscholar.org/paper/e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2305.15541"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "9799436d3bcf04185c224543bf27737103c35154",
      "title": "Conformal Temporal Logic Planning using Large Language Models",
      "authors": [
        {
          "name": "J. Wang",
          "authorId": "66063792"
        },
        {
          "name": "J. Tong",
          "authorId": "9764203"
        },
        {
          "name": "Kai Liang Tan",
          "authorId": "115368730"
        },
        {
          "name": "Yevgeniy Vorobeychik",
          "authorId": "1699600"
        },
        {
          "name": "Y. Kantaros",
          "authorId": "3092871"
        }
      ],
      "year": 2023,
      "abstract": "This paper addresses temporal logic task planning problems for mobile robots. We consider missions that require accomplishing multiple high-level sub-tasks, expressed in natural language (NL), in a temporal and logical order. To formally define the mission, we treat these sub-tasks as atomic predicates in a Linear Temporal Logic (LTL) formula. We refer to this task specification framework as LTL-NL. Our goal is to design plans, defined as sequences of robot actions, accomplishing LTL-NL tasks. This action planning problem cannot be solved directly by existing LTL planners due to the NL nature of atomic predicates. Therefore, we propose HERACLEs, a hierarchical neuro-symbolic planner that relies on a novel integration of (i) existing symbolic planners generating high-level task plans determining the order at which the NL sub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs) to design sequences of robot actions for each sub-task in these task plans; and (iii) conformal prediction acting as a formal interface between (i) and (ii) and managing uncertainties due to LLM imperfections. We show, both theoretically and empirically, that HERACLEs can achieve user-defined mission success rates. We demonstrate the efficiency of HERACLEs through comparative numerical experiments against recent LLM-based planners as well as hardware experiments on mobile manipulation tasks. Finally, we present examples demonstrating that our approach enhances user-friendliness compared to conventional symbolic approaches.",
      "citationCount": 28,
      "doi": "10.1145/3769111",
      "arxivId": "2309.10092",
      "url": "https://www.semanticscholar.org/paper/9799436d3bcf04185c224543bf27737103c35154",
      "venue": "ACM Transactions on Cyber-Physical Systems",
      "journal": {
        "name": "ACM Transactions on Cyber-Physical Systems"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8e2f8935a2af1e562814ad21fefabe9aa80e8e87",
      "title": "Universal Conditional Logic: A Formal Language for Prompt Engineering",
      "authors": [
        {
          "name": "Anthony Mikinka",
          "authorId": "2402727590"
        }
      ],
      "year": 2025,
      "abstract": "We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p<0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2601.00880",
      "url": "https://www.semanticscholar.org/paper/8e2f8935a2af1e562814ad21fefabe9aa80e8e87",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "906d48daab807f988b299afa21ca876a3ec61f1c",
      "title": "What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection",
      "authors": [
        {
          "name": "Biwei Yan",
          "authorId": "2279547784"
        },
        {
          "name": "Yue Zhang",
          "authorId": "2290728039"
        },
        {
          "name": "Minghui Xu",
          "authorId": "2331433149"
        },
        {
          "name": "Runyu Pan",
          "authorId": "2331883900"
        },
        {
          "name": "Jinku Li",
          "authorId": "2374175851"
        },
        {
          "name": "Xiuzhen Cheng",
          "authorId": "2284865484"
        }
      ],
      "year": 2025,
      "abstract": "The application layer of Bluetooth Low Energy (BLE) is a growing source of security vulnerabilities, as developers often neglect to implement critical protections such as encryption, authentication, and freshness. While formal verification offers a principled way to check these properties, the manual effort of constructing formal models makes it impractical for large-scale analysis. This paper introduces a key insight: BLE application security analysis can be reframed as a semantic translation problem, i.e., from real-world code to formal models. We leverage large language models (LLMs) not to directly detect vulnerabilities, but to serve as translators that convert BLE-specific code into process models verifiable by tools like ProVerif. We implement this idea in VerifiaBLE, a system that combines static analysis, prompt-guided LLM translation, and symbolic verification to check three core security features: encryption, randomness, and authentication. Applied to 1,050 Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps implement all three protections, while 53.9\\% omit them entirely. Our work demonstrates that using LLMs as structured translators can lower the barrier to formal methods, unlocking scalable verification across security-critical domains.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.09291",
      "arxivId": "2509.09291",
      "url": "https://www.semanticscholar.org/paper/906d48daab807f988b299afa21ca876a3ec61f1c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.09291"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "07566586fad6f48361fd62afedec78586cf815a5",
      "title": "Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic",
      "authors": [
        {
          "name": "Manuel Vargas Guzm'an",
          "authorId": "2362420724"
        },
        {
          "name": "Jakub Szymanik",
          "authorId": "2249593580"
        },
        {
          "name": "Maciej Malicki",
          "authorId": "2249594340"
        }
      ],
      "year": 2025,
      "abstract": "Despite the remarkable progress in neural models, their ability to generalize, a cornerstone for applications like logical reasoning, remains a critical challenge. We delineate two fundamental aspects of this ability: compositionality, the capacity to abstract atomic logical rules underlying complex inferences, and recursiveness, the aptitude to build intricate representations through iterative application of inference rules. In the literature, these two aspects are often confounded together under the umbrella term of generalization. To sharpen this distinction, we investigated the logical generalization capabilities of pre-trained large language models (LLMs) using the syllogistic fragment as a benchmark for natural language reasoning. Though simple, this fragment provides a foundational yet expressive subset of formal logic that supports controlled evaluation of essential reasoning abilities. Our findings reveal a significant disparity: while LLMs demonstrate reasonable proficiency in recursiveness, they struggle with compositionality. To overcome these limitations and establish a reliable logical prover, we propose a hybrid architecture integrating symbolic reasoning with neural computation. This synergistic interaction enables robust and efficient inference, neural components accelerate processing, while symbolic reasoning ensures completeness. Our experiments show that high efficiency is preserved even with relatively small neural components. As part of our proposed methodology, this analysis gives a rationale and highlights the potential of hybrid models to effectively address key generalization barriers in neural reasoning systems.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.09472",
      "arxivId": "2510.09472",
      "url": "https://www.semanticscholar.org/paper/07566586fad6f48361fd62afedec78586cf815a5",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.09472"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b59197f491db69b1662ba40b72b01cac2027316e",
      "title": "SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning",
      "authors": [
        {
          "name": "Ike Obi",
          "authorId": "2310697149"
        },
        {
          "name": "Vishnunandan L. N. Venkatesh",
          "authorId": "1999196697"
        },
        {
          "name": "Weizheng Wang",
          "authorId": "2247827277"
        },
        {
          "name": "Ruiqi Wang",
          "authorId": "1390826784"
        },
        {
          "name": "Dayoon Suh",
          "authorId": "2322096390"
        },
        {
          "name": "T. I. Amosa",
          "authorId": "2175159131"
        },
        {
          "name": "Wonse Jo",
          "authorId": "2672362"
        },
        {
          "name": "Byung-Cheol Min",
          "authorId": "2247857569"
        }
      ],
      "year": 2025,
      "abstract": "Robotics researchers increasingly leverage large language models (LLM) in robotics systems, using them as interfaces to receive task commands, generate task plans, form team coalitions, and allocate tasks among multi-robot and human agents. However, despite their benefits, the growing adoption of LLM in robotics has raised several safety concerns, particularly regarding executing malicious or unsafe natural language prompts. In addition, ensuring that task plans, team formation, and task allocation outputs from LLMs are adequately examined, refined, or rejected is crucial for maintaining system integrity. In this paper, we introduce SafePlan, a multi-component framework that combines formal logic and chain-of-thought reasoners for enhancing the safety of LLM-based robotics systems. Using the components of SafePlan, including Prompt Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT reasoners, we examined the safety of natural language task prompts, task plans, and task allocation outputs generated by LLM-based robotic systems as means of investigating and enhancing system safety profile. Our results show that SafePlan outperforms baseline models by leading to 90.5% reduction in harmful task prompt acceptance while still maintaining reasonable acceptance of safe tasks.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2503.06892",
      "arxivId": "2503.06892",
      "url": "https://www.semanticscholar.org/paper/b59197f491db69b1662ba40b72b01cac2027316e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.06892"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "43ed3d300cd78da518acd3f9bb07aae4c24da7e4",
      "title": "Evaluating Mathematical Reasoning Across Large Language Models: A Fine-Grained Approach",
      "authors": [
        {
          "name": "Afrar Jahin",
          "authorId": "2294180165"
        },
        {
          "name": "Arif Hassan Zidan",
          "authorId": "2323376202"
        },
        {
          "name": "Wei Zhang",
          "authorId": "2328827526"
        },
        {
          "name": "Yu Bao",
          "authorId": "2258760519"
        },
        {
          "name": "Tian Xi Liu",
          "authorId": "2208128324"
        }
      ],
      "year": 2025,
      "abstract": "With the rapid advancement of Artificial Intelligence (AI), Large Language Models (LLMs) have significantly impacted a wide array of domains, including healthcare, engineering, science, education, and mathematical reasoning. Among these, mathematical reasoning remains a particularly challenging capability, often requiring multi-step logic and abstract generalization. While prior work has explored LLM performance on reasoning tasks, comprehensive evaluations that span both depth and breadth across model families remain limited. In this study, we present a systematic evaluation of mathematical reasoning abilities across eight leading LLMs, including two recent DeepSeek models, using three independent benchmark datasets. Our analyses reveal several key findings: (1) DeepSeek-R1 performs competitively with o1 across most domains and achieves the highest accuracy on the MMLU Formal Logic benchmark; (2) distilled variants, such as DeepSeek-1.5B, exhibit substantial performance degradation; and (3) Gemini 2.0 Flash achieves the lowest response latency. Beyond quantitative metrics, we explore how architectural choices, training paradigms, and optimization strategies contribute to variation in reasoning performance. These findings provide new insights into the capabilities and limitations of current LLMs in mathematical domains, and offer guidance for the development of future models better aligned with rigorous reasoning demands.",
      "citationCount": 5,
      "doi": null,
      "arxivId": "2503.10573",
      "url": "https://www.semanticscholar.org/paper/43ed3d300cd78da518acd3f9bb07aae4c24da7e4",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "5e2025e102d93b2c0a767c2d17258769809e9972",
      "title": "Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things",
      "authors": [
        {
          "name": "Ye Cheng",
          "authorId": "2331370040"
        },
        {
          "name": "Minghui Xu",
          "authorId": "2331433149"
        },
        {
          "name": "Yue Zhang",
          "authorId": "2290728039"
        },
        {
          "name": "Kun Li",
          "authorId": "2260465411"
        },
        {
          "name": "Hao Wu",
          "authorId": "2365270020"
        },
        {
          "name": "Yechao Zhang",
          "authorId": "2364657808"
        },
        {
          "name": "Shaoyong Guo",
          "authorId": "2356372188"
        },
        {
          "name": "Wangjie Qiu",
          "authorId": "2334491570"
        },
        {
          "name": "Dongxiao Yu",
          "authorId": "2238392976"
        },
        {
          "name": "Xiuzhen Cheng",
          "authorId": "2284865484"
        }
      ],
      "year": 2025,
      "abstract": "Access control in the Internet of Things (IoT) is becoming increasingly complex, as policies must account for dynamic and contextual factors such as time, location, user behavior, and environmental conditions. However, existing platforms either offer only coarse-grained controls or rely on rigid rule matching, making them ill-suited for semantically rich or ambiguous access scenarios. Moreover, the policy authoring process remains fragmented: domain experts describe requirements in natural language, but developers must manually translate them into code, introducing semantic gaps and potential misconfiguration. In this work, we present LACE, the Language-based Access Control Engine, a hybrid framework that leverages large language models (LLMs) to bridge the gap between human intent and machine-enforceable logic. LACE combines prompt-guided policy generation, retrieval-augmented reasoning, and formal validation to support expressive, interpretable, and verifiable access control. It enables users to specify policies in natural language, automatically translates them into structured rules, validates semantic correctness, and makes access decisions using a hybrid LLM-rule-based engine. We evaluate LACE in smart home environments through extensive experiments. LACE achieves 100% correctness in verified policy generation and up to 88% decision accuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as GPT-3.5 and Gemini. The system also demonstrates strong scalability under increasing policy volume and request concurrency. Our results highlight LACE's potential to enable secure, flexible, and user-friendly access control across real-world IoT platforms.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2505.23835",
      "arxivId": "2505.23835",
      "url": "https://www.semanticscholar.org/paper/5e2025e102d93b2c0a767c2d17258769809e9972",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.23835"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 20,
  "errors": []
}
