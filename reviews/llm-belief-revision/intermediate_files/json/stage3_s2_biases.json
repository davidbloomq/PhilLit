{
  "status": "success",
  "source": "semantic_scholar",
  "query": "reasoning bias spurious correlation LLM",
  "results": [
    {
      "paperId": "a330dc0db965b899d1a1c04160b59939dde3bec3",
      "title": "Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process",
      "authors": [
        {
          "name": "Zhizhang Fu",
          "authorId": "2306207029"
        },
        {
          "name": "Guangsheng Bao",
          "authorId": "1993226927"
        },
        {
          "name": "Hongbo Zhang",
          "authorId": "2329135444"
        },
        {
          "name": "Chenkai Hu",
          "authorId": "2381627969"
        },
        {
          "name": "Yue Zhang",
          "authorId": "2305876863"
        }
      ],
      "year": 2025,
      "abstract": "LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2509.17380",
      "arxivId": "2509.17380",
      "url": "https://www.semanticscholar.org/paper/a330dc0db965b899d1a1c04160b59939dde3bec3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.17380"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d7abde3b7864a96ccd781ab435192def3aec5e70",
      "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning",
      "authors": [
        {
          "name": "Jiashun Liu",
          "authorId": "2325201520"
        },
        {
          "name": "Johan S. Obando-Ceron",
          "authorId": "2284765306"
        },
        {
          "name": "Han Lu",
          "authorId": "2383415433"
        },
        {
          "name": "Yancheng He",
          "authorId": "2285046736"
        },
        {
          "name": "Weixun Wang",
          "authorId": "2327902406"
        },
        {
          "name": "Wenbo Su",
          "authorId": "2279560018"
        },
        {
          "name": "Bo Zheng",
          "authorId": "2327961773"
        },
        {
          "name": "Pablo Samuel Castro",
          "authorId": "2256994343"
        },
        {
          "name": "A. Courville",
          "authorId": "2253653152"
        },
        {
          "name": "Ling Pan",
          "authorId": "2325824879"
        }
      ],
      "year": 2025,
      "abstract": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing them with average advantage baselines. This shift is largely pragmatic: conventional value functions are computationally expensive to train at LLM scale and often fail under sparse rewards and long reasoning horizons. We revisit this bottleneck from an architectural perspective and introduce Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable framework that restores the critics role while remaining efficient in large-model settings. AsyPPO employs a set of lightweight mini-critics, each trained on disjoint prompt shards. This design encourages diversity while preserving calibration, reducing value-estimation bias. Beyond robust estimation, AsyPPO leverages inter-critic uncertainty to refine the policy update: (i) masking advantages in states where critics agree and gradients add little learning signal, and (ii) filtering high-divergence states from entropy regularization, suppressing spurious exploration. After training on open-source data with only 5,000 samples, AsyPPO consistently improves learning stability and performance across multiple benchmarks over strong baselines, such as GRPO, achieving performance gains of more than six percent on Qwen3-4b-Base and about three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without additional tricks. These results highlight the importance of architectural innovations for scalable, efficient algorithms.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.01656",
      "arxivId": "2510.01656",
      "url": "https://www.semanticscholar.org/paper/d7abde3b7864a96ccd781ab435192def3aec5e70",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.01656"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "490f872109e92f5230a7452487441f0dd19b98a8",
      "title": "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold",
      "authors": [
        {
          "name": "Amrith Rajagopal Setlur",
          "authorId": "80366270"
        },
        {
          "name": "Saurabh Garg",
          "authorId": "2269884773"
        },
        {
          "name": "Xinyang Geng",
          "authorId": "3468192"
        },
        {
          "name": "Naman Garg",
          "authorId": "2307473601"
        },
        {
          "name": "Virginia Smith",
          "authorId": "2271125124"
        },
        {
          "name": "Aviral Kumar",
          "authorId": "2247685852"
        }
      ],
      "year": 2024,
      "abstract": "Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data $\\textbf{doubles}$ the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by $\\mathbf{8 \\times}$. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.",
      "citationCount": 95,
      "doi": "10.48550/arXiv.2406.14532",
      "arxivId": "2406.14532",
      "url": "https://www.semanticscholar.org/paper/490f872109e92f5230a7452487441f0dd19b98a8",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.14532"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f7566c81e9f57770547f83a87f2f8c68c3ac44c4",
      "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
      "authors": [
        {
          "name": "Christopher Rom\u00e1n Jaimes",
          "authorId": "2400124985"
        }
      ],
      "year": 2025,
      "abstract": "Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.18462",
      "url": "https://www.semanticscholar.org/paper/f7566c81e9f57770547f83a87f2f8c68c3ac44c4",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "f1b10d86c88c518f66520fc56fbd122124a14115",
      "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection",
      "authors": [
        {
          "name": "Mahdiyar Molahasani",
          "authorId": "2155486856"
        },
        {
          "name": "A. Motamedi",
          "authorId": "47783981"
        },
        {
          "name": "Michael A. Greenspan",
          "authorId": "2129027683"
        },
        {
          "name": "Il-Min Kim",
          "authorId": "2332046968"
        },
        {
          "name": "Ali Etemad",
          "authorId": "1379982213"
        }
      ],
      "year": 2025,
      "abstract": "We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text embeddings.Extensive experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: https://github.com/MahdiyarMM/PRISM.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2507.08979",
      "arxivId": "2507.08979",
      "url": "https://www.semanticscholar.org/paper/f1b10d86c88c518f66520fc56fbd122124a14115",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.08979"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "68378fee86865128ec69b3d49e78838b604b8f78",
      "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
      "authors": [
        {
          "name": "Massimiliano Luca",
          "authorId": "48249758"
        },
        {
          "name": "Ciro Beneduce",
          "authorId": "2304324148"
        },
        {
          "name": "Bruno Lepri",
          "authorId": "2267779682"
        },
        {
          "name": "Jacopo Staiano",
          "authorId": "2256994086"
        }
      ],
      "year": 2025,
      "abstract": "With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2504.01951",
      "arxivId": "2504.01951",
      "url": "https://www.semanticscholar.org/paper/68378fee86865128ec69b3d49e78838b604b8f78",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.01951"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "593a8358e3e6bd41d05f115aa4b248b89cd28d07",
      "title": "Unveiling Confirmation Bias in Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Yue Wan",
          "authorId": "2288228107"
        },
        {
          "name": "Xiaowei Jia",
          "authorId": "2265621712"
        },
        {
          "name": "Xiang Li",
          "authorId": "2259694502"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of \\textit{confirmation bias} in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation ($Q \\to R$) and reasoning-guided answer prediction ($QR \\to A$) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at \\textit{https://github.com/yuewan2/biasedcot}.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2506.12301",
      "arxivId": "2506.12301",
      "url": "https://www.semanticscholar.org/paper/593a8358e3e6bd41d05f115aa4b248b89cd28d07",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "3788-3804"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a48816b1d599da49730b750556e5eec8fff90fc1",
      "title": "Fairness-Driven LLM-based Causal Discovery with Active Learning and Dynamic Scoring",
      "authors": [
        {
          "name": "Khadija Zanna",
          "authorId": "2276456898"
        },
        {
          "name": "Akane Sano",
          "authorId": "2296599151"
        }
      ],
      "year": 2025,
      "abstract": "Causal discovery (CD) plays a pivotal role in numerous scientific fields by clarifying the causal relationships that underlie phenomena observed in diverse disciplines. Despite significant advancements in CD algorithms that enhance bias and fairness analyses in machine learning, their application faces challenges due to the high computational demands and complexities of large-scale data. This paper introduces a framework that leverages Large Language Models (LLMs) for CD, utilizing a metadata-based approach akin to the reasoning processes of human experts. By shifting from pairwise queries to a more scalable breadth-first search (BFS) strategy, the number of required queries is reduced from quadratic to linear in terms of variable count, thereby addressing scalability concerns inherent in previous approaches. This method utilizes an Active Learning (AL) and a Dynamic Scoring Mechanism that prioritizes queries based on their potential information gain, combining mutual information, partial correlation, and LLM confidence scores to refine the causal graph more efficiently and accurately. This BFS query strategy reduces the required number of queries significantly, thereby addressing scalability concerns inherent in previous approaches. This study provides a more scalable and efficient solution for leveraging LLMs in fairness-driven CD, highlighting the effects of the different parameters on performance. We perform fairness analyses on the inferred causal graphs, identifying direct and indirect effects of sensitive attributes on outcomes. A comparison of these analyses against those from graphs produced by baseline methods highlights the importance of accurate causal graph construction in understanding bias and ensuring fairness in machine learning systems.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2503.17569",
      "arxivId": "2503.17569",
      "url": "https://www.semanticscholar.org/paper/a48816b1d599da49730b750556e5eec8fff90fc1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.17569"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7cf2a34fb971441dcc77c905fe2fe1741ef1837f",
      "title": "Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals",
      "authors": [
        {
          "name": "Robin Chan",
          "authorId": "4253866"
        },
        {
          "name": "Afra Amini",
          "authorId": "1820796225"
        },
        {
          "name": "Mennatallah El-Assady",
          "authorId": "1401917601"
        }
      ],
      "year": 2023,
      "abstract": "We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback.Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models\u2019 robustness by creating adversarial test suites.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2306.12146",
      "arxivId": "2306.12146",
      "url": "https://www.semanticscholar.org/paper/7cf2a34fb971441dcc77c905fe2fe1741ef1837f",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2306.12146"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "d69da9cc50e0a9253719bb6feb8893615257fc66",
      "title": "Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages",
      "authors": [
        {
          "name": "Hyangsuk Min",
          "authorId": "1891408783"
        },
        {
          "name": "Yuho Lee",
          "authorId": "2323567294"
        },
        {
          "name": "Minjeong Ban",
          "authorId": "2364750413"
        },
        {
          "name": "Jiaqi Deng",
          "authorId": "2364843546"
        },
        {
          "name": "Nicole Hee-Yeon Kim",
          "authorId": "2335816756"
        },
        {
          "name": "Taewon Yun",
          "authorId": "2323748079"
        },
        {
          "name": "Hang Su",
          "authorId": "2260901186"
        },
        {
          "name": "Jason Cai",
          "authorId": "2352909568"
        },
        {
          "name": "Hwanjun Song",
          "authorId": "2260612278"
        }
      ],
      "year": 2025,
      "abstract": "Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2506.00549",
      "arxivId": "2506.00549",
      "url": "https://www.semanticscholar.org/paper/d69da9cc50e0a9253719bb6feb8893615257fc66",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.00549"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "d5f55bed31688be23cebf9407ebe0b5740253384",
      "title": "CorrSteer: Generation-Time LLM Steering via Correlated Sparse Autoencoder Features",
      "authors": [
        {
          "name": "Seonglae Cho",
          "authorId": "2361429341"
        },
        {
          "name": "Zekun Wu",
          "authorId": "2284029866"
        },
        {
          "name": "A. Koshiyama",
          "authorId": "2268316579"
        }
      ],
      "year": 2025,
      "abstract": "Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby reducing spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma-2 2B and LLaMA-3.1 8B, notably achieving a +3.3% improvement in MMLU performance with 4000 samples and a +27.2% improvement in HarmBench with only 108 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlation-based selection as an effective and scalable approach for automated SAE steering across language model applications.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2508.12535",
      "url": "https://www.semanticscholar.org/paper/d5f55bed31688be23cebf9407ebe0b5740253384",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "f7b07e4ae631c929f9a19f83eca8e4279a7db054",
      "title": "Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment",
      "authors": [
        {
          "name": "Hongbin Zhang",
          "authorId": "2305734239"
        },
        {
          "name": "Kehai Chen",
          "authorId": "2266796043"
        },
        {
          "name": "Xuefeng Bai",
          "authorId": "2320289516"
        },
        {
          "name": "Yang Xiang",
          "authorId": "2273965768"
        },
        {
          "name": "Min Zhang",
          "authorId": "2273887691"
        }
      ],
      "year": 2025,
      "abstract": "Reward models (RMs) are crucial for aligning large language models (LLMs) with diverse cultures. Consequently, evaluating their cultural awareness is essential for further advancing global alignment of LLMs. However, existing RM evaluations fall short in assessing cultural awareness due to the scarcity of culturally relevant evaluation datasets. To fill this gap, we propose Cultural Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs reveals their deficiencies in modeling cultural awareness and demonstrates a positive correlation between performance on CARB and downstream multilingual cultural alignment tasks. Further analysis identifies the spurious correlations within culture-aware reward modeling, wherein RM's scoring relies predominantly on surface-level features rather than authentic cultural nuance understanding. To address these, we propose Think-as-Locals to elicit deeper culturally grounded reasoning from generative RMs via reinforcement learning from verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate preference judgments and high-quality structured evaluation criteria generation. Experimental results validate its efficacy in mitigating spurious features interference and advancing culture-aware reward modeling.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.21798",
      "arxivId": "2509.21798",
      "url": "https://www.semanticscholar.org/paper/f7b07e4ae631c929f9a19f83eca8e4279a7db054",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.21798"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a2f9a5ca54d4c2085c73a834c446286747d31312",
      "title": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs",
      "authors": [
        {
          "name": "Shaowen Wang",
          "authorId": "2310497362"
        },
        {
          "name": "Yiqi Dong",
          "authorId": "2391839030"
        },
        {
          "name": "Ruinian Chang",
          "authorId": "2391750936"
        },
        {
          "name": "Tansheng Zhu",
          "authorId": "2392602590"
        },
        {
          "name": "Yuebo Sun",
          "authorId": "2391730291"
        },
        {
          "name": "Kaifeng Lyu",
          "authorId": "2151259481"
        },
        {
          "name": "Jian Li",
          "authorId": "2333277664"
        }
      ],
      "year": 2025,
      "abstract": "Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.07318",
      "arxivId": "2511.07318",
      "url": "https://www.semanticscholar.org/paper/a2f9a5ca54d4c2085c73a834c446286747d31312",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.07318"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bfd101814602a3c58f955ead34c575bb08fcab3b",
      "title": "Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing",
      "authors": [
        {
          "name": "Zichen Wu",
          "authorId": "2109687047"
        },
        {
          "name": "Hsiu-Yuan Huang",
          "authorId": "2292056032"
        },
        {
          "name": "Yunfang Wu",
          "authorId": "2257128093"
        }
      ],
      "year": 2025,
      "abstract": "Multimodal Large Language Models (MLLMs) have shown substantial capabilities in integrating visual and textual information, yet frequently rely on spurious correlations, undermining their robustness and generalization in complex multimodal reasoning tasks. This paper addresses the critical challenge of superficial correlation bias in MLLMs through a novel causal mediation-based debiasing framework. Specially, we distinguishing core semantics from spurious textual and visual contexts via counterfactual examples to activate training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture with dynamic routing to selectively engages modality-specific debiasing experts. Empirical evaluation on multimodal sarcasm detection and sentiment analysis tasks demonstrates that our framework significantly surpasses unimodal debiasing strategies and existing state-of-the-art models.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2509.15361",
      "arxivId": "2509.15361",
      "url": "https://www.semanticscholar.org/paper/bfd101814602a3c58f955ead34c575bb08fcab3b",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.15361"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f5f7c5ffc6990bb38e8bd25ebde7941550642591",
      "title": "Rethinking LLM-based Preference Evaluation",
      "authors": [
        {
          "name": "Zhengyu Hu",
          "authorId": null
        },
        {
          "name": "Linxin Song",
          "authorId": "2355252250"
        },
        {
          "name": "Jieyu Zhang",
          "authorId": "2358116645"
        },
        {
          "name": "Zheyuan Xiao",
          "authorId": "2311315868"
        },
        {
          "name": "Jingang Wang",
          "authorId": "2398619215"
        },
        {
          "name": "Zhenyu Chen",
          "authorId": "2309176938"
        },
        {
          "name": "Jieyu Zhao",
          "authorId": "2309202283"
        },
        {
          "name": "Hui Xiong",
          "authorId": "2269470756"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 8,
      "doi": "10.48550/arXiv.2407.01085",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f5f7c5ffc6990bb38e8bd25ebde7941550642591",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.01085"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "211ba4a0b12fbb97eedcc2649861d9073e289b23",
      "title": "STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning",
      "authors": [
        {
          "name": "Chenghao Wu",
          "authorId": "2378074619"
        },
        {
          "name": "Ruiyang Ren",
          "authorId": "1708171825"
        },
        {
          "name": "Junjie Zhang",
          "authorId": "2362751379"
        },
        {
          "name": "Ruirui Wang",
          "authorId": "2357005595"
        },
        {
          "name": "Zhongrui Ma",
          "authorId": "2283769104"
        },
        {
          "name": "Qi Ye",
          "authorId": "2357989054"
        },
        {
          "name": "Wayne Xin Zhao",
          "authorId": "2362310540"
        }
      ],
      "year": 2025,
      "abstract": "While modern recommender systems are instrumental in navigating information abundance, they remain fundamentally limited by static user modeling and reactive decision-making paradigms. Current large language model (LLM)-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in sparse-data scenarios. We introduce STARec, a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training-a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping. This hybrid approach scaffolds agents in acquiring foundational capabilities (preference summarization, rationale generation) while enabling dynamic policy adaptation through simulated feedback loops. Experiments on MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.",
      "citationCount": 0,
      "doi": "10.1145/3746252.3760995",
      "arxivId": "2508.18812",
      "url": "https://www.semanticscholar.org/paper/211ba4a0b12fbb97eedcc2649861d9073e289b23",
      "venue": "International Conference on Information and Knowledge Management",
      "journal": {
        "name": "Proceedings of the 34th ACM International Conference on Information and Knowledge Management"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a0f2704bab838d808a7aca0806371d02196e0157",
      "title": "How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations",
      "authors": [
        {
          "name": "Yoshiki Takenami",
          "authorId": "2378130854"
        },
        {
          "name": "Yin Jou Huang",
          "authorId": "2366154615"
        },
        {
          "name": "Yugo Murawaki",
          "authorId": "2606962"
        },
        {
          "name": "Chenhui Chu",
          "authorId": "2378225800"
        }
      ],
      "year": 2025,
      "abstract": "Cognitive biases, well-studied in humans, can also be observed in LLMs, affecting their reliability in real-world applications. This paper investigates the anchoring effect in LLM-driven price negotiations. To this end, we instructed seller LLM agents to apply the anchoring effect and evaluated negotiations using not only an objective metric but also a subjective metric. Experimental results show that LLMs are influenced by the anchoring effect like humans. Additionally, we investigated the relationship between the anchoring effect and factors such as reasoning and personality. It was shown that reasoning models are less prone to the anchoring effect, suggesting that the long chain of thought mitigates the effect. However, we found no significant correlation between personality traits and susceptibility to the anchoring effect. These findings contribute to a deeper understanding of cognitive biases in LLMs and to the realization of safe and responsible application of LLMs in society.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.21137",
      "arxivId": "2508.21137",
      "url": "https://www.semanticscholar.org/paper/a0f2704bab838d808a7aca0806371d02196e0157",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.21137"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7dd900abf8ed87dcaa65eeead3e8251d5c43eab9",
      "title": "Enhancing Large Language Model with Decomposed Reasoning for Emotion Cause Pair Extraction",
      "authors": [
        {
          "name": "Jialiang Wu",
          "authorId": "2282096955"
        },
        {
          "name": "Yi Shen",
          "authorId": "2115382769"
        },
        {
          "name": "Ziheng Zhang",
          "authorId": "2282178999"
        },
        {
          "name": "Longjun Cai",
          "authorId": "2651246"
        }
      ],
      "year": 2024,
      "abstract": "Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs representing emotions and their causes in a document. Existing methods tend to overfit spurious correlations, such as positional bias in existing benchmark datasets, rather than capturing semantic features. Inspired by recent work, we explore leveraging large language model (LLM) to address ECPE task without additional training. Despite strong capabilities, LLMs suffer from uncontrollable outputs, resulting in mediocre performance. To address this, we introduce chain-of-thought to mimic human cognitive process and propose the Decomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference and logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance the framework by incorporating in-context learning. Experiment results demonstrate the strength of DECC compared to state-of-the-art supervised fine-tuning methods. Finally, we analyze the effectiveness of each component and the robustness of the method in various scenarios, including different LLM bases, rebalanced datasets, and multi-pair extraction.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2401.17716",
      "arxivId": "2401.17716",
      "url": "https://www.semanticscholar.org/paper/7dd900abf8ed87dcaa65eeead3e8251d5c43eab9",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2401.17716"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "be135c3b7177ea502e6de54adf34fd3dd0dff474",
      "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications",
      "authors": [
        {
          "name": "Hongliu Cao",
          "authorId": "3130250"
        },
        {
          "name": "Ilias Driouich",
          "authorId": "2189304999"
        },
        {
          "name": "Robin Singh",
          "authorId": "2353998032"
        },
        {
          "name": "Eoin Thomas",
          "authorId": "2087735285"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across diverse domains, yet they still encounter challenges such as insufficient domain-specific knowledge, biases, and hallucinations. This underscores the need for robust evaluation methodologies to accurately assess LLM-based applications. Traditional evaluation methods, which rely on word overlap or text embeddings, are inadequate for capturing the nuanced semantic information necessary to evaluate dynamic, open-ended text generation. Recent research has explored leveraging LLMs to mimic human reasoning and decision-making processes for evaluation purposes known as LLM-as-a-judge framework. However, these existing frameworks have two significant limitations. First, they lack the flexibility to adapt to different text styles, including various answer and ground truth styles, thereby reducing their generalization performance. Second, the evaluation scores produced by these frameworks are often skewed and hard to interpret, showing a low correlation with human judgment. To address these challenges, we propose a novel dynamic multi-agent system that automatically designs personalized LLM judges for various natural language generation applications. This system iteratively refines evaluation prompts and balances the trade-off between the adaptive requirements of downstream tasks and the alignment with human perception. Our experimental results show that the proposed multi-agent LLM Judge framework not only enhances evaluation accuracy compared to existing methods but also produces evaluation scores that better align with human perception.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2504.02867",
      "arxivId": "2504.02867",
      "url": "https://www.semanticscholar.org/paper/be135c3b7177ea502e6de54adf34fd3dd0dff474",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.02867"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9c9eb9d73333ba9ecbfcbe7cf1e0a8971f9222c2",
      "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
      "authors": [
        {
          "name": "Yuanning Feng",
          "authorId": "2367245614"
        },
        {
          "name": "Sinan Wang",
          "authorId": "2375814880"
        },
        {
          "name": "Zhengxiang Cheng",
          "authorId": "2347697187"
        },
        {
          "name": "Yao Wan",
          "authorId": "2349959514"
        },
        {
          "name": "Dongping Chen",
          "authorId": "2279219833"
        }
      ],
      "year": 2025,
      "abstract": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.",
      "citationCount": 3,
      "doi": null,
      "arxivId": "2512.16041",
      "url": "https://www.semanticscholar.org/paper/9c9eb9d73333ba9ecbfcbe7cf1e0a8971f9222c2",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "cb8ac70443da5e074914cbd4a3692fe463cf4978",
      "title": "When an LLM is apprehensive about its answers - and when its uncertainty is justified",
      "authors": [
        {
          "name": "Petr Sychev",
          "authorId": "2348473124"
        },
        {
          "name": "Andrey Goncharov",
          "authorId": "2348474041"
        },
        {
          "name": "Daniil Vyazhev",
          "authorId": "2348473708"
        },
        {
          "name": "Edvard Khalafyan",
          "authorId": "2348471562"
        },
        {
          "name": "Alexey Zaytsev",
          "authorId": "2348426054"
        }
      ],
      "year": 2025,
      "abstract": "Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and $14$ topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is $0.73$. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is $0.55$. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2503.01688",
      "arxivId": "2503.01688",
      "url": "https://www.semanticscholar.org/paper/cb8ac70443da5e074914cbd4a3692fe463cf4978",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.01688"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "11b5e803a47b725933b3c46262dec3789b255a1f",
      "title": "SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs",
      "authors": [
        {
          "name": "Parsa Hosseini",
          "authorId": "2288261415"
        },
        {
          "name": "Sumit Nawathe",
          "authorId": "2336877156"
        },
        {
          "name": "Mazda Moayeri",
          "authorId": "104644443"
        },
        {
          "name": "S. Balasubramanian",
          "authorId": "144021807"
        },
        {
          "name": "S. Feizi",
          "authorId": "34389431"
        }
      ],
      "year": 2025,
      "abstract": "Unimodal vision models are known to rely on spurious correlations, but it remains unclear to what extent Multimodal Large Language Models (MLLMs) exhibit similar biases despite language supervision. In this paper, we investigate spurious bias in MLLMs and introduce SpurLens, a pipeline that leverages GPT-4 and open-set object detectors to automatically identify spurious visual cues without human supervision. Our findings reveal that spurious correlations cause two major failure modes in MLLMs: (1) over-reliance on spurious cues for object recognition, where removing these cues reduces accuracy, and (2) object hallucination, where spurious cues amplify the hallucination by over 10x. We validate our findings in various MLLMs and datasets. Beyond diagnosing these failures, we explore potential mitigation strategies, such as prompt ensembling and reasoning-based prompting, and conduct ablation studies to examine the root causes of spurious bias in MLLMs. By exposing the persistence of spurious correlations, our study calls for more rigorous evaluation methods and mitigation strategies to enhance the reliability of MLLMs.",
      "citationCount": 6,
      "doi": null,
      "arxivId": "2503.08884",
      "url": "https://www.semanticscholar.org/paper/11b5e803a47b725933b3c46262dec3789b255a1f",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "d0616e912d58a2ea67060c8cabddbab8ff626a3a",
      "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection",
      "authors": [
        {
          "name": "Seonglae Cho",
          "authorId": "2361429341"
        },
        {
          "name": "Zekun Wu",
          "authorId": "2284029866"
        },
        {
          "name": "A. Koshiyama",
          "authorId": "2268316579"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.12535",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d0616e912d58a2ea67060c8cabddbab8ff626a3a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.12535"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8efabec881e3ffdb90abaf7afc69acf8e6704843",
      "title": "Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "authors": [
        {
          "name": "Peter Chen",
          "authorId": "2359907160"
        },
        {
          "name": "Xiaopeng Li",
          "authorId": "2362288897"
        },
        {
          "name": "Ziniu Li",
          "authorId": "2377514405"
        },
        {
          "name": "Wotao Yin",
          "authorId": "2363408656"
        },
        {
          "name": "Xi Chen",
          "authorId": "2359690666"
        },
        {
          "name": "Tianyi Lin",
          "authorId": "2359716309"
        }
      ],
      "year": 2025,
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2512.16912",
      "url": "https://www.semanticscholar.org/paper/8efabec881e3ffdb90abaf7afc69acf8e6704843",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "cde94812bf1525006612b2cd69ebd3ebd3ebc049",
      "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models",
      "authors": [
        {
          "name": "Julia Shuieh",
          "authorId": "2360173984"
        },
        {
          "name": "Prasann Singhal",
          "authorId": "2187681933"
        },
        {
          "name": "Apaar Shanker",
          "authorId": "1390689296"
        },
        {
          "name": "John Heyer",
          "authorId": "2323512547"
        },
        {
          "name": "George Pu",
          "authorId": "2323513733"
        },
        {
          "name": "Sam Denton",
          "authorId": "2323515242"
        }
      ],
      "year": 2025,
      "abstract": "Supervised and preference-based fine-tuning techniques have become popular for aligning large language models (LLMs) with user intent and correctness criteria. However, real-world training data often exhibits spurious correlations -- arising from biases, dataset artifacts, or other\"shortcut\"features -- that can compromise a model's performance or generalization. In this paper, we systematically evaluate three post-training algorithms -- Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO (Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and spuriousness conditions. Our tasks span mathematical reasoning, constrained instruction-following, and document-grounded question answering. We vary the degree of spurious correlation (10% vs. 90%) and investigate two forms of artifacts:\"Feature Ambiguity\"and\"Distributional Narrowness.\"Our results show that the models often but not always degrade under higher spuriousness. The preference-based methods (DPO/KTO) can demonstrate relative robustness in mathematical reasoning tasks. By contrast, SFT maintains stronger performance in complex, context-intensive tasks. These findings highlight that no single post-training strategy universally outperforms in all scenarios; the best choice depends on the type of target task and the nature of spurious correlations.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.05704",
      "arxivId": "2505.05704",
      "url": "https://www.semanticscholar.org/paper/cde94812bf1525006612b2cd69ebd3ebd3ebc049",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.05704"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 25,
  "errors": []
}
