{
  "status": "success",
  "source": "openalex",
  "query": "model collapse fine-tuning language models",
  "results": [
    {
      "openalex_id": "W4392873657",
      "doi": "10.48550/arxiv.2403.09167",
      "title": "Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse",
      "authors": [
        {
          "name": "Jianwei Sun",
          "openalex_id": "A5100429020",
          "orcid": "https://orcid.org/0000-0002-2470-1077"
        },
        {
          "name": "Chaoyang Mei",
          "openalex_id": "A5111148379"
        },
        {
          "name": "Linlin Wei",
          "openalex_id": "A5101492806",
          "orcid": "https://orcid.org/0000-0003-4601-8487"
        },
        {
          "name": "Kaiyu Zheng",
          "openalex_id": "A5101914277",
          "orcid": "https://orcid.org/0000-0002-9002-6292"
        },
        {
          "name": "Liu Na",
          "openalex_id": "A5031044955",
          "orcid": "https://orcid.org/0000-0002-8777-0707"
        },
        {
          "name": "Ming Cui",
          "openalex_id": "A5107093915",
          "orcid": "https://orcid.org/0009-0001-8514-9845"
        },
        {
          "name": "Tianyi Li",
          "openalex_id": "A5100460612",
          "orcid": "https://orcid.org/0009-0001-3265-9921"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-14",
      "abstract": "The efficacy of large language models (LLMs) is heavily dependent on the quality of the underlying data, particularly within specialized domains. A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities. To address these issues, we propose a two-stage approach for the construction of production prompts designed to yield high-quality data. This method involves the generation of a diverse array of prompts that encompass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that the domain-specific proficiency of general LLMs can be enhanced through fine-tuning with data produced via our proposed method, without compromising their overall generalization abilities, even when exclusively domain-specific data is employed for fine-tuning.",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2403.09167"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4392873657"
    },
    {
      "openalex_id": "W4308243242",
      "doi": "10.48550/arxiv.2211.01642",
      "title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively",
      "authors": [
        {
          "name": "Haojie Zhang",
          "openalex_id": "A5100707830",
          "orcid": "https://orcid.org/0000-0002-4577-3600"
        },
        {
          "name": "Ge Li",
          "openalex_id": "A5100447691",
          "orcid": "https://orcid.org/0000-0003-0140-0949"
        },
        {
          "name": "Jia Li",
          "openalex_id": "A5100405681",
          "orcid": "https://orcid.org/0000-0002-3108-8645"
        },
        {
          "name": "Zhongjin Zhang",
          "openalex_id": "A5083055398",
          "orcid": "https://orcid.org/0000-0002-6666-3224"
        },
        {
          "name": "Yuqi Zhu",
          "openalex_id": "A5102421790",
          "orcid": "https://orcid.org/0009-0007-1299-1744"
        },
        {
          "name": "Zhi Jin",
          "openalex_id": "A5049100391",
          "orcid": "https://orcid.org/0000-0003-1087-226X"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-11-03",
      "abstract": "Large-scale pre-trained language models have achieved impressive results on a wide range of downstream tasks recently. However, fine-tuning an extremely large-scale pre-trained language model on limited target datasets is often plagued by overfitting and representation degradation. In this paper, we propose a Dynamic Parameter Selection (DPS) algorithm for the large-scale pre-trained models during fine-tuning, which adaptively selects a more promising subnetwork to perform staging updates based on gradients of back-propagation. Experiments on the GLUE benchmark show that DPS outperforms previous fine-tuning methods in terms of overall performance and stability, and consistently achieves better results with variable pre-trained language models. In addition, DPS brings a large magnitude of improvement in out-of-domain transferring experiments and low-resource scenarios, which shows that it can maintain stable general contextual features and reduce the representation collapse. We release our code at https://github.com/ZhangHaojie077/DPS",
      "cited_by_count": 13,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2211.01642"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Multimodal Machine Learning Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4308243242"
    },
    {
      "openalex_id": "W4224211001",
      "doi": "10.48550/arxiv.2204.09179",
      "title": "On the Representation Collapse of Sparse Mixture of Experts",
      "authors": [
        {
          "name": "Zewen Chi",
          "openalex_id": "A5077329331",
          "orcid": "https://orcid.org/0000-0003-1615-1885"
        },
        {
          "name": "Dong Li",
          "openalex_id": "A5100407418",
          "orcid": "https://orcid.org/0000-0002-2599-6065"
        },
        {
          "name": "Shaohan Huang",
          "openalex_id": "A5061624006",
          "orcid": "https://orcid.org/0000-0003-4324-6337"
        },
        {
          "name": "Damai Dai",
          "openalex_id": "A5020456783"
        },
        {
          "name": "Shuming Ma",
          "openalex_id": "A5113130010",
          "orcid": "https://orcid.org/0000-0003-1091-1206"
        },
        {
          "name": "Barun Patra",
          "openalex_id": "A5076387701"
        },
        {
          "name": "Saksham Singhal",
          "openalex_id": "A5023837971"
        },
        {
          "name": "Payal Bajaj",
          "openalex_id": "A5022825653"
        },
        {
          "name": "Song Xia",
          "openalex_id": "A5087490418",
          "orcid": "https://orcid.org/0000-0002-1962-4273"
        },
        {
          "name": "Furu Wei",
          "openalex_id": "A5014662947",
          "orcid": "https://orcid.org/0000-0002-7810-5852"
        },
        {
          "name": "Huang, Heyan",
          "openalex_id": ""
        },
        {
          "name": "Wei, Furu",
          "openalex_id": ""
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-04-20",
      "abstract": "Sparse mixture of experts provides larger model capacity while requiring a constant computational overhead. It employs the routing mechanism to distribute input tokens to the best-matched experts according to their hidden representations. However, learning such a routing mechanism encourages token clustering around expert centroids, implying a trend toward representation collapse. In this work, we propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. We conduct extensive experiments on cross-lingual language model pre-training and fine-tuning on downstream tasks. Experimental results across seven multilingual benchmarks show that our method achieves consistent gains. We also present a comprehensive analysis on the representation and routing behaviors of our models. Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.",
      "cited_by_count": 16,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2204.09179"
      },
      "topics": [
        "Expert finding and Q&A systems",
        "Topic Modeling",
        "Multimodal Machine Learning Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4224211001"
    },
    {
      "openalex_id": "W4390871743",
      "doi": "10.1109/iccv51070.2023.00080",
      "title": "Global Knowledge Calibration for Fast Open-Vocabulary Segmentation",
      "authors": [
        {
          "name": "Kunyang Han",
          "openalex_id": "A5091778667",
          "orcid": "https://orcid.org/0000-0003-3522-4816",
          "institutions": [
            "Beijing Jiaotong University"
          ]
        },
        {
          "name": "Yong Liu",
          "openalex_id": "A5072988055",
          "orcid": "https://orcid.org/0000-0001-9031-9696",
          "institutions": [
            "Tsinghua University",
            "Tsinghua\u2013Berkeley Shenzhen Institute"
          ]
        },
        {
          "name": "Jun Hao Liew",
          "openalex_id": "A5014937540",
          "orcid": "https://orcid.org/0000-0002-7538-6759"
        },
        {
          "name": "Henghui Ding",
          "openalex_id": "A5036631624",
          "orcid": "https://orcid.org/0000-0003-4868-6526",
          "institutions": [
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Jiajun Liu",
          "openalex_id": "A5100661974"
        },
        {
          "name": "Yitong Wang",
          "openalex_id": "A5100650009",
          "orcid": "https://orcid.org/0000-0002-7559-4152"
        },
        {
          "name": "Yansong Tang",
          "openalex_id": "A5101926293",
          "orcid": "https://orcid.org/0000-0002-1534-4549",
          "institutions": [
            "Tsinghua University",
            "Tsinghua\u2013Berkeley Shenzhen Institute"
          ]
        },
        {
          "name": "Yujiu Yang",
          "openalex_id": "A5020953714",
          "orcid": "https://orcid.org/0000-0002-6427-1024",
          "institutions": [
            "Tsinghua University",
            "Tsinghua\u2013Berkeley Shenzhen Institute"
          ]
        },
        {
          "name": "Jiashi Feng",
          "openalex_id": "A5100668696",
          "orcid": "https://orcid.org/0000-0001-6843-0064"
        },
        {
          "name": "Yao Zhao",
          "openalex_id": "A5100362745",
          "orcid": "https://orcid.org/0000-0002-8581-9554",
          "institutions": [
            "Beijing Jiaotong University"
          ]
        },
        {
          "name": "Yunchao Wei",
          "openalex_id": "A5087043856",
          "orcid": "https://orcid.org/0000-0002-2812-8781",
          "institutions": [
            "Beijing Jiaotong University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-01",
      "abstract": "Recent advancements in pre-trained vision-language models, such as CLIP, have enabled the segmentation of arbitrary concepts solely from textual inputs, a process commonly referred to as open-vocabulary semantic segmentation (OVS). However, existing OVS techniques confront a fundamental challenge: the trained classifier tends to over-fit on the base classes observed during training, resulting in suboptimal generalization performance to unseen classes. To mitigate this issue, recent studies have proposed the use of an additional frozen pre-trained CLIP for classification. Nonetheless, this approach incurs heavy computational overheads as the CLIP vision encoder must be repeatedly forward-passed for each mask, rendering it impractical for real-world applications. To address this challenge, our objective is to develop a fast OVS model that can perform comparably or better without the extra computational burden of the CLIP image encoder during inference. To this end, we propose a core idea of preserving the generalizable representation when fine-tuning on known classes. Specifically, we introduce a text diversification strategy that generates a set of synonyms for each training category, which prevents the learned representation from collapsing onto specific known category names. Additionally, we employ a text-guided knowledge distillation method to preserve the generalizable knowledge of CLIP. Extensive experiments demonstrate that our proposed model achieves robust generalization performance across various datasets. Furthermore, we perform a preliminary exploration of open-vocabulary video segmentation and present a benchmark that can facilitate future open-vocabulary research in the video domain.",
      "cited_by_count": 26,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Multimodal Machine Learning Applications",
        "Domain Adaptation and Few-Shot Learning",
        "Human Pose and Action Recognition"
      ],
      "referenced_works_count": 63,
      "url": "https://openalex.org/W4390871743"
    },
    {
      "openalex_id": "W4387968461",
      "doi": "10.1145/3581783.3611987",
      "title": "Beyond Generic: Enhancing Image Captioning with Real-World Knowledge using Vision-Language Pre-Training Model",
      "authors": [
        {
          "name": "Kanzhi Cheng",
          "openalex_id": "A5001863177",
          "orcid": "https://orcid.org/0009-0004-4532-1446",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Wenpo Song",
          "openalex_id": "A5078320517",
          "orcid": "https://orcid.org/0009-0006-2344-7931",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Zheng Ma",
          "openalex_id": "A5101942699",
          "orcid": "https://orcid.org/0009-0006-1101-3263",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Wenhao Zhu",
          "openalex_id": "A5058553449",
          "orcid": "https://orcid.org/0009-0007-9608-6281",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Zixuan Zhu",
          "openalex_id": "A5102955762",
          "orcid": "https://orcid.org/0009-0004-3970-0319",
          "institutions": [
            "University of Glasgow"
          ]
        },
        {
          "name": "Jianbing Zhang",
          "openalex_id": "A5046733861",
          "orcid": "https://orcid.org/0009-0005-8193-8344",
          "institutions": [
            "Nanjing University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-26",
      "abstract": "Current captioning approaches tend to generate correct but \"generic\" descriptions that lack real-world knowledge, e.g., named entities and contextual information. Considering that Vision-Language Pre-Training (VLP) models master massive such knowledge from large-scale web-harvested data, it is promising to utilize the generalizability of VLP models to incorporate knowledge into image descriptions. However, using VLP models faces challenges: zero-shot inference suffers from knowledge hallucination that leads to low-quality descriptions, but the generic bias in downstream task fine-tuning hinders the VLP model from expressing knowledge. To address these concerns, we propose a simple yet effective method called Knowledge-guided Replay (K-Replay), which enables the retention of pre-training knowledge during fine-tuning. Our approach consists of two parts: (1) a knowledge prediction task on automatically collected replay exemplars to continuously awaken the VLP model's memory about knowledge, thus preventing the model from collapsing into the generic pattern; (2) a knowledge distillation constraint to improve the faithfulness of generated descriptions hence alleviating the knowledge hallucination. To evaluate knowledge-enhanced descriptions, we construct a novel captioning benchmark KnowCap, containing knowledge of landmarks, famous brands, special foods and movie characters. Experimental results show that our approach effectively incorporates knowledge into descriptions, outperforming strong VLP baseline by 20.9 points (78.7 \u2192 99.6) in CIDEr score and 20.5 percentage points (34.0% \u2192 54.5%) in knowledge recognition accuracy. Our code and data is available at https://github.com/njucckevin/KnowCap.",
      "cited_by_count": 11,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Multimodal Machine Learning Applications",
        "Domain Adaptation and Few-Shot Learning",
        "Advanced Image and Video Retrieval Techniques"
      ],
      "referenced_works_count": 40,
      "url": "https://openalex.org/W4387968461"
    },
    {
      "openalex_id": "W7117554779",
      "doi": "10.5281/zenodo.18080764",
      "title": "Quantifying Model Collapse in Small Language Models via Recursive Fine-Tuning",
      "authors": [
        {
          "name": "Satvika Pragada",
          "openalex_id": "A5121353906"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-12-29",
      "abstract": "This study investigates the phenomenon of Model Collapse, the degenerative process where generative artificial intelligence models lose their ability to represent the true underlying data distribution after being trained on synthetic, model-generated content. The experiment utilized DistilGPT2 (82M parameters) as a base architecture. The model was initially fine-tuned on a curated \"ground truth\" dataset of 1,000 structured knock-knock jokes. In a recursive loop, the artifical outputs of Generation $n$ were used as the primary training data for Generation n+1. Performance was monitored via Training Loss convergence and Lexical Diversity (Type-Token Ratio). Forensic analysis of the training logs revealed a rapid \"collapse of the latent space.\" Training loss dropped significantly from an initial baseline of 3.22 to 0.06 within four epochs, indicating a shift from generative learning to memorization. As the diversity score plummeted, the model\u2019s variable nature failed, resulting in Mode Collapse, where the model produced deterministic, repetitive sequences regardless of input temperature. The results demonstrate that recursive training without the injection of fresh, human-curated data leads to a \"death spiral\" of variety. This experiment highlights the existential risk to Large Language Models (LLMs) as the internet becomes increasingly saturated with synthetic data, potentially leading to a permanent loss of the \"long tail\" of human creativity.",
      "cited_by_count": 0,
      "type": "preprint",
      "source": {
        "name": "Zenodo (CERN European Organization for Nuclear Research)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.5281/zenodo.18080764"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Computational and Text Analysis Methods",
        "Language and cultural evolution"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W7117554779"
    },
    {
      "openalex_id": "W7117576107",
      "doi": "10.5281/zenodo.18080765",
      "title": "Quantifying Model Collapse in Small Language Models via Recursive Fine-Tuning",
      "authors": [
        {
          "name": "Satvika Pragada",
          "openalex_id": "A5121353906"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-12-29",
      "abstract": "This study investigates the phenomenon of Model Collapse, the degenerative process where generative artificial intelligence models lose their ability to represent the true underlying data distribution after being trained on synthetic, model-generated content. The experiment utilized DistilGPT2 (82M parameters) as a base architecture. The model was initially fine-tuned on a curated \"ground truth\" dataset of 1,000 structured knock-knock jokes. In a recursive loop, the artifical outputs of Generation $n$ were used as the primary training data for Generation n+1. Performance was monitored via Training Loss convergence and Lexical Diversity (Type-Token Ratio). Forensic analysis of the training logs revealed a rapid \"collapse of the latent space.\" Training loss dropped significantly from an initial baseline of 3.22 to 0.06 within four epochs, indicating a shift from generative learning to memorization. As the diversity score plummeted, the model\u2019s variable nature failed, resulting in Mode Collapse, where the model produced deterministic, repetitive sequences regardless of input temperature. The results demonstrate that recursive training without the injection of fresh, human-curated data leads to a \"death spiral\" of variety. This experiment highlights the existential risk to Large Language Models (LLMs) as the internet becomes increasingly saturated with synthetic data, potentially leading to a permanent loss of the \"long tail\" of human creativity.",
      "cited_by_count": 0,
      "type": "preprint",
      "source": {
        "name": "Zenodo (CERN European Organization for Nuclear Research)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.5281/zenodo.18080765"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Computational and Text Analysis Methods",
        "Language and cultural evolution"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W7117576107"
    },
    {
      "openalex_id": "W4385573519",
      "doi": "10.18653/v1/2022.findings-emnlp.77",
      "title": "RL with KL penalties is better viewed as Bayesian inference",
      "authors": [
        {
          "name": "Tomasz Korbak",
          "openalex_id": "A5002211679",
          "orcid": "https://orcid.org/0000-0002-6258-2013",
          "institutions": [
            "New York University",
            "Sussex County Community College"
          ]
        },
        {
          "name": "Ethan Perez",
          "openalex_id": "A5091112967",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Christopher L. Buckley",
          "openalex_id": "A5026149273",
          "orcid": "https://orcid.org/0000-0002-8551-9121",
          "institutions": [
            "University of Sussex"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.",
      "cited_by_count": 3,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2022.findings-emnlp.77.pdf"
      },
      "topics": [
        "Machine Learning and Algorithms",
        "Machine Learning and Data Classification",
        "Neural Networks and Applications"
      ],
      "referenced_works_count": 43,
      "url": "https://openalex.org/W4385573519"
    },
    {
      "openalex_id": "W4206484811",
      "doi": "10.1109/access.2021.3140175",
      "title": "A Metaverse: Taxonomy, Components, Applications, and Open Challenges",
      "authors": [
        {
          "name": "Sangmin Park",
          "openalex_id": "A5100671422",
          "orcid": "https://orcid.org/0000-0002-3484-9833",
          "institutions": [
            "Korea University"
          ]
        },
        {
          "name": "Young\u2010Gab Kim",
          "openalex_id": "A5061277148",
          "orcid": "https://orcid.org/0000-0001-9585-8808",
          "institutions": [
            "Sejong University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to connectivity with reality using virtual currency. The integration of enhanced social activities and neural-net methods requires a new definition of Metaverse suitable for the present, different from the previous Metaverse. This paper divides the concepts and essential techniques necessary for realizing the Metaverse into three components (i.e., hardware, software, and contents) and three approaches (i.e., user interaction, implementation, and application) rather than marketing or hardware approach to conduct a comprehensive analysis. Furthermore, we describe essential methods based on three components and techniques to Metaverse&#x2019;s representative Ready Player One, Roblox, and Facebook research in the domain of films, games, and studies. Finally, we summarize the limitations and directions for implementing the immersive Metaverse as social influences, constraints, and open challenges.",
      "cited_by_count": 1649,
      "type": "article",
      "source": {
        "name": "IEEE Access",
        "type": "journal",
        "issn": [
          "2169-3536"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://ieeexplore.ieee.org/ielx7/6287639/9668973/09667507.pdf"
      },
      "topics": [
        "Multimodal Machine Learning Applications",
        "Advanced Neural Network Applications",
        "Domain Adaptation and Few-Shot Learning"
      ],
      "referenced_works_count": 424,
      "url": "https://openalex.org/W4206484811"
    },
    {
      "openalex_id": "W4397049077",
      "doi": "10.1145/3638529.3654138",
      "title": "Generative Design through Quality-Diversity Data Synthesis and Language Models",
      "authors": [
        {
          "name": "Adam Gaier",
          "openalex_id": "A5087609177",
          "orcid": "https://orcid.org/0000-0002-4632-0929",
          "institutions": [
            "Autodesk (United States)"
          ]
        },
        {
          "name": "James Stoddart",
          "openalex_id": "A5102013340",
          "orcid": "https://orcid.org/0009-0007-7157-7869",
          "institutions": [
            "Autodesk (United States)"
          ]
        },
        {
          "name": "Lorenzo Villaggi",
          "openalex_id": "A5028237713",
          "institutions": [
            "Autodesk (United States)"
          ]
        },
        {
          "name": "Shyam Sudhakaran",
          "openalex_id": "A5030303571",
          "orcid": "https://orcid.org/0009-0008-4658-903X",
          "institutions": [
            "Autodesk (United States)"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-07-08",
      "abstract": "Two fundamental challenges face generative models in engineering applications: the acquisition of high-performing, diverse datasets, and the adherence to precise constraints in generated designs. We propose a novel approach combining optimization, constraint satisfaction, and language models to tackle these challenges in architectural design. Our method uses Quality-Diversity (QD) to generate a diverse, high-performing dataset. We then fine-tune a language model with this dataset to generate high-level designs. These designs are then refined into detailed, constraint-compliant layouts using the Wave Function Collapse algorithm. Our system demonstrates reliable adherence to textual guidance, enabling the generation of layouts with targeted architectural and performance features. Crucially, our results indicate that data synthesized through the evolutionary search of QD not only improves overall model performance but is essential for the model's ability to closely adhere to textual guidance. This improvement underscores the pivotal role evolutionary computation can play in creating the datasets key to training generative models for design. Web article at https://tilegpt.github.io",
      "cited_by_count": 6,
      "type": "article",
      "source": {
        "name": "Proceedings of the Genetic and Evolutionary Computation Conference",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1145/3638529.3654138"
      },
      "topics": [
        "Design Education and Practice",
        "Manufacturing Process and Optimization",
        "Model-Driven Software Engineering Techniques"
      ],
      "referenced_works_count": 27,
      "url": "https://openalex.org/W4397049077"
    },
    {
      "openalex_id": "W3203528425",
      "doi": "10.1109/taslp.2022.3193292",
      "title": "NoiER: An Approach for Training More Reliable Fine-Tuned Downstream Task Models",
      "authors": [
        {
          "name": "Myeongjun Jang",
          "openalex_id": "A5055458201",
          "orcid": "https://orcid.org/0000-0002-9352-4799",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Thomas Lukasiewicz",
          "openalex_id": "A5091549352",
          "institutions": [
            "University of Oxford",
            "TU Wien"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "The recent development in pretrained language models that are trained in a self-supervised fashion, such as BERT, is driving rapid progress in natural language processing. However, their brilliant performance is based on leveraging syntactic artefacts of the training data rather than fully understanding the intrinsic meaning of language. The excessive exploitation of spurious artefacts is a problematic issue: the distribution collapse problem, which is the phenomenon that the model fine-tuned on downstream tasks is unable to distinguish out-of-distribution sentences while producing a high-confidence score. In this paper, we argue that the distribution collapse is a prevalent issue in pretrained language models and propose <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">noise entropy regularisation (NoiER)</i> as an efficient learning paradigm that solves the problem without auxiliary models and additional data. The proposed approach improved traditional out-of-distribution detection evaluation metrics by 55% on average compared to the original fine-tuned models.",
      "cited_by_count": 3,
      "type": "article",
      "source": {
        "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "type": "journal",
        "issn": [
          "2329-9290",
          "2329-9304"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1109/taslp.2022.3193292"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Speech Recognition and Synthesis"
      ],
      "referenced_works_count": 65,
      "url": "https://openalex.org/W3203528425"
    },
    {
      "openalex_id": "W4416452367",
      "doi": "10.48550/arxiv.2505.16559",
      "title": "CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning",
      "authors": [
        {
          "name": "Biao Yi",
          "openalex_id": "A5022273713"
        },
        {
          "name": "T. Huang",
          "openalex_id": "A5038187292",
          "orcid": "https://orcid.org/0000-0002-2971-8185"
        },
        {
          "name": "Baolei Zhang",
          "openalex_id": "A5052982232",
          "orcid": "https://orcid.org/0009-0007-3477-1697"
        },
        {
          "name": "Tong Li",
          "openalex_id": "A5100358988",
          "orcid": "https://orcid.org/0000-0001-6269-5117"
        },
        {
          "name": "L. S. Nie",
          "openalex_id": "A5101673893",
          "orcid": "https://orcid.org/0000-0003-0932-5461"
        },
        {
          "name": "Zheli Liu",
          "openalex_id": "A5060212061",
          "orcid": "https://orcid.org/0000-0002-2984-2661"
        },
        {
          "name": "Lianguan Shen",
          "openalex_id": "A5101744923",
          "orcid": "https://orcid.org/0000-0002-5871-3121"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-05-22",
      "abstract": "Fine-tuning-as-a-service, while commercially successful for Large Language Model (LLM) providers, exposes models to harmful fine-tuning attacks. As a widely explored defense paradigm against such attacks, unlearning attempts to remove malicious knowledge from LLMs, thereby essentially preventing them from being used to perform malicious tasks. However, we highlight a critical flaw: the powerful general adaptability of LLMs allows them to easily bypass selective unlearning by rapidly relearning or repurposing their capabilities for harmful tasks. To address this fundamental limitation, we propose a paradigm shift: instead of selective removal, we advocate for inducing model collapse--effectively forcing the model to \"unlearn everything\"--specifically in response to updates characteristic of malicious adaptation. This collapse directly neutralizes the very general capabilities that attackers exploit, tackling the core issue unaddressed by selective unlearning. We introduce the Collapse Trap (CTRAP) as a practical mechanism to implement this concept conditionally. Embedded during alignment, CTRAP pre-configures the model's reaction to subsequent fine-tuning dynamics. If updates during fine-tuning constitute a persistent attempt to reverse safety alignment, the pre-configured trap triggers a progressive degradation of the model's core language modeling abilities, ultimately rendering it inert and useless for the attacker. Crucially, this collapse mechanism remains dormant during benign fine-tuning, ensuring the model's utility and general capabilities are preserved for legitimate users. Extensive empirical results demonstrate that CTRAP effectively counters harmful fine-tuning risks across various LLMs and attack settings, while maintaining high performance in benign scenarios. Our code is available at https://anonymous.4open.science/r/CTRAP.",
      "cited_by_count": 0,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2505.16559"
      },
      "topics": [],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4416452367"
    },
    {
      "openalex_id": "W4312206130",
      "doi": "10.48550/arxiv.2212.12206",
      "title": "Understanding and Improving Transfer Learning of Deep Models via Neural Collapse",
      "authors": [
        {
          "name": "Xiao Li",
          "openalex_id": "A5100375332",
          "orcid": "https://orcid.org/0000-0003-2565-9883"
        },
        {
          "name": "Sheng Liu",
          "openalex_id": "A5100319993",
          "orcid": "https://orcid.org/0000-0001-9608-0524"
        },
        {
          "name": "Jin\u2010Xin Zhou",
          "openalex_id": "A5011391021",
          "orcid": "https://orcid.org/0000-0002-8353-896X"
        },
        {
          "name": "Xinyu Lu",
          "openalex_id": "A5063368064",
          "orcid": "https://orcid.org/0000-0002-0034-4191"
        },
        {
          "name": "Carlos Fernandez\u2010Granda",
          "openalex_id": "A5044336556",
          "orcid": "https://orcid.org/0000-0001-7039-8606"
        },
        {
          "name": "Zhihui Zhu",
          "openalex_id": "A5011989964",
          "orcid": "https://orcid.org/0000-0002-3856-0375"
        },
        {
          "name": "Qing Qu",
          "openalex_id": "A5019924950",
          "orcid": "https://orcid.org/0000-0001-9136-558X"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-12-23",
      "abstract": "With the ever-increasing complexity of large-scale pre-trained models coupled with a shortage of labeled data for downstream training, transfer learning has become the primary approach in many fields, including natural language processing, computer vision, and multi-modal learning. Despite recent progress, the fine-tuning process for large-scale pre-trained models in vision still mostly relies on trial and error. This work investigates the relationship between neural collapse (NC) and transfer learning for classification problems. NC is an intriguing while prevalent phenomenon that has been recently discovered in terms of the final-layer features and linear classifiers of trained neural networks. Specifically, during the terminal phase of training, NC implies that the variability of the features within each class diminishes to zero, while the means of features between classes are maximally and equally distanced. In this work, we examine the NC attributes of pre-trained models on both downstream and source data for transfer learning, and we find strong correlation between feature collapse and downstream performance. In particular, we discovered a systematic pattern that emerges when linear probing pre-trained models on downstream training data: the more feature collapse of pre-trained models on downstream training data, the higher the transfer accuracy. Additionally, we also studied the relationship between NC and transfer accuracy on the source data. Moreover, these findings allow us to develop a principled, parameter-efficient fine-tuning method that employs skip-connection to induce the last-layer feature collapse on downstream data. Our proposed fine-tuning methods deliver good performances while reducing fine-tuning parameters by at least 90% and mitigating overfitting in situations especially when the downstream data is scarce.",
      "cited_by_count": 3,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2212.12206"
      },
      "topics": [
        "Domain Adaptation and Few-Shot Learning",
        "Machine Learning and ELM",
        "Machine Learning and Data Classification"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4312206130"
    },
    {
      "openalex_id": "W4378225851",
      "doi": "10.3390/app13116410",
      "title": "Self Attention Networks in Speaker Recognition",
      "authors": [
        {
          "name": "Pooyan Safari",
          "openalex_id": "A5052874187",
          "orcid": "https://orcid.org/0000-0002-5916-8575",
          "institutions": [
            "Universitat Polit\u00e8cnica de Catalunya"
          ]
        },
        {
          "name": "Miquel India",
          "openalex_id": "A5085739769",
          "orcid": "https://orcid.org/0000-0002-3107-3662",
          "institutions": [
            "Universitat Polit\u00e8cnica de Catalunya"
          ]
        },
        {
          "name": "Javier Hernando",
          "openalex_id": "A5025633053",
          "orcid": "https://orcid.org/0000-0002-1730-8154",
          "institutions": [
            "Universitat Polit\u00e8cnica de Catalunya"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-24",
      "abstract": "Recently, there has been a significant surge of interest in Self-Attention Networks (SANs) based on the Transformer architecture. This can be attributed to their notable ability for parallelization and their impressive performance across various Natural Language Processing applications. On the other hand, the utilization of large-scale, multi-purpose language models trained through self-supervision is progressively more prevalent, for tasks like speech recognition. In this context, the pre-trained model, which has been trained on extensive speech data, can be fine-tuned for particular downstream tasks like speaker verification. These massive models typically rely on SANs as their foundational architecture. Therefore, studying the potential capabilities and training challenges of such models is of utmost importance for the future generation of speaker verification systems. In this direction, we propose a speaker embedding extractor based on SANs to obtain a discriminative speaker representation given non-fixed length speech utterances. With the advancements suggested in this work, we could achieve up to 41% relative performance improvement in terms of EER compared to the naive SAN which was proposed in our previous work. Moreover, we empirically show the training instability in such architectures in terms of rank collapse and further investigate the potential solutions to alleviate this shortcoming.",
      "cited_by_count": 4,
      "type": "article",
      "source": {
        "name": "Applied Sciences",
        "type": "journal",
        "issn": [
          "2076-3417"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2076-3417/13/11/6410/pdf?version=1684923003"
      },
      "topics": [
        "Speech Recognition and Synthesis",
        "Music and Audio Processing",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 55,
      "url": "https://openalex.org/W4378225851"
    },
    {
      "openalex_id": "W4403885103",
      "doi": "10.48550/arxiv.2409.12917",
      "title": "Training Language Models to Self-Correct via Reinforcement Learning",
      "authors": [
        {
          "name": "Aviral Kumar",
          "openalex_id": "A5102493293"
        },
        {
          "name": "Vincent Zhuang",
          "openalex_id": "A5068788467",
          "orcid": "https://orcid.org/0009-0007-2931-3069"
        },
        {
          "name": "Rishabh Agarwal",
          "openalex_id": "A5070953294",
          "orcid": "https://orcid.org/0000-0003-2817-0231"
        },
        {
          "name": "Yi Su",
          "openalex_id": "A5047993178",
          "orcid": "https://orcid.org/0000-0001-9793-6953"
        },
        {
          "name": "John D. Co-Reyes",
          "openalex_id": "A5007992087"
        },
        {
          "name": "Avi Singh",
          "openalex_id": "A5071640510"
        },
        {
          "name": "Kate Baumli",
          "openalex_id": "A5083212839"
        },
        {
          "name": "Shariq Iqbal",
          "openalex_id": "A5036422995",
          "orcid": "https://orcid.org/0000-0003-2766-8425"
        },
        {
          "name": "Colton Bishop",
          "openalex_id": "A5108960667"
        },
        {
          "name": "Rebecca Roelofs",
          "openalex_id": "A5008334918"
        },
        {
          "name": "Lei Zhang",
          "openalex_id": "A5100433931",
          "orcid": "https://orcid.org/0000-0002-5808-5313"
        },
        {
          "name": "Kay McKinney",
          "openalex_id": "A5093511571"
        },
        {
          "name": "Disha Shrivastava",
          "openalex_id": "A5058234964"
        },
        {
          "name": "Cosmin P\u0103duraru",
          "openalex_id": "A5056097740",
          "orcid": "https://orcid.org/0000-0002-1746-5467"
        },
        {
          "name": "George Tucker",
          "openalex_id": "A5048032272"
        },
        {
          "name": "Doina Precup",
          "openalex_id": "A5065836447"
        },
        {
          "name": "Feryal Behbahani",
          "openalex_id": "A5002098080"
        },
        {
          "name": "Aleksandra Faust",
          "openalex_id": "A5002971435",
          "orcid": "https://orcid.org/0000-0002-3268-8685"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-09-19",
      "abstract": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.",
      "cited_by_count": 6,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2409.12917"
      },
      "topics": [
        "Speech and dialogue systems",
        "Topic Modeling",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4403885103"
    },
    {
      "openalex_id": "W4400949264",
      "doi": "10.1038/s41586-024-07566-y",
      "title": "AI models collapse when trained on recursively generated data",
      "authors": [
        {
          "name": "Ilia Shumailov",
          "openalex_id": "A5069844959",
          "orcid": "https://orcid.org/0000-0003-3100-0727"
        },
        {
          "name": "Zakhar Shumaylov",
          "openalex_id": "A5087214686",
          "orcid": "https://orcid.org/0000-0001-7087-4393"
        },
        {
          "name": "Yiren Zhao",
          "openalex_id": "A5101093262",
          "orcid": "https://orcid.org/0000-0002-3727-7463",
          "institutions": [
            "Imperial College London"
          ]
        },
        {
          "name": "Nicolas Papernot",
          "openalex_id": "A5018809423",
          "orcid": "https://orcid.org/0000-0001-5078-7233",
          "institutions": [
            "Vector Institute",
            "University of Toronto"
          ]
        },
        {
          "name": "Ross Anderson",
          "openalex_id": "A5046983053",
          "orcid": "https://orcid.org/0000-0001-8697-5682",
          "institutions": [
            "University of Cambridge",
            "University of Edinburgh"
          ]
        },
        {
          "name": "Yarin Gal",
          "openalex_id": "A5029186201",
          "orcid": "https://orcid.org/0000-0002-2733-2078"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-07-24",
      "abstract": null,
      "cited_by_count": 386,
      "type": "article",
      "source": {
        "name": "Nature",
        "type": "journal",
        "issn": [
          "0028-0836",
          "1476-4687"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s41586-024-07566-y.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Generative Adversarial Networks and Image Synthesis",
        "Domain Adaptation and Few-Shot Learning"
      ],
      "referenced_works_count": 9,
      "url": "https://openalex.org/W4400949264"
    },
    {
      "openalex_id": "W4379259759",
      "doi": "10.48550/arxiv.2306.00813",
      "title": "UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning",
      "authors": [
        {
          "name": "Xiao Dong",
          "openalex_id": "A5101961088",
          "orcid": "https://orcid.org/0000-0001-9519-612X"
        },
        {
          "name": "Runhui Huang",
          "openalex_id": "A5001888307"
        },
        {
          "name": "Xiaoyong Wei",
          "openalex_id": "A5034340826",
          "orcid": "https://orcid.org/0000-0001-9677-4227"
        },
        {
          "name": "Zequn Jie",
          "openalex_id": "A5075329194",
          "orcid": "https://orcid.org/0000-0002-3038-5891"
        },
        {
          "name": "Jianxing Yu",
          "openalex_id": "A5101899054",
          "orcid": "https://orcid.org/0000-0002-9696-2460"
        },
        {
          "name": "Jian Yin",
          "openalex_id": "A5102961198",
          "orcid": "https://orcid.org/0000-0002-4820-0226"
        },
        {
          "name": "Xiaodan Liang",
          "openalex_id": "A5047878798",
          "orcid": "https://orcid.org/0000-0003-3213-3062"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-01",
      "abstract": "Recent advances in vision-language pre-training have enabled machines to perform better in multimodal object discrimination (e.g., image-text semantic alignment) and image synthesis (e.g., text-to-image generation). On the other hand, fine-tuning pre-trained models with discriminative or generative capabilities such as CLIP and Stable Diffusion on domain-specific datasets has shown to be effective in various tasks by adapting to specific domains. However, few studies have explored the possibility of learning both discriminative and generative capabilities and leveraging their synergistic effects to create a powerful and personalized multimodal model during fine-tuning. This paper presents UniDiff, a unified multi-modal model that integrates image-text contrastive learning (ITC), text-conditioned image synthesis learning (IS), and reciprocal semantic consistency modeling (RSC). UniDiff effectively learns aligned semantics and mitigates the issue of semantic collapse during fine-tuning on small datasets by leveraging RSC on visual features from CLIP and diffusion models, without altering the pre-trained model's basic architecture. UniDiff demonstrates versatility in both multi-modal understanding and generative tasks. Experimental results on three datasets (Fashion-man, Fashion-woman, and E-commercial Product) showcase substantial enhancements in vision-language retrieval and text-to-image generation, illustrating the advantages of combining discriminative and generative fine-tuning. The proposed UniDiff model establishes a robust pipeline for personalized modeling and serves as a benchmark for future comparisons in the field.",
      "cited_by_count": 2,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2306.00813"
      },
      "topics": [
        "Multimodal Machine Learning Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4379259759"
    },
    {
      "openalex_id": "W4385416665",
      "doi": "10.3390/fi15080260",
      "title": "The Power of Generative AI: A Review of Requirements, Models, Input\u2013Output Formats, Evaluation Metrics, and Challenges",
      "authors": [
        {
          "name": "Ajay Bandi",
          "openalex_id": "A5047345391",
          "orcid": "https://orcid.org/0000-0003-2434-736X",
          "institutions": [
            "Northwest Missouri State University"
          ]
        },
        {
          "name": "Pydi Venkata Satya Ramesh Adapa",
          "openalex_id": "A5092780821",
          "institutions": [
            "Northwest Missouri State University"
          ]
        },
        {
          "name": "Yudu Eswar Vinay Pratap Kumar Kuchi",
          "openalex_id": "A5092780822",
          "institutions": [
            "Northwest Missouri State University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-31",
      "abstract": "Generative artificial intelligence (AI) has emerged as a powerful technology with numerous applications in various domains. There is a need to identify the requirements and evaluation metrics for generative AI models designed for specific tasks. The purpose of the research aims to investigate the fundamental aspects of generative AI systems, including their requirements, models, input\u2013output formats, and evaluation metrics. The study addresses key research questions and presents comprehensive insights to guide researchers, developers, and practitioners in the field. Firstly, the requirements necessary for implementing generative AI systems are examined and categorized into three distinct categories: hardware, software, and user experience. Furthermore, the study explores the different types of generative AI models described in the literature by presenting a taxonomy based on architectural characteristics, such as variational autoencoders (VAEs), generative adversarial networks (GANs), diffusion models, transformers, language models, normalizing flow models, and hybrid models. A comprehensive classification of input and output formats used in generative AI systems is also provided. Moreover, the research proposes a classification system based on output types and discusses commonly used evaluation metrics in generative AI. The findings contribute to advancements in the field, enabling researchers, developers, and practitioners to effectively implement and evaluate generative AI models for various applications. The significance of the research lies in understanding that generative AI system requirements are crucial for effective planning, design, and optimal performance. A taxonomy of models aids in selecting suitable options and driving advancements. Classifying input\u2013output formats enables leveraging diverse formats for customized systems, while evaluation metrics establish standardized methods to assess model quality and performance.",
      "cited_by_count": 471,
      "type": "review",
      "source": {
        "name": "Future Internet",
        "type": "journal",
        "issn": [
          "1999-5903"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/1999-5903/15/8/260/pdf?version=1690812126"
      },
      "topics": [
        "Generative Adversarial Networks and Image Synthesis",
        "Explainable Artificial Intelligence (XAI)",
        "Advanced Data Storage Technologies"
      ],
      "referenced_works_count": 135,
      "url": "https://openalex.org/W4385416665"
    },
    {
      "openalex_id": "W4389518736",
      "doi": "10.18653/v1/2023.findings-emnlp.975",
      "title": "Representation Projection Invariance Mitigates Representation Collapse",
      "authors": [
        {
          "name": "Anastasia Razdaibiedina",
          "openalex_id": "A5090519936",
          "orcid": "https://orcid.org/0000-0002-0572-1136",
          "institutions": [
            "Amazon (United States)",
            "University of Toronto",
            "Johns Hopkins University"
          ]
        },
        {
          "name": "Ashish Khetan",
          "openalex_id": "A5056299576",
          "orcid": "https://orcid.org/0000-0002-8856-1595",
          "institutions": [
            "Amazon (United States)",
            "University of Toronto",
            "Johns Hopkins University"
          ]
        },
        {
          "name": "Zohar Karnin",
          "openalex_id": "A5048818669",
          "orcid": "https://orcid.org/0009-0009-0108-289X",
          "institutions": [
            "Johns Hopkins University",
            "Amazon (United States)",
            "University of Toronto"
          ]
        },
        {
          "name": "Daniel Khashabi",
          "openalex_id": "A5043628255",
          "orcid": "https://orcid.org/0009-0009-7664-2230",
          "institutions": [
            "Johns Hopkins University",
            "University of Toronto",
            "Amazon (United States)"
          ]
        },
        {
          "name": "Vivek Madan",
          "openalex_id": "A5030726280",
          "institutions": [
            "Amazon (United States)",
            "Johns Hopkins University",
            "University of Toronto"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": "Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization. In this paper, we propose Representation Projection Invariance (REPINA), a novel regularization method to maintain the information content of representation and reduce representation collapse during fine-tuning by discouraging undesirable changes in the representations. We study the empirical behavior of the proposed regularization in comparison to 5 comparable baselines across 13 language understanding tasks (GLUE benchmark and six additional datasets). When evaluating in-domain performance, REPINA consistently outperforms other baselines on most tasks (10 out of 13). Additionally, REPINA improves out-of-distribution performance. We also demonstrate its effectiveness in few-shot settings and robustness to label perturbation. As a by-product, we extend previous studies of representation collapse and propose several metrics to quantify it. Our empirical findings show that our approach is significantly more effective at mitigating representation collapse.",
      "cited_by_count": 1,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2023.findings-emnlp.975.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Domain Adaptation and Few-Shot Learning"
      ],
      "referenced_works_count": 53,
      "url": "https://openalex.org/W4389518736"
    },
    {
      "openalex_id": "W4213212652",
      "doi": "10.1103/revmodphys.94.015004",
      "title": "Noisy intermediate-scale quantum algorithms",
      "authors": [
        {
          "name": "Kishor Bharti",
          "openalex_id": "A5061413142",
          "orcid": "https://orcid.org/0000-0002-7776-6608",
          "institutions": [
            "Centre for Quantum Technologies",
            "National University of Singapore"
          ]
        },
        {
          "name": "Alba Cervera-Lierta",
          "openalex_id": "A5031306679",
          "orcid": "https://orcid.org/0000-0002-8835-2910",
          "institutions": [
            "Centre for Quantum Technologies",
            "National University of Singapore",
            "University of Toronto"
          ]
        },
        {
          "name": "Thi Ha Kyaw",
          "openalex_id": "A5088770397",
          "institutions": [
            "Centre for Quantum Technologies",
            "University of Toronto",
            "National University of Singapore"
          ]
        },
        {
          "name": "Tobias Haug",
          "openalex_id": "A5030920804",
          "orcid": "https://orcid.org/0000-0003-2707-9962",
          "institutions": [
            "Centre for Quantum Technologies",
            "National University of Singapore",
            "Imperial College London"
          ]
        },
        {
          "name": "Sumner Alperin-Lea",
          "openalex_id": "A5045363686",
          "orcid": "https://orcid.org/0000-0003-1082-0400",
          "institutions": [
            "Centre for Quantum Technologies",
            "University of Toronto",
            "National University of Singapore"
          ]
        },
        {
          "name": "Abhinav Anand",
          "openalex_id": "A5061367654",
          "orcid": "https://orcid.org/0000-0002-8081-2310",
          "institutions": [
            "University of Toronto",
            "Centre for Quantum Technologies",
            "National University of Singapore"
          ]
        },
        {
          "name": "Matthias Degroote",
          "openalex_id": "A5000821684",
          "orcid": "https://orcid.org/0000-0002-8850-7708",
          "institutions": [
            "National University of Singapore",
            "Centre for Quantum Technologies",
            "University of Toronto"
          ]
        },
        {
          "name": "Hermanni Heimonen",
          "openalex_id": "A5013912304",
          "institutions": [
            "National University of Singapore",
            "Centre for Quantum Technologies"
          ]
        },
        {
          "name": "Jakob S. Kottmann",
          "openalex_id": "A5091863640",
          "orcid": "https://orcid.org/0000-0002-4156-2048",
          "institutions": [
            "University of Toronto",
            "National University of Singapore",
            "Centre for Quantum Technologies"
          ]
        },
        {
          "name": "Tim Menke",
          "openalex_id": "A5007778441",
          "orcid": "https://orcid.org/0000-0002-7205-752X",
          "institutions": [
            "National University of Singapore",
            "Centre for Quantum Technologies",
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Wai\u2010Keong Mok",
          "openalex_id": "A5015947580",
          "orcid": "https://orcid.org/0000-0002-1920-5407",
          "institutions": [
            "Centre for Quantum Technologies",
            "National University of Singapore"
          ]
        },
        {
          "name": "Sukin Sim",
          "openalex_id": "A5071358233",
          "orcid": "https://orcid.org/0000-0001-5324-5165",
          "institutions": [
            "Centre for Quantum Technologies",
            "Harvard University",
            "National University of Singapore"
          ]
        },
        {
          "name": "L. C. Kwek",
          "openalex_id": "A5037387694",
          "orcid": "https://orcid.org/0000-0002-0879-0591",
          "institutions": [
            "Centre for Quantum Technologies",
            "National University of Singapore"
          ]
        },
        {
          "name": "Al\u00e1n Aspuru\u2010Guzik",
          "openalex_id": "A5071495561",
          "orcid": "https://orcid.org/0000-0002-8277-4434",
          "institutions": [
            "University of Toronto",
            "National University of Singapore",
            "Centre for Quantum Technologies",
            "Vector Institute"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-02-15",
      "abstract": "A universal fault-tolerant quantum computer that can efficiently solve problems such as integer factorization and unstructured database search requires millions of qubits with low error rates and long coherence times. While the experimental advancement toward realizing such devices will potentially take decades of research, noisy intermediate-scale quantum (NISQ) computers already exist. These computers are composed of hundreds of noisy qubits, i.e., qubits that are not error corrected, and therefore perform imperfect operations within a limited coherence time. In the search for achieving quantum advantage with these devices, algorithms have been proposed for applications in various disciplines spanning physics, machine learning, quantum chemistry, and combinatorial optimization. The overarching goal of such algorithms is to leverage the limited available resources to perform classically challenging tasks. In this review, a thorough summary of NISQ computational paradigms and algorithms is provided. The key structure of these algorithms and their limitations and advantages are discussed. Finally, a comprehensive overview of various benchmarking and software tools useful for programming and testing NISQ devices is additionally provided.",
      "cited_by_count": 1429,
      "type": "article",
      "source": {
        "name": "Reviews of Modern Physics",
        "type": "journal",
        "issn": [
          "0034-6861",
          "1538-4527",
          "1539-0756"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "http://link.aps.org/pdf/10.1103/RevModPhys.94.015004"
      },
      "topics": [
        "Quantum Computing Algorithms and Architecture",
        "Quantum Information and Cryptography",
        "Quantum many-body systems"
      ],
      "referenced_works_count": 896,
      "url": "https://openalex.org/W4213212652"
    }
  ],
  "count": 20,
  "errors": []
}
