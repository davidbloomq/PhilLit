{
  "status": "success",
  "source": "semantic_scholar",
  "query": "reasoning benchmark LLM evaluation",
  "results": [
    {
      "paperId": "40b420cad2fa52491d0d001351ce18764d20eec1",
      "title": "LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts",
      "authors": [
        {
          "name": "Yijia Xiao",
          "authorId": "95289709"
        },
        {
          "name": "Edward Sun",
          "authorId": "2310336411"
        },
        {
          "name": "Tianyu Liu",
          "authorId": "2310398131"
        },
        {
          "name": "Wei Wang",
          "authorId": "2256611611"
        }
      ],
      "year": 2024,
      "abstract": "We propose LogicVista, an evaluation benchmark that assesses the integrated logical reasoning capabilities of multimodal large language models (MLLMs) in Visual contexts. Recent advancements in MLLMs have demonstrated various fascinating abilities, from crafting poetry based on an image to performing mathematical reasoning. However, there is still a lack of systematic evaluation of MLLMs' proficiency in logical reasoning tasks, which are essential for activities like navigation and puzzle-solving. Thus we evaluate general logical cognition abilities across 5 logical reasoning tasks encompassing 9 different capabilities, using a sample of 448 multiple-choice questions. Each question is annotated with the correct answer and the human-written reasoning behind the selection, enabling both open-ended and multiple-choice evaluation. A total of 8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available at https://github.com/Yijia-Xiao/LogicVista.",
      "citationCount": 110,
      "doi": "10.48550/arXiv.2407.04973",
      "arxivId": "2407.04973",
      "url": "https://www.semanticscholar.org/paper/40b420cad2fa52491d0d001351ce18764d20eec1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.04973"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f68a65df5f8527dc27ca8da79e7e06b599a5ff5b",
      "title": "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks",
      "authors": [
        {
          "name": "Eva S\u00e1nchez-Salido",
          "authorId": "2326306397"
        },
        {
          "name": "Julio Gonzalo",
          "authorId": "2309178337"
        },
        {
          "name": "Guillermo Marco",
          "authorId": "2075206254"
        }
      ],
      "year": 2025,
      "abstract": "In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs'answers.",
      "citationCount": 12,
      "doi": "10.48550/arXiv.2502.12896",
      "arxivId": "2502.12896",
      "url": "https://www.semanticscholar.org/paper/f68a65df5f8527dc27ca8da79e7e06b599a5ff5b",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.12896"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b8f3e8b85bef7b0abb187ca4cbcca677cccbcb44",
      "title": "Evaluation of large language model performance on the Biomedical Language Understanding and Reasoning Benchmark",
      "authors": [
        {
          "name": "Hui Feng",
          "authorId": "2302818048"
        },
        {
          "name": "Francesco Ronzano",
          "authorId": "2302410294"
        },
        {
          "name": "Jude LaFleur",
          "authorId": "2302410044"
        },
        {
          "name": "Matthew Garber",
          "authorId": "2302409246"
        },
        {
          "name": "Rodrigo de Oliveira",
          "authorId": "2302817730"
        },
        {
          "name": "Kathryn Rough",
          "authorId": "2296687891"
        },
        {
          "name": "Katharine Roth",
          "authorId": "2302409056"
        },
        {
          "name": "Jay Nanavati",
          "authorId": "2302409209"
        },
        {
          "name": "Khaldoun Zine",
          "authorId": "2302409150"
        },
        {
          "name": "El Abidine",
          "authorId": "2302410051"
        },
        {
          "name": "C. Mack",
          "authorId": "2270547506"
        }
      ],
      "year": 2024,
      "abstract": "Background: The ability of large language models (LLMs) to interpret and generate human-like text has been accompanied with speculation about their application in medicine and clinical research. There is limited data available to inform evidence-based decisions on the appropriateness for specific use cases. Methods: We evaluated and compared four general-purpose LLMs (GPT-4, GPT-3.5-turbo, Flan-T5-XXL, and Zephyr-7B-Beta) and a healthcare-specific LLM (MedLLaMA-13B) on a set of 13 datasets - referred to as the Biomedical Language Understanding and Reasoning Benchmark (BLURB) - covering six commonly needed medical natural language processing tasks: named entity recognition (NER); relation extraction; population, interventions, comparators, and outcomes (PICO); sentence similarity; document classification; and question-answering. All models were evaluated without modification. Model performance was assessed according to a range of prompting strategies (formalised as a systematic, reusable prompting framework) and relied on the standard, task-specific evaluation metrics defined by BLURB. Results: Across all tasks, GPT-4 outperformed other LLMs, followed by Flan-T5-XXL and GPT-3.5- turbo, then Zephyr-7b-Beta and MedLLaMA-13B. The most performant prompts for GPT-4 and Flan-T5-XXL both outperformed the previously-reported best results for the PubMedQA task. The domain-specific MedLLaMA-13B achieved lower scores for most tasks except for question-answering tasks. We observed a substantial impact of strategically editing the prompt describing the task and a consistent improvement in performance when including examples semantically similar to the input text in the prompt. Conclusion: These results provide evidence of the potential LLMs may have for medical application and highlight the importance of robust evaluation before adopting LLMs for any specific use cases. Continuing to explore how these emerging technologies can be adapted for the healthcare setting, paired with human expertise, and enhanced through quality control measures will be important research to allow responsible innovation with LLMs in the medical area.",
      "citationCount": 11,
      "doi": "10.1101/2024.05.17.24307411",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b8f3e8b85bef7b0abb187ca4cbcca677cccbcb44",
      "venue": "medRxiv",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "1dd20f5d548555a863164824ef225b27f8ff8f8d",
      "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
      "authors": [
        {
          "name": "Meng-Hao Guo",
          "authorId": "2287922663"
        },
        {
          "name": "Jiajun Xu",
          "authorId": "2292063190"
        },
        {
          "name": "Yi Zhang",
          "authorId": "2359944029"
        },
        {
          "name": "Jiaxi Song",
          "authorId": "2359313248"
        },
        {
          "name": "Hao-Yang Peng",
          "authorId": "2347692775"
        },
        {
          "name": "Yi-Xuan Deng",
          "authorId": "2359212251"
        },
        {
          "name": "Xinzhi Dong",
          "authorId": "2359447853"
        },
        {
          "name": "Kiyohiro Nakayama",
          "authorId": "2215985722"
        },
        {
          "name": "Zhengyang Geng",
          "authorId": "2359148264"
        },
        {
          "name": "Chen Wang",
          "authorId": "2359816333"
        },
        {
          "name": "Bolin Ni",
          "authorId": "2359148247"
        },
        {
          "name": "Guo-Wei Yang",
          "authorId": "2109944421"
        },
        {
          "name": "Yongming Rao",
          "authorId": "2359146815"
        },
        {
          "name": "Houwen Peng",
          "authorId": "2243685256"
        },
        {
          "name": "Han Hu",
          "authorId": "2359181758"
        },
        {
          "name": "Gordon Wetzstein",
          "authorId": "2297763521"
        },
        {
          "name": "Shi-Min Hu",
          "authorId": "2263152288"
        }
      ],
      "year": 2025,
      "abstract": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here.",
      "citationCount": 20,
      "doi": "10.48550/arXiv.2505.02018",
      "arxivId": "2505.02018",
      "url": "https://www.semanticscholar.org/paper/1dd20f5d548555a863164824ef225b27f8ff8f8d",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.02018"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "33c4b67d4d2088c7ec1a6bc11934d7b795b8b145",
      "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark",
      "authors": [
        {
          "name": "Minglai Yang",
          "authorId": "2345288933"
        },
        {
          "name": "Ethan Huang",
          "authorId": "2363499007"
        },
        {
          "name": "Liang Zhang",
          "authorId": "2364763378"
        },
        {
          "name": "Mihai Surdeanu",
          "authorId": "2289756268"
        },
        {
          "name": "W. Wang",
          "authorId": "2257130314"
        },
        {
          "name": "Liangming Pan",
          "authorId": "2364705996"
        }
      ],
      "year": 2025,
      "abstract": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models'(LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions.",
      "citationCount": 18,
      "doi": "10.48550/arXiv.2505.18761",
      "arxivId": "2505.18761",
      "url": "https://www.semanticscholar.org/paper/33c4b67d4d2088c7ec1a6bc11934d7b795b8b145",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.18761"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "27c53381af4d06fc3327dd8d138a0b9e0acdf27e",
      "title": "LLM ethics benchmark: a three-dimensional assessment system for evaluating moral reasoning in large language models",
      "authors": [
        {
          "name": "Junfeng Jiao",
          "authorId": "2279986827"
        },
        {
          "name": "Saleh Afroogh",
          "authorId": "1573652229"
        },
        {
          "name": "Abhejay Murali",
          "authorId": "2205455964"
        },
        {
          "name": "Kevin Chen",
          "authorId": "2308532913"
        },
        {
          "name": "David Atkinson",
          "authorId": "2308468640"
        },
        {
          "name": "Amit Dhurandhar",
          "authorId": "2308468375"
        }
      ],
      "year": 2025,
      "abstract": "This study establishes a novel framework for systematically evaluating the moral reasoning capabilities of large language models (LLMs) as they increasingly integrate into critical societal domains. Current assessment methodologies lack the precision needed to evaluate nuanced ethical decision-making in AI systems, creating significant accountability gaps. Our framework addresses this challenge by quantifying alignment with human ethical standards through three dimensions: foundational moral principles, reasoning robustness, and value consistency across diverse scenarios. This approach enables precise identification of ethical strengths and weaknesses in LLMs, facilitating targeted improvements and stronger alignment with societal values. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at https://github.com/The-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git.",
      "citationCount": 6,
      "doi": "10.1038/s41598-025-18489-7",
      "arxivId": "2505.00853",
      "url": "https://www.semanticscholar.org/paper/27c53381af4d06fc3327dd8d138a0b9e0acdf27e",
      "venue": "Scientific Reports",
      "journal": {
        "name": "Scientific Reports",
        "volume": "15"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "47aacaab789e80388d22598b4810213655e62888",
      "title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
      "authors": [
        {
          "name": "Shihan Deng",
          "authorId": "2309690070"
        },
        {
          "name": "Weikai Xu",
          "authorId": "2262867071"
        },
        {
          "name": "Hongda Sun",
          "authorId": "2257127695"
        },
        {
          "name": "Wei Liu",
          "authorId": "2257333016"
        },
        {
          "name": "Tao Tan",
          "authorId": "2309299004"
        },
        {
          "name": "Jianfeng Liu",
          "authorId": "2309199381"
        },
        {
          "name": "Ang Li",
          "authorId": "2309681549"
        },
        {
          "name": "Jian Luan",
          "authorId": "2257013742"
        },
        {
          "name": "Bin Wang",
          "authorId": "2257388949"
        },
        {
          "name": "Rui Yan",
          "authorId": "2257014132"
        },
        {
          "name": "Shuo Shang",
          "authorId": "2232780079"
        }
      ],
      "year": 2024,
      "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps.",
      "citationCount": 36,
      "doi": "10.48550/arXiv.2407.00993",
      "arxivId": "2407.00993",
      "url": "https://www.semanticscholar.org/paper/47aacaab789e80388d22598b4810213655e62888",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.00993"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "2a59d62cd06286cb2ba6d5314290b54edf56c978",
      "title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks",
      "authors": [
        {
          "name": "Yifei Zhou",
          "authorId": "2288585404"
        },
        {
          "name": "Song Jiang",
          "authorId": "2327145097"
        },
        {
          "name": "Yuandong Tian",
          "authorId": "2285362895"
        },
        {
          "name": "Jason E. Weston",
          "authorId": "2267341626"
        },
        {
          "name": "Sergey Levine",
          "authorId": "2268967207"
        },
        {
          "name": "Sainbayar Sukhbaatar",
          "authorId": "2265067"
        },
        {
          "name": "Xian Li",
          "authorId": "2327159625"
        }
      ],
      "year": 2025,
      "abstract": "Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.",
      "citationCount": 49,
      "doi": "10.48550/arXiv.2503.15478",
      "arxivId": "2503.15478",
      "url": "https://www.semanticscholar.org/paper/2a59d62cd06286cb2ba6d5314290b54edf56c978",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.15478"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5f2dbd43ed12147b5d97a6978156779dddbce93a",
      "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators",
      "authors": [
        {
          "name": "Yilun Zhou",
          "authorId": "2307023535"
        },
        {
          "name": "Austin Xu",
          "authorId": "2351058697"
        },
        {
          "name": "PeiFeng Wang",
          "authorId": "2322520737"
        },
        {
          "name": "Caiming Xiong",
          "authorId": "2267728986"
        },
        {
          "name": "Shafiq Joty",
          "authorId": "2313536726"
        }
      ],
      "year": 2025,
      "abstract": "Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses.",
      "citationCount": 21,
      "doi": "10.48550/arXiv.2504.15253",
      "arxivId": "2504.15253",
      "url": "https://www.semanticscholar.org/paper/5f2dbd43ed12147b5d97a6978156779dddbce93a",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.15253"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "6311ff8a02daf56232ce7d4fac640b912ec9e670",
      "title": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs",
      "authors": [
        {
          "name": "Ved Sirdeshmukh",
          "authorId": "2342686059"
        },
        {
          "name": "Kaustubh Deshpande",
          "authorId": "2342685945"
        },
        {
          "name": "Johannes Mols",
          "authorId": "2342686369"
        },
        {
          "name": "Lifeng Jin",
          "authorId": "2343702236"
        },
        {
          "name": "E. Cardona",
          "authorId": "2342685923"
        },
        {
          "name": "Dean Lee",
          "authorId": "2299292563"
        },
        {
          "name": "Jeremy Kritz",
          "authorId": "2342686324"
        },
        {
          "name": "Willow E. Primack",
          "authorId": "97781280"
        },
        {
          "name": "Summer Yue",
          "authorId": "2290014338"
        },
        {
          "name": "Chen Xing",
          "authorId": "2342686217"
        }
      ],
      "year": 2025,
      "abstract": "We present MultiChallenge, a pioneering benchmark evaluating large language models (LLMs) on conducting multi-turn conversations with human users, a crucial yet underexamined capability for their applications. MultiChallenge identifies four categories of challenges in multi-turn conversations that are not only common and realistic among current human-LLM interactions, but are also challenging to all current frontier LLMs. All 4 challenges require accurate instruction-following, context allocation, and in-context reasoning at the same time. We also develop LLM as judge with instance-level rubrics to facilitate an automatic evaluation method with fair agreement with experienced human raters. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving just a 41.4% average accuracy.",
      "citationCount": 71,
      "doi": "10.48550/arXiv.2501.17399",
      "arxivId": "2501.17399",
      "url": "https://www.semanticscholar.org/paper/6311ff8a02daf56232ce7d4fac640b912ec9e670",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "18632-18702"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "8a451b268de1cf303b8dc5152a796c22701ecaef",
      "title": "Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction",
      "authors": [
        {
          "name": "Suma Bailis",
          "authorId": "2187686249"
        },
        {
          "name": "Jane Friedhoff",
          "authorId": "2278645876"
        },
        {
          "name": "Feiyang Chen",
          "authorId": "2279155764"
        }
      ],
      "year": 2024,
      "abstract": "This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game's complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework's utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models' strategic reasoning and communication. These findings highlight Werewolf Arena's potential as a challenging and scalable LLM benchmark.",
      "citationCount": 19,
      "doi": "10.48550/arXiv.2407.13943",
      "arxivId": "2407.13943",
      "url": "https://www.semanticscholar.org/paper/8a451b268de1cf303b8dc5152a796c22701ecaef",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.13943"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "03a93e2e7f57d3e1d1c1aee921ccf2161ac03872",
      "title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents",
      "authors": [
        {
          "name": "Hanjun Luo",
          "authorId": "2357857537"
        },
        {
          "name": "Shenyu Dai",
          "authorId": "2365045099"
        },
        {
          "name": "Chiming Ni",
          "authorId": "2364739076"
        },
        {
          "name": "Xinfeng Li",
          "authorId": "2349829666"
        },
        {
          "name": "Gui-Min Zhang",
          "authorId": "2340212538"
        },
        {
          "name": "Kun Wang",
          "authorId": "2363281493"
        },
        {
          "name": "Tongliang Liu",
          "authorId": "2353560912"
        },
        {
          "name": "H. Salam",
          "authorId": "2364747273"
        }
      ],
      "year": 2025,
      "abstract": "Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents'step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce AgentAuditor, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. AgentAuditor constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator's assessment of new cases. Moreover, we developed ASSEBench, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. ASSEBench comprises 2293 meticulously annotated interaction records, covering 15 risk types across 29 application scenarios. A key feature of ASSEBench is its nuanced approach to ambiguous risk situations, employing\"Strict\"and\"Lenient\"judgment standards. Experiments demonstrate that AgentAuditor not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly accessible at https://github.com/Astarojth/AgentAuditor.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2506.00641",
      "arxivId": "2506.00641",
      "url": "https://www.semanticscholar.org/paper/03a93e2e7f57d3e1d1c1aee921ccf2161ac03872",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.00641"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "088ab579bf490691eea7ac92e122ee11c9b9d131",
      "title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges",
      "authors": [
        {
          "name": "Sijun Tan",
          "authorId": "2296003107"
        },
        {
          "name": "Siyuan Zhuang",
          "authorId": "92721493"
        },
        {
          "name": "Kyle Montgomery",
          "authorId": "2254275867"
        },
        {
          "name": "William Y. Tang",
          "authorId": "2326253583"
        },
        {
          "name": "Alejandro Cuadron",
          "authorId": "2326113379"
        },
        {
          "name": "Chenguang Wang",
          "authorId": "2255299714"
        },
        {
          "name": "R. Popa",
          "authorId": "2315123815"
        },
        {
          "name": "Ion Stoica",
          "authorId": "2316152835"
        }
      ],
      "year": 2024,
      "abstract": "LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at https://github.com/ScalerLab/JudgeBench.",
      "citationCount": 145,
      "doi": null,
      "arxivId": "2410.12784",
      "url": "https://www.semanticscholar.org/paper/088ab579bf490691eea7ac92e122ee11c9b9d131",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.12784"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e0c7f4fdce68813a19757eab4cd42b3ef2b9b29a",
      "title": "MathConstruct: Challenging LLM Reasoning with Constructive Proofs",
      "authors": [
        {
          "name": "Mislav Balunovi'c",
          "authorId": "2138580250"
        },
        {
          "name": "Jasper Dekoninck",
          "authorId": "2268310707"
        },
        {
          "name": "Nikola Jovanovi'c",
          "authorId": "2324062790"
        },
        {
          "name": "Ivo Petrov",
          "authorId": "2345699423"
        },
        {
          "name": "Martin T. Vechev",
          "authorId": "1736447"
        }
      ],
      "year": 2025,
      "abstract": "While Large Language Models (LLMs) demonstrate impressive performance in mathematics, existing math benchmarks come with significant limitations. Many focus on problems with fixed ground-truth answers, and are often saturated due to problem simplicity or the viability of guessing or memorization. Crucially, they capture only a narrow subset of relevant math problems. To address this research gap, we introduce MathConstruct, a new benchmark of 121 challenging problems sourced from various math competitions, which targets constructive proofs, a widely encountered problem type requiring the construction of mathematical objects with specific properties. These proofs are particularly suitable for LLM evaluation, as solution correctness can be easily verified. Our automated verifiers also enable MathConstruct to generate problem variations, used to evaluate robustness. State-of-the-art LLMs solve only 60% of MathConstruct problems, highlighting its complexity and importance for LLM evaluation.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2502.10197",
      "arxivId": "2502.10197",
      "url": "https://www.semanticscholar.org/paper/e0c7f4fdce68813a19757eab4cd42b3ef2b9b29a",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.10197"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "5763879570c5deef6b242908f16a303d54d9adcf",
      "title": "TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues",
      "authors": [
        {
          "name": "Yubin Ge",
          "authorId": "2346651462"
        },
        {
          "name": "Salvatore Romeo",
          "authorId": "2343755167"
        },
        {
          "name": "Jason Cai",
          "authorId": "2344716903"
        },
        {
          "name": "Raphael Shu",
          "authorId": "2324784546"
        },
        {
          "name": "Monica Sunkara",
          "authorId": "2327863906"
        },
        {
          "name": "Yassine Benajiba",
          "authorId": "2287927460"
        },
        {
          "name": "Yi Zhang",
          "authorId": "2334517915"
        }
      ],
      "year": 2025,
      "abstract": "Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs time-aware memorization through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate neuro-symbolic temporal reasoning, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.",
      "citationCount": 12,
      "doi": "10.48550/arXiv.2502.01630",
      "arxivId": "2502.01630",
      "url": "https://www.semanticscholar.org/paper/5763879570c5deef6b242908f16a303d54d9adcf",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.01630"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "774d01e152003f342596031c0c0fbf1936dee41a",
      "title": "LiveBench: A Challenging, Contamination-Limited LLM Benchmark",
      "authors": [
        {
          "name": "Colin White",
          "authorId": "2258965437"
        },
        {
          "name": "Samuel Dooley",
          "authorId": "2315128608"
        },
        {
          "name": "Manley Roberts",
          "authorId": "2179329459"
        },
        {
          "name": "Arka Pal",
          "authorId": "2284763248"
        },
        {
          "name": "Ben Feuer",
          "authorId": "120557092"
        },
        {
          "name": "Siddhartha Jain",
          "authorId": "2307560471"
        },
        {
          "name": "Ravid Shwartz-Ziv",
          "authorId": "1411405900"
        },
        {
          "name": "Neel Jain",
          "authorId": "2204647912"
        },
        {
          "name": "Khalid Saifullah",
          "authorId": "2203810783"
        },
        {
          "name": "Sreemanti Dey",
          "authorId": "2356552876"
        },
        {
          "name": "Shubh-Agrawal",
          "authorId": "2356551053"
        },
        {
          "name": "S. Sandha",
          "authorId": "2353239"
        },
        {
          "name": "Siddartha Naidu",
          "authorId": "2060450397"
        },
        {
          "name": "Chinmay Hegde",
          "authorId": "2262216898"
        },
        {
          "name": "Yann LeCun",
          "authorId": "2265899558"
        },
        {
          "name": "Tom Goldstein",
          "authorId": "2306981546"
        },
        {
          "name": "Willie Neiswanger",
          "authorId": "2934259"
        },
        {
          "name": "Micah Goldblum",
          "authorId": "121592562"
        }
      ],
      "year": 2024,
      "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.",
      "citationCount": 75,
      "doi": null,
      "arxivId": "2406.19314",
      "url": "https://www.semanticscholar.org/paper/774d01e152003f342596031c0c0fbf1936dee41a",
      "venue": "International Conference on Learning Representations",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4fd7dfbb3400ce60cdedfb679185c35f41bdde62",
      "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation",
      "authors": [
        {
          "name": "Weihao Xuan",
          "authorId": "1387974253"
        },
        {
          "name": "Rui Yang",
          "authorId": "2279666961"
        },
        {
          "name": "Heli Qi",
          "authorId": "2292393163"
        },
        {
          "name": "Qingcheng Zeng",
          "authorId": "2268696297"
        },
        {
          "name": "Yunze Xiao",
          "authorId": "2152958412"
        },
        {
          "name": "Yun Xing",
          "authorId": "2326482972"
        },
        {
          "name": "Junjue Wang",
          "authorId": "2327165426"
        },
        {
          "name": "Huitao Li",
          "authorId": "2326063945"
        },
        {
          "name": "Xin Li",
          "authorId": "2349817920"
        },
        {
          "name": "Kunyu Yu",
          "authorId": "2311833673"
        },
        {
          "name": "Nan Liu",
          "authorId": "2351821127"
        },
        {
          "name": "Qingyu Chen",
          "authorId": "2347043169"
        },
        {
          "name": "Douglas Teodoro",
          "authorId": "2349809632"
        },
        {
          "name": "Edison Marrese-Taylor",
          "authorId": "2355404306"
        },
        {
          "name": "Shijian Lu",
          "authorId": "2237947102"
        },
        {
          "name": "Yusuke Iwasawa",
          "authorId": "1715282"
        },
        {
          "name": "Yutaka Matsuo",
          "authorId": "2253398720"
        },
        {
          "name": "Irene Li",
          "authorId": "2248289605"
        }
      ],
      "year": 2025,
      "abstract": "Existing large language model (LLM) evaluation benchmarks primarily focus on English, while current multilingual tasks lack parallel questions that specifically assess cross-linguistic reasoning abilities. This dual limitation makes it challenging to comprehensively assess LLMs' performance in the multilingual setting. To fill this gap, we introduce MMLU-ProX, a comprehensive benchmark covering 29 languages, built on an English benchmark. Each language version consists of 11,829 identical questions, enabling direct cross-linguistic comparisons. Additionally, to meet efficient evaluation needs, we provide a lite version containing 658 questions per language. To ensure the high quality of MMLU-ProX, we employ a rigorous development process that involves multiple powerful LLMs for translation, followed by expert review to ensure accurate expression, consistent terminology, and cultural relevance. Building on this, we systematically evaluate 36 state-of-the-art LLMs, including reasoning-enhanced and multilingual-optimized LLMs. The results reveal significant disparities in the multilingual capabilities of LLMs: While they perform well in high-resource languages, their performance declines markedly in low-resource languages, with gaps of up to 24.3%. Through MMLU-ProX, we aim to advance the development of more inclusive AI systems and promote equitable access to technology across global contexts.",
      "citationCount": 29,
      "doi": "10.48550/arXiv.2503.10497",
      "arxivId": "2503.10497",
      "url": "https://www.semanticscholar.org/paper/4fd7dfbb3400ce60cdedfb679185c35f41bdde62",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.10497"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
      "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
      "authors": [
        {
          "name": "Davide Paglieri",
          "authorId": "2280905545"
        },
        {
          "name": "Bartlomiej Cupial",
          "authorId": "2265579252"
        },
        {
          "name": "Samuel Coward",
          "authorId": "2321869100"
        },
        {
          "name": "Ulyana Piterbarg",
          "authorId": "2047734290"
        },
        {
          "name": "Maciej Wolczyk",
          "authorId": "2276436804"
        },
        {
          "name": "Akbir Khan",
          "authorId": "49213201"
        },
        {
          "name": "Eduardo Pignatelli",
          "authorId": "2269468910"
        },
        {
          "name": "Lukasz Kuci'nski",
          "authorId": "2331509998"
        },
        {
          "name": "Lerrel Pinto",
          "authorId": "2273667564"
        },
        {
          "name": "Rob Fergus",
          "authorId": "2273646684"
        },
        {
          "name": "Jakob N. Foerster",
          "authorId": "2330187322"
        },
        {
          "name": "Jack Parker-Holder",
          "authorId": "1410302742"
        },
        {
          "name": "Tim Rocktaschel",
          "authorId": "1389854357"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as several models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community. Code and Leaderboard at balrogai.com.",
      "citationCount": 68,
      "doi": "10.48550/arXiv.2411.13543",
      "arxivId": "2411.13543",
      "url": "https://www.semanticscholar.org/paper/faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.13543"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "456ffcc9efed5cb36bd01058ea6617173aa02a55",
      "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization",
      "authors": [
        {
          "name": "Hongzheng Chen",
          "authorId": "2294927908"
        },
        {
          "name": "Yingheng Wang",
          "authorId": "2322603767"
        },
        {
          "name": "Yaohui Cai",
          "authorId": "2276454314"
        },
        {
          "name": "Hins Hu",
          "authorId": "2296228974"
        },
        {
          "name": "Jiajie Li",
          "authorId": "2248355005"
        },
        {
          "name": "Shirley Huang",
          "authorId": "2366629818"
        },
        {
          "name": "Chenhui Deng",
          "authorId": "2337854993"
        },
        {
          "name": "Rongjian Liang",
          "authorId": "2291240060"
        },
        {
          "name": "Shufeng Kong",
          "authorId": "2292321134"
        },
        {
          "name": "Haoxing Ren",
          "authorId": "2339655489"
        },
        {
          "name": "Samitha Samaranayake",
          "authorId": "145188761"
        },
        {
          "name": "Carla P. Gomes",
          "authorId": "2237792433"
        },
        {
          "name": "Zhiru Zhang",
          "authorId": "2284031854"
        }
      ],
      "year": 2025,
      "abstract": "While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2506.07972",
      "arxivId": "2506.07972",
      "url": "https://www.semanticscholar.org/paper/456ffcc9efed5cb36bd01058ea6617173aa02a55",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.07972"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "89cff9492b2fea767eadffd297baafdb61b8ca2a",
      "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis",
      "authors": [
        {
          "name": "Anjiang Wei",
          "authorId": "2028616391"
        },
        {
          "name": "Tarun Suresh",
          "authorId": "2352942885"
        },
        {
          "name": "Jiannan Cao",
          "authorId": "2346112911"
        },
        {
          "name": "Naveen Kannan",
          "authorId": "2352940958"
        },
        {
          "name": "Yuheng Wu",
          "authorId": "2354175098"
        },
        {
          "name": "Kai Yan",
          "authorId": "2352995252"
        },
        {
          "name": "Thiago S. F. X. Teixeira",
          "authorId": "2326989082"
        },
        {
          "name": "Ke Wang",
          "authorId": "2327322280"
        },
        {
          "name": "Alex Aiken",
          "authorId": "2277502914"
        }
      ],
      "year": 2025,
      "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/CodeARC",
      "citationCount": 6,
      "doi": "10.48550/arXiv.2503.23145",
      "arxivId": "2503.23145",
      "url": "https://www.semanticscholar.org/paper/89cff9492b2fea767eadffd297baafdb61b8ca2a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.23145"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "752f684371c9901791259dc4afd04b9754e803d1",
      "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
      "authors": [
        {
          "name": "Yizhuo Zhang",
          "authorId": "2129515763"
        },
        {
          "name": "Heng Wang",
          "authorId": "2256778370"
        },
        {
          "name": "Shangbin Feng",
          "authorId": "2284701198"
        },
        {
          "name": "Zhaoxuan Tan",
          "authorId": "2093186816"
        },
        {
          "name": "Xiaochuang Han",
          "authorId": "2257023881"
        },
        {
          "name": "Tianxing He",
          "authorId": "2249540815"
        },
        {
          "name": "Yulia Tsvetkov",
          "authorId": "2249583325"
        }
      ],
      "year": 2024,
      "abstract": "Large language models (LLMs) demonstrate great potential for problems with implicit graphical structures, while recent works seek to enhance the graph reasoning capabilities of LLMs through specialized instruction tuning. The resulting 'graph LLMs' are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data. To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph reasoning generalization: whether LLMs could go beyond semantic, numeric, structural, reasoning patterns in the synthetic training data and improve utility on real-world graph-based tasks. Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures. We explore three strategies to improve LLM graph reasoning generalization, and we find that while post-training alignment is most promising for real-world tasks, empowering LLM graph reasoning to go beyond pattern memorization remains an open research question.",
      "citationCount": 47,
      "doi": "10.48550/arXiv.2406.15992",
      "arxivId": "2406.15992",
      "url": "https://www.semanticscholar.org/paper/752f684371c9901791259dc4afd04b9754e803d1",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.15992"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e8ae4a277aed1bbf5e92fe4422d3e6bde0f7ae28",
      "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains",
      "authors": [
        {
          "name": "Xuzhao Li",
          "authorId": "2326984494"
        },
        {
          "name": "Xuchen Li",
          "authorId": "2288040881"
        },
        {
          "name": "Shiyu Hu",
          "authorId": "1491622261"
        },
        {
          "name": "Yongzhen Guo",
          "authorId": "2325459050"
        },
        {
          "name": "Wentao Zhang",
          "authorId": "2372959120"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) increasingly rely on reinforcement learning (RL) to enhance their reasoning capabilities through feedback. A critical challenge is verifying the consistency of model-generated responses and reference answers, since these responses are often lengthy, diverse, and nuanced. Rule-based verifiers struggle with complexity, prompting the use of model-based verifiers. However, specialized verifiers lack flexibility, while general LLM judges can be inconsistent. Existing research primarily focuses on building better verifiers, yet a systematic evaluation of different types of verifiers'performance across domains remains lacking, severely constraining the reliable development of Reinforcement Learning with Verifiable Reward (RLVR). To address this, we propose VerifyBench--a cross-domain comprehensive benchmark for systematically evaluating verifiers. We construct 4,000 expert-level questions covering mathematics, physics, chemistry, and biology. Each question is equipped with reference answers and diverse responses. The reliability of the evaluation is ensured through a rigorous annotation process conducted by a multidisciplinary expert team. We design a four-dimensional experimental framework to comprehensively compare the performance boundaries of specialized verifiers and general LLMs under combined conditions of extracted answers vs. complete responses, and short vs. long outputs. Our evaluation uncovers fundamental trade-offs in verifiers: while specialized verifiers achieve leading accuracy, they exhibit deficiencies in recall; general models show stronger inclusivity but unstable precision. More importantly, we discover verifiers'high sensitivity to input structure and inherent limitations in cross-domain generalization, providing critical insights into the bottlenecks of current verifier technology.",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2507.09884",
      "arxivId": "2507.09884",
      "url": "https://www.semanticscholar.org/paper/e8ae4a277aed1bbf5e92fe4422d3e6bde0f7ae28",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.09884"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7706de6db48c21e19895fb2907359aeb2c450476",
      "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning",
      "authors": [
        {
          "name": "Alex Egg",
          "authorId": "2333361813"
        },
        {
          "name": "Martin Iglesias Goyanes",
          "authorId": "2371069251"
        },
        {
          "name": "Friso Kingma",
          "authorId": "2371069907"
        },
        {
          "name": "Andreu Mora",
          "authorId": "2371070056"
        },
        {
          "name": "L. V. Werra",
          "authorId": "51128119"
        },
        {
          "name": "Thomas Wolf",
          "authorId": "2354240405"
        }
      ],
      "year": 2025,
      "abstract": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic multi-step data analysis tasks. DABstep comprises over 450 real-world challenges derived from a financial analytics platform, requiring models to combine code-based data processing with contextual reasoning over heterogeneous documentation. Each task demands an iterative, multi-step problem-solving approach, testing capabilities in data manipulation, cross-referencing multiple sources, and precise result reporting. The benchmark provides a factoid-style answer format with automatic correctness checks for objective scoring at scale. We evaluate leading LLM-based agents, revealing a substantial performance gap: even the best agent achieves only 14.55% accuracy on the hardest tasks. We detail our benchmark's design, dataset composition, task formulation, evaluation protocol, report baseline results and analyze failure modes. DABstep is released with a public leaderboard and toolkit to accelerate research in autonomous data analysis.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2506.23719",
      "arxivId": "2506.23719",
      "url": "https://www.semanticscholar.org/paper/7706de6db48c21e19895fb2907359aeb2c450476",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.23719"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b65d6bc8c7184db100d7a881f4d6c2bebfdea9dc",
      "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
      "authors": [
        {
          "name": "Jiashuo Yu",
          "authorId": "2116034742"
        },
        {
          "name": "Yue Wu",
          "authorId": "2367137677"
        },
        {
          "name": "Meng Chu",
          "authorId": "2366539382"
        },
        {
          "name": "Zhifei Ren",
          "authorId": "2342398635"
        },
        {
          "name": "Zizheng Huang",
          "authorId": "2267535629"
        },
        {
          "name": "Pei Chu",
          "authorId": "2288188121"
        },
        {
          "name": "Ruijie Zhang",
          "authorId": "2342306068"
        },
        {
          "name": "Yinan He",
          "authorId": "2118918324"
        },
        {
          "name": "Qirui Li",
          "authorId": "2366548027"
        },
        {
          "name": "Songze Li",
          "authorId": "2293357052"
        },
        {
          "name": "Zhenxiang Li",
          "authorId": "2290966898"
        },
        {
          "name": "Zhongying Tu",
          "authorId": "2289992992"
        },
        {
          "name": "Conghui He",
          "authorId": "2289699353"
        },
        {
          "name": "Yu Qiao",
          "authorId": "2268701394"
        },
        {
          "name": "Yali Wang",
          "authorId": "2298732012"
        },
        {
          "name": "Yi Wang",
          "authorId": "2263510675"
        },
        {
          "name": "Limin Wang",
          "authorId": "2141353278"
        }
      ],
      "year": 2025,
      "abstract": "We present VRBench, the first long narrative video benchmark crafted for evaluating large models'multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 960 long videos (with an average duration of 1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs and 25,106 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2506.10857",
      "arxivId": "2506.10857",
      "url": "https://www.semanticscholar.org/paper/b65d6bc8c7184db100d7a881f4d6c2bebfdea9dc",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.10857"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "814a15a34d3bfc5cf6496ccc222598f1b85361b5",
      "title": "CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Yuefei Chen",
          "authorId": "2345916356"
        },
        {
          "name": "Vivek K. Singh",
          "authorId": "2350853448"
        },
        {
          "name": "Jing Ma",
          "authorId": "2346131223"
        },
        {
          "name": "Ruxiang Tang",
          "authorId": "2346993681"
        }
      ],
      "year": 2025,
      "abstract": "Counterfactual reasoning is widely recognized as one of the most challenging and intricate aspects of causality in artificial intelligence. In this paper, we evaluate the performance of large language models (LLMs) in counterfactual reasoning. In contrast to previous studies that primarily focus on commonsense causal reasoning, where LLMs often rely on prior knowledge for inference, we specifically assess their ability to perform counterfactual inference using a set of formal rules. To support this evaluation, we introduce a new benchmark dataset, CounterBench, comprising 1K counterfactual reasoning questions. The dataset is designed with varying levels of difficulty, diverse causal graph structures, distinct types of counterfactual questions, and multiple nonsensical name variants. Our experiments demonstrate that counterfactual reasoning poses a significant challenge for LLMs, with most models performing at levels comparable to random guessing. To enhance LLM's counterfactual reasoning ability, we propose a novel reasoning paradigm, CoIn, which guides LLMs through iterative reasoning and backtracking to systematically explore counterfactual solutions. Experimental results show that our method significantly improves LLM performance on counterfactual reasoning tasks and consistently enhances performance across different LLMs.Our dataset is available at https://huggingface.co/datasets/CounterBench/CounterBench.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2502.11008",
      "arxivId": "2502.11008",
      "url": "https://www.semanticscholar.org/paper/814a15a34d3bfc5cf6496ccc222598f1b85361b5",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.11008"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0aa160093f13c6c6ca444d1e72660b494e10c309",
      "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
      "authors": [
        {
          "name": "Peijie Dong",
          "authorId": "2265581222"
        },
        {
          "name": "Zhenheng Tang",
          "authorId": "66873962"
        },
        {
          "name": "Xiang-Hong Liu",
          "authorId": "2243330447"
        },
        {
          "name": "Lujun Li",
          "authorId": "2240002722"
        },
        {
          "name": "Xiaowen Chu",
          "authorId": "2259315629"
        },
        {
          "name": "Bo Li",
          "authorId": "2343704731"
        }
      ],
      "year": 2025,
      "abstract": "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2505.19433",
      "arxivId": "2505.19433",
      "url": "https://www.semanticscholar.org/paper/0aa160093f13c6c6ca444d1e72660b494e10c309",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.19433"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "2454302acea271345d2ba5dc548a55c5e4c2d39d",
      "title": "HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning",
      "authors": [
        {
          "name": "Xiaoyuan Li",
          "authorId": "2304613272"
        },
        {
          "name": "Moxin Li",
          "authorId": "2118769749"
        },
        {
          "name": "Rui Men",
          "authorId": "47447639"
        },
        {
          "name": "Yichang Zhang",
          "authorId": "29343468"
        },
        {
          "name": "Keqin Bao",
          "authorId": "2387215667"
        },
        {
          "name": "Wenjie Wang",
          "authorId": "2117833732"
        },
        {
          "name": "Fuli Feng",
          "authorId": "2280911299"
        },
        {
          "name": "Dayiheng Liu",
          "authorId": "2248487202"
        },
        {
          "name": "Junyang Lin",
          "authorId": "2326803484"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2502.11393",
      "arxivId": "2502.11393",
      "url": "https://www.semanticscholar.org/paper/2454302acea271345d2ba5dc548a55c5e4c2d39d",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.11393"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ba0eb6e108a33ffa7c9a848ec453a84d40932ec3",
      "title": "ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large Language Models",
      "authors": [
        {
          "name": "Boyang Xue",
          "authorId": "2248039956"
        },
        {
          "name": "Qi Zhu",
          "authorId": "2269768949"
        },
        {
          "name": "Rui Wang",
          "authorId": "2248766573"
        },
        {
          "name": "Sheng Wang",
          "authorId": "2373686987"
        },
        {
          "name": "Hongru Wang",
          "authorId": "22642319"
        },
        {
          "name": "Fei Mi",
          "authorId": "2258717400"
        },
        {
          "name": "Yasheng Wang",
          "authorId": "2136912252"
        },
        {
          "name": "Lifeng Shang",
          "authorId": "2362626718"
        },
        {
          "name": "Qun Liu",
          "authorId": "2249841180"
        },
        {
          "name": "Kam-Fai Wong",
          "authorId": "2293717684"
        }
      ],
      "year": 2025,
      "abstract": "Although demonstrating remarkable performance on reasoning tasks, Large Language Models (LLMs) still tend to fabricate unreliable responses when confronted with problems that are unsolvable or beyond their capability, severely undermining the reliability. Prior studies of LLM reliability have primarily focused on knowledge tasks to identify unanswerable questions, while mathematical reasoning tasks have remained unexplored due to the dearth of unsolvable math problems. To systematically investigate LLM reliability in mathematical reasoning tasks, we formulate the reliability evaluation for both solvable and unsolvable problems. We then develop a ReliableMath dataset which incorporates open-source solvable problems and high-quality unsolvable problems synthesized by our proposed construction workflow with human evaluations. Experiments are conducted on various LLMs with several key findings uncovered. LLMs fail to directly identify unsolvable problems and always generate fabricated responses. When instructing LLMs to indicate unsolvability using a reliable prompt, the reliability of larger-sized LLMs remains on solvable problems, but notably improves on unsolvable problems yet still falls short of solvable problems. However, small LLMs rarely show any progress despite employing reliable prompts. Therefore, we further propose an alignment strategy to enhance small LLMs'reliability, which can significantly improve LLM reliability performances on both in-domain and out-of-domain tasks.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2507.03133",
      "arxivId": "2507.03133",
      "url": "https://www.semanticscholar.org/paper/ba0eb6e108a33ffa7c9a848ec453a84d40932ec3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.03133"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6ab849bba2a40e706ebacd4e666ce7e21d170c0f",
      "title": "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents",
      "authors": [
        {
          "name": "Luca Gioacchini",
          "authorId": "93647972"
        },
        {
          "name": "G. Siracusano",
          "authorId": "2009237"
        },
        {
          "name": "D. Sanvito",
          "authorId": "3109801"
        },
        {
          "name": "Kiril Gashteovski",
          "authorId": "24868638"
        },
        {
          "name": "David Friede",
          "authorId": "2295731200"
        },
        {
          "name": "Roberto Bifulco",
          "authorId": "2269460793"
        },
        {
          "name": "Carolin Lawrence",
          "authorId": "2261363939"
        }
      ],
      "year": 2024,
      "abstract": "The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest \u2013 a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.",
      "citationCount": 20,
      "doi": "10.48550/arXiv.2404.06411",
      "arxivId": "2404.06411",
      "url": "https://www.semanticscholar.org/paper/6ab849bba2a40e706ebacd4e666ce7e21d170c0f",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "journal": {
        "pages": "185-193"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a94b17d099b4eb9e713107e6daebde12980922ae",
      "title": "DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning",
      "authors": [
        {
          "name": "Shangqing Tu",
          "authorId": "2116520118"
        },
        {
          "name": "Kejian Zhu",
          "authorId": "2305113513"
        },
        {
          "name": "Yushi Bai",
          "authorId": "2141377570"
        },
        {
          "name": "Zijun Yao",
          "authorId": "2273946831"
        },
        {
          "name": "Lei Hou",
          "authorId": "2284777109"
        },
        {
          "name": "Juanzi Li",
          "authorId": "2133353675"
        }
      ],
      "year": 2024,
      "abstract": "The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance. Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training. Besides, prior work has already shown that even training on data similar to benchmark data inflates performance, namely \\emph{In-distribution contamination}. In this work, we argue that in-distribution contamination can lead to the performance drop on OOD benchmarks. To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination. DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer. Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets. We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions. Additionally, we find that DICE's predictions correlate with the performance of LLMs fine-tuned by either us or other organizations, achieving a coefficient of determination ($R^2$) between 0.61 and 0.75. The code and data are available at https://github.com/THU-KEG/DICE.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2406.04197",
      "arxivId": "2406.04197",
      "url": "https://www.semanticscholar.org/paper/a94b17d099b4eb9e713107e6daebde12980922ae",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.04197"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
