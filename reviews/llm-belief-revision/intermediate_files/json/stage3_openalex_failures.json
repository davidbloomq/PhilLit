{
  "status": "success",
  "source": "openalex",
  "query": "LLM reasoning failures",
  "results": [
    {
      "openalex_id": "W4391136507",
      "doi": "10.1145/3641289",
      "title": "A Survey on Evaluation of Large Language Models",
      "authors": [
        {
          "name": "Yupeng Chang",
          "openalex_id": "A5102930702",
          "orcid": "https://orcid.org/0000-0001-7178-6088",
          "institutions": [
            "Jilin University"
          ]
        },
        {
          "name": "Xu Wang",
          "openalex_id": "A5100407966",
          "orcid": "https://orcid.org/0009-0001-5904-5313",
          "institutions": [
            "Jilin University"
          ]
        },
        {
          "name": "Jindong Wang",
          "openalex_id": "A5100700956",
          "orcid": "https://orcid.org/0000-0002-4833-0880",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Yuan Wu",
          "openalex_id": "A5102024536",
          "orcid": "https://orcid.org/0000-0001-6289-5872",
          "institutions": [
            "Jilin University"
          ]
        },
        {
          "name": "Linyi Yang",
          "openalex_id": "A5035082722",
          "orcid": "https://orcid.org/0000-0003-0667-7349",
          "institutions": [
            "Westlake University"
          ]
        },
        {
          "name": "Kaijie Zhu",
          "openalex_id": "A5103056110",
          "orcid": "https://orcid.org/0009-0002-6220-1476",
          "institutions": [
            "Shandong Institute of Automation",
            "Chinese Academy of Sciences"
          ]
        },
        {
          "name": "Hao Chen",
          "openalex_id": "A5100353526",
          "orcid": "https://orcid.org/0000-0002-1960-4803",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Xiaoyuan Yi",
          "openalex_id": "A5026595284",
          "orcid": "https://orcid.org/0000-0003-2710-1613",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Cunxiang Wang",
          "openalex_id": "A5010758235",
          "orcid": "https://orcid.org/0000-0002-3023-8082",
          "institutions": [
            "Westlake University"
          ]
        },
        {
          "name": "Yidong Wang",
          "openalex_id": "A5100685716",
          "orcid": "https://orcid.org/0009-0007-9969-8259",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Wei Ye",
          "openalex_id": "A5101538186",
          "orcid": "https://orcid.org/0000-0002-9331-4716",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Yue Zhang",
          "openalex_id": "A5100333729",
          "orcid": "https://orcid.org/0000-0002-5214-2268",
          "institutions": [
            "Westlake University"
          ]
        },
        {
          "name": "Yi Chang",
          "openalex_id": "A5029392006",
          "orcid": "https://orcid.org/0000-0003-2697-8093",
          "institutions": [
            "Jilin University"
          ]
        },
        {
          "name": "Philip S. Yu",
          "openalex_id": "A5036357902",
          "orcid": "https://orcid.org/0000-0002-3491-5968",
          "institutions": [
            "University of Illinois Chicago"
          ]
        },
        {
          "name": "Qiang Yang",
          "openalex_id": "A5100636286",
          "orcid": "https://orcid.org/0000-0001-5059-8360",
          "institutions": [
            "University of Hong Kong",
            "Hong Kong University of Science and Technology"
          ]
        },
        {
          "name": "Xing Xie",
          "openalex_id": "A5044651577",
          "orcid": "https://orcid.org/0000-0002-8608-8482",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-23",
      "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate , where to evaluate , and how to evaluate . Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the \u2018where\u2019 and \u2018how\u2019 questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey",
      "cited_by_count": 1873,
      "type": "article",
      "source": {
        "name": "ACM Transactions on Intelligent Systems and Technology",
        "type": "journal",
        "issn": [
          "2157-6904",
          "2157-6912"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 280,
      "url": "https://openalex.org/W4391136507"
    },
    {
      "openalex_id": "W4405785256",
      "doi": "10.1109/iros58592.2024.10801328",
      "title": "LLM<sup>3</sup>: Large Language Model-based Task and Motion Planning with Motion Failure Reasoning",
      "authors": [
        {
          "name": "Shu Wang",
          "openalex_id": "A5070346269",
          "orcid": "https://orcid.org/0000-0002-7920-7025",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Muzhi Han",
          "openalex_id": "A5070386263",
          "orcid": "https://orcid.org/0000-0002-2649-4577",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Ziyuan Jiao",
          "openalex_id": "A5084328887",
          "orcid": "https://orcid.org/0000-0003-3404-3810",
          "institutions": [
            "Beijing Academy of Artificial Intelligence"
          ]
        },
        {
          "name": "Zeyu Zhang",
          "openalex_id": "A5052460961",
          "orcid": "https://orcid.org/0000-0001-8929-134X",
          "institutions": [
            "Beijing Academy of Artificial Intelligence"
          ]
        },
        {
          "name": "Ying Wu",
          "openalex_id": "A5101780958",
          "orcid": "https://orcid.org/0009-0001-6768-5118",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Song-Chun Zhu",
          "openalex_id": "A5031660884",
          "orcid": "https://orcid.org/0009-0009-9458-5583",
          "institutions": [
            "Beijing Academy of Artificial Intelligence"
          ]
        },
        {
          "name": "Hangxin Liu",
          "openalex_id": "A5043423420",
          "orcid": "https://orcid.org/0000-0002-3003-8611",
          "institutions": [
            "Beijing Academy of Artificial Intelligence"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-10-14",
      "abstract": null,
      "cited_by_count": 17,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Natural Language Processing Techniques",
        "Multimodal Machine Learning Applications",
        "Speech and dialogue systems"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W4405785256"
    },
    {
      "openalex_id": "W4378942271",
      "doi": "10.48550/arxiv.2305.18354",
      "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",
      "authors": [
        {
          "name": "Yudong Xu",
          "openalex_id": "A5100572590"
        },
        {
          "name": "Wenhao Li",
          "openalex_id": "A5107145907"
        },
        {
          "name": "Pashootan Vaezipoor",
          "openalex_id": "A5091641915"
        },
        {
          "name": "Scott Sanner",
          "openalex_id": "A5028174137",
          "orcid": "https://orcid.org/0000-0001-7984-8394"
        },
        {
          "name": "Elias B. Khalil",
          "openalex_id": "A5009528063",
          "orcid": "https://orcid.org/0000-0001-5844-9642"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-26",
      "abstract": "Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some \"core knowledge\" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability. Visualizations, GPT logs, and data are available at https://khalil-research.github.io/LLM4ARC.",
      "cited_by_count": 5,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.18354"
      },
      "topics": [
        "Topic Modeling",
        "Explainable Artificial Intelligence (XAI)",
        "Software Engineering Research"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4378942271"
    },
    {
      "openalex_id": "W4378509375",
      "doi": "10.48550/arxiv.2305.14279",
      "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
      "authors": [
        {
          "name": "Angelica Chen",
          "openalex_id": "A5034489893"
        },
        {
          "name": "Jason Phang",
          "openalex_id": "A5047763717",
          "orcid": "https://orcid.org/0000-0003-3522-1869"
        },
        {
          "name": "Alicia Parrish",
          "openalex_id": "A5082569485",
          "orcid": "https://orcid.org/0000-0002-1054-0516"
        },
        {
          "name": "Vishakh Padmakumar",
          "openalex_id": "A5052649262",
          "orcid": "https://orcid.org/0000-0002-3396-3589"
        },
        {
          "name": "Chen Zhao",
          "openalex_id": "A5100351992",
          "orcid": "https://orcid.org/0000-0001-9843-6758"
        },
        {
          "name": "Samuel R. Bowman",
          "openalex_id": "A5112713734"
        },
        {
          "name": "Kyunghyun Cho",
          "openalex_id": "A5091175785",
          "orcid": "https://orcid.org/0000-0003-1669-3211"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-23",
      "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.",
      "cited_by_count": 6,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.14279"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4378509375"
    },
    {
      "openalex_id": "W4383605161",
      "doi": "10.48550/arxiv.2307.03109",
      "title": "A Survey on Evaluation of Large Language Models",
      "authors": [
        {
          "name": "Yupeng Chang",
          "openalex_id": "A5102930702",
          "orcid": "https://orcid.org/0000-0001-7178-6088"
        },
        {
          "name": "Xu Wang",
          "openalex_id": "A5100639862",
          "orcid": "https://orcid.org/0000-0002-5251-0534"
        },
        {
          "name": "Jindong Wang",
          "openalex_id": "A5100700956",
          "orcid": "https://orcid.org/0000-0002-4833-0880"
        },
        {
          "name": "Yuan-Hsuan Wu",
          "openalex_id": "A5110426370"
        },
        {
          "name": "Kaijie Zhu",
          "openalex_id": "A5103056110",
          "orcid": "https://orcid.org/0009-0002-6220-1476"
        },
        {
          "name": "Hao Chen",
          "openalex_id": "A5100353526",
          "orcid": "https://orcid.org/0000-0002-1960-4803"
        },
        {
          "name": "Linyi Yang",
          "openalex_id": "A5035082722",
          "orcid": "https://orcid.org/0000-0003-0667-7349"
        },
        {
          "name": "Xiaoyuan Yi",
          "openalex_id": "A5040961637"
        },
        {
          "name": "Cunxiang Wang",
          "openalex_id": "A5010758235",
          "orcid": "https://orcid.org/0000-0002-3023-8082"
        },
        {
          "name": "Yidong Wang",
          "openalex_id": "A5100685716",
          "orcid": "https://orcid.org/0009-0007-9969-8259"
        },
        {
          "name": "Wei Ye",
          "openalex_id": "A5026098803",
          "orcid": "https://orcid.org/0000-0002-1074-2169"
        },
        {
          "name": "Yue Zhang",
          "openalex_id": "A5100333646",
          "orcid": "https://orcid.org/0000-0001-6875-7761"
        },
        {
          "name": "Yi Chang",
          "openalex_id": "A5114168422"
        },
        {
          "name": "Philip S. Yu",
          "openalex_id": "A5036357902",
          "orcid": "https://orcid.org/0000-0002-3491-5968"
        },
        {
          "name": "Qiang Yang",
          "openalex_id": "A5041210716",
          "orcid": "https://orcid.org/0000-0003-3762-5015"
        },
        {
          "name": "Xing Xie",
          "openalex_id": "A5111456159"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-06",
      "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.",
      "cited_by_count": 193,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2307.03109"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4383605161"
    },
    {
      "openalex_id": "W4304195432",
      "doi": "10.5281/zenodo.17432640",
      "title": "Persona-Driven Benchmarking for Generalizable and Human-Aware Artificial General Intelligence",
      "authors": [
        {
          "name": "Shunyu Yao",
          "openalex_id": "A5034251656",
          "orcid": "https://orcid.org/0000-0002-8732-7844"
        },
        {
          "name": "Jeffrey Zhao",
          "openalex_id": "A5080095155",
          "orcid": "https://orcid.org/0000-0001-7046-5434"
        },
        {
          "name": "Dian Yu",
          "openalex_id": "A5113226487"
        },
        {
          "name": "Nan Du",
          "openalex_id": "A5100721790",
          "orcid": "https://orcid.org/0000-0003-2855-7452"
        },
        {
          "name": "Izhak Shafran",
          "openalex_id": "A5025419994"
        },
        {
          "name": "Karthik Narasimhan",
          "openalex_id": "A5025205227"
        },
        {
          "name": "Yuan Cao",
          "openalex_id": "A5100400922",
          "orcid": "https://orcid.org/0000-0002-8775-0626"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-10-24",
      "abstract": "This research paper, \"Persona-Driven Benchmarking for Generalizable and Human-Aware Artificial General Intelligence,\" proposes a novel architectural solution to transition current Large Language Models (LLMs) from sophisticated pattern-matchers to genuine Artificial General Intelligence (AGI) agents. The core argument of the paper is that achieving AGI is primarily an architectural challenge (creating a \"gearbox\" of metacognitive functions) rather than solely a scaling or fundamental modeling problem (the \"engine\"). The proposed framework is an architectural overlay composed of three synergistic components that address the key limitations of contemporary LLMs: lack of persistent memory, poor accountability, and narrow human-centric reasoning. Key Components of the Proposed Architecture Persona Module (for Dynamic Contextualization and Self-Awareness): Function: Handles the dynamic contextualization of all interactions, performs causal intent inference regarding the user's needs, and manages the AGI's evolving, internal self-identity (or \"self-model\"). Goal: To enable the AGI to move beyond simple response generation to goal-oriented optimization based on a deep, human-aware understanding of the user and its own capabilities. Benchmarking Loop (for Metacognitive Self-Correction): Function: This component acts as the AGI's metacognitive self-correction engine. It rigorously audits the LLM's output against a goal-specific metric, performs causal inference to determine why a failure occurred, and then dictates a permanent learning path. Goal: To foster an internal, continuous learning cycle that ensures the AGI is accountable and capable of generalized skill transfer from one task to a similar, new one. FedRAG (Federated Reflective Augmented Generation): Function: A novel, persistent external memory layer that functions as a dynamic knowledge graph. It is crucial because it decouples learned skills and state from the static LLM weights. It stores successful outcomes, critical causal inferences from the Benchmarking Loop, user preferences, and Persona state updates. Goal: To provide the AGI with persistent, generalizable memory and ensure that learning is permanent, efficient, and does not lead to catastrophic forgetting or excessive model scaling. Central Thesis The paper asserts that the integrated, recursive loop formed by these three components\u2014where the Benchmarking Loop's reflective output is written to the FedRAG memory for use by the Persona Module in future interactions\u2014is the necessary architectural scaffold to achieve true, generalizable, and human-aware AGI.",
      "cited_by_count": 503,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2210.03629"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4304195432"
    },
    {
      "openalex_id": "W4404347840",
      "doi": "10.48550/arxiv.2410.23884",
      "title": "Failure Modes of LLMs for Causal Reasoning on Narratives",
      "authors": [
        {
          "name": "Khurram Yamin",
          "openalex_id": "A5114534443"
        },
        {
          "name": "Shantanu Gupta",
          "openalex_id": "A5064330876",
          "orcid": "https://orcid.org/0000-0002-9931-1612"
        },
        {
          "name": "Gaurav R. Ghosal",
          "openalex_id": "A5114635773"
        },
        {
          "name": "Zachary C. Lipton",
          "openalex_id": "A5029448258",
          "orcid": "https://orcid.org/0000-0002-3824-4241"
        },
        {
          "name": "Bryan Wilder",
          "openalex_id": "A5079207566",
          "orcid": "https://orcid.org/0000-0002-9410-020X"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-10-31",
      "abstract": "The ability to robustly identify causal relationships is essential for autonomous decision-making and adaptation to novel scenarios. However, accurately inferring causal structure requires integrating both world knowledge and abstract logical reasoning. In this work, we investigate the interaction between these two capabilities through the representative task of causal reasoning over narratives. Through controlled synthetic, semi-synthetic, and real-world experiments, we find that state-of-the-art large language models (LLMs) often rely on superficial heuristics -- for example, inferring causality from event order or recalling memorized world knowledge without attending to context. Furthermore, we show that simple reformulations of the task can elicit more robust reasoning behavior. Our evaluation spans a range of causal structures, from linear chains to complex graphs involving colliders and forks. These findings uncover systematic patterns in how LLMs perform causal reasoning and lay the groundwork for developing methods that better align LLM behavior with principled causal inference.",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2410.23884"
      },
      "topics": [
        "Topic Modeling",
        "Multi-Agent Systems and Negotiation",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4404347840"
    },
    {
      "openalex_id": "W4387876242",
      "doi": "10.1007/s10514-023-10132-6",
      "title": "Semantic anomaly detection with large language models",
      "authors": [
        {
          "name": "Amine Elhafsi",
          "openalex_id": "A5083943136",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Rohan Sinha",
          "openalex_id": "A5101568385",
          "orcid": "https://orcid.org/0000-0002-3606-7181",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Christopher Agia",
          "openalex_id": "A5084514643",
          "orcid": "https://orcid.org/0000-0002-1208-2539",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Edward Schmerling",
          "openalex_id": "A5103887320",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Issa Nesnas",
          "openalex_id": "A5037801956",
          "orcid": "https://orcid.org/0000-0002-2616-5001",
          "institutions": [
            "Jet Propulsion Laboratory"
          ]
        },
        {
          "name": "Marco Pavone",
          "openalex_id": "A5050003000",
          "orcid": "https://orcid.org/0000-0002-0206-4337",
          "institutions": [
            "Stanford University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-23",
      "abstract": null,
      "cited_by_count": 66,
      "type": "article",
      "source": {
        "name": "Autonomous Robots",
        "type": "journal",
        "issn": [
          "0929-5593",
          "1573-7527"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Anomaly Detection Techniques and Applications",
        "Network Security and Intrusion Detection",
        "Software System Performance and Reliability"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W4387876242"
    },
    {
      "openalex_id": "W4321277158",
      "doi": "10.48550/arxiv.2302.08399",
      "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
      "authors": [
        {
          "name": "Tomer Ullman",
          "openalex_id": "A5086092571",
          "orcid": "https://orcid.org/0000-0003-1722-2382"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-02-16",
      "abstract": "Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.",
      "cited_by_count": 79,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2302.08399"
      },
      "topics": [
        "Topic Modeling",
        "Advanced Graph Neural Networks",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4321277158"
    },
    {
      "openalex_id": "W4378942694",
      "doi": "10.48550/arxiv.2305.18654",
      "title": "Faith and Fate: Limits of Transformers on Compositionality",
      "authors": [
        {
          "name": "Nouha Dziri",
          "openalex_id": "A5049618494"
        },
        {
          "name": "Ximing Lu",
          "openalex_id": "A5102930940",
          "orcid": "https://orcid.org/0000-0001-6671-4573"
        },
        {
          "name": "Melanie Sclar",
          "openalex_id": "A5039472047"
        },
        {
          "name": "Xiang Lorraine Li",
          "openalex_id": "A5034858125"
        },
        {
          "name": "Liwei Jian",
          "openalex_id": "A5108840339"
        },
        {
          "name": "Bill Yuchen Lin",
          "openalex_id": "A5013041264",
          "orcid": "https://orcid.org/0000-0002-1149-0186"
        },
        {
          "name": "Peter West",
          "openalex_id": "A5112759123"
        },
        {
          "name": "Chandra Bhagavatula",
          "openalex_id": "A5044250030",
          "orcid": "https://orcid.org/0000-0001-6264-0378"
        },
        {
          "name": "Ronan Le Bras",
          "openalex_id": "A5024879161",
          "orcid": "https://orcid.org/0000-0003-2439-6938"
        },
        {
          "name": "Jena D. Hwang",
          "openalex_id": "A5080544237",
          "orcid": "https://orcid.org/0000-0003-3801-294X"
        },
        {
          "name": "Soumya Sanyal",
          "openalex_id": "A5102487773"
        },
        {
          "name": "Sean Welleck",
          "openalex_id": "A5019030424"
        },
        {
          "name": "Xiang Ren",
          "openalex_id": "A5009408707",
          "orcid": "https://orcid.org/0000-0001-8655-663X"
        },
        {
          "name": "Allyson Ettinger",
          "openalex_id": "A5012068726"
        },
        {
          "name": "Za\u00efd Harchaoui",
          "openalex_id": "A5010613767",
          "orcid": "https://orcid.org/0000-0003-1186-1343"
        },
        {
          "name": "Yejin Choi",
          "openalex_id": "A5102992157",
          "orcid": "https://orcid.org/0000-0003-3032-5378"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-29",
      "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
      "cited_by_count": 71,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.18654"
      },
      "topics": [
        "Topic Modeling",
        "Machine Learning in Materials Science",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4378942694"
    },
    {
      "openalex_id": "W4321074112",
      "doi": "10.2139/ssrn.4358789",
      "title": "AI as Agency Without Intelligence: On ChatGPT, Large Language Models, and Other Generative Models",
      "authors": [
        {
          "name": "Luciano Floridi",
          "openalex_id": "A5046574356",
          "orcid": "https://orcid.org/0000-0002-5444-2280",
          "institutions": [
            "University of Bologna",
            "Yale University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": null,
      "cited_by_count": 69,
      "type": "article",
      "source": {
        "name": "SSRN Electronic Journal",
        "type": "repository",
        "issn": [
          "1556-5068"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.2139/ssrn.4358789"
      },
      "topics": [
        "Topic Modeling",
        "Artificial Intelligence in Healthcare and Education",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4321074112"
    },
    {
      "openalex_id": "W4367692219",
      "doi": "10.48550/arxiv.2305.00050",
      "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
      "authors": [
        {
          "name": "Emre K\u0131c\u0131man",
          "openalex_id": "A5079458476",
          "orcid": "https://orcid.org/0000-0001-5429-468X"
        },
        {
          "name": "Robert A. Van Ness",
          "openalex_id": "A5090458533",
          "orcid": "https://orcid.org/0000-0002-8023-6421"
        },
        {
          "name": "Amit Sharma",
          "openalex_id": "A5080592530",
          "orcid": "https://orcid.org/0000-0003-1451-5892"
        },
        {
          "name": "Chenhao Tan",
          "openalex_id": "A5079270249",
          "orcid": "https://orcid.org/0000-0002-3981-2116"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-28",
      "abstract": "The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a \"behavorial\" study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.",
      "cited_by_count": 86,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.00050"
      },
      "topics": [
        "Topic Modeling"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4367692219"
    },
    {
      "openalex_id": "W4389523767",
      "doi": "10.18653/v1/2023.emnlp-main.13",
      "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
      "authors": [
        {
          "name": "Huao Li",
          "openalex_id": "A5020780914",
          "institutions": [
            "University of Pittsburgh"
          ]
        },
        {
          "name": "Chong Yu",
          "openalex_id": "A5102769116",
          "orcid": "https://orcid.org/0009-0000-5084-3768",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Simon Stepputtis",
          "openalex_id": "A5048668442",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Joseph Campbell",
          "openalex_id": "A5048453008",
          "orcid": "https://orcid.org/0000-0002-7924-8548",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Dana Hughes",
          "openalex_id": "A5036897495",
          "orcid": "https://orcid.org/0000-0003-4493-959X",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Charles Lewis",
          "openalex_id": "A5113067510",
          "institutions": [
            "University of Pittsburgh"
          ]
        },
        {
          "name": "Katia Sycara",
          "openalex_id": "A5087505541",
          "orcid": "https://orcid.org/0000-0001-5635-1406",
          "institutions": [
            "Carnegie Mellon University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
      "cited_by_count": 48,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2023.emnlp-main.13.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Language and cultural evolution",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 35,
      "url": "https://openalex.org/W4389523767"
    },
    {
      "openalex_id": "W7106244966",
      "doi": "10.1016/j.jii.2025.101012",
      "title": "Harnessing collective intelligence of multi-agent LLM systems for sensor failure reasoning in smart manufacturing",
      "authors": [
        {
          "name": "Wei Gong",
          "openalex_id": ""
        },
        {
          "name": "Shuang Qiao",
          "openalex_id": "",
          "orcid": "https://orcid.org/"
        },
        {
          "name": "Chenhong Cao",
          "openalex_id": ""
        },
        {
          "name": "Shilei Tan",
          "openalex_id": ""
        },
        {
          "name": "Junliang Ye",
          "openalex_id": ""
        },
        {
          "name": "Haoxiang Liu",
          "openalex_id": "",
          "orcid": "https://orcid.org/0009-0005-8120-3751"
        },
        {
          "name": "Si Chen",
          "openalex_id": ""
        },
        {
          "name": "Xuesong Wang",
          "openalex_id": "",
          "orcid": "https://orcid.org/0000-0003-0568-8089"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-11-21",
      "abstract": null,
      "cited_by_count": 1,
      "type": "article",
      "source": {
        "name": "Journal of Industrial Information Integration",
        "type": "journal",
        "issn": [
          "2452-414X",
          "2467-964X"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Flexible and Reconfigurable Manufacturing Systems",
        "Multi-Agent Systems and Negotiation",
        "Digital Transformation in Industry"
      ],
      "referenced_works_count": 16,
      "url": "https://openalex.org/W7106244966"
    },
    {
      "openalex_id": "W4319662928",
      "doi": "10.1371/journal.pdig.0000198",
      "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
      "authors": [
        {
          "name": "Tiffany H. Kung",
          "openalex_id": "A5055949685",
          "institutions": [
            "Massachusetts General Hospital"
          ]
        },
        {
          "name": "Morgan Cheatham",
          "openalex_id": "A5080358515",
          "orcid": "https://orcid.org/0000-0002-4688-058X",
          "institutions": [
            "Warren Alpert Foundation",
            "Brown University"
          ]
        },
        {
          "name": "Arielle Medenilla",
          "openalex_id": "A5015894459"
        },
        {
          "name": "Czarina Sillos",
          "openalex_id": "A5087687954"
        },
        {
          "name": "Lorie De Leon",
          "openalex_id": "A5003016349"
        },
        {
          "name": "Camille Elepa\u00f1o",
          "openalex_id": "A5072923710"
        },
        {
          "name": "Maria Madriaga",
          "openalex_id": "A5069780423"
        },
        {
          "name": "Rimel Aggabao",
          "openalex_id": "A5071379154"
        },
        {
          "name": "Giezel Diaz-Candido",
          "openalex_id": "A5079156966"
        },
        {
          "name": "James Maningo",
          "openalex_id": "A5001189652"
        },
        {
          "name": "Victor Tseng",
          "openalex_id": "A5015132009",
          "orcid": "https://orcid.org/0000-0003-0211-512X"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-02-09",
      "abstract": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.",
      "cited_by_count": 3161,
      "type": "article",
      "source": {
        "name": "PLOS Digital Health",
        "type": "journal",
        "issn": [
          "2767-3170"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000198&type=printable"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 25,
      "url": "https://openalex.org/W4319662928"
    },
    {
      "openalex_id": "W4385682136",
      "doi": "10.48550/arxiv.2308.03688",
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": [
        {
          "name": "Xiao Liu",
          "openalex_id": "A5075936732",
          "orcid": "https://orcid.org/0000-0001-8400-5754"
        },
        {
          "name": "Hao Yu",
          "openalex_id": "A5055267677",
          "orcid": "https://orcid.org/0000-0002-7261-5448"
        },
        {
          "name": "Hanchen Zhang",
          "openalex_id": "A5001724504",
          "orcid": "https://orcid.org/0000-0003-0498-429X"
        },
        {
          "name": "Yifan Xu",
          "openalex_id": "A5067206559",
          "orcid": "https://orcid.org/0000-0003-1591-0384"
        },
        {
          "name": "Xuanyu Lei",
          "openalex_id": "A5102586784"
        },
        {
          "name": "Hanyu Lai",
          "openalex_id": "A5108915645"
        },
        {
          "name": "Yu\u2010Cheng Gu",
          "openalex_id": "A5008346603",
          "orcid": "https://orcid.org/0000-0002-6400-6167"
        },
        {
          "name": "Hangliang Ding",
          "openalex_id": "A5076572469",
          "orcid": "https://orcid.org/0000-0001-9479-5040"
        },
        {
          "name": "Kaiwen Men",
          "openalex_id": "A5114189369"
        },
        {
          "name": "Kejuan Yang",
          "openalex_id": "A5111021767"
        },
        {
          "name": "Shudan Zhang",
          "openalex_id": "A5113258512"
        },
        {
          "name": "Xiang Deng",
          "openalex_id": "A5111694676"
        },
        {
          "name": "Aohan Zeng",
          "openalex_id": "A5005277155"
        },
        {
          "name": "Zhengxiao Du",
          "openalex_id": "A5018336656",
          "orcid": "https://orcid.org/0000-0002-8223-4147"
        },
        {
          "name": "Chenhui Zhang",
          "openalex_id": "A5082488018",
          "orcid": "https://orcid.org/0000-0002-0124-6315"
        },
        {
          "name": "Sheng Shen",
          "openalex_id": "A5100784818",
          "orcid": "https://orcid.org/0000-0003-4734-1008"
        },
        {
          "name": "Tianjun Zhang",
          "openalex_id": "A5101571112",
          "orcid": "https://orcid.org/0000-0001-6137-2340"
        },
        {
          "name": "Yu Su",
          "openalex_id": "A5111043220"
        },
        {
          "name": "Huan Sun",
          "openalex_id": "A5100824702"
        },
        {
          "name": "Minlie Huang",
          "openalex_id": "A5044042138",
          "orcid": "https://orcid.org/0000-0001-7111-1849"
        },
        {
          "name": "Yuxiao Dong",
          "openalex_id": "A5052284218",
          "orcid": "https://orcid.org/0000-0002-6092-2002"
        },
        {
          "name": "Jie Tang",
          "openalex_id": "A5044791875",
          "orcid": "https://orcid.org/0000-0003-3487-4593"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-08-07",
      "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \\textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \\num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.",
      "cited_by_count": 39,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2308.03688"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4385682136"
    },
    {
      "openalex_id": "W4384071683",
      "doi": "10.1038/s41586-023-06291-2",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        {
          "name": "Karan Singhal",
          "openalex_id": "A5027454515",
          "orcid": "https://orcid.org/0000-0001-9002-7490",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Shekoofeh Azizi",
          "openalex_id": "A5047463591",
          "orcid": "https://orcid.org/0000-0002-7447-6031",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Tao Tu",
          "openalex_id": "A5059213795",
          "orcid": "https://orcid.org/0000-0003-3420-7889",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "S. Sara Mahdavi",
          "openalex_id": "A5063201022",
          "orcid": "https://orcid.org/0000-0001-6823-598X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jason Lee",
          "openalex_id": "A5100657725",
          "orcid": "https://orcid.org/0000-0003-4042-795X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Hyung Won Chung",
          "openalex_id": "A5051828575",
          "orcid": "https://orcid.org/0000-0002-1280-9953",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathan Scales",
          "openalex_id": "A5030765685",
          "orcid": "https://orcid.org/0000-0002-9535-7138",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Ajay Kumar Tanwani",
          "openalex_id": "A5088063475",
          "orcid": "https://orcid.org/0000-0002-6365-8315",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Heather Cole-Lewis",
          "openalex_id": "A5069557194",
          "orcid": "https://orcid.org/0000-0002-7275-1810",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Stephen Pfohl",
          "openalex_id": "A5021812637",
          "orcid": "https://orcid.org/0000-0003-0551-9664",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Perry W. Payne",
          "openalex_id": "A5014637990",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Martin Seneviratne",
          "openalex_id": "A5058677067",
          "orcid": "https://orcid.org/0000-0003-0435-3738",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Paul Gamble",
          "openalex_id": "A5090718376",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Kelly",
          "openalex_id": "A5026540467",
          "orcid": "https://orcid.org/0000-0002-1246-844X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Abubakr Babiker",
          "openalex_id": "A5066029226",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathanael Sch\u00e4rli",
          "openalex_id": "A5007588003",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Aakanksha Chowdhery",
          "openalex_id": "A5055969617",
          "orcid": "https://orcid.org/0000-0002-0628-5225",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "P. Mansfield",
          "openalex_id": "A5086361722",
          "orcid": "https://orcid.org/0000-0003-4969-0543",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dina Demner\u2010Fushman",
          "openalex_id": "A5046764593",
          "institutions": [
            "United States National Library of Medicine"
          ]
        },
        {
          "name": "Blaise Ag\u00fcera y Arcas",
          "openalex_id": "A5044698998",
          "orcid": "https://orcid.org/0000-0003-2256-9823",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dale R. Webster",
          "openalex_id": "A5060000122",
          "orcid": "https://orcid.org/0000-0002-3023-8824",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Greg S. Corrado",
          "openalex_id": "A5068955381",
          "orcid": "https://orcid.org/0000-0001-8817-0992",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Yossi Matias",
          "openalex_id": "A5065128060",
          "orcid": "https://orcid.org/0000-0003-3960-6002",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Katherine Chou",
          "openalex_id": "A5070366042",
          "orcid": "https://orcid.org/0000-0002-0318-7857",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Juraj Gottweis",
          "openalex_id": "A5057932939",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nenad Toma\u0161ev",
          "openalex_id": "A5057195145",
          "orcid": "https://orcid.org/0000-0003-1624-0220",
          "institutions": [
            "DeepMind (United Kingdom)"
          ]
        },
        {
          "name": "Yun Liu",
          "openalex_id": "A5078784976",
          "orcid": "https://orcid.org/0000-0003-4079-8275",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alvin Rajkomar",
          "openalex_id": "A5022388476",
          "orcid": "https://orcid.org/0000-0001-5750-5016",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jo\u00eblle Barral",
          "openalex_id": "A5043862316",
          "orcid": "https://orcid.org/0009-0009-0432-5148",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Semturs",
          "openalex_id": "A5010171106",
          "orcid": "https://orcid.org/0000-0001-6108-2773",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alan Karthikesalingam",
          "openalex_id": "A5003509342",
          "orcid": "https://orcid.org/0000-0001-5074-898X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Vivek Natarajan",
          "openalex_id": "A5103234563",
          "orcid": "https://orcid.org/0000-0001-7849-2074",
          "institutions": [
            "Google (United States)"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-12",
      "abstract": null,
      "cited_by_count": 2399,
      "type": "article",
      "source": {
        "name": "Nature",
        "type": "journal",
        "issn": [
          "0028-0836",
          "1476-4687"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s41586-023-06291-2.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Artificial Intelligence in Healthcare and Education",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 91,
      "url": "https://openalex.org/W4384071683"
    },
    {
      "openalex_id": "W4385570291",
      "doi": "10.18653/v1/2023.acl-long.361",
      "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language",
      "authors": [
        {
          "name": "Mehran Kazemi",
          "openalex_id": "A5061716911",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Najoung Kim",
          "openalex_id": "A5071542832",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Deepti Bhatia",
          "openalex_id": "A5075099055",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Xin Xu",
          "openalex_id": "A5100319817",
          "orcid": "https://orcid.org/0009-0002-2319-1682",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Deepak Ramachandran",
          "openalex_id": "A5111238931",
          "orcid": "https://orcid.org/0000-0001-5412-6133",
          "institutions": [
            "Google (United States)"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.",
      "cited_by_count": 27,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2023.acl-long.361.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Semantic Web and Ontologies"
      ],
      "referenced_works_count": 42,
      "url": "https://openalex.org/W4385570291"
    },
    {
      "openalex_id": "W4381797997",
      "doi": "10.48550/arxiv.2306.13063",
      "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
      "authors": [
        {
          "name": "Miao Xiong",
          "openalex_id": "A5101122252"
        },
        {
          "name": "Zhiyuan Hu",
          "openalex_id": "A5010390926",
          "orcid": "https://orcid.org/0000-0002-1688-6032"
        },
        {
          "name": "Xinyang Lu",
          "openalex_id": "A5024457748"
        },
        {
          "name": "Yifei Li",
          "openalex_id": "A5100355213",
          "orcid": "https://orcid.org/0000-0001-6238-8176"
        },
        {
          "name": "Jie Fu",
          "openalex_id": "A5100666923",
          "orcid": "https://orcid.org/0000-0001-5864-3488"
        },
        {
          "name": "Junxian He",
          "openalex_id": "A5015879697",
          "orcid": "https://orcid.org/0009-0007-9559-6941"
        },
        {
          "name": "Bryan Hooi",
          "openalex_id": "A5065675832",
          "orcid": "https://orcid.org/0000-0002-5645-1754"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-22",
      "abstract": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.",
      "cited_by_count": 45,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2306.13063"
      },
      "topics": [
        "Artificial Intelligence in Law",
        "Financial Distress and Bankruptcy Prediction"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4381797997"
    },
    {
      "openalex_id": "W4382603221",
      "doi": "10.48550/arxiv.2306.15724",
      "title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
      "authors": [
        {
          "name": "Zeyi Liu",
          "openalex_id": "A5101772264",
          "orcid": "https://orcid.org/0009-0008-2763-3023"
        },
        {
          "name": "Arpit Bahety",
          "openalex_id": "A5051511277"
        },
        {
          "name": "Shuran Song",
          "openalex_id": "A5004644695",
          "orcid": "https://orcid.org/0000-0002-8768-7356"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-27",
      "abstract": "The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.",
      "cited_by_count": 18,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2306.15724"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Biomedical Text Mining and Ontologies"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4382603221"
    },
    {
      "openalex_id": "W4389519108",
      "doi": "10.18653/v1/2023.emnlp-main.468",
      "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
      "authors": [
        {
          "name": "Daman Arora",
          "openalex_id": "A5104331002",
          "institutions": [
            "University of California, Berkeley",
            "Indian Institute of Technology Delhi",
            "Berkeley College",
            "Microsoft Research (United Kingdom)"
          ]
        },
        {
          "name": "Himanshu Singh",
          "openalex_id": "A5033422078",
          "orcid": "https://orcid.org/0000-0002-0410-0602",
          "institutions": [
            "University of California, Berkeley",
            "Berkeley College",
            "Indian Institute of Technology Delhi",
            "Microsoft Research (United Kingdom)"
          ]
        },
        {
          "name": "Mausam Mausam",
          "openalex_id": "A5042262991",
          "orcid": "https://orcid.org/0000-0003-4088-4296"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
      "cited_by_count": 30,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2023.emnlp-main.468.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Software Engineering Research"
      ],
      "referenced_works_count": 20,
      "url": "https://openalex.org/W4389519108"
    },
    {
      "openalex_id": "W4389518758",
      "doi": "10.18653/v1/2023.emnlp-main.313",
      "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
      "authors": [
        {
          "name": "Theo Olausson",
          "openalex_id": "A5035758082",
          "orcid": "https://orcid.org/0000-0001-6653-2227"
        },
        {
          "name": "Alex Gu",
          "openalex_id": "A5007842855",
          "orcid": "https://orcid.org/0000-0002-4814-0796"
        },
        {
          "name": "Benjamin Lipkin",
          "openalex_id": "A5011209038",
          "orcid": "https://orcid.org/0000-0001-7465-5315"
        },
        {
          "name": "Cedegao Zhang",
          "openalex_id": "A5057683300"
        },
        {
          "name": "Armando Solar-Lezama",
          "openalex_id": "A5010786661",
          "orcid": "https://orcid.org/0000-0001-7604-8252"
        },
        {
          "name": "Joshua B. Tenenbaum",
          "openalex_id": "A5071093940",
          "orcid": "https://orcid.org/0000-0002-1925-2035"
        },
        {
          "name": "Roger L\u00e9vy",
          "openalex_id": "A5090215557",
          "orcid": "https://orcid.org/0000-0002-4493-8864"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.",
      "cited_by_count": 23,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2023.emnlp-main.313.pdf"
      },
      "topics": [
        "Ferroelectric and Negative Capacitance Devices",
        "Advanced Memory and Neural Computing",
        "Computability, Logic, AI Algorithms"
      ],
      "referenced_works_count": 59,
      "url": "https://openalex.org/W4389518758"
    },
    {
      "openalex_id": "W4327946446",
      "doi": "10.3390/healthcare11060887",
      "title": "ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns",
      "authors": [
        {
          "name": "Malik Sallam",
          "openalex_id": "A5086563544",
          "orcid": "https://orcid.org/0000-0002-0165-9670",
          "institutions": [
            "University of Jordan",
            "Jordan Hospital"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-03-19",
      "abstract": "ChatGPT is an artificial intelligence (AI)-based conversational large language model (LLM). The potential applications of LLMs in health care education, research, and practice could be promising if the associated valid concerns are proactively examined and addressed. The current systematic review aimed to investigate the utility of ChatGPT in health care education, research, and practice and to highlight its potential limitations. Using the PRIMSA guidelines, a systematic search was conducted to retrieve English records in PubMed/MEDLINE and Google Scholar (published research or preprints) that examined ChatGPT in the context of health care education, research, or practice. A total of 60 records were eligible for inclusion. Benefits of ChatGPT were cited in 51/60 (85.0%) records and included: (1) improved scientific writing and enhancing research equity and versatility; (2) utility in health care research (efficient analysis of datasets, code generation, literature reviews, saving time to focus on experimental design, and drug discovery and development); (3) benefits in health care practice (streamlining the workflow, cost saving, documentation, personalized medicine, and improved health literacy); and (4) benefits in health care education including improved personalized learning and the focus on critical thinking and problem-based learning. Concerns regarding ChatGPT use were stated in 58/60 (96.7%) records including ethical, copyright, transparency, and legal issues, the risk of bias, plagiarism, lack of originality, inaccurate content with risk of hallucination, limited knowledge, incorrect citations, cybersecurity issues, and risk of infodemics. The promising applications of ChatGPT can induce paradigm shifts in health care education, research, and practice. However, the embrace of this AI chatbot should be conducted with extreme caution considering its potential limitations. As it currently stands, ChatGPT does not qualify to be listed as an author in scientific articles unless the ICMJE/COPE guidelines are revised or amended. An initiative involving all stakeholders in health care education, research, and practice is urgently needed. This will help to set a code of ethics to guide the responsible use of ChatGPT among other LLMs in health care and academia.",
      "cited_by_count": 2445,
      "type": "review",
      "source": {
        "name": "Healthcare",
        "type": "journal",
        "issn": [
          "2227-9032"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2227-9032/11/6/887/pdf?version=1679359384"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "COVID-19 diagnosis using AI"
      ],
      "referenced_works_count": 114,
      "url": "https://openalex.org/W4327946446"
    },
    {
      "openalex_id": "W4319323461",
      "doi": "10.48550/arxiv.2302.01560",
      "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
      "authors": [
        {
          "name": "Zihao Wang",
          "openalex_id": "A5100413085",
          "orcid": "https://orcid.org/0000-0002-3919-0396"
        },
        {
          "name": "Shaofei Cai",
          "openalex_id": "A5074233262",
          "orcid": "https://orcid.org/0000-0002-1195-7276"
        },
        {
          "name": "Anji Liu",
          "openalex_id": "A5083180587"
        },
        {
          "name": "Xiaojian Ma",
          "openalex_id": "A5100689683",
          "orcid": "https://orcid.org/0000-0002-2611-6568"
        },
        {
          "name": "Yitao Liang",
          "openalex_id": "A5079752482"
        },
        {
          "name": "Liang, Yitao",
          "openalex_id": ""
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-02-03",
      "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose \"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
      "cited_by_count": 51,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2302.01560"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Multimodal Machine Learning Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4319323461"
    },
    {
      "openalex_id": "W4391709329",
      "doi": "10.48550/arxiv.2402.05200",
      "title": "Are LLMs Ready for Real-World Materials Discovery?",
      "authors": [
        {
          "name": "Santiago Miret",
          "openalex_id": "A5013678286",
          "orcid": "https://orcid.org/0000-0002-5121-3853"
        },
        {
          "name": "N. M. Anoop Krishnan",
          "openalex_id": "A5065129881",
          "orcid": "https://orcid.org/0000-0003-1500-4947"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-02-07",
      "abstract": "Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction challenges which need to be overcome in order to build large-scale, multi-modal datasets that capture valuable materials science knowledge. Finally, we outline a roadmap for applying future MatSci-LLMs for real-world materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials Laboratories.",
      "cited_by_count": 19,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2402.05200"
      },
      "topics": [
        "Machine Learning in Materials Science",
        "Mineral Processing and Grinding"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4391709329"
    },
    {
      "openalex_id": "W4378942217",
      "doi": "10.48550/arxiv.2305.18323",
      "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
      "authors": [
        {
          "name": "Binfeng Xu",
          "openalex_id": "A5102518304"
        },
        {
          "name": "Zhiyuan Peng",
          "openalex_id": "A5075223216",
          "orcid": "https://orcid.org/0000-0002-9870-4422"
        },
        {
          "name": "Bowen Lei",
          "openalex_id": "A5087003069",
          "orcid": "https://orcid.org/0000-0003-2882-9753"
        },
        {
          "name": "Subhabrata Mukherjee",
          "openalex_id": "A5103853262"
        },
        {
          "name": "Yuchen Liu",
          "openalex_id": "A5100373052",
          "orcid": "https://orcid.org/0000-0002-5376-0017"
        },
        {
          "name": "Dongkuan Xu",
          "openalex_id": "A5068433690",
          "orcid": "https://orcid.org/0000-0002-1456-9658"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-23",
      "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.",
      "cited_by_count": 14,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.18323"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4378942217"
    },
    {
      "openalex_id": "W4408184619",
      "doi": "10.1016/j.jii.2025.100807",
      "title": "Knowledge graph enhanced retrieval-augmented generation for failure mode and effects analysis",
      "authors": [
        {
          "name": "Lukas Bahr",
          "openalex_id": "A5099611070"
        },
        {
          "name": "Christoph Wehner",
          "openalex_id": "A5001408301",
          "orcid": "https://orcid.org/0000-0003-0421-4113"
        },
        {
          "name": "Judith Wewerka",
          "openalex_id": "A5017870049",
          "orcid": "https://orcid.org/0000-0002-4809-2480"
        },
        {
          "name": "Jos\u00e9 Bittencourt",
          "openalex_id": "A5099611071"
        },
        {
          "name": "Ute Schmid",
          "openalex_id": "A5027156356",
          "orcid": "https://orcid.org/0000-0002-1301-0326"
        },
        {
          "name": "R\u00fcdiger Daub",
          "openalex_id": "A5024110966",
          "orcid": "https://orcid.org/0000-0002-5120-4228"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-03-06",
      "abstract": "Failure mode and effects analysis (FMEA) is an essential tool for mitigating potential failures, particularly during the ramp-up phases of new products. However, its effectiveness is often limited by the reasoning capabilities of the FMEA tools, which are usually tabular structured. Meanwhile, large language models (LLMs) offer novel prospects for advanced natural language processing tasks. However, LLMs face challenges in tasks that require factual knowledge, a gap that retrieval-augmented generation (RAG) approaches aim to fill. RAG retrieves information from a non-parametric data store and uses a language model to generate responses. Building on this concept, we propose to enhance the non-parametric data store with a knowledge graph (KG). By integrating a KG into the RAG framework, we aim to leverage analytical and semantic question-answering capabilities for FMEA data. This paper contributes by presenting set-theoretic standardization and a schema for FMEA data, an algorithm for creating vector embeddings from the FMEA-KG, and a KG-enhanced RAG framework. Our approach is validated through a user experience design study, and we measure the precision and performance of the context retrieval recall.",
      "cited_by_count": 21,
      "type": "article",
      "source": {
        "name": "Journal of Industrial Information Integration",
        "type": "journal",
        "issn": [
          "2452-414X",
          "2467-964X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1016/j.jii.2025.100807"
      },
      "topics": [
        "Advanced Graph Neural Networks",
        "Rough Sets and Fuzzy Logic",
        "Semantic Web and Ontologies"
      ],
      "referenced_works_count": 62,
      "url": "https://openalex.org/W4408184619"
    },
    {
      "openalex_id": "W4412505619",
      "doi": "10.70777/si.v2i3.15161",
      "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges",
      "authors": [
        {
          "name": "Ranjan Sapkota",
          "openalex_id": "A5025722346",
          "orcid": "https://orcid.org/0000-0002-5417-6744",
          "institutions": [
            "Cornell University"
          ]
        },
        {
          "name": "Konstantinos I. Roumeliotis",
          "openalex_id": "A5077820227",
          "orcid": "https://orcid.org/0000-0002-8098-1616",
          "institutions": [
            "University of Peloponnese"
          ]
        },
        {
          "name": "Manoj Karkee",
          "openalex_id": "A5013737840",
          "orcid": "https://orcid.org/0000-0001-5337-4848",
          "institutions": [
            "Cornell University"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-07-20",
      "abstract": "This review critically distinguishes between AI Agents and Agentic AI, offering a structured, conceptual taxonomy, application mapping, and analysis of opportunities and challenges to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven and enabled by LLMs and LIMs for taskspecific automation. Generative AI is positioned as a precursor providing the foundation, with AI agents advancing through tool integration, prompt engineering, and reasoning enhancements. We then characterize Agentic AI systems, which, in contrast to AI Agents, represent a paradigm shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and coordinated autonomy. Through a chronological evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both AI agents and agentic AI paradigms. Application domains enabled by AI Agents such as customer support, scheduling, and data summarization are then contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure, and propose targeted solutions such as ReAct loops, retrieval-augmented generation (RAG), automation coordination layers, and causal modeling. This work aims to provide a roadmap for developing robust, scalable, and explainable AI-driven systems.",
      "cited_by_count": 44,
      "type": "article",
      "source": {
        "name": "SuperIntelligence - Robotics - Safety & Alignment",
        "type": "journal",
        "issn": [
          "3067-2627"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://s-rsa.com/index.php/agi/article/download/15161/11079"
      },
      "topics": [
        "Multi-Agent Systems and Negotiation"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4412505619"
    },
    {
      "openalex_id": "W4411272807",
      "doi": "10.1073/pnas.2501660122",
      "title": "Take caution in using LLMs as human surrogates",
      "authors": [
        {
          "name": "Yuan Gao",
          "openalex_id": "A5114619452",
          "institutions": [
            "Quest University Canada"
          ]
        },
        {
          "name": "Dokyun Lee",
          "openalex_id": "A5088913516",
          "orcid": "https://orcid.org/0000-0002-3186-3349",
          "institutions": [
            "Quest University Canada"
          ]
        },
        {
          "name": "Gordon Burtch",
          "openalex_id": "A5057319034",
          "orcid": "https://orcid.org/0000-0001-9798-1113",
          "institutions": [
            "Quest University Canada"
          ]
        },
        {
          "name": "Sina Fazelpour",
          "openalex_id": "A5118546792",
          "institutions": [
            "Universidad del Noreste"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-06-13",
      "abstract": "Recent studies suggest large language models (LLMs) can generate human-like responses, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LLMs can be used as surrogates or simulations for humans in social science research. However, LLMs differ fundamentally from humans, relying on probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth of LLMs using the 11-20 money request game. Nearly all advanced approaches fail to replicate human behavior distributions across many models. The causes of failure are diverse and unpredictable, relating to input language, roles, safeguarding, and more. These results warrant caution in using LLMs as surrogates or for simulating human behavior in research.",
      "cited_by_count": 14,
      "type": "article",
      "source": {
        "name": "Proceedings of the National Academy of Sciences",
        "type": "journal",
        "issn": [
          "0027-8424",
          "1091-6490"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1073/pnas.2501660122"
      },
      "topics": [
        "Biomedical Text Mining and Ontologies",
        "Explainable Artificial Intelligence (XAI)",
        "Topic Modeling"
      ],
      "referenced_works_count": 46,
      "url": "https://openalex.org/W4411272807"
    },
    {
      "openalex_id": "W4401863510",
      "doi": "10.1145/3637528.3671650",
      "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
      "authors": [
        {
          "name": "Mingzhe Xing",
          "openalex_id": "A5060626806",
          "orcid": "https://orcid.org/0000-0002-2065-9852",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Rongkai Zhang",
          "openalex_id": "A5061105350",
          "orcid": "https://orcid.org/0009-0003-1652-7202",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Hui Xue",
          "openalex_id": "A5050108117",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Qi Chen",
          "openalex_id": "A5070766481",
          "orcid": "https://orcid.org/0009-0006-7394-0185",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Fan Yang",
          "openalex_id": "A5100346602",
          "orcid": "https://orcid.org/0000-0002-0378-060X",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Zhen Xiao",
          "openalex_id": "A5102979232",
          "orcid": "https://orcid.org/0000-0002-6784-9709",
          "institutions": [
            "Peking University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-08-24",
      "abstract": "Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e. understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.",
      "cited_by_count": 10,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671650"
      },
      "topics": [
        "Topic Modeling",
        "Software Engineering Research",
        "Software System Performance and Reliability"
      ],
      "referenced_works_count": 7,
      "url": "https://openalex.org/W4401863510"
    },
    {
      "openalex_id": "W4394769383",
      "doi": "10.1145/3597503.3639157",
      "title": "Enhancing Exploratory Testing by Large Language Model and Knowledge Graph",
      "authors": [
        {
          "name": "Yanqi Su",
          "openalex_id": "A5086023162",
          "orcid": "https://orcid.org/0000-0002-2410-3229",
          "institutions": [
            "Australian National University"
          ]
        },
        {
          "name": "Dianshu Liao",
          "openalex_id": "A5003595989",
          "orcid": "https://orcid.org/0009-0000-0865-0444",
          "institutions": [
            "Jiangxi Normal University"
          ]
        },
        {
          "name": "Zhenchang Xing",
          "openalex_id": "A5028641941",
          "orcid": "https://orcid.org/0000-0001-7663-1421",
          "institutions": [
            "Commonwealth Scientific and Industrial Research Organisation",
            "Data61",
            "Australian National University"
          ]
        },
        {
          "name": "Qing Huang",
          "openalex_id": "A5087875790",
          "orcid": "https://orcid.org/0000-0002-8877-4267",
          "institutions": [
            "Jiangxi Normal University"
          ]
        },
        {
          "name": "Mulong Xie",
          "openalex_id": "A5040915887",
          "orcid": "https://orcid.org/0000-0002-0481-2167",
          "institutions": [
            "Commonwealth Scientific and Industrial Research Organisation",
            "Data61"
          ]
        },
        {
          "name": "Qinghua Lu",
          "openalex_id": "A5100652461",
          "orcid": "https://orcid.org/0000-0002-9466-1672",
          "institutions": [
            "Commonwealth Scientific and Industrial Research Organisation",
            "Data61"
          ]
        },
        {
          "name": "Xiwei Xu",
          "openalex_id": "A5006841485",
          "orcid": "https://orcid.org/0000-0002-2273-1862",
          "institutions": [
            "Data61",
            "Commonwealth Scientific and Industrial Research Organisation"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-04-12",
      "abstract": "Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.",
      "cited_by_count": 15,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Software Engineering Research",
        "Software Testing and Debugging Techniques",
        "Software Reliability and Analysis Research"
      ],
      "referenced_works_count": 29,
      "url": "https://openalex.org/W4394769383"
    },
    {
      "openalex_id": "W4393065402",
      "doi": "10.1007/s11704-024-40231-1",
      "title": "A survey on large language model based autonomous agents",
      "authors": [
        {
          "name": "Lei Wang",
          "openalex_id": "A5101394605",
          "orcid": "https://orcid.org/0009-0001-6108-9235",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Chen Ma",
          "openalex_id": "A5100410908",
          "orcid": "https://orcid.org/0000-0001-7933-9813",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Xueyang Feng",
          "openalex_id": "A5101929118",
          "orcid": "https://orcid.org/0000-0003-4426-5732",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Zeyu Zhang",
          "openalex_id": "A5100358735",
          "orcid": "https://orcid.org/0009-0006-2834-3007",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Hao Yang",
          "openalex_id": "A5101551028",
          "orcid": "https://orcid.org/0009-0008-5365-897X",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Jingsen Zhang",
          "openalex_id": "A5101658564",
          "orcid": "https://orcid.org/0000-0003-2997-3386",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Zhiyuan Chen",
          "openalex_id": "A5070590692",
          "orcid": "https://orcid.org/0000-0001-9897-8848",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Jiakai Tang",
          "openalex_id": "A5043446092",
          "orcid": "https://orcid.org/0000-0001-9543-8889",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Xu Chen",
          "openalex_id": "A5100385692",
          "orcid": "https://orcid.org/0000-0001-9943-6020",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Yankai Lin",
          "openalex_id": "A5043098453",
          "orcid": "https://orcid.org/0000-0002-0151-6178",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Wayne Xin Zhao",
          "openalex_id": "A5037145565",
          "orcid": "https://orcid.org/0000-0002-8333-6196",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Zhewei Wei",
          "openalex_id": "A5074858555",
          "orcid": "https://orcid.org/0000-0003-3620-5086",
          "institutions": [
            "Renmin University of China"
          ]
        },
        {
          "name": "Ji-Rong Wen",
          "openalex_id": "A5025631695",
          "orcid": "https://orcid.org/0000-0002-9777-9676",
          "institutions": [
            "Renmin University of China"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-22",
      "abstract": "Abstract Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.",
      "cited_by_count": 721,
      "type": "article",
      "source": {
        "name": "Frontiers of Computer Science",
        "type": "journal",
        "issn": [
          "2095-2228",
          "2095-2236"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s11704-024-40231-1.pdf"
      },
      "topics": [
        "Reinforcement Learning in Robotics",
        "Topic Modeling",
        "Multimodal Machine Learning Applications"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W4393065402"
    },
    {
      "openalex_id": "W4407308237",
      "doi": "10.1038/s41598-025-22940-0",
      "title": "Limitations of large language models in clinical problem-solving arising from inflexible reasoning",
      "authors": [
        {
          "name": "Jonathan Kim",
          "openalex_id": "A5101455605",
          "orcid": "https://orcid.org/0000-0001-5817-348X",
          "institutions": [
            "Palo Alto University",
            "Stanford University"
          ]
        },
        {
          "name": "Anna Podlasek",
          "openalex_id": "A5036597666",
          "orcid": "https://orcid.org/0000-0001-7297-7169",
          "institutions": [
            "University of Dundee"
          ]
        },
        {
          "name": "Kie Shidara",
          "openalex_id": "A5116210769",
          "institutions": [
            "University of California, San Francisco"
          ]
        },
        {
          "name": "Feng Liu",
          "openalex_id": "A5100610446",
          "orcid": "https://orcid.org/0000-0002-1074-2601",
          "institutions": [
            "Stevens Institute of Technology"
          ]
        },
        {
          "name": "Ahmed Alaa",
          "openalex_id": "A5027819729",
          "orcid": "https://orcid.org/0000-0003-4900-3635",
          "institutions": [
            "University of California, Berkeley"
          ]
        },
        {
          "name": "Danilo Bernardo",
          "openalex_id": "A5105924929",
          "orcid": "https://orcid.org/0000-0003-1077-2963"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-11-11",
      "abstract": null,
      "cited_by_count": 6,
      "type": "preprint",
      "source": {
        "name": "Scientific Reports",
        "type": "journal",
        "issn": [
          "2045-2322"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.nature.com/articles/s41598-025-22940-0.pdf"
      },
      "topics": [
        "Clinical Reasoning and Diagnostic Skills",
        "Education and Critical Thinking Development"
      ],
      "referenced_works_count": 40,
      "url": "https://openalex.org/W4407308237"
    },
    {
      "openalex_id": "W4392974098",
      "doi": "10.48550/arxiv.2403.11552",
      "title": "LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning",
      "authors": [
        {
          "name": "Shu Wang",
          "openalex_id": "A5070346269",
          "orcid": "https://orcid.org/0000-0002-7920-7025"
        },
        {
          "name": "Muzhi Han",
          "openalex_id": "A5070386263",
          "orcid": "https://orcid.org/0000-0002-2649-4577"
        },
        {
          "name": "Ziyuan Jiao",
          "openalex_id": "A5084328887",
          "orcid": "https://orcid.org/0000-0003-3404-3810"
        },
        {
          "name": "Zeyu Zhang",
          "openalex_id": "A5100358738",
          "orcid": "https://orcid.org/0000-0002-7583-1867"
        },
        {
          "name": "Ying Wu",
          "openalex_id": "A5101780958",
          "orcid": "https://orcid.org/0009-0001-6768-5118"
        },
        {
          "name": "Song\u2010Chun Zhu",
          "openalex_id": "A5034228010",
          "orcid": "https://orcid.org/0009-0008-0876-8117"
        },
        {
          "name": "Hangxin Liu",
          "openalex_id": "A5043423420",
          "orcid": "https://orcid.org/0000-0002-3003-8611"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-18",
      "abstract": "Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feedback through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain-specific messages between them. Through a series of simulations in a box-packing domain, we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP problems and the efficiency in selecting action parameters. Ablation studies underscore the significant contribution of motion failure reasoning to the success of LLM^3. Furthermore, we conduct qualitative experiments on a physical manipulator, demonstrating the practical applicability of our approach in real-world settings.",
      "cited_by_count": 7,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2403.11552"
      },
      "topics": [
        "Natural Language Processing Techniques",
        "Multimodal Machine Learning Applications",
        "Speech and dialogue systems"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4392974098"
    },
    {
      "openalex_id": "W4367859573",
      "doi": "10.48550/arxiv.2305.00633",
      "title": "Self-Evaluation Guided Beam Search for Reasoning",
      "authors": [
        {
          "name": "Yuxi Xie",
          "openalex_id": "A5101498987",
          "orcid": "https://orcid.org/0009-0009-0843-6507"
        },
        {
          "name": "Kenji Kawaguchi",
          "openalex_id": "A5003184366",
          "orcid": "https://orcid.org/0000-0002-5361-9793"
        },
        {
          "name": "Yiran Zhao",
          "openalex_id": "A5110759003",
          "orcid": "https://orcid.org/0009-0006-8514-192X"
        },
        {
          "name": "Xu Zhao",
          "openalex_id": "A5100440525",
          "orcid": "https://orcid.org/0000-0002-8176-623X"
        },
        {
          "name": "Min\u2010Yen Kan",
          "openalex_id": "A5066305082",
          "orcid": "https://orcid.org/0000-0001-8507-3716"
        },
        {
          "name": "Junxian He",
          "openalex_id": "A5015879697",
          "orcid": "https://orcid.org/0009-0007-9559-6941"
        },
        {
          "name": "Qizhe Xie",
          "openalex_id": "A5029288401"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-01",
      "abstract": "Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.",
      "cited_by_count": 10,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.00633"
      },
      "topics": [
        "Natural Language Processing Techniques",
        "Topic Modeling",
        "Multimodal Machine Learning Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4367859573"
    },
    {
      "openalex_id": "W4389520279",
      "doi": "10.18653/v1/2023.findings-emnlp.717",
      "title": "Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Yufan Wu",
          "openalex_id": "A5101208758",
          "institutions": [
            "University of Michigan\u2013Ann Arbor",
            "Westlake Health Center"
          ]
        },
        {
          "name": "Yinghui He",
          "openalex_id": "A5100839368",
          "institutions": [
            "Westlake Health Center",
            "University of Michigan\u2013Ann Arbor"
          ]
        },
        {
          "name": "Yilin Jia",
          "openalex_id": "A5100571209",
          "institutions": [
            "University of Michigan\u2013Ann Arbor",
            "Westlake Health Center"
          ]
        },
        {
          "name": "Rada Mihalcea",
          "openalex_id": "A5082450455",
          "orcid": "https://orcid.org/0000-0002-0767-6703",
          "institutions": [
            "Westlake Health Center",
            "University of Michigan\u2013Ann Arbor"
          ]
        },
        {
          "name": "Yulong Chen",
          "openalex_id": "A5100777438",
          "orcid": "https://orcid.org/0000-0002-4960-5241",
          "institutions": [
            "University of Michigan\u2013Ann Arbor",
            "Westlake Health Center"
          ]
        },
        {
          "name": "Naihao Deng",
          "openalex_id": "A5007774447",
          "institutions": [
            "University of Michigan\u2013Ann Arbor",
            "Westlake Health Center"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": "Theory of Mind (ToM) is the ability to reason about one\u2019s own and others\u2019 mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others\u2019 beliefs. %We also incorporate a new deception mechanism in ToM reasoning. We introduce Hi-ToM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.",
      "cited_by_count": 6,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2023.findings-emnlp.717.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Multimodal Machine Learning Applications"
      ],
      "referenced_works_count": 35,
      "url": "https://openalex.org/W4389520279"
    },
    {
      "openalex_id": "W4392181659",
      "doi": "10.48550/arxiv.2402.14848",
      "title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
      "authors": [
        {
          "name": "Mosh Levy",
          "openalex_id": "A5007803354"
        },
        {
          "name": "Alon Jacoby",
          "openalex_id": "A5094008567"
        },
        {
          "name": "Yoav Goldberg",
          "openalex_id": "A5028476919",
          "orcid": "https://orcid.org/0000-0002-6497-829X"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-02-19",
      "abstract": "This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.",
      "cited_by_count": 7,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2402.14848"
      },
      "topics": [
        "Natural Language Processing Techniques",
        "Topic Modeling",
        "Semantic Web and Ontologies"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4392181659"
    },
    {
      "openalex_id": "W4389991792",
      "doi": "10.1038/s41586-023-06792-0",
      "title": "Autonomous chemical research with large language models",
      "authors": [
        {
          "name": "Daniil A. Boiko",
          "openalex_id": "A5065327102",
          "orcid": "https://orcid.org/0000-0003-4140-4645",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Robert MacKnight",
          "openalex_id": "A5060793099",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Ben Kline",
          "openalex_id": "A5004653421"
        },
        {
          "name": "Gabriel dos Passos Gomes",
          "openalex_id": "A5048633127",
          "orcid": "https://orcid.org/0000-0002-8235-5969",
          "institutions": [
            "Carnegie Mellon University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-20",
      "abstract": null,
      "cited_by_count": 601,
      "type": "article",
      "source": {
        "name": "Nature",
        "type": "journal",
        "issn": [
          "0028-0836",
          "1476-4687"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s41586-023-06792-0.pdf"
      },
      "topics": [
        "Machine Learning in Materials Science",
        "Ferroelectric and Negative Capacitance Devices",
        "Innovative Microfluidic and Catalytic Techniques Innovation"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W4389991792"
    },
    {
      "openalex_id": "W4380715535",
      "doi": "10.48550/arxiv.2306.07929",
      "title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents",
      "authors": [
        {
          "name": "Danyang Zhang",
          "openalex_id": "A5100629051",
          "orcid": "https://orcid.org/0000-0003-3433-7157"
        },
        {
          "name": "Chen L\u00fc",
          "openalex_id": "A5100605099",
          "orcid": "https://orcid.org/0000-0003-1927-2391"
        },
        {
          "name": "Situo Zhang",
          "openalex_id": "A5108750212"
        },
        {
          "name": "Hongshen Xu",
          "openalex_id": "A5100574387"
        },
        {
          "name": "Zihan Zhao",
          "openalex_id": "A5048045743",
          "orcid": "https://orcid.org/0009-0000-6832-8107"
        },
        {
          "name": "Kai Yu",
          "openalex_id": "A5043098653",
          "orcid": "https://orcid.org/0000-0002-7102-9826"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-09",
      "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.",
      "cited_by_count": 10,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2306.07929"
      },
      "topics": [
        "Topic Modeling",
        "Domain Adaptation and Few-Shot Learning",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4380715535"
    },
    {
      "openalex_id": "W4392736029",
      "doi": "10.1145/3639477.3639745",
      "title": "Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach",
      "authors": [
        {
          "name": "Jinxi Kuang",
          "openalex_id": "A5098954772",
          "orcid": "https://orcid.org/0000-0002-1987-2533",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        },
        {
          "name": "Jinyang Liu",
          "openalex_id": "A5100705518",
          "orcid": "https://orcid.org/0000-0003-0037-1912",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        },
        {
          "name": "Junjie Huang",
          "openalex_id": "A5040653412",
          "orcid": "https://orcid.org/0009-0004-6962-5292",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        },
        {
          "name": "Renyi Zhong",
          "openalex_id": "A5098969559",
          "orcid": "https://orcid.org/0000-0001-6626-4437",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        },
        {
          "name": "Jiazhen Gu",
          "openalex_id": "A5042468232",
          "orcid": "https://orcid.org/0000-0002-5831-9474",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        },
        {
          "name": "L. Yu",
          "openalex_id": "A5037440313",
          "orcid": "https://orcid.org/0009-0000-5821-865X",
          "institutions": [
            "Cloud Computing Center"
          ]
        },
        {
          "name": "R.B. Tan",
          "openalex_id": "A5065645159",
          "orcid": "https://orcid.org/0009-0001-1531-0074",
          "institutions": [
            "Cloud Computing Center"
          ]
        },
        {
          "name": "Zengyin Yang",
          "openalex_id": "A5071672297",
          "orcid": "https://orcid.org/0000-0001-6307-7310",
          "institutions": [
            "Cloud Computing Center"
          ]
        },
        {
          "name": "Michael R. Lyu",
          "openalex_id": "A5069596903",
          "orcid": "https://orcid.org/0000-0002-3666-5798",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-04-14",
      "abstract": "Due to the scale and complexity of cloud systems, a system failure would trigger an \"alert storm\", i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts. To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining module effectively captures the temporal and spatial relations between alerts, measuring their correlations in an efficient manner. Subsequently, only uncertain pairs with low confidence are forwarded to the LLM reasoning module for detailed analysis. This hybrid design harnesses both statistical evidence for frequent alerts and the reasoning capabilities of computationally intensive LLMs, ensuring the overall efficiency of COLA in handling large volumes of alerts in practical scenarios. We evaluate COLA on three datasets collected from the production environment of a large-scale cloud platform. The experimental results show COLA achieves F1-scores from 0.901 to 0.930, outperforming state-of-the-art methods and achieving comparable efficiency. We also share our experience in deploying COLA in our real-world cloud system, Cloud X.",
      "cited_by_count": 9,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3639477.3639745"
      },
      "topics": [
        "Software System Performance and Reliability",
        "Cloud Computing and Resource Management",
        "Network Security and Intrusion Detection"
      ],
      "referenced_works_count": 21,
      "url": "https://openalex.org/W4392736029"
    }
  ],
  "count": 40,
  "errors": []
}
