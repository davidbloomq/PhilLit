{
  "status": "success",
  "source": "semantic_scholar",
  "query": "distributional shift language model safety",
  "results": [
    {
      "paperId": "d0a02239a3003015f4844a4b647505f86841e008",
      "title": "Distraction is All You Need for Multimodal Large Language Model Jailbreaking",
      "authors": [
        {
          "name": "Zuopeng Yang",
          "authorId": "2305033291"
        },
        {
          "name": "Jiluan Fan",
          "authorId": "2328827372"
        },
        {
          "name": "Anli Yan",
          "authorId": "2328404869"
        },
        {
          "name": "Erdun Gao",
          "authorId": "2257190977"
        },
        {
          "name": "Xin Lin",
          "authorId": "2115252929"
        },
        {
          "name": "Tao Li",
          "authorId": "2305382426"
        },
        {
          "name": "Kanghua Mo",
          "authorId": "2345819426"
        },
        {
          "name": "Changyu Dong",
          "authorId": "2289908109"
        }
      ],
      "year": 2025,
      "abstract": "Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce vulnerabilities, which may be exploited to bypass safety mechanisms. To address this, we analyze the relationship between image content and task and find that the complexity of subimages, rather than their content, is key. Building on this insight, we propose the Distraction Hypothesis, followed by a novel framework called Contrasting Subimage Distraction Jailbreaking (CS-DJ), to achieve jailbreaking by disrupting MLLMs alignment through multi-level distraction strategies. CS-DJ consists of two components: structured distraction, achieved through query decomposition that induces a distributional shift by fragmenting harmful prompts into sub-queries, and visual-enhanced distraction, realized by constructing contrasting subimages to disrupt the interactions among visual elements within the model. This dual strategy disperses the model\u2019s attention, reducing its ability to detect and mitigate harmful content. Extensive experiments across five representative scenarios and four popular closed-source MLLMs, including GPT-4o-mini, GPT-4o, GPT-4V, and Gemini-1.5-Flash, demonstrate that CS-DJ achieves average success rates of 52.40% for the attack success rate and 74.10% for the ensemble attack success rate. These results reveal the potential of distraction-based approaches to exploit and bypass MLLMs\u2019 defenses, offering new insights for attack strategies. Our code is available at https://github.com/TeamPigeonLab/CS-DJ.Warning: This paper contains unfiltered content generated by MLLMs that may be offensive to readers",
      "citationCount": 16,
      "doi": "10.1109/CVPR52734.2025.00884",
      "arxivId": "2502.10794",
      "url": "https://www.semanticscholar.org/paper/d0a02239a3003015f4844a4b647505f86841e008",
      "venue": "Computer Vision and Pattern Recognition",
      "journal": {
        "name": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "pages": "9467-9476"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "17f9fea1d9fd31d111a1eb8f6fc7328f046a3cee",
      "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
      "authors": [
        {
          "name": "Edmund Mills",
          "authorId": "2275158766"
        },
        {
          "name": "Shiye Su",
          "authorId": "2275412153"
        },
        {
          "name": "Stuart Russell",
          "authorId": "2237426863"
        },
        {
          "name": "Scott Emmons",
          "authorId": "2237427074"
        }
      ],
      "year": 2023,
      "abstract": "How do we measure the efficacy of language model explainability methods? While many explainability methods have been developed, they are typically evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help fill this gap, we present ALMANACS, a language model explainability benchmark. ALMANACS scores explainability methods on simulatability, i.e., how well the explanations improve behavior prediction on new inputs. The ALMANACS scenarios span twelve safety-relevant topics such as ethical reasoning and advanced AI behaviors; they have idiosyncratic premises to invoke model-specific behavior; and they have a train-test distributional shift to encourage faithful explanations. By using another language model to predict behavior based on the explanations, ALMANACS is a fully automated benchmark. While not a replacement for human evaluations, we aim for ALMANACS to be a complementary, automated tool that allows for fast, scalable evaluation. Using ALMANACS, we evaluate counterfactual, rationalization, attention, and Integrated Gradients explanations. Our results are sobering: when averaged across all topics, no explanation method outperforms the explanation-free control. We conclude that despite modest successes in prior work, developing an explanation method that aids simulatability in ALMANACS remains an open challenge.",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2312.12747",
      "arxivId": "2312.12747",
      "url": "https://www.semanticscholar.org/paper/17f9fea1d9fd31d111a1eb8f6fc7328f046a3cee",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2312.12747"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "11f0e05fb15a151d9a62418a52c87582c9402d43",
      "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
      "authors": [
        {
          "name": "Tanqiu Jiang",
          "authorId": "2328252315"
        },
        {
          "name": "Jiacheng Liang",
          "authorId": "2273583474"
        },
        {
          "name": "Rongyi Zhu",
          "authorId": "2343691700"
        },
        {
          "name": "Jiawei Zhou",
          "authorId": "2339423984"
        },
        {
          "name": "Fenglong Ma",
          "authorId": "2304717793"
        },
        {
          "name": "Ting Wang",
          "authorId": "2343363436"
        }
      ],
      "year": 2025,
      "abstract": "Large vision-language models (VLMs) are highly vulnerable to jailbreak attacks that exploit visual-textual interactions to bypass safety guardrails. In this paper, we present DTR, a novel inference-time defense that mitigates multimodal jailbreak attacks through optimizing the model's key-value (KV) caches. Rather than relying on curated safety-specific data or costly image-to-text conversion, we introduce a new formulation of the safety-relevant distributional shift induced by the visual modality. This formulation enables DTR to dynamically adjust visual token weights, minimizing the impact of adversarial visual inputs while preserving the model's general capabilities and inference efficiency. Extensive evaluation across diverse VLMs and attack benchmarks demonstrates that \\sys outperforms existing defenses in both attack robustness and benign task performance, marking the first successful application of KV cache optimization for safety enhancement in multimodal foundation models. (warning: this paper contains potentially harmful content generated by VLMs.)",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.17132",
      "arxivId": "2505.17132",
      "url": "https://www.semanticscholar.org/paper/11f0e05fb15a151d9a62418a52c87582c9402d43",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.17132"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a7c7c5bfd0442e0aa159992d87f2c8b829a734da",
      "title": "SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge",
      "authors": [
        {
          "name": "Adeel Yousaf",
          "authorId": "10747984"
        },
        {
          "name": "Joseph Fioresi",
          "authorId": "2345188655"
        },
        {
          "name": "James Beetham",
          "authorId": "69379632"
        },
        {
          "name": "A. S. Bedi",
          "authorId": "3387859"
        },
        {
          "name": "Mubarak Shah",
          "authorId": "2325749619"
        }
      ],
      "year": 2025,
      "abstract": "Improving the safety of vision-language models like CLIP via fine-tuning often comes at a steep price, causing significant drops in their generalization performance. We find this trade-off stems from rigid alignment strategies that force unsafe concepts toward single, predefined safe targets, disrupting the model's learned semantic structure. To address this, we propose a proximity-aware approach: redirecting unsafe concepts to their semantically closest safe alternatives to minimize representational change. We introduce SaFeR-CLIP, a fine-tuning framework that applies this principle of minimal intervention. SaFeR-CLIP successfully reconciles safety and performance, recovering up to 8.0% in zero-shot accuracy over prior methods while maintaining robust safety. To support more rigorous evaluation, we also contribute NSFW-Caps, a new benchmark of 1,000 highly-aligned pairs for testing safety under distributional shift. Our work shows that respecting the geometry of pretrained representations is key to achieving safety without sacrificing performance.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.16743",
      "arxivId": "2511.16743",
      "url": "https://www.semanticscholar.org/paper/a7c7c5bfd0442e0aa159992d87f2c8b829a734da",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.16743"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ccb70eb3381330c04f893471bb7b55b465a8bbbc",
      "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner",
      "authors": [
        {
          "name": "Bolian Li",
          "authorId": "2133146583"
        },
        {
          "name": "Yanran Wu",
          "authorId": "2376525112"
        },
        {
          "name": "Xinyu Luo",
          "authorId": "2366163227"
        },
        {
          "name": "Ruqi Zhang",
          "authorId": "2257078903"
        }
      ],
      "year": 2025,
      "abstract": "Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling acceleration, which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-shifted speculative sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2508.15044",
      "arxivId": "2508.15044",
      "url": "https://www.semanticscholar.org/paper/ccb70eb3381330c04f893471bb7b55b465a8bbbc",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.15044"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "dff7a3989c41bd0200095e6560f73923b0f3bf32",
      "title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment",
      "authors": [
        {
          "name": "Thom Lake",
          "authorId": "67055103"
        },
        {
          "name": "Eunsol Choi",
          "authorId": "2266842055"
        },
        {
          "name": "Greg Durrett",
          "authorId": "1814094"
        }
      ],
      "year": 2024,
      "abstract": "The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.",
      "citationCount": 27,
      "doi": "10.48550/arXiv.2406.17692",
      "arxivId": "2406.17692",
      "url": "https://www.semanticscholar.org/paper/dff7a3989c41bd0200095e6560f73923b0f3bf32",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.17692"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "12a9b981bb445835a3b8253531659d8f42fc88eb",
      "title": "Mitigating exposure bias in large language model distillation: an imitation learning approach",
      "authors": [
        {
          "name": "A. Pozzi",
          "authorId": "2226580092"
        },
        {
          "name": "A. Incremona",
          "authorId": "146092686"
        },
        {
          "name": "D. Tessera",
          "authorId": "2298656797"
        },
        {
          "name": "Daniele Toti",
          "authorId": "2451187"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 16,
      "doi": "10.1007/s00521-025-11162-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/12a9b981bb445835a3b8253531659d8f42fc88eb",
      "venue": "Neural computing & applications (Print)",
      "journal": {
        "name": "Neural Computing and Applications",
        "pages": "12013 - 12029",
        "volume": "37"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bedec42a1d49c52daf1b601f9eff0ec8755f7d22",
      "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models",
      "authors": [
        {
          "name": "Jinchuan Zhang",
          "authorId": "2144141948"
        },
        {
          "name": "Lu Yin",
          "authorId": "2365457139"
        },
        {
          "name": "Yan Zhou",
          "authorId": "2150920806"
        },
        {
          "name": "Songlin Hu",
          "authorId": "2364078619"
        }
      ],
      "year": 2025,
      "abstract": "The acquisition of agentic capabilities has transformed LLMs from\"knowledge providers\"to\"action executors\", a trend that while expanding LLMs' capability boundaries, significantly increases their susceptibility to malicious use. Previous work has shown that current LLM-based agents execute numerous malicious tasks even without being attacked, indicating a deficiency in agentic use safety alignment during the post-training phase. To address this gap, we propose AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis. By instantiating these behavior chains in simulated environments with diverse tool instances, our framework enables the generation of highly authentic and executable instructions while capturing complex multi-step dynamics. The framework further ensures model utility by proportionally synthesizing benign instructions through non-malicious interpretations of behavior chains, precisely calibrating the boundary between helpfulness and harmlessness. Evaluation results on AgentHarm demonstrate that fine-tuning three families of open-source models using our method substantially improves their safety (35.8% to 79.5% improvement) while minimally impacting or even positively enhancing their helpfulness, outperforming various prompting methods. The dataset and code have both been open-sourced.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2505.23020",
      "arxivId": "2505.23020",
      "url": "https://www.semanticscholar.org/paper/bedec42a1d49c52daf1b601f9eff0ec8755f7d22",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.23020"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "118a9b10647cbf4c68ae708dfea50b62011f2662",
      "title": "An Ensemble Framework for Unbiased Language Model Watermarking",
      "authors": [
        {
          "name": "Yihan Wu",
          "authorId": "2254326623"
        },
        {
          "name": "Ruibo Chen",
          "authorId": "2262968852"
        },
        {
          "name": "Georgios Milis",
          "authorId": "2367038055"
        },
        {
          "name": "Heng Huang",
          "authorId": "2382926439"
        }
      ],
      "year": 2025,
      "abstract": "As large language models become increasingly capable and widely deployed, verifying the provenance of machine-generated content is critical to ensuring trust, safety, and accountability. Watermarking techniques have emerged as a promising solution by embedding imperceptible statistical signals into the generation process. Among them, unbiased watermarking is particularly attractive due to its theoretical guarantee of preserving the language model's output distribution, thereby avoiding degradation in fluency or detectability through distributional shifts. However, existing unbiased watermarking schemes often suffer from weak detection power and limited robustness, especially under short text lengths or distributional perturbations. In this work, we propose ENS, a novel ensemble framework that enhances the detectability and robustness of logits-based unbiased watermarks while strictly preserving their unbiasedness. ENS sequentially composes multiple independent watermark instances, each governed by a distinct key, to amplify the watermark signal. We theoretically prove that the ensemble construction remains unbiased in expectation and demonstrate how it improves the signal-to-noise ratio for statistical detectors. Empirical evaluations on multiple LLM families show that ENS substantially reduces the number of tokens needed for reliable detection and increases resistance to smoothing and paraphrasing attacks without compromising generation quality.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2509.24043",
      "arxivId": "2509.24043",
      "url": "https://www.semanticscholar.org/paper/118a9b10647cbf4c68ae708dfea50b62011f2662",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.24043"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "71bae1ffe7691f9164d1d38ef0090782051f4d71",
      "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
      "authors": [
        {
          "name": "Zongxia Li",
          "authorId": "2375157272"
        },
        {
          "name": "Wenhao Yu",
          "authorId": "2265843326"
        },
        {
          "name": "Chengsong Huang",
          "authorId": "31937655"
        },
        {
          "name": "Rui Liu",
          "authorId": "2377939457"
        },
        {
          "name": "Zhenwen Liang",
          "authorId": "2362316761"
        },
        {
          "name": "Fuxiao Liu",
          "authorId": "52220309"
        },
        {
          "name": "Jingxi Che",
          "authorId": "2377559556"
        },
        {
          "name": "Dian Yu",
          "authorId": "2265968215"
        },
        {
          "name": "J. Boyd-Graber",
          "authorId": "2240779865"
        },
        {
          "name": "Haitao Mi",
          "authorId": "2238955130"
        },
        {
          "name": "Dong Yu",
          "authorId": "2303999735"
        }
      ],
      "year": 2025,
      "abstract": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
      "citationCount": 38,
      "doi": "10.48550/arXiv.2508.19652",
      "arxivId": "2508.19652",
      "url": "https://www.semanticscholar.org/paper/71bae1ffe7691f9164d1d38ef0090782051f4d71",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.19652"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "be34f5fd2c05266421ef234a3cd65572cda60efc",
      "title": "Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation From Unlabeled Data",
      "authors": [
        {
          "name": "David S. W. Williams",
          "authorId": "2152728183"
        },
        {
          "name": "Daniele De Martini",
          "authorId": "2261392392"
        },
        {
          "name": "Matthew Gadd",
          "authorId": "2346231"
        },
        {
          "name": "Paul Newman",
          "authorId": "2135739162"
        }
      ],
      "year": 2024,
      "abstract": "Knowing when a trained segmentation model is encountering data that is different to its training data is important. Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective\u2014this being a safety concern in applications such as autonomous vehicles. This article presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass. As annotation costs limit the diversity of labeled datasets, we use easy-to-obtain, uncurated and unlabeled data to learn to perform uncertainty estimation by selectively enforcing consistency over data augmentation. To this end, a novel segmentation benchmark based on the sense-assess-eXplain (SAX) is used, which includes labeled test data spanning three autonomous-driving domains, ranging in appearance from dense urban to off-road. The proposed method, named $\\mathrm{\\gamma }\\text{-}\\text{SSL}$, consistently outperforms uncertainty estimation and out-of-distribution techniques on this difficult benchmark\u2014by up to 10.7% in area under the receiver operating characteristic curve and 19.2% in area under the precision-recall curve in the most challenging of the three scenarios.",
      "citationCount": 6,
      "doi": "10.1109/TRO.2024.3401020",
      "arxivId": "2402.17653",
      "url": "https://www.semanticscholar.org/paper/be34f5fd2c05266421ef234a3cd65572cda60efc",
      "venue": "IEEE Transactions on robotics",
      "journal": {
        "name": "IEEE Transactions on Robotics",
        "pages": "3146-3165",
        "volume": "40"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f06ff02b943c8d4bc5d71fdc5ee7f781438a1a0a",
      "title": "Retrospective validation study of a large language model approach to screening for intraabdominal surgical site infections for quality and safety reporting",
      "authors": [
        {
          "name": "Logan Pierce",
          "authorId": "1630985813"
        },
        {
          "name": "Christy Pak",
          "authorId": "2159483823"
        },
        {
          "name": "Kim Stanley",
          "authorId": "2219996984"
        },
        {
          "name": "Lusha Wang",
          "authorId": "2304623078"
        },
        {
          "name": "Caroline Erickson",
          "authorId": "2299806842"
        },
        {
          "name": "Deborah S Yokoe",
          "authorId": "2399966543"
        },
        {
          "name": "Elizabeth C. Wick",
          "authorId": "2257417850"
        }
      ],
      "year": 2025,
      "abstract": "Surgical site infections (SSIs), particularly intra-abdominal (IAB) infections, are challenging to identify and remain a resource-intensive focus of infection prevention programmes. Current automated screening measures rely on discrete data from the electronic health record (EHR), such as microbiology results, diagnosis codes and/or return to the operating room. This approach has poor specificity, and therefore surveillance methods depend heavily on additional manual chart review by trained infection preventionists. Large language models (LLMs) offer an opportunity to improve surveillance by synthesising complex clinical documentation alongside structured data elements. We evaluated the performance of a locally hosted LLM (gpt-35-turbo-16k) to improve IAB SSI screening using perioperative clinical notes and microbiology results. The model analysed documentation across the perioperative period (3\u2009days before through 30 days after surgery) to generate case-level SSI summaries and likelihood assessments. We compared the performance of this tool against the current EHR-based screening workflow. Among 1977 abdominal surgical cases, including 56 with confirmed IAB SSIs, the LLM screened 104\u2009cases as high risk, identifying all infections (negative predictive value (NPV) 100%) and achieving a positive predictive value (PPV) of 53.8%. In contrast, the EHR-based workflow identified 288 cases for further review, with a PPV of only 19.4% and the same NPV of 100%. Analysis of 57\u2009224 notes required ~107\u2009million tokens, translating to approximately USD 0.05 per case. An LLM-based approach to SSI surveillance has the potential to substantially improve efficiency while remaining highly accurate and cost-effective. By reducing reliance on manual chart review, this strategy could allow infection preventionists to shift their attention from surveillance toward quality improvement and patient safety initiatives.",
      "citationCount": 0,
      "doi": "10.1136/bmjqs-2025-019493",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f06ff02b943c8d4bc5d71fdc5ee7f781438a1a0a",
      "venue": "BMJ Quality & Safety",
      "journal": {
        "name": "BMJ Quality & Safety"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "2e6813cad2e41c683277aa2d400dc2a2761309a2",
      "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
      "authors": [
        {
          "name": "Atsuyuki Miyai",
          "authorId": "2188735058"
        },
        {
          "name": "Jingkang Yang",
          "authorId": "2295601"
        },
        {
          "name": "Jingyang Zhang",
          "authorId": "2267788653"
        },
        {
          "name": "Yifei Ming",
          "authorId": "2143847067"
        },
        {
          "name": "Yueqian Lin",
          "authorId": "2223973348"
        },
        {
          "name": "Qing Yu",
          "authorId": "2087411522"
        },
        {
          "name": "Go Irie",
          "authorId": "2240003271"
        },
        {
          "name": "Shafiq R. Joty",
          "authorId": "2708940"
        },
        {
          "name": "Yixuan Li",
          "authorId": "2273756302"
        },
        {
          "name": "Hai Li",
          "authorId": "2294901591"
        },
        {
          "name": "Ziwei Liu",
          "authorId": "2279869111"
        },
        {
          "name": "T. Yamasaki",
          "authorId": "145572097"
        },
        {
          "name": "Kiyoharu Aizawa",
          "authorId": "2237794190"
        }
      ],
      "year": 2024,
      "abstract": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the safety of machine learning systems and has shaped the field of OOD detection. Meanwhile, several other problems are closely related to OOD detection, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). To unify these problems, a generalized OOD detection framework was proposed, taxonomically categorizing these five problems. However, Vision Language Models (VLMs) such as CLIP have significantly changed the paradigm and blurred the boundaries between these fields, again confusing researchers. In this survey, we first present a generalized OOD detection v2, encapsulating the evolution of these fields in the VLM era. Our framework reveals that, with some field inactivity and integration, the demanding challenges have become OOD detection and AD. Then, we highlight the significant shift in the definition, problem settings, and benchmarks; we thus feature a comprehensive review of the methodology for OOD detection and related tasks to clarify their relationship to OOD detection. Finally, we explore the advancements in the emerging Large Vision Language Model (LVLM) era, such as GPT-4V. We conclude with open challenges and future directions. The resource is available at https://github.com/AtsuMiyai/Awesome-OOD-VLM.",
      "citationCount": 29,
      "doi": "10.48550/arXiv.2407.21794",
      "arxivId": "2407.21794",
      "url": "https://www.semanticscholar.org/paper/2e6813cad2e41c683277aa2d400dc2a2761309a2",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "Trans. Mach. Learn. Res.",
        "volume": "2025"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "986752e0de257d7798e052285081bf8b124442e0",
      "title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
      "authors": [
        {
          "name": "Sakhinana Sagar Srinivas",
          "authorId": "2203079037"
        },
        {
          "name": "Venkataramana Runkana",
          "authorId": "2139833562"
        }
      ],
      "year": 2024,
      "abstract": "Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes. Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models. Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task. Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions. Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2408.11866",
      "arxivId": "2408.11866",
      "url": "https://www.semanticscholar.org/paper/986752e0de257d7798e052285081bf8b124442e0",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2408.11866"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d313d3981a7cc76c02a68fdca8a28be1b93f0235",
      "title": "A Language Model-Guided Framework for Mining Time Series with Distributional Shifts",
      "authors": [
        {
          "name": "Haibei Zhu",
          "authorId": "2265503576"
        },
        {
          "name": "Yousef El-Laham",
          "authorId": "2346113879"
        },
        {
          "name": "Elizabeth Fons",
          "authorId": "73771260"
        },
        {
          "name": "Svitlana Vyetrenko",
          "authorId": "2264978144"
        }
      ],
      "year": 2024,
      "abstract": "Effective utilization of time series data is often constrained by the scarcity of data quantity that reflects complex dynamics, especially under the condition of distributional shifts. Existing datasets may not encompass the full range of statistical properties required for robust and comprehensive analysis. And privacy concerns can further limit their accessibility in domains such as finance and healthcare. This paper presents an approach that utilizes large language models and data source interfaces to explore and collect time series datasets. While obtained from external sources, the collected data share critical statistical properties with primary time series datasets, making it possible to model and adapt to various scenarios. This method enlarges the data quantity when the original data is limited or lacks essential properties. It suggests that collected datasets can effectively supplement existing datasets, especially involving changes in data distribution. We demonstrate the effectiveness of the collected datasets through practical examples and show how time series forecasting foundation models fine-tuned on these datasets achieve comparable performance to those models without fine-tuning.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2406.05249",
      "arxivId": "2406.05249",
      "url": "https://www.semanticscholar.org/paper/d313d3981a7cc76c02a68fdca8a28be1b93f0235",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.05249"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "81f39ee405d5748ea76eb5e001116f3ebe71ad3f",
      "title": "Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model's Prediction Rationality",
      "authors": [
        {
          "name": "Qitong Wang",
          "authorId": "2335989946"
        },
        {
          "name": "Tang Li",
          "authorId": "2149202298"
        },
        {
          "name": "Kien X. Nguyen",
          "authorId": "2329142494"
        },
        {
          "name": "Xi Peng",
          "authorId": "2310399716"
        }
      ],
      "year": 2024,
      "abstract": "Vision-Language Models (VLMs), such as CLIP, have already seen widespread applications. Researchers actively engage in further fine-tuning VLMs in safety-critical domains. In these domains, prediction rationality is crucial: the prediction should be correct and based on valid evidence. Yet, for VLMs, the impact of fine-tuning on prediction rationality is seldomly investigated. To study this problem, we proposed two new metrics called Prediction Trustworthiness and Inference Reliability. We conducted extensive experiments on various settings and observed some interesting phenomena. On the one hand, we found that the well-adopted fine-tuning methods led to more correct predictions based on invalid evidence. This potentially undermines the trustworthiness of correct predictions from fine-tuned VLMs. On the other hand, having identified valid evidence of target objects, fine-tuned VLMs were more likely to make correct predictions. Moreover, the findings are also consistent under distributional shifts and across various experimental settings. We hope our research offer fresh insights to VLM fine-tuning.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2412.13333",
      "arxivId": "2412.13333",
      "url": "https://www.semanticscholar.org/paper/81f39ee405d5748ea76eb5e001116f3ebe71ad3f",
      "venue": "AAAI Conference on Artificial Intelligence",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2412.13333"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "2296629527ebbd6f8c897df7cf5cdbac3f0cc15b",
      "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
      "authors": [
        {
          "name": "A. Ustun",
          "authorId": "82290814"
        },
        {
          "name": "Viraat Aryabumi",
          "authorId": "2283848534"
        },
        {
          "name": "Zheng-Xin Yong",
          "authorId": "1725420331"
        },
        {
          "name": "Wei-Yin Ko",
          "authorId": "1500573542"
        },
        {
          "name": "Daniel D'souza",
          "authorId": "1432838153"
        },
        {
          "name": "Gbemileke Onilude",
          "authorId": "2190104489"
        },
        {
          "name": "Neel Bhandari",
          "authorId": "2283848415"
        },
        {
          "name": "Shivalika Singh",
          "authorId": "2283844788"
        },
        {
          "name": "Hui-Lee Ooi",
          "authorId": "2329582025"
        },
        {
          "name": "Amr Kayid",
          "authorId": "2024731522"
        },
        {
          "name": "Freddie Vargus",
          "authorId": "7140106"
        },
        {
          "name": "Phil Blunsom",
          "authorId": "2283848746"
        },
        {
          "name": "Shayne Longpre",
          "authorId": "2283848744"
        },
        {
          "name": "Niklas Muennighoff",
          "authorId": "2037383772"
        },
        {
          "name": "Marzieh Fadaee",
          "authorId": "2818759"
        },
        {
          "name": "Julia Kreutzer",
          "authorId": "3422710"
        },
        {
          "name": "Sara Hooker",
          "authorId": "2261493078"
        }
      ],
      "year": 2024,
      "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101",
      "citationCount": 318,
      "doi": "10.48550/arXiv.2402.07827",
      "arxivId": "2402.07827",
      "url": "https://www.semanticscholar.org/paper/2296629527ebbd6f8c897df7cf5cdbac3f0cc15b",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "15894-15939"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a7f1be908de231bb72928fa984289501c0796823",
      "title": "One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion",
      "authors": [
        {
          "name": "Marc Rigter",
          "authorId": "51300315"
        },
        {
          "name": "Bruno Lacerda",
          "authorId": "145350537"
        },
        {
          "name": "Nick Hawes",
          "authorId": "2072387078"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 1,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a7f1be908de231bb72928fa984289501c0796823",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "105716739d5b198316cd75bdaec775a904fcdda5",
      "title": "TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift",
      "authors": [
        {
          "name": "Dipesh Mahato",
          "authorId": "2268066654"
        },
        {
          "name": "Rohan Poudel",
          "authorId": "2367276657"
        },
        {
          "name": "Pramod Dhungana",
          "authorId": "95044422"
        }
      ],
      "year": 2025,
      "abstract": "Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2506.14217",
      "arxivId": "2506.14217",
      "url": "https://www.semanticscholar.org/paper/105716739d5b198316cd75bdaec775a904fcdda5",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.14217"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c2578f5ced46da44eb68253293d8e6ce2397f043",
      "title": "Measuring Distributional Shifts in Text: The Advantage of Language Model-Based Embeddings",
      "authors": [
        {
          "name": "Gyandev Gupta",
          "authorId": "2269733839"
        },
        {
          "name": "Bashir Rastegarpanah",
          "authorId": "2365952"
        },
        {
          "name": "Amalendu Iyer",
          "authorId": "2269735028"
        },
        {
          "name": "Joshua Rubin",
          "authorId": "2269735459"
        },
        {
          "name": "K. Kenthapadi",
          "authorId": "1769861"
        }
      ],
      "year": 2023,
      "abstract": "An essential part of monitoring machine learning models in production is measuring input and output data drift. In this paper, we present a system for measuring distributional shifts in natural language data and highlight and investigate the potential advantage of using large language models (LLMs) for this problem. Recent advancements in LLMs and their successful adoption in different domains indicate their effectiveness in capturing semantic relationships for solving various natural language processing problems. The power of LLMs comes largely from the encodings (embeddings) generated in the hidden layers of the corresponding neural network. First we propose a clustering-based algorithm for measuring distributional shifts in text data by exploiting such embeddings. Then we study the effectiveness of our approach when applied to text embeddings generated by both LLMs and classical embedding algorithms. Our experiments show that general-purpose LLM-based embeddings provide a high sensitivity to data drift compared to other embedding methods. We propose drift sensitivity as an important evaluation metric to consider when comparing language models. Finally, we present insights and lessons learned from deploying our framework as part of the Fiddler ML Monitoring platform over a period of 18 months.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2312.02337",
      "arxivId": "2312.02337",
      "url": "https://www.semanticscholar.org/paper/c2578f5ced46da44eb68253293d8e6ce2397f043",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2312.02337"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "3bee3eb0209c0d1d0c0ac5e6737a8fc642f34268",
      "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning",
      "authors": [
        {
          "name": "Borong Zhang",
          "authorId": "152705071"
        },
        {
          "name": "Yuhao Zhang",
          "authorId": "2348552424"
        },
        {
          "name": "Jiaming Ji",
          "authorId": "2273548793"
        },
        {
          "name": "Yingshan Lei",
          "authorId": "2263752063"
        },
        {
          "name": "Josef Dai",
          "authorId": "2260610683"
        },
        {
          "name": "Yuanpei Chen",
          "authorId": "2261728034"
        },
        {
          "name": "Yaodong Yang",
          "authorId": "2260432856"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 11,
      "doi": "10.48550/arXiv.2503.03480",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/3bee3eb0209c0d1d0c0ac5e6737a8fc642f34268",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.03480"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0cba64a3454f9a3093b6a6fa7dc9a306b971c212",
      "title": "On Large Language Model Continual Unlearning",
      "authors": [
        {
          "name": "Chongyang Gao",
          "authorId": "2311833838"
        },
        {
          "name": "Lixu Wang",
          "authorId": "2108631414"
        },
        {
          "name": "Chenkai Weng",
          "authorId": "2148353350"
        },
        {
          "name": "Xiao Wang",
          "authorId": "2276121035"
        },
        {
          "name": "Qi Zhu",
          "authorId": "2275773112"
        }
      ],
      "year": 2024,
      "abstract": "While large language models have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning has emerged as a representative approach for model safety and security by removing the influence of undesired data on the target model. However, these methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging, especially in the context of LLMs, which may lead to accumulated model utility loss that eventually becomes unacceptable. Moreover, existing LLM unlearning methods often ignore previous data access limitations due to privacy concerns and copyright protection. Without previous data, the utility preservation during unlearning is much harder. To overcome these challenges, we propose the OOO framework that includes an Orthogonal low-rank adapter (LoRA) for continually unlearning requested data and an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. The OOD detector is trained with a novel contrastive entropy loss and utilizes a glocal-aware scoring mechanism. During inference, our OOO framework can decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predicted similarity between the input and the unlearned knowledge. Notably, OOO's effectiveness does not rely on any retained data. We conducted extensive experiments on OOO and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that OOO consistently achieves the best unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. The source codes can be found at https://github.com/GCYZSL/O3-LLM-UNLEARNING.",
      "citationCount": 24,
      "doi": null,
      "arxivId": "2407.10223",
      "url": "https://www.semanticscholar.org/paper/0cba64a3454f9a3093b6a6fa7dc9a306b971c212",
      "venue": "International Conference on Learning Representations",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a8230bc314c6d4042240e04a67e3afb10a82b27",
      "title": "Lightweight Vision Language Model-Guided Gesture Recognition Based on Electromyography",
      "authors": [
        {
          "name": "Mustapha Deji Dere",
          "authorId": "2367335554"
        },
        {
          "name": "Saehyung Cheong",
          "authorId": "2359484133"
        },
        {
          "name": "Ji-Hun Jo",
          "authorId": "2258514613"
        },
        {
          "name": "Giwon Ku",
          "authorId": "2259021148"
        },
        {
          "name": "Boreom Lee",
          "authorId": "3069730"
        }
      ],
      "year": 2025,
      "abstract": "Neuromuscular dysfunction poses a critical health challenge, affecting both patients and caregivers. Current active rehabilitation devices relying on electromyography (EMG) face significant hurdles due to high data acquisition demands, inaccurate decoders, and model degradation from sensor distribution shifts across participants. We address these limitations with a vision language model (VLM) that pseudolabels motor intent predictions, enhancing decoding accuracy and reliability under sensor distribution shift across participants. Our proposed pipeline integrates visual information with EMG decoding by employing a parallel processing framework: vision data are processed through the VLM to determine user intent, object, and grasp type, which, in turn, provides pseudolabels for the EMG decoder\u2019s output. The pseudolabel is then utilized to infer motor intent, enabling precise external control of assistive devices. In our experimental analysis, our proposed pipeline demonstrated superior performance compared to an EMG-only decoder, achieving 100% decoding accuracy across four upper limb gestures in four participants despite sensor distribution shifts across them. Our approach underscores the potential of integrating visual understanding with neural signal analysis to enhance the reliability and effectiveness of active rehabilitation devices, ultimately improving patient outcomes by providing more accurate and consistent assistive technologies. The project page including the code can be found at https://github.com/deremustapha/VLM-EMG4Gesture",
      "citationCount": 1,
      "doi": "10.1109/JSEN.2025.3565766",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0a8230bc314c6d4042240e04a67e3afb10a82b27",
      "venue": "IEEE Sensors Journal",
      "journal": {
        "name": "IEEE Sensors Journal",
        "pages": "22677-22685",
        "volume": "25"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a",
      "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
      "authors": [
        {
          "name": "Zhaorui Yang",
          "authorId": "2284865968"
        },
        {
          "name": "Qian Liu",
          "authorId": "1409707585"
        },
        {
          "name": "Tianyu Pang",
          "authorId": "19201674"
        },
        {
          "name": "Han Wang",
          "authorId": "2285032001"
        },
        {
          "name": "H. Feng",
          "authorId": "46854712"
        },
        {
          "name": "Minfeng Zhu",
          "authorId": "145314938"
        },
        {
          "name": "Wei Chen",
          "authorId": "2256716159"
        }
      ],
      "year": 2024,
      "abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.",
      "citationCount": 77,
      "doi": "10.48550/arXiv.2402.13669",
      "arxivId": "2402.13669",
      "url": "https://www.semanticscholar.org/paper/c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "1028-1043"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "440a9bb08c42763f3f370c5d2676a195bff57e35",
      "title": "LLM as Dataset Analyst: Subpopulation Structure Discovery with Large Language Model",
      "authors": [
        {
          "name": "Yulin Luo",
          "authorId": "2276754886"
        },
        {
          "name": "Ruichuan An",
          "authorId": "2269472998"
        },
        {
          "name": "Bocheng Zou",
          "authorId": "2269737481"
        },
        {
          "name": "Yiming Tang",
          "authorId": "2300085280"
        },
        {
          "name": "Jiaming Liu",
          "authorId": "2267506419"
        },
        {
          "name": "Shanghang Zhang",
          "authorId": "2242835846"
        }
      ],
      "year": 2024,
      "abstract": "The distribution of subpopulations is an important property hidden within a dataset. Uncovering and analyzing the subpopulation distribution within datasets provides a comprehensive understanding of the datasets, standing as a powerful tool beneficial to various downstream tasks, including Dataset Subpopulation Organization, Subpopulation Shift, and Slice Discovery. Despite its importance, there has been no work that systematically explores the subpopulation distribution of datasets to our knowledge. To address the limitation and solve all the mentioned tasks in a unified way, we introduce a novel concept of subpopulation structures to represent, analyze, and utilize subpopulation distributions within datasets. To characterize the structures in an interpretable manner, we propose the Subpopulation Structure Discovery with Large Language Models (SSD-LLM) framework, which employs world knowledge and instruction-following capabilities of Large Language Models (LLMs) to linguistically analyze informative image captions and summarize the structures. Furthermore, we propose complete workflows to address downstream tasks, named Task-specific Tuning, showcasing the application of the discovered structure to a spectrum of subpopulation-related tasks, including dataset subpopulation organization, subpopulation shift, and slice discovery. Furthermore, we propose complete workflows to address downstream tasks, named Task-specific Tuning, showcasing the application of the discovered structure to a spectrum of subpopulation-related tasks, including dataset subpopulation organization, subpopulation shift, and slice discovery.",
      "citationCount": 43,
      "doi": "10.48550/arXiv.2405.02363",
      "arxivId": "2405.02363",
      "url": "https://www.semanticscholar.org/paper/440a9bb08c42763f3f370c5d2676a195bff57e35",
      "venue": "European Conference on Computer Vision",
      "journal": {
        "pages": "235-252"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "66718e87b70de80cbc2a4120050ca36fda49f8d6",
      "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
      "authors": [
        {
          "name": "Shushan Arakelyan",
          "authorId": "46221106"
        },
        {
          "name": "Rocktim Jyoti Das",
          "authorId": "2211732585"
        },
        {
          "name": "Yi Mao",
          "authorId": "145469202"
        },
        {
          "name": "Xiang Ren",
          "authorId": "1384550891"
        }
      ],
      "year": 2023,
      "abstract": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain",
      "citationCount": 25,
      "doi": "10.48550/arXiv.2303.09128",
      "arxivId": "2303.09128",
      "url": "https://www.semanticscholar.org/paper/66718e87b70de80cbc2a4120050ca36fda49f8d6",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "pages": "16298-16314"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "63d6e950979d8eb1d45951220b24f148deb85de5",
      "title": "Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization",
      "authors": [
        {
          "name": "Ru Wang",
          "authorId": "2347184384"
        },
        {
          "name": "Wei Huang",
          "authorId": "2347499606"
        },
        {
          "name": "Selena Song",
          "authorId": "2347162050"
        },
        {
          "name": "Haoyu Zhang",
          "authorId": "2377680155"
        },
        {
          "name": "Yusuke Iwasawa",
          "authorId": "1715282"
        },
        {
          "name": "Yutaka Matsuo",
          "authorId": "2241471533"
        },
        {
          "name": "Jiaxian Guo",
          "authorId": "2342508970"
        }
      ],
      "year": 2025,
      "abstract": "Generalization to novel compound tasks under distribution shift is important for deploying transformer-based language models (LMs). This work investigates Chain-of-Thought (CoT) reasoning as a means to enhance OOD generalization. Through controlled experiments across several compound tasks, we reveal three key insights: (1) While QA-trained models achieve near-perfect in-distribution accuracy, their OOD performance degrades catastrophically, even with 10000k+ training examples; (2) the granularity of CoT data strongly correlates with generalization performance; finer-grained CoT data leads to better generalization; (3) CoT exhibits remarkable sample efficiency, matching QA performance with much less (even 80%) data. Theoretically, we demonstrate that compound tasks inherently permit shortcuts in Q-A data that misalign with true reasoning principles, while CoT forces internalization of valid dependency structures, and thus can achieve better generalization. Further, we show that transformer positional embeddings can amplify generalization by emphasizing subtask condition recurrence in long CoT sequences. Our combined theoretical and empirical analysis provides compelling evidence for CoT reasoning as a crucial training paradigm for enabling LM generalization under real-world distributional shifts for compound tasks.",
      "citationCount": 6,
      "doi": "10.48550/arXiv.2502.18273",
      "arxivId": "2502.18273",
      "url": "https://www.semanticscholar.org/paper/63d6e950979d8eb1d45951220b24f148deb85de5",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.18273"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "47387abd3fe650391ee84f8189e9c655c3dd067f",
      "title": "Large Language Model driven Policy Exploration for Recommender Systems",
      "authors": [
        {
          "name": "Jie Wang",
          "authorId": "2293547342"
        },
        {
          "name": "Alexandros Karatzoglou",
          "authorId": "2257295049"
        },
        {
          "name": "Ioannis Arapakis",
          "authorId": "2309480030"
        },
        {
          "name": "Joemon M. Jose",
          "authorId": "2286309062"
        }
      ],
      "year": 2025,
      "abstract": "Recent advancements in Recommender Systems (RS) have incorporated Reinforcement Learning (RL), framing the recommendation as a Markov Decision Process (MDP). However, offline RL policies trained on static user data are vulnerable to distribution shift when deployed in dynamic online environments. Additionally, excessive focus on exploiting short-term relevant items can hinder exploration, leading to sub-optimal recommendations and negatively impacting long-term user gains. Online RL-based RS also face challenges in production deployment, due to the risks of exposing users to untrained or unstable policies. Large Language Models (LLMs) offer a promising solution to mimic user objectives and preferences for pre-training policies offline to enhance the initial recommendations in online settings. Effectively managing distribution shift and balancing exploration are crucial for improving RL-based RS, especially when leveraging LLM-based pre-training. To address these challenges, we propose an Interaction-Augmented Learned Policy (iALP) that utilizes user preferences distilled from an LLM. Our approach involves prompting the LLM with user states to extract item preferences, learning rewards based on feedback, and updating the RL policy using an actor-critic framework. Furthermore, to deploy iALP in an online scenario, we introduce an adaptive variant, A-iALP, that implements a simple fine-tuning strategy (A-iALPft), and an adaptive approach (A-iALPap) designed to mitigate issues with compromised policies and limited exploration. Experiments across three simulated environments demonstrate that A-iALP introduces substantial performance improvements.",
      "citationCount": 6,
      "doi": "10.1145/3701551.3703496",
      "arxivId": "2501.13816",
      "url": "https://www.semanticscholar.org/paper/47387abd3fe650391ee84f8189e9c655c3dd067f",
      "venue": "Web Search and Data Mining",
      "journal": {
        "name": "Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "d97aab5f94a1d224b00d19028e5b81901eab0adb",
      "title": "Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift",
      "authors": [
        {
          "name": "Shuai Yuan",
          "authorId": "2374100385"
        },
        {
          "name": "Zhibo Zhang",
          "authorId": "2271977255"
        },
        {
          "name": "Yuxi Li",
          "authorId": "2268093091"
        },
        {
          "name": "Guangdong Bai",
          "authorId": "2261336931"
        },
        {
          "name": "Kailong Wang",
          "authorId": "2265727621"
        }
      ],
      "year": 2025,
      "abstract": "The widespread distribution of Large Language Models (LLMs) through public platforms like Hugging Face introduces significant security challenges. While these platforms perform basic security scans, they often fail to detect subtle manipulations within the embedding layer. This work identifies a novel class of deployment phase attacks that exploit this vulnerability by injecting imperceptible perturbations directly into the embedding layer outputs without modifying model weights or input text. These perturbations, though statistically benign, systematically bypass safety alignment mechanisms and induce harmful behaviors during inference. We propose Search based Embedding Poisoning(SEP), a practical, model agnostic framework that introduces carefully optimized perturbations into embeddings associated with high risk tokens. SEP leverages a predictable linear transition in model responses, from refusal to harmful output to semantic deviation to identify a narrow perturbation window that evades alignment safeguards. Evaluated across six aligned LLMs, SEP achieves an average attack success rate of 96.43% while preserving benign task performance and evading conventional detection mechanisms. Our findings reveal a critical oversight in deployment security and emphasize the urgent need for embedding level integrity checks in future LLM defense strategies.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.06338",
      "arxivId": "2509.06338",
      "url": "https://www.semanticscholar.org/paper/d97aab5f94a1d224b00d19028e5b81901eab0adb",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.06338"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4c1b7cf0550130a2aca6215b759cb09c76b19978",
      "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning",
      "authors": [
        {
          "name": "Borong Zhang",
          "authorId": "152705071"
        },
        {
          "name": "Yuhao Zhang",
          "authorId": "2348552424"
        },
        {
          "name": "Jiaming Ji",
          "authorId": "2273548793"
        },
        {
          "name": "Yingshan Lei",
          "authorId": "2263752063"
        },
        {
          "name": "Josef Dai",
          "authorId": "2260610683"
        },
        {
          "name": "Yuanpei Chen",
          "authorId": "2261728034"
        },
        {
          "name": "Yaodong Yang",
          "authorId": "2260432856"
        }
      ],
      "year": 2025,
      "abstract": "Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, reducing the cumulative cost of safety violations by 83.58% compared to the state-of-the-art method, while also maintaining task success rate (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. The effectiveness is evaluated on long-horizon mobile manipulation tasks. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.",
      "citationCount": 7,
      "doi": null,
      "arxivId": "2503.03480",
      "url": "https://www.semanticscholar.org/paper/4c1b7cf0550130a2aca6215b759cb09c76b19978",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    }
  ],
  "count": 30,
  "errors": []
}
