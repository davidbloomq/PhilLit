{
  "status": "success",
  "source": "semantic_scholar",
  "query": "arithmetic reasoning language models",
  "results": [
    {
      "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Jason Wei",
          "authorId": "119640649"
        },
        {
          "name": "Xuezhi Wang",
          "authorId": "2275277634"
        },
        {
          "name": "Dale Schuurmans",
          "authorId": "1714772"
        },
        {
          "name": "Maarten Bosma",
          "authorId": "40377863"
        },
        {
          "name": "Ed H. Chi",
          "authorId": "2226805"
        },
        {
          "name": "F. Xia",
          "authorId": "144956443"
        },
        {
          "name": "Quoc Le",
          "authorId": "1998340269"
        },
        {
          "name": "Denny Zhou",
          "authorId": "65855107"
        }
      ],
      "year": 2022,
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "citationCount": 14519,
      "doi": null,
      "arxivId": "2201.11903",
      "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2201.11903"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": [
        {
          "name": "Xuezhi Wang",
          "authorId": "2275277634"
        },
        {
          "name": "Jason Wei",
          "authorId": "119640649"
        },
        {
          "name": "D. Schuurmans",
          "authorId": "50319359"
        },
        {
          "name": "Quoc Le",
          "authorId": "1998340269"
        },
        {
          "name": "Ed H. Chi",
          "authorId": "2226805"
        },
        {
          "name": "Denny Zhou",
          "authorId": "65855107"
        }
      ],
      "year": 2022,
      "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
      "citationCount": 5523,
      "doi": null,
      "arxivId": "2203.11171",
      "url": "https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2203.11171"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a4b45e0df7992e137f85b442dc45467f7d474da7",
      "title": "How Good Are Large Language Models at Arithmetic Reasoning in Low-Resource Language Settings?\u2014A Study on Yor\u00f9b\u00e1 Numerical Probes with Minimal Contamination",
      "authors": [
        {
          "name": "Fiyinfoluwa Oyesanmi",
          "authorId": "2210517132"
        },
        {
          "name": "Peter O. Olukanmi",
          "authorId": "31627181"
        }
      ],
      "year": 2025,
      "abstract": "We study the performance of large language models (LLMs) in natural language understanding and natural language reasoning tasks in a low-resourced-language (LRL) setting. Using Yor\u00f9b\u00e1, an LRL, we curated a set of numerical probes with minimal contamination. The probes comprise three sets of questions\u2014the first covers basic arithmetic, the second covers date and time (calendar system), and the last focuses on numerals and counting systems. Assessed in a zero-shot setup, three LLMs (ChatGPT, Gemini, and PaLM) were evaluated based on several metrics. The best-performing model, ChatGPT, generated some correct answers, showing logical steps in attaining the answers in Yor\u00f9b\u00e1 (with an accuracy of 56% in set one, and 44% in set two). The second-best model (with an accuracy of 56% in set one, and 32% in set two) is Gemini. PaLM (with an accuracy of 16% in set one, and 8% in set two) showed the answers without logic. The three models performed poorly on the Yor\u00f9b\u00e1 numerals question set (ChatGPT scored 8%, and Gemini and PaLM each had 0% accuracy). The study also revealed that there is significant room for improvement in the state of the art of LLMs when it comes to Yor\u00f9b\u00e1 numerals.",
      "citationCount": 1,
      "doi": "10.3390/app15084459",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a4b45e0df7992e137f85b442dc45467f7d474da7",
      "venue": "Applied Sciences",
      "journal": {
        "name": "Applied Sciences"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "753d66d67c14e3d831a17917f6b88df49bb6e68d",
      "title": "BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models",
      "authors": [
        {
          "name": "Chuyuan Li",
          "authorId": "2321148692"
        },
        {
          "name": "Giuseppe Carenini",
          "authorId": "2291365789"
        }
      ],
      "year": 2025,
      "abstract": "We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.13095",
      "arxivId": "2511.13095",
      "url": "https://www.semanticscholar.org/paper/753d66d67c14e3d831a17917f6b88df49bb6e68d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.13095"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "881b763caaa228f0725ae1cd93e0ebd5d137a1d0",
      "title": "Towards Analysis and Interpretation of Large Language Models for Arithmetic Reasoning",
      "authors": [
        {
          "name": "Mst. Shapna Akter",
          "authorId": "2127959940"
        },
        {
          "name": "Hossain Shahriar",
          "authorId": "2277554652"
        },
        {
          "name": "Alfredo Cuzzocrea",
          "authorId": "2293984775"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) have recently conquered the research scene, with particular regards to the Transformer architecture in the context of arithmetic reasoning. In this so-delineated scenario, this paper puts the basis for a causal mediation analysis about the approach of Transformer-based LLMs to complex arithmetic problems. In particular, we try to discover which parameters are crucial for complex reasoning tasks such as model activations. Our preliminary results state that, for complex arithmetic operations, information is channeled from mid-layer activations to the final token through enhanced attention mechanisms. Preliminary experiments are reported.",
      "citationCount": 2,
      "doi": "10.1109/SDS60720.2024.00049",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/881b763caaa228f0725ae1cd93e0ebd5d137a1d0",
      "venue": "Swiss Conference on Data Science",
      "journal": {
        "name": "2024 11th IEEE Swiss Conference on Data Science (SDS)",
        "pages": "267-270"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "00e4098e8cba9fb2342109ba3028294c8b687c03",
      "title": "Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges",
      "authors": [
        {
          "name": "Safal Shrestha",
          "authorId": "2345187520"
        },
        {
          "name": "Minwu Kim",
          "authorId": "2345328816"
        },
        {
          "name": "Keith Ross",
          "authorId": "2345187775"
        }
      ],
      "year": 2025,
      "abstract": "Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal a significant increase in logical error rates-up to 14 percentage points-as numerical complexity rises, demonstrating a general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide a comprehensive evaluation of LLMs' mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2502.08680",
      "arxivId": "2502.08680",
      "url": "https://www.semanticscholar.org/paper/00e4098e8cba9fb2342109ba3028294c8b687c03",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.08680"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "45ebe6cba6fdd7cfb61f2cc6df3178f65d9146ad",
      "title": "Self-training Language Models for Arithmetic Reasoning",
      "authors": [
        {
          "name": "Marek Kadlc\u00edk",
          "authorId": "2194666242"
        },
        {
          "name": "Michal \u0160tef\u00e1nik",
          "authorId": "31023375"
        }
      ],
      "year": 2024,
      "abstract": "Recent language models achieve impressive results in tasks involving complex multistep reasoning, but scaling these capabilities further traditionally requires expensive collection of more annotated data. In this work, we explore the potential of improving models' reasoning capabilities without new data, merely using automated feedback to the validity of their predictions in arithmetic reasoning (self-training). In systematic experimentation across six different arithmetic reasoning datasets, we find that models can substantially improve in both single-round (offline) and online self-training, reaching a correct result in +13.9% and +25.9% more cases, respectively, underlining the importance of actuality of self-training feedback. We further find that in the single-round, offline self-training, traditional supervised training can deliver gains comparable to preference optimization, but in online self-training, preference optimization methods largely outperform supervised training thanks to their superior stability and robustness on unseen types of problems.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2407.08400",
      "arxivId": "2407.08400",
      "url": "https://www.semanticscholar.org/paper/45ebe6cba6fdd7cfb61f2cc6df3178f65d9146ad",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.08400"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "5dc15ac1c92ab7492f121471823fb13a95d273ba",
      "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
      "authors": [
        {
          "name": "Alessandro Stolfo",
          "authorId": "2175480389"
        },
        {
          "name": "Yonatan Belinkov",
          "authorId": "2083259"
        },
        {
          "name": "Mrinmaya Sachan",
          "authorId": "2790926"
        }
      ],
      "year": 2023,
      "abstract": "Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.",
      "citationCount": 67,
      "doi": "10.18653/v1/2023.emnlp-main.435",
      "arxivId": "2305.15054",
      "url": "https://www.semanticscholar.org/paper/5dc15ac1c92ab7492f121471823fb13a95d273ba",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "pages": "7035-7052"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "1fec37227a5e731e7a1ca65ca92e21344511efaf",
      "title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
      "authors": [
        {
          "name": "Zhongtao Miao",
          "authorId": "2294717957"
        },
        {
          "name": "Kaiyan Zhao",
          "authorId": "2242928509"
        },
        {
          "name": "Yoshimasa Tsuruoka",
          "authorId": "2294717770"
        }
      ],
      "year": 2024,
      "abstract": "Current representations used in reasoning steps of large language models can mostly be categorized into two main types: (1) natural language, which is difficult to verify; and (2) non-natural language, usually programming code, which is difficult for people who are unfamiliar with coding to read. In this paper, we propose to use a semi-structured form to represent reasoning steps of large language models. Specifically, we use relation tuples, which are not only human-readable but also machine-friendly and easier to verify than natural language. We implement a framework that includes three main components: (1) introducing relation tuples into the reasoning steps of large language models; (2) implementing an automatic verification process of reasoning steps with a local code interpreter based on relation tuples; and (3) integrating a simple and effective dynamic feedback mechanism, which we found helpful for self-improvement of large language models. The experimental results on various arithmetic datasets demonstrate the effectiveness of our method in improving the arithmetic reasoning ability of large language models. The source code is available at https://github.com/gpgg/art.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2406.17873",
      "arxivId": "2406.17873",
      "url": "https://www.semanticscholar.org/paper/1fec37227a5e731e7a1ca65ca92e21344511efaf",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.17873"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f7b9c9b987204a88379708a9ab38e865ec6cb5f8",
      "title": "Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic",
      "authors": [
        {
          "name": "Jiuxiang Gu",
          "authorId": "2260056944"
        },
        {
          "name": "Chenyang Li",
          "authorId": "2268335390"
        },
        {
          "name": "Yingyu Liang",
          "authorId": "2260827689"
        },
        {
          "name": "Zhenmei Shi",
          "authorId": "113515522"
        },
        {
          "name": "Zhao Song",
          "authorId": "2284489474"
        },
        {
          "name": "Tianyi Zhou",
          "authorId": "2190694474"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 8,
      "doi": "10.48550/arXiv.2402.09469",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f7b9c9b987204a88379708a9ab38e865ec6cb5f8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.09469"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "80f06ce2be5dd43211045627aed587baac109007",
      "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective",
      "authors": [
        {
          "name": "Yiyao Yu",
          "authorId": "2255733564"
        },
        {
          "name": "Yuxiang Zhang",
          "authorId": "2108080174"
        },
        {
          "name": "Dongdong Zhang",
          "authorId": "2273919921"
        },
        {
          "name": "Xiao Liang",
          "authorId": "2367462247"
        },
        {
          "name": "Hengyuan Zhang",
          "authorId": "2290987204"
        },
        {
          "name": "Xingxing Zhang",
          "authorId": "2284863493"
        },
        {
          "name": "Ziyi Yang",
          "authorId": "2291073936"
        },
        {
          "name": "Mahmoud Khademi",
          "authorId": "2268760479"
        },
        {
          "name": "H. Awadalla",
          "authorId": "3032929"
        },
        {
          "name": "Junjie Wang",
          "authorId": "2143183255"
        },
        {
          "name": "Yujiu Yang",
          "authorId": "2296746860"
        },
        {
          "name": "Furu Wei",
          "authorId": "2290016262"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.",
      "citationCount": 21,
      "doi": "10.48550/arXiv.2501.11110",
      "arxivId": "2501.11110",
      "url": "https://www.semanticscholar.org/paper/80f06ce2be5dd43211045627aed587baac109007",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "24914-24937"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "34a3e108301c84167fe877842fac3cec4a87b38c",
      "title": "Large Language Models and Mathematical Reasoning Failures",
      "authors": [
        {
          "name": "Johan Boye",
          "authorId": "2345818720"
        },
        {
          "name": "Birger Mo\u00ebll",
          "authorId": "48134541"
        }
      ],
      "year": 2025,
      "abstract": "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2502.11574",
      "arxivId": "2502.11574",
      "url": "https://www.semanticscholar.org/paper/34a3e108301c84167fe877842fac3cec4a87b38c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.11574"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ddd1b5e92e0464bd775fc52218a7fda150ea6149",
      "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?",
      "authors": [
        {
          "name": "Kai Yan",
          "authorId": "2342409708"
        },
        {
          "name": "Yufei Xu",
          "authorId": "2338864255"
        },
        {
          "name": "Zhengyin Du",
          "authorId": "2336923378"
        },
        {
          "name": "Xuesong Yao",
          "authorId": "2338877338"
        },
        {
          "name": "Zheyu Wang",
          "authorId": "2353209211"
        },
        {
          "name": "Xiaowen Guo",
          "authorId": "2353236457"
        },
        {
          "name": "Jiecao Chen",
          "authorId": "2341524466"
        }
      ],
      "year": 2025,
      "abstract": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs'remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60 percent performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2504.00509",
      "arxivId": "2504.00509",
      "url": "https://www.semanticscholar.org/paper/ddd1b5e92e0464bd775fc52218a7fda150ea6149",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.00509"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f122115613ff4f9b1b01b35821737e2f261776fb",
      "title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding",
      "authors": [
        {
          "name": "Kung-Hsiang Huang",
          "authorId": "1420116116"
        },
        {
          "name": "Can Qin",
          "authorId": "2316388054"
        },
        {
          "name": "Haoyi Qiu",
          "authorId": "2261278256"
        },
        {
          "name": "Philippe Laban",
          "authorId": "46180754"
        },
        {
          "name": "Shafiq Joty",
          "authorId": "2313536726"
        },
        {
          "name": "Caiming Xiong",
          "authorId": "2266753302"
        },
        {
          "name": "Chien-Sheng Wu",
          "authorId": "2325507100"
        }
      ],
      "year": 2025,
      "abstract": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks like chart understanding and geometric reasoning. In this work, we first investigate the root causes of this deficiency through a suite of probing tasks focusing on basic visual arithmetic. Our analysis reveals that while pre-trained vision encoders typically capture sufficient information, the text decoder often fails to decode it correctly for arithmetic reasoning. To address this, we propose CogAlign, a novel post-training strategy inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to recognize invariant properties under visual transformations. We demonstrate that this approach significantly improves the performance of three diverse VLMs on our proposed probing tasks. Furthermore, CogAlign enhances performance by an average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching supervised fine-tuning methods while requiring only 60% less training data. These results highlight the effectiveness and generalizability of CogAlign in improving fundamental visual arithmetic capabilities and their transfer to downstream tasks.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2502.11492",
      "arxivId": "2502.11492",
      "url": "https://www.semanticscholar.org/paper/f122115613ff4f9b1b01b35821737e2f261776fb",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "4830-4843"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "73df3e273452aacfe90f0c4bf22ff24249de0e07",
      "title": "A Survey on Mathematical Reasoning and Optimization with Large Language Models",
      "authors": [
        {
          "name": "Ali Forootani",
          "authorId": "31183264"
        }
      ],
      "year": 2025,
      "abstract": "Mathematical reasoning and optimization are fundamental to artificial intelligence and computational problem-solving. Recent advancements in Large Language Models (LLMs) have significantly improved AI-driven mathematical reasoning, theorem proving, and optimization techniques. This survey explores the evolution of mathematical problem-solving in AI, from early statistical learning approaches to modern deep learning and transformer-based methodologies. We review the capabilities of pretrained language models and LLMs in performing arithmetic operations, complex reasoning, theorem proving, and structured symbolic computation. A key focus is on how LLMs integrate with optimization and control frameworks, including mixed-integer programming, linear quadratic control, and multi-agent optimization strategies. We examine how LLMs assist in problem formulation, constraint generation, and heuristic search, bridging theoretical reasoning with practical applications. We also discuss enhancement techniques such as Chain-of-Thought reasoning, instruction tuning, and tool-augmented methods that improve LLM's problem-solving performance. Despite their progress, LLMs face challenges in numerical precision, logical consistency, and proof verification. Emerging trends such as hybrid neural-symbolic reasoning, structured prompt engineering, and multi-step self-correction aim to overcome these limitations. Future research should focus on interpretability, integration with domain-specific solvers, and improving the robustness of AI-driven decision-making. This survey offers a comprehensive review of the current landscape and future directions of mathematical reasoning and optimization with LLMs, with applications across engineering, finance, and scientific research.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2503.17726",
      "arxivId": "2503.17726",
      "url": "https://www.semanticscholar.org/paper/73df3e273452aacfe90f0c4bf22ff24249de0e07",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.17726"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "48e669c2679b9acf7beb8abdb789167d61ceca49",
      "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics",
      "authors": [
        {
          "name": "Yaniv Nikankin",
          "authorId": "2191617821"
        },
        {
          "name": "Anja Reusch",
          "authorId": "2328076715"
        },
        {
          "name": "Aaron Mueller",
          "authorId": "2261670263"
        },
        {
          "name": "Yonatan Belinkov",
          "authorId": "2083259"
        }
      ],
      "year": 2024,
      "abstract": "Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a\"bag of heuristics\".",
      "citationCount": 61,
      "doi": "10.48550/arXiv.2410.21272",
      "arxivId": "2410.21272",
      "url": "https://www.semanticscholar.org/paper/48e669c2679b9acf7beb8abdb789167d61ceca49",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.21272"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "da5d2425cdef713b105ceb92cd86e9528aa079d5",
      "title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
      "authors": [
        {
          "name": "Wei Zhang",
          "authorId": "2319188665"
        },
        {
          "name": "Chaoqun Wan",
          "authorId": "29001337"
        },
        {
          "name": "Yonggang Zhang",
          "authorId": "2319178563"
        },
        {
          "name": "Yiu-ming Cheung",
          "authorId": "2319152002"
        },
        {
          "name": "Xinmei Tian",
          "authorId": "2257165685"
        },
        {
          "name": "Xu Shen",
          "authorId": "2319393988"
        },
        {
          "name": "Jieping Ye",
          "authorId": "2316672136"
        }
      ],
      "year": 2024,
      "abstract": "Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remain mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (<5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.",
      "citationCount": 36,
      "doi": "10.48550/arXiv.2409.01659",
      "arxivId": "2409.01659",
      "url": "https://www.semanticscholar.org/paper/da5d2425cdef713b105ceb92cd86e9528aa079d5",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2409.01659"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "53ea798a5d5ac0060cbcd2044aa19188c629fb40",
      "title": "TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Yiran Zhang",
          "authorId": "2364802948"
        },
        {
          "name": "Mo Wang",
          "authorId": "2364830886"
        },
        {
          "name": "Xiaoyang Li",
          "authorId": "2365323266"
        },
        {
          "name": "Kaixuan Ren",
          "authorId": "2364746924"
        },
        {
          "name": "Chencheng Zhu",
          "authorId": "2364632591"
        },
        {
          "name": "Usman Naseem",
          "authorId": "2363576119"
        }
      ],
      "year": 2025,
      "abstract": "Despite impressive advances in large language models (LLMs), existing benchmarks often focus on single-turn or single-step tasks, failing to capture the kind of iterative reasoning required in real-world settings. To address this limitation, we introduce TurnBench, a novel benchmark that evaluates multi-turn, multi-step reasoning through an interactive code-breaking task inspired by the\"Turing Machine Board Game.\"In each episode, a model must uncover hidden logical or arithmetic rules by making sequential guesses, receiving structured feedback, and integrating clues across multiple rounds. This dynamic setup requires models to reason over time, adapt based on past information, and maintain consistency across steps-capabilities underexplored in current benchmarks. TurnBench includes two modes: Classic, which tests standard reasoning, and Nightmare, which introduces increased complexity and requires robust inferential chains. To support fine-grained analysis, we provide ground-truth annotations for intermediate reasoning steps. Our evaluation of state-of-the-art LLMs reveals significant gaps: the best model achieves 84% accuracy in Classic mode, but performance drops to 18% in Nightmare mode. In contrast, human participants achieve 100% in both, underscoring the challenge TurnBench poses to current models. By incorporating feedback loops and hiding task rules, TurnBench reduces contamination risks and provides a rigorous testbed for diagnosing and advancing multi-step, multi-turn reasoning in LLMs.",
      "citationCount": 5,
      "doi": "10.18653/v1/2025.findings-emnlp.1084",
      "arxivId": "2506.01341",
      "url": "https://www.semanticscholar.org/paper/53ea798a5d5ac0060cbcd2044aa19188c629fb40",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.01341"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "35b9dbb9fa0c0d21a2a8c1a237754eeb58ed4ef2",
      "title": "On Representational Dissociation of Language and Arithmetic in Large Language Models",
      "authors": [
        {
          "name": "Riku Kisako",
          "authorId": "2345922495"
        },
        {
          "name": "Tatsuki Kuribayashi",
          "authorId": "83446147"
        },
        {
          "name": "Ryohei Sasano",
          "authorId": "2375541979"
        }
      ],
      "year": 2025,
      "abstract": "The association between language and (non-linguistic) thinking ability in humans has long been debated, and recently, neuroscientific evidence of brain activity patterns has been considered. Such a scientific context naturally raises an interdisciplinary question -- what about such a language-thought dissociation in large language models (LLMs)? In this paper, as an initial foray, we explore this question by focusing on simple arithmetic skills (e.g., $1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in LLMs' representation space. Our experiments with linear classifiers and cluster separability tests demonstrate that simple arithmetic equations and general language input are encoded in completely separated regions in LLMs' internal representation space across all the layers, which is also supported with more controlled stimuli (e.g., spelled-out equations). These tentatively suggest that arithmetic reasoning is mapped into a distinct region from general language input, which is in line with the neuroscientific observations of human brain activations, while we also point out their somewhat cognitively implausible geometric properties.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2502.11932",
      "arxivId": "2502.11932",
      "url": "https://www.semanticscholar.org/paper/35b9dbb9fa0c0d21a2a8c1a237754eeb58ed4ef2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.11932"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c13d100ccb7a759f7c31d9f369b775e3b7de5652",
      "title": "Probing for Arithmetic Errors in Language Models",
      "authors": [
        {
          "name": "Yucheng Sun",
          "authorId": "2373564638"
        },
        {
          "name": "Alessandro Stolfo",
          "authorId": "2175480389"
        },
        {
          "name": "Mrinmaya Sachan",
          "authorId": "2790926"
        }
      ],
      "year": 2025,
      "abstract": "We investigate whether internal activations in language models can be used to detect arithmetic errors. Starting with a controlled setting of 3-digit addition, we show that simple probes can accurately decode both the model's predicted output and the correct answer from hidden states, regardless of whether the model's output is correct. Building on this, we train lightweight error detectors that predict model correctness with over 90% accuracy. We then extend our analysis to structured chain-of-thought traces on addition-only GSM8K problems and find that probes trained on simple arithmetic generalize well to this more complex setting, revealing consistent internal representations. Finally, we demonstrate that these probes can guide selective re-prompting of erroneous reasoning steps, improving task accuracy with minimal disruption to correct outputs. Our findings suggest that arithmetic errors can be anticipated from internal activations alone, and that simple probes offer a viable path toward lightweight model self-correction.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2507.12379",
      "arxivId": "2507.12379",
      "url": "https://www.semanticscholar.org/paper/c13d100ccb7a759f7c31d9f369b775e3b7de5652",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.12379"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    }
  ],
  "count": 20,
  "errors": []
}
