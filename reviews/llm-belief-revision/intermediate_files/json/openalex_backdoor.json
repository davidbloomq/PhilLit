{
  "status": "success",
  "source": "openalex",
  "query": "backdoor attacks neural networks",
  "results": [
    {
      "openalex_id": "W3107337211",
      "doi": "10.1007/978-3-030-58607-2_11",
      "title": "Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks",
      "authors": [
        {
          "name": "Yunfei Liu",
          "openalex_id": "A5100704143",
          "orcid": "https://orcid.org/0000-0001-6898-0058",
          "institutions": [
            "Beihang University"
          ]
        },
        {
          "name": "Xingjun Ma",
          "openalex_id": "A5078711649",
          "orcid": "https://orcid.org/0000-0003-2099-4973",
          "institutions": [
            "Deakin University"
          ]
        },
        {
          "name": "James Bailey",
          "openalex_id": "A5101609697",
          "orcid": "https://orcid.org/0000-0002-3769-3811",
          "institutions": [
            "University of Melbourne"
          ]
        },
        {
          "name": "Feng Lu",
          "openalex_id": "A5050527785",
          "orcid": "https://orcid.org/0000-0001-9064-7964",
          "institutions": [
            "Beihang University",
            "Peng Cheng Laboratory"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": null,
      "cited_by_count": 414,
      "type": "book-chapter",
      "source": {
        "name": "Lecture notes in computer science",
        "type": "book series",
        "issn": [
          "0302-9743",
          "1611-3349"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Domain Adaptation and Few-Shot Learning",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W3107337211"
    },
    {
      "openalex_id": "W3167334189",
      "doi": "10.1145/3450569.3463560",
      "title": "Backdoor Attacks to Graph Neural Networks",
      "authors": [
        {
          "name": "Zaixi Zhang",
          "openalex_id": "A5063733770",
          "orcid": "https://orcid.org/0000-0002-0380-6558",
          "institutions": [
            "Duke University"
          ]
        },
        {
          "name": "Jinyuan Jia",
          "openalex_id": "A5087464080",
          "orcid": "https://orcid.org/0000-0002-7772-4766",
          "institutions": [
            "Duke University"
          ]
        },
        {
          "name": "Binghui Wang",
          "openalex_id": "A5101789833",
          "orcid": "https://orcid.org/0000-0001-5616-060X",
          "institutions": [
            "Duke University"
          ]
        },
        {
          "name": "Neil Zhenqiang Gong",
          "openalex_id": "A5009102659",
          "orcid": "https://orcid.org/0000-0002-9900-9309",
          "institutions": [
            "Duke University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-06-11",
      "abstract": "In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a subgraph based backdoor attack to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.",
      "cited_by_count": 187,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3450569.3463560"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Graph Neural Networks",
        "Terrorism, Counterterrorism, and Political Violence"
      ],
      "referenced_works_count": 55,
      "url": "https://openalex.org/W3167334189"
    },
    {
      "openalex_id": "W3083185154",
      "doi": "10.1109/tdsc.2020.3021407",
      "title": "Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization",
      "authors": [
        {
          "name": "Shaofeng Li",
          "openalex_id": "A5101513174",
          "orcid": "https://orcid.org/0000-0002-1491-4319",
          "institutions": [
            "Shanghai Jiao Tong University"
          ]
        },
        {
          "name": "Minhui Xue",
          "openalex_id": "A5009850797",
          "orcid": "https://orcid.org/0000-0002-9172-4252",
          "institutions": [
            "University of Adelaide"
          ]
        },
        {
          "name": "Benjamin Zi Hao Zhao",
          "openalex_id": "A5016670591",
          "orcid": "https://orcid.org/0000-0002-2774-2675",
          "institutions": [
            "University of Adelaide"
          ]
        },
        {
          "name": "Haojin Zhu",
          "openalex_id": "A5039106671",
          "orcid": "https://orcid.org/0000-0001-5079-4556",
          "institutions": [
            "UNSW Sydney"
          ]
        },
        {
          "name": "Xinpeng Zhang",
          "openalex_id": "A5071724015",
          "orcid": "https://orcid.org/0000-0001-5867-1315",
          "institutions": [
            "Data61",
            "Commonwealth Scientific and Industrial Research Organisation"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": "Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks, where hidden features (patterns) trained to a normal model, which is only activated by some specific input (called triggers), trick the model into producing unexpected behavior. In this article, we create covert and scattered triggers for backdoor attacks, invisible backdoors, where triggers can fool both DNN models and human inspection. We apply our invisible backdoors through two state-of-the-art methods of embedding triggers for backdoor attacks. The first approach on Badnets embeds the trigger into DNNs through steganography. The second approach of a trojan attack uses two types of additional regularization terms to generate the triggers with irregular shape and size. We use the Attack Success Rate and Functionality to measure the performance of our attacks. We introduce two novel definitions of invisibility for human perception; one is conceptualized by the Perceptual Adversarial Similarity Score (PASS) and the other is Learned Perceptual Image Patch Similarity (LPIPS). We show that the proposed invisible backdoors can be fairly effective across various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100, and GTSRB, by measuring their attack success rates for the adversary, functionality for the normal users, and invisibility scores for the administrators. We finally argue that the proposed invisible backdoor attacks can effectively thwart the state-of-the-art trojan backdoor detection approaches.",
      "cited_by_count": 261,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Dependable and Secure Computing",
        "type": "journal",
        "issn": [
          "1545-5971",
          "1941-0018",
          "2160-9209"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications",
        "Domain Adaptation and Few-Shot Learning"
      ],
      "referenced_works_count": 80,
      "url": "https://openalex.org/W3083185154"
    },
    {
      "openalex_id": "W3106646114",
      "doi": "10.1145/3372297.3423362",
      "title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features",
      "authors": [
        {
          "name": "Junyu Lin",
          "openalex_id": "A5101737739",
          "orcid": "https://orcid.org/0000-0002-5555-3935",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Lei Xu",
          "openalex_id": "A5100781025",
          "orcid": "https://orcid.org/0000-0001-6435-6055",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Yingqi Liu",
          "openalex_id": "A5080886129",
          "orcid": "https://orcid.org/0000-0002-8312-0088",
          "institutions": [
            "Purdue University West Lafayette"
          ]
        },
        {
          "name": "Xiangyu Zhang",
          "openalex_id": "A5107249133",
          "orcid": "https://orcid.org/0000-0002-9544-2500",
          "institutions": [
            "Purdue University West Lafayette"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-10-30",
      "abstract": "With the prevalent use of Deep Neural Networks (DNNs) in many applications, security of these networks is of importance. Pre-trained DNNs may contain backdoors that are injected through poisoned training. These trojaned models perform well when regular inputs are provided, but misclassify to a target output label when the input is stamped with a unique pattern called trojan trigger. Recently various backdoor detection and mitigation systems for DNN based AI applications have been proposed. However, many of them are limited to trojan attacks that require a specific patch trigger. In this paper, we introduce composite attack, a more flexible and stealthy trojan attack that eludes backdoor scanners using trojan triggers composed from existing benign features of multiple labels. We show that a neural network with a composed backdoor can achieve accuracy comparable to its original version on benign data and misclassifies when the composite trigger is present in the input. Our experiments on 7 different tasks show that this attack poses a severe threat. We evaluate our attack with two state-of-the-art backdoor scanners. The results show none of the injected backdoors can be detected by either scanner. We also study in details why the scanners are not effective. In the end, we discuss the essence of our attack and propose possible defense.",
      "cited_by_count": 186,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications",
        "Advanced Neural Network Applications"
      ],
      "referenced_works_count": 40,
      "url": "https://openalex.org/W3106646114"
    },
    {
      "openalex_id": "W3174908416",
      "doi": "10.1145/3468218.3469046",
      "title": "Explainability-based Backdoor Attacks Against Graph Neural Networks",
      "authors": [
        {
          "name": "Jing Xu",
          "openalex_id": "A5041333761",
          "orcid": "https://orcid.org/0000-0002-9900-4081",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Minhui Xue",
          "openalex_id": "A5009850797",
          "orcid": "https://orcid.org/0000-0002-9172-4252",
          "institutions": [
            "University of Adelaide"
          ]
        },
        {
          "name": "Stjepan Picek",
          "openalex_id": "A5024072796",
          "orcid": "https://orcid.org/0000-0001-7509-4337",
          "institutions": [
            "Delft University of Technology"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-06-21",
      "abstract": "&lt;p&gt;Backdoor attacks represent a serious threat to neural network models. A backdoored model will misclassify the trigger-embedded inputs into an attacker-chosen target label while performing normally on other benign inputs. There are already numerous works on backdoor attacks on neural networks, but only a few works consider graph neural networks (GNNs). As such, there is no intensive research on explaining the impact of trigger injecting position on the performance of backdoor attacks on GNNs. To bridge this gap, we conduct an experimental investigation on the performance of backdoor attacks on GNNs. We apply two powerful GNN explainability approaches to select the optimal trigger injecting position to achieve two attacker objectives - high attack success rate and low clean accuracy drop. Our empirical results on benchmark datasets and state-of-the-art neural network models demonstrate the proposed method's effectiveness in selecting trigger injecting position for backdoor attacks on GNNs. For instance, on the node classification task, the backdoor attack with trigger injecting position selected by GraphLIME reaches over 84% attack success rate with less than 2.5% accuracy drop. &lt;/p&gt;",
      "cited_by_count": 68,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3468218.3469046"
      },
      "topics": [
        "Advanced Graph Neural Networks",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 18,
      "url": "https://openalex.org/W3174908416"
    },
    {
      "openalex_id": "W4323066547",
      "doi": "10.1145/3543507.3583392",
      "title": "Unnoticeable Backdoor Attacks on Graph Neural Networks",
      "authors": [
        {
          "name": "Enyan Dai",
          "openalex_id": "A5091395218",
          "orcid": "https://orcid.org/0000-0001-9715-0280",
          "institutions": [
            "Pennsylvania State University"
          ]
        },
        {
          "name": "Minhua Lin",
          "openalex_id": "A5039804358",
          "orcid": "https://orcid.org/0000-0003-1591-7172",
          "institutions": [
            "Pennsylvania State University"
          ]
        },
        {
          "name": "X. D. Zhang",
          "openalex_id": "A5060725887",
          "orcid": "https://orcid.org/0000-0003-0940-6595",
          "institutions": [
            "Pennsylvania State University"
          ]
        },
        {
          "name": "Suhang Wang",
          "openalex_id": "A5011048500",
          "orcid": "https://orcid.org/0000-0003-3448-4878",
          "institutions": [
            "Pennsylvania State University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-26",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising results in various tasks\\nsuch as node classification and graph classification. Recent studies find that\\nGNNs are vulnerable to adversarial attacks. However, effective backdoor attacks\\non graphs are still an open problem. In particular, backdoor attack poisons the\\ngraph by attaching triggers and the target class label to a set of nodes in the\\ntraining graph. The backdoored GNNs trained on the poisoned graph will then be\\nmisled to predict test nodes to target class once attached with triggers.\\nThough there are some initial efforts in graph backdoor attacks, our empirical\\nanalysis shows that they may require a large attack budget for effective\\nbackdoor attacks and the injected triggers can be easily detected and pruned.\\nTherefore, in this paper, we study a novel problem of unnoticeable graph\\nbackdoor attacks with limited attack budget. To fully utilize the attack\\nbudget, we propose to deliberately select the nodes to inject triggers and\\ntarget class labels in the poisoning phase. An adaptive trigger generator is\\ndeployed to obtain effective triggers that are difficult to be noticed.\\nExtensive experiments on real-world datasets against various defense strategies\\ndemonstrate the effectiveness of our proposed method in conducting effective\\nunnoticeable backdoor attacks.\\n",
      "cited_by_count": 58,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2303.01263"
      },
      "topics": [
        "Advanced Graph Neural Networks",
        "Adversarial Robustness in Machine Learning",
        "Domain Adaptation and Few-Shot Learning"
      ],
      "referenced_works_count": 52,
      "url": "https://openalex.org/W4323066547"
    },
    {
      "openalex_id": "W3212158867",
      "doi": "10.1109/ojsp.2022.3190213",
      "title": "An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences",
      "authors": [
        {
          "name": "Wei Guo",
          "openalex_id": "A5030603717",
          "orcid": "https://orcid.org/0000-0002-6224-0953",
          "institutions": [
            "ORCID",
            "University of Siena"
          ]
        },
        {
          "name": "Benedetta Tondi",
          "openalex_id": "A5026139768",
          "orcid": "https://orcid.org/0000-0002-7518-046X",
          "institutions": [
            "ORCID",
            "University of Siena"
          ]
        },
        {
          "name": "Mauro Barni",
          "openalex_id": "A5007836692",
          "orcid": "https://orcid.org/0000-0002-7368-0866",
          "institutions": [
            "University of Siena",
            "ORCID"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "Together with impressive advances touching every aspect of our society, AI technology based on Deep Neural Networks (DNN) is bringing increasing security concerns. While attacks operating at test time have monopolised the initial attention of researchers, backdoor attacks, exploiting the possibility of corrupting DNN models by interfering with the training process, represent a further serious threat undermining the dependability of AI techniques. In backdoor attacks, the attacker corrupts the training data to induce an erroneous behaviour at test time. Test-time errors, however, are activated only in the presence of a triggering event. In this way, the corrupted network continues to work as expected for regular inputs, and the malicious behaviour occurs only when the attacker decides to activate the backdoor hidden within the network. Recently, backdoor attacks have been an intense research domain focusing on both the development of new classes of attacks, and the proposal of possible countermeasures. The goal of this overview is to review the works published until now, classifying the different types of attacks and defences proposed so far. The classification guiding the analysis is based on the amount of control that the attacker has on the training process, and the capability of the defender to verify the integrity of the data used for training, and to monitor the operations of the DNN at training and test time. Hence, the proposed analysis is suited to highlight the strengths and weaknesses of both attacks and defences with reference to the application scenarios they are operating in.",
      "cited_by_count": 71,
      "type": "article",
      "source": {
        "name": "IEEE Open Journal of Signal Processing",
        "type": "journal",
        "issn": [
          "2644-1322"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://ieeexplore.ieee.org/ielx7/8782710/9006934/09827581.pdf"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Network Security and Intrusion Detection",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 214,
      "url": "https://openalex.org/W3212158867"
    },
    {
      "openalex_id": "W3155871095",
      "doi": "10.1109/infocom42981.2021.9488902",
      "title": "Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks",
      "authors": [
        {
          "name": "Rui Ning",
          "openalex_id": "A5061056153",
          "orcid": "https://orcid.org/0000-0003-4050-6252"
        },
        {
          "name": "Li Jiang",
          "openalex_id": "A5100392353",
          "orcid": "https://orcid.org/0000-0001-7058-6957",
          "institutions": [
            "Old Dominion University"
          ]
        },
        {
          "name": "Chunsheng Xin",
          "openalex_id": "A5074327315",
          "orcid": "https://orcid.org/0000-0001-5575-2849",
          "institutions": [
            "Old Dominion University"
          ]
        },
        {
          "name": "Hongyi Wu",
          "openalex_id": "A5061078522",
          "orcid": "https://orcid.org/0000-0001-8596-1342",
          "institutions": [
            "Old Dominion University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-05-10",
      "abstract": "This paper reports a new clean-label data poisoning backdoor attack, named Invisible Poison, which stealthily and aggressively plants a backdoor in neural networks. It converts a regular trigger to a noised trigger that can be easily concealed inside images for training NN, with the objective to plant a backdoor that can be later activated by the trigger. Compared with existing data poisoning backdoor attacks, this newfound attack has the following distinct properties. First, it is a blackbox attack, requiring zero-knowledge of the target model. Second, this attack utilizes \"invisible poison\" to achieve stealthiness where the trigger is disguised as `noise', and thus can easily evade human inspection. On the other hand, this noised trigger remains effective in the feature space to poison training data. Third, the attack is practical and aggressive. A backdoor can be effectively planted with a small amount of poisoned data and is robust to most data augmentation methods during training. The attack is fully tested on multiple benchmark datasets including MNIST, Cifar10, and ImageNet10, as well as application specific data sets such as Yahoo Adblocker and GTSRB. Two countermeasures, namely Supervised and Unsupervised Poison Sample Detection, are introduced to defend the attack.",
      "cited_by_count": 49,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications",
        "Advanced Neural Network Applications"
      ],
      "referenced_works_count": 53,
      "url": "https://openalex.org/W3155871095"
    },
    {
      "openalex_id": "W3008191164",
      "doi": "10.48550/arxiv.2002.12162",
      "title": "Defending against Backdoor Attack on Deep Neural Networks",
      "authors": [
        {
          "name": "Hao Cheng",
          "openalex_id": "A5101511712",
          "orcid": "https://orcid.org/0000-0003-4823-0908"
        },
        {
          "name": "Kaidi Xu",
          "openalex_id": "A5102775611",
          "orcid": "https://orcid.org/0000-0003-4437-0671"
        },
        {
          "name": "Sijia Liu",
          "openalex_id": "A5100321835",
          "orcid": "https://orcid.org/0000-0001-9763-1164"
        },
        {
          "name": "Pin\u2010Yu Chen",
          "openalex_id": "A5050344371",
          "orcid": "https://orcid.org/0000-0003-1039-8369"
        },
        {
          "name": "Pu Zhao",
          "openalex_id": "A5073885088",
          "orcid": "https://orcid.org/0000-0001-5018-2859"
        },
        {
          "name": "Xue Lin",
          "openalex_id": "A5043582832",
          "orcid": "https://orcid.org/0000-0001-6210-8883"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-02-26",
      "abstract": "Although deep neural networks (DNNs) have achieved a great success in various computer vision tasks, it is recently found that they are vulnerable to adversarial attacks. In this paper, we focus on the so-called \\textit{backdoor attack}, which injects a backdoor trigger to a small portion of training data (also known as data poisoning) such that the trained DNN induces misclassification while facing examples with this trigger. To be specific, we carefully study the effect of both real and synthetic backdoor attacks on the internal response of vanilla and backdoored DNNs through the lens of Gard-CAM. Moreover, we show that the backdoor attack induces a significant bias in neuron activation in terms of the $\\ell_\\infty$ norm of an activation map compared to its $\\ell_1$ and $\\ell_2$ norm. Spurred by our results, we propose the \\textit{$\\ell_\\infty$-based neuron pruning} to remove the backdoor from the backdoored DNN. Experiments show that our method could effectively decrease the attack success rate, and also hold a high classification accuracy for clean images.",
      "cited_by_count": 37,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2002.12162"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications",
        "Advanced Malware Detection Techniques"
      ],
      "referenced_works_count": 23,
      "url": "https://openalex.org/W3008191164"
    },
    {
      "openalex_id": "W4367146727",
      "doi": "10.1109/tcss.2023.3267094",
      "title": "Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs",
      "authors": [
        {
          "name": "Haibin Zheng",
          "openalex_id": "A5072212265",
          "orcid": "https://orcid.org/0000-0002-8997-5343",
          "institutions": [
            "Zhejiang University of Technology"
          ]
        },
        {
          "name": "Haiyang Xiong",
          "openalex_id": "A5072130386",
          "institutions": [
            "Zhejiang University of Technology"
          ]
        },
        {
          "name": "Jinyin Chen",
          "openalex_id": "A5101758654",
          "orcid": "https://orcid.org/0000-0002-7153-2755",
          "institutions": [
            "Zhejiang University of Technology"
          ]
        },
        {
          "name": "Haonan Ma",
          "openalex_id": "A5057073497",
          "orcid": "https://orcid.org/0009-0002-7451-9166",
          "institutions": [
            "Zhejiang University of Technology"
          ]
        },
        {
          "name": "Guohan Huang",
          "openalex_id": "A5068404923",
          "orcid": "https://orcid.org/0000-0003-2850-6268",
          "institutions": [
            "Zhejiang University of Technology"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-26",
      "abstract": "Graph neural network (GNN) with a powerful representation capability has been widely applied to various areas. Recent works have exposed that GNN is vulnerable to the backdoor attack, i.e., models trained with maliciously crafted training samples are easily fooled by patched samples. Most of the proposed studies launch the backdoor attack using a trigger that is either the randomly generated subgraph [e.g., erd\u0151s-r\u00e9nyi backdoor (ER-B)] for less computational burden or the gradient-based generative subgraph [e.g., graph trojaning attack (GTA)] to enable a more effective attack. However, the interpretation of how is the trigger structure and the effect of the backdoor attack related has been overlooked in the current literature. Motifs, recurrent and statistically significant subgraphs in graphs, contain rich structure information. In this article, we are rethinking the trigger from the perspective of motifs and propose a motif-based backdoor attack, denoted as Motif-Backdoor. It contributes from three aspects. First, Interpretation, it provides an in-depth explanation for backdoor effectiveness by the validity of the trigger structure from motifs, leading to some novel insights, e.g., using subgraphs that appear less frequently in the graph as the trigger can achieve better attack performance. Second, Effectiveness, Motif-Backdoor reaches the state-of-the-art (SOTA) attack performance in both black-box and defensive scenarios. Third, Efficiency, based on the graph motif distribution, Motif-Backdoor can quickly obtain an effective trigger structure without target model feedback or subgraph model generation. Extensive experimental results show that Motif-Backdoor realizes the SOTA performance on three popular models and four public datasets compared with five baselines, e.g., Motif-Backdoor improves the attack success rate (ASR) by 14.73% compared with baselines on average. In addition, under a possible defense, Motif-Backdoor still implements satisfying performance, highlighting the requirement of defenses against backdoor attacks on GNNs. The datasets and code are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/Seaocn/Motif-Backdoor</uri> .",
      "cited_by_count": 34,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Computational Social Systems",
        "type": "journal",
        "issn": [
          "2329-924X",
          "2373-7476"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Advanced Graph Neural Networks",
        "Adversarial Robustness in Machine Learning",
        "Machine Learning in Materials Science"
      ],
      "referenced_works_count": 56,
      "url": "https://openalex.org/W4367146727"
    },
    {
      "openalex_id": "W3166022359",
      "doi": "10.1109/jsac.2021.3087237",
      "title": "Defense-Resistant Backdoor Attacks Against Deep Neural Networks in Outsourced Cloud Environment",
      "authors": [
        {
          "name": "Xueluan Gong",
          "openalex_id": "A5046712504",
          "orcid": "https://orcid.org/0000-0003-2190-8117",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Yanjiao Chen",
          "openalex_id": "A5015275781",
          "orcid": "https://orcid.org/0000-0002-1382-0679",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Qian Wang",
          "openalex_id": "A5100391116",
          "orcid": "https://orcid.org/0000-0002-8967-8525",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Huayang Huang",
          "openalex_id": "A5001692314",
          "orcid": "https://orcid.org/0000-0001-6267-9846",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Lingshuo Meng",
          "openalex_id": "A5103028194",
          "orcid": "https://orcid.org/0009-0002-7187-0153",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Chao Shen",
          "openalex_id": "A5101843177",
          "orcid": "https://orcid.org/0000-0003-2783-5529",
          "institutions": [
            "Xi'an Jiaotong University"
          ]
        },
        {
          "name": "Qian Zhang",
          "openalex_id": "A5114378267",
          "orcid": "https://orcid.org/0000-0001-9205-1881",
          "institutions": [
            "University of Hong Kong",
            "Hong Kong University of Science and Technology"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-06-09",
      "abstract": "The time and monetary costs of training sophisticated deep neural networks are exorbitant, which motivates resource-limited users to outsource the training process to the cloud. Concerning that an untrustworthy cloud service provider may inject backdoors to the returned model, the user can leverage state-of-the-art defense strategies to examine the model. In this paper, we aim to develop robust backdoor attacks (named RobNet) that can evade existing defense strategies from the standpoint of malicious cloud providers. The key rationale is to diversify the triggers and strengthen the model structure so that the backdoor is hard to be detected or removed. To attain this objective, we refine the trigger generation algorithm by selecting the neuron(s) with large weights and activations and then computing the triggers via gradient descent to maximize the value of the selected neuron(s). In stark contrast to existing works that fix the trigger location, we design a multi-location patching method to make the model less sensitive to mild displacement of triggers in real attacks. Furthermore, we extend the attack space by proposing multi-trigger backdoor attacks that can misclassify inputs with different triggers into the same or different target label(s). We evaluate the performance of RobNet on MNIST, GTSRB, and CIFAR-10 datasets, against four representative defense strategies Pruning, NeuralCleanse, Strip, and ABS. The comparison with two state-of-the-art baselines BadNets and Hidden Backdoors demonstrates that RobNet achieves higher attack success rate and is more resistant to potential defenses. \u00a9 1983-2012 IEEE.",
      "cited_by_count": 44,
      "type": "article",
      "source": {
        "name": "IEEE Journal on Selected Areas in Communications",
        "type": "journal",
        "issn": [
          "0733-8716",
          "1558-0008"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://ieeexplore.ieee.org/ielx7/49/9486979/09450029.pdf"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Neural Network Applications",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W3166022359"
    },
    {
      "openalex_id": "W3094933146",
      "doi": "10.1145/3411508.3421375",
      "title": "Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks",
      "authors": [
        {
          "name": "Kota Yoshida",
          "openalex_id": "A5063582139",
          "orcid": "https://orcid.org/0000-0003-1293-6415",
          "institutions": [
            "Ritsumeikan University"
          ]
        },
        {
          "name": "Takeshi Fujino",
          "openalex_id": "A5007179822",
          "orcid": "https://orcid.org/0000-0001-9441-3137",
          "institutions": [
            "Ritsumeikan University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-11-02",
      "abstract": "Backdoor attacks are poisoning attacks and serious threats to deep neural networks. When an adversary mixes poison data into a training dataset, the training dataset is called a poison training dataset. A model trained with the poison training dataset becomes a backdoor model and it achieves high stealthiness and attack-feasibility. The backdoor model classifies only a poison image into an adversarial target class and other images into the correct classes. We propose an additional procedure to our previously proposed countermeasure against backdoor attacks by using knowledge distillation. Our procedure removes poison data from a poison training dataset and recovers the accuracy of the distillation model. Our countermeasure differs from previous ones in that it does not require detecting and identifying backdoor models, backdoor neurons, and poison data. A characteristic assumption in our defense scenario is that the defender can collect clean images without labels. A defender distills clean knowledge from a backdoor model (teacher model) to a distillation model (student model) with knowledge distillation. Subsequently, the defender removes poison-data candidates from the poison training dataset by comparing the predictions of the backdoor and distillation models. The defender fine-tunes the distillation model with the detoxified training dataset to improve classification accuracy. We evaluated our countermeasure by using two datasets. The backdoor is disabled by distillation and fine-tuning further improves the classification accuracy of the distillation model. The fine-tuning model achieved comparable accuracy to a baseline model when the number of clean images for a distillation dataset was more than 13% of the training data. Our results indicate that our countermeasure can be applied for general image-classification tasks and that it works well whether the defender's received training dataset is a poison dataset or not.",
      "cited_by_count": 40,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 10,
      "url": "https://openalex.org/W3094933146"
    },
    {
      "openalex_id": "W3093748630",
      "doi": "10.1109/access.2020.3032411",
      "title": "Detecting Backdoor Attacks via Class Difference in Deep Neural Networks",
      "authors": [
        {
          "name": "Hyun Kwon",
          "openalex_id": "A5040423887",
          "orcid": "https://orcid.org/0000-0003-1169-9892",
          "institutions": [
            "Korea Military Academy"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": "A backdoor attack implies that deep neural networks misrecognize data that have a specific trigger by additionally training the malicious training data, including the specific trigger to the deep neural network model. In this method, the deep neural network correctly recognizes normal data without triggers, but the network misrecognizes data containing a specific trigger as a target class chosen by the attacker. In this paper, I propose a defense method against backdoor attacks using a detection model. This method detects the backdoor sample by comparing the output result of the target model with that of the model that trained the original secure training dataset. This is a defense method without trigger reverse or access to the entire training dataset. As an experimental environment, I used the Tensorflow machine-learning library, MNIST, and Fashion-MNIST as datasets. The results show that when the partial training data for the detection model are 200, the proposed method showed detection rates of 70.1% and 74.4% for the backdoor samples in MNIST and Fashion-MNIST, respectively.",
      "cited_by_count": 34,
      "type": "article",
      "source": {
        "name": "IEEE Access",
        "type": "journal",
        "issn": [
          "2169-3536"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09233317.pdf"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications",
        "Advanced Malware Detection Techniques"
      ],
      "referenced_works_count": 30,
      "url": "https://openalex.org/W3093748630"
    },
    {
      "openalex_id": "W4206244849",
      "doi": "10.1007/s11042-021-11135-0",
      "title": "BlindNet backdoor: Attack on deep neural network using blind watermark",
      "authors": [
        {
          "name": "Hyun Kwon",
          "openalex_id": "A5040423887",
          "orcid": "https://orcid.org/0000-0003-1169-9892",
          "institutions": [
            "Korea Military Academy"
          ]
        },
        {
          "name": "Yongchul Kim",
          "openalex_id": "A5102001083",
          "orcid": "https://orcid.org/0000-0003-1393-8711",
          "institutions": [
            "Korea Military Academy"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-07",
      "abstract": null,
      "cited_by_count": 36,
      "type": "article",
      "source": {
        "name": "Multimedia Tools and Applications",
        "type": "journal",
        "issn": [
          "1380-7501",
          "1573-7721"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Digital Media Forensic Detection",
        "Advanced Malware Detection Techniques"
      ],
      "referenced_works_count": 31,
      "url": "https://openalex.org/W4206244849"
    },
    {
      "openalex_id": "W3039176595",
      "doi": "10.48550/arxiv.2007.02343",
      "title": "Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks",
      "authors": [
        {
          "name": "Yunfei Liu",
          "openalex_id": "A5100704143",
          "orcid": "https://orcid.org/0000-0001-6898-0058",
          "institutions": [
            "Beihang University"
          ]
        },
        {
          "name": "Xingjun Ma",
          "openalex_id": "A5078711649",
          "orcid": "https://orcid.org/0000-0003-2099-4973",
          "institutions": [
            "Beihang University"
          ]
        },
        {
          "name": "James Bailey",
          "openalex_id": "A5101609697",
          "orcid": "https://orcid.org/0000-0002-3769-3811",
          "institutions": [
            "Deakin University"
          ]
        },
        {
          "name": "Feng Lu",
          "openalex_id": "A5075632709",
          "orcid": "https://orcid.org/0000-0002-5236-2287",
          "institutions": [
            "University of Melbourne"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-07-05",
      "abstract": "Recent studies have shown that DNNs can be compromised by backdoor attacks crafted at training time. A backdoor attack installs a backdoor into the victim model by injecting a backdoor pattern into a small proportion of the training data. At test time, the victim model behaves normally on clean test data, yet consistently predicts a specific (likely incorrect) target class whenever the backdoor pattern is present in a test example. While existing backdoor attacks are effective, they are not stealthy. The modifications made on training data or labels are often suspicious and can be easily detected by simple data filtering or human inspection. In this paper, we present a new type of backdoor attack inspired by an important natural phenomenon: reflection. Using mathematical modeling of physical reflection models, we propose reflection backdoor (Refool) to plant reflections as backdoor into a victim model. We demonstrate on 3 computer vision tasks and 5 datasets that, Refool can attack state-of-the-art DNNs with high success rate, and is resistant to state-of-the-art backdoor defenses.",
      "cited_by_count": 38,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2007.02343"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Domain Adaptation and Few-Shot Learning",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 69,
      "url": "https://openalex.org/W3039176595"
    },
    {
      "openalex_id": "W3015815227",
      "doi": "10.1109/mnet.011.1900577",
      "title": "Backdoor Attacks and Defenses for Deep Neural Networks in Outsourced Cloud Environments",
      "authors": [
        {
          "name": "Yanjiao Chen",
          "openalex_id": "A5015275781",
          "orcid": "https://orcid.org/0000-0002-1382-0679",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Xueluan Gong",
          "openalex_id": "A5046712504",
          "orcid": "https://orcid.org/0000-0003-2190-8117",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Qian Wang",
          "openalex_id": "A5100391116",
          "orcid": "https://orcid.org/0000-0002-8967-8525",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Xing Di",
          "openalex_id": "A5013497247",
          "orcid": "https://orcid.org/0000-0001-7232-2330",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Huayang Huang",
          "openalex_id": "A5001692314",
          "orcid": "https://orcid.org/0000-0001-6267-9846",
          "institutions": [
            "Wuhan University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-08",
      "abstract": "Deep neural networks have achieved tremendous success in various fields, especially in recognition and classification applications. However, faced with the difficulty of training millions of parameters of such networks, many users outsource the training procedure of a specific prediction work to the powerful cloud servers that own abundant computation and storage resources. Although such outsourced training can significantly simplify and expedite the development circles, it also introduces many security risks. In recent years, a new type of attack, the so-called backdoor attack, has attracted much attention, where the attacker's goal is to create a maliciously deep neural network to make misclassification on the special inputs with the backdoor trigger. For its concealment, such attacks can potentially cause disastrous consequences. Subsequently, many defense mechanisms against this attack are also appearing. In this article, we conduct a retrospective review on the existing schemes of the backdoor attacks and defenses in outsourced cloud environments. According to the resources the adversary has, and whether the detection time is during run-time or not, we classify the attack and defense approaches into multiple categories. We present a detailed overview of each category, and we provide a comparison of these approaches and evaluate part of the attack schemes by the experiments. We also highlight various future research directions in this field. These views shed light on possible avenues for future research.",
      "cited_by_count": 34,
      "type": "article",
      "source": {
        "name": "IEEE Network",
        "type": "journal",
        "issn": [
          "0890-8044",
          "1558-156X"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Privacy-Preserving Technologies in Data",
        "Advanced Malware Detection Techniques"
      ],
      "referenced_works_count": 19,
      "url": "https://openalex.org/W3015815227"
    },
    {
      "openalex_id": "W4393132009",
      "doi": "10.1016/j.patcog.2024.110449",
      "title": "Multi-target label backdoor attacks on graph neural networks",
      "authors": [
        {
          "name": "Kaiyang Wang",
          "openalex_id": "A5102860174",
          "orcid": "https://orcid.org/0009-0009-4982-2893",
          "institutions": [
            "Sichuan University"
          ]
        },
        {
          "name": "Huaxin Deng",
          "openalex_id": "A5112889367",
          "institutions": [
            "Sichuan University"
          ]
        },
        {
          "name": "Yijia Xu",
          "openalex_id": "A5024278894",
          "orcid": "https://orcid.org/0000-0003-2843-4225",
          "institutions": [
            "Sichuan University"
          ]
        },
        {
          "name": "Zhonglin Liu",
          "openalex_id": "A5101660631",
          "orcid": "https://orcid.org/0000-0002-3239-5647",
          "institutions": [
            "Sichuan University"
          ]
        },
        {
          "name": "Yong Fang",
          "openalex_id": "A5100606641",
          "orcid": "https://orcid.org/0000-0003-2736-0615",
          "institutions": [
            "Sichuan University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-24",
      "abstract": null,
      "cited_by_count": 27,
      "type": "article",
      "source": {
        "name": "Pattern Recognition",
        "type": "journal",
        "issn": [
          "0031-3203",
          "1873-5142"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Advanced Graph Neural Networks",
        "Adversarial Robustness in Machine Learning",
        "Machine Learning in Materials Science"
      ],
      "referenced_works_count": 53,
      "url": "https://openalex.org/W4393132009"
    },
    {
      "openalex_id": "W4221155263",
      "doi": "10.1145/3564625.3567999",
      "title": "More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks",
      "authors": [
        {
          "name": "Jing Xu",
          "openalex_id": "A5041333761",
          "orcid": "https://orcid.org/0000-0002-9900-4081",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Rui Wang",
          "openalex_id": "A5100431156",
          "orcid": "https://orcid.org/0000-0001-8495-3631",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Stefanos Koffas",
          "openalex_id": "A5101725689",
          "orcid": "https://orcid.org/0000-0001-6543-4801",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Kaitai Liang",
          "openalex_id": "A5001485404",
          "orcid": "https://orcid.org/0000-0003-0262-7678",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Stjepan Picek",
          "openalex_id": "A5024072796",
          "orcid": "https://orcid.org/0000-0001-7509-4337",
          "institutions": [
            "Delft University of Technology",
            "Radboud University Nijmegen"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-12-03",
      "abstract": "Graph Neural Networks (GNNs) are a class of deep learning-based methods for processing graph domain information. GNNs have recently become a widely used graph analysis method due to their superior ability to learn representations for complex graph data. Due to privacy concerns and regulation restrictions, centralized GNNs can be difficult to apply to data-sensitive scenarios. Federated learning (FL) is an emerging technology developed for privacy-preserving settings when several parties need to train a shared global model collaboratively. Although several research works have applied FL to train GNNs (Federated GNNs), there is no research on their robustness to backdoor attacks.&lt;br/&gt;&lt;br/&gt;This paper bridges this gap by conducting two types of backdoor attacks in Federated GNNs: centralized backdoor attacks (CBA) and distributed backdoor attacks (DBA). Our experiments show that the DBA attack success rate is higher than CBA in almost all cases. For CBA, the attack success rate of all local triggers is similar to the global trigger, even if the training set of the adversarial party is embedded with the global trigger. To explore the properties of two backdoor attacks in Federated GNNs, we evaluate the attack performance for a different number of clients, trigger sizes, poisoning intensities, and trigger densities. Finally, we explore the robustness of DBA and CBA against two state-of-the-art defenses. We find that both attacks are robust against the investigated defenses, necessitating the need to consider backdoor attacks in Federated GNNs as a novel threat that requires custom defenses.",
      "cited_by_count": 27,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3564625.3567999"
      },
      "topics": [
        "Advanced Graph Neural Networks",
        "Privacy-Preserving Technologies in Data",
        "Access Control and Trust"
      ],
      "referenced_works_count": 17,
      "url": "https://openalex.org/W4221155263"
    },
    {
      "openalex_id": "W3215147048",
      "doi": "10.1049/cje.2021.00.126",
      "title": "Backdoor Attacks on Image Classification Models in Deep Neural Networks",
      "authors": [
        {
          "name": "Quanxin ZHANG",
          "openalex_id": "A5046779128",
          "institutions": [
            "Aerospace Technology Institute",
            "Beijing Institute of Technology"
          ]
        },
        {
          "name": "Wencong MA",
          "openalex_id": "A5102138147",
          "institutions": [
            "Beijing Institute of Technology",
            "Aerospace Technology Institute"
          ]
        },
        {
          "name": "Yajie Wang",
          "openalex_id": "A5100455413",
          "orcid": "https://orcid.org/0000-0002-0962-4464",
          "institutions": [
            "Aerospace Technology Institute",
            "Beijing Institute of Technology"
          ]
        },
        {
          "name": "Yaoyuan Zhang",
          "openalex_id": "A5047829642",
          "orcid": "https://orcid.org/0000-0003-3732-0115",
          "institutions": [
            "Aerospace Technology Institute",
            "Beijing Institute of Technology"
          ]
        },
        {
          "name": "Zhiwei Shi",
          "openalex_id": "A5101815702",
          "orcid": "https://orcid.org/0000-0002-9111-5342",
          "institutions": [
            "China Information Technology Security Evaluation Center"
          ]
        },
        {
          "name": "Yuanzhang Li",
          "openalex_id": "A5026287089",
          "orcid": "https://orcid.org/0000-0002-1931-366X",
          "institutions": [
            "Beijing Institute of Technology",
            "Aerospace Technology Institute"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-03-01",
      "abstract": "Deep neural network (DNN) is applied widely in many applications and achieves state-of-the-art performance. However, DNN lacks transparency and interpretability for users in structure. Attackers can use this feature to embed trojan horses in the DNN structure, such as inserting a backdoor into the DNN, so that DNN can learn both the normal main task and additional malicious tasks at the same time. Besides, DNN relies on data set for training. Attackers can tamper with training data to interfere with DNN training process, such as attaching a trigger on input data. Because of defects in DNN structure and data, the backdoor attack can be a serious threat to the security of DNN. The DNN attacked by backdoor performs well on benign inputs while it outputs an attacker-specified label on trigger attached inputs. Backdoor attack can be conducted in almost every stage of the machine learning pipeline. Although there are a few researches in the backdoor attack on image classification, a systematic review is still rare in this field. This paper is a comprehensive review of backdoor attacks. According to whether attackers have access to the training data, we divide various backdoor attacks into two types: poisoning-based attacks and non-poisoning-based attacks. We go through the details of each work in the timeline, discussing its contribution and deficiencies. We propose a detailed mathematical backdoor model to summary all kinds of backdoor attacks. In the end, we provide some insights about future studies.",
      "cited_by_count": 25,
      "type": "article",
      "source": {
        "name": "Chinese Journal of Electronics",
        "type": "journal",
        "issn": [
          "1022-4653",
          "2075-5597"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/cje.2021.00.126"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Malware Detection Techniques",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 49,
      "url": "https://openalex.org/W3215147048"
    },
    {
      "openalex_id": "W4223940398",
      "doi": "10.1016/j.cose.2022.102726",
      "title": "PTB: Robust physical backdoor attacks against deep neural networks in real world",
      "authors": [
        {
          "name": "Mingfu Xue",
          "openalex_id": "A5022068526",
          "orcid": "https://orcid.org/0000-0003-2408-503X",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Can He",
          "openalex_id": "A5102943054",
          "orcid": "https://orcid.org/0000-0002-7462-8402",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Yinghao Wu",
          "openalex_id": "A5024861014",
          "orcid": "https://orcid.org/0000-0003-1181-5670",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Shichang Sun",
          "openalex_id": "A5005224500",
          "orcid": "https://orcid.org/0000-0002-0802-8708",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Yushu Zhang",
          "openalex_id": "A5085698418",
          "orcid": "https://orcid.org/0000-0001-8183-8435",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Jian Wang",
          "openalex_id": "A5100370498",
          "orcid": "https://orcid.org/0000-0002-8376-5898",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Weiqiang Liu",
          "openalex_id": "A5023734605",
          "orcid": "https://orcid.org/0000-0001-8398-8648",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-04-15",
      "abstract": null,
      "cited_by_count": 32,
      "type": "article",
      "source": {
        "name": "Computers & Security",
        "type": "journal",
        "issn": [
          "0167-4048",
          "1872-6208"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2104.07395"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Network Security and Intrusion Detection",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 84,
      "url": "https://openalex.org/W4223940398"
    },
    {
      "openalex_id": "W3014697957",
      "doi": "10.1587/transinf.2019edl8170",
      "title": "Multi-Targeted Backdoor: Indentifying Backdoor Attack for Multiple Deep Neural Networks",
      "authors": [
        {
          "name": "Hyun Kwon",
          "openalex_id": "A5040423887",
          "orcid": "https://orcid.org/0000-0003-1169-9892",
          "institutions": [
            "Korea Military Academy",
            "Korea Advanced Institute of Science and Technology"
          ]
        },
        {
          "name": "Hyunsoo Yoon",
          "openalex_id": "A5101921593",
          "orcid": "https://orcid.org/0000-0002-2671-6491",
          "institutions": [
            "Korea Advanced Institute of Science and Technology"
          ]
        },
        {
          "name": "Ki-Woong Park",
          "openalex_id": "A5037312953",
          "orcid": "https://orcid.org/0000-0002-3377-223X",
          "institutions": [
            "Sejong University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-03-31",
      "abstract": "We propose a multi-targeted backdoor that misleads different models to different classes. The method trains multiple models with data that include specific triggers that will be misclassified by different models into different classes. For example, an attacker can use a single multi-targeted backdoor sample to make model A recognize it as a stop sign, model B as a left-turn sign, model C as a right-turn sign, and model D as a U-turn sign. We used MNIST and Fashion-MNIST as experimental datasets and Tensorflow as a machine learning library. Experimental results show that the proposed method with a trigger can cause misclassification as different classes by different models with a 100% attack success rate on MNIST and Fashion-MNIST while maintaining the 97.18% and 91.1% accuracy, respectively, on data without a trigger.",
      "cited_by_count": 30,
      "type": "article",
      "source": {
        "name": "IEICE Transactions on Information and Systems",
        "type": "journal",
        "issn": [
          "0916-8532",
          "1745-1361"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://www.jstage.jst.go.jp/article/transinf/E103.D/4/E103.D_2019EDL8170/_pdf"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications",
        "Advanced Malware Detection Techniques"
      ],
      "referenced_works_count": 12,
      "url": "https://openalex.org/W3014697957"
    },
    {
      "openalex_id": "W4304945145",
      "doi": "10.1016/j.eswa.2022.118990",
      "title": "A defense method against backdoor attacks on neural networks",
      "authors": [
        {
          "name": "Sara Kaviani",
          "openalex_id": "A5110727347",
          "institutions": [
            "Dongguk University"
          ]
        },
        {
          "name": "Samaneh Shamshiri",
          "openalex_id": "A5054656260",
          "institutions": [
            "Dongguk University"
          ]
        },
        {
          "name": "Insoo Sohn",
          "openalex_id": "A5064247136",
          "orcid": "https://orcid.org/0000-0003-3616-6283",
          "institutions": [
            "Dongguk University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-10-13",
      "abstract": null,
      "cited_by_count": 20,
      "type": "article",
      "source": {
        "name": "Expert Systems with Applications",
        "type": "journal",
        "issn": [
          "0957-4174",
          "1873-6793"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications",
        "Network Security and Intrusion Detection"
      ],
      "referenced_works_count": 64,
      "url": "https://openalex.org/W4304945145"
    },
    {
      "openalex_id": "W4378189129",
      "doi": "10.1016/j.asoc.2023.110389",
      "title": "Universal backdoor attack on deep neural networks for malware detection",
      "authors": [
        {
          "name": "Yunchun Zhang",
          "openalex_id": "A5045752561",
          "orcid": "https://orcid.org/0000-0001-9738-4802",
          "institutions": [
            "Yunnan University"
          ]
        },
        {
          "name": "Fan Feng",
          "openalex_id": "A5072479287",
          "orcid": "https://orcid.org/0000-0003-1344-3727",
          "institutions": [
            "Yunnan University"
          ]
        },
        {
          "name": "Zikun Liao",
          "openalex_id": "A5009669605",
          "orcid": "https://orcid.org/0009-0009-7256-2540",
          "institutions": [
            "Yunnan University"
          ]
        },
        {
          "name": "Zixuan Li",
          "openalex_id": "A5100674834",
          "orcid": "https://orcid.org/0000-0003-0242-0984",
          "institutions": [
            "Yunnan University"
          ]
        },
        {
          "name": "Shaowen Yao",
          "openalex_id": "A5000436275",
          "orcid": "https://orcid.org/0000-0003-1516-4246",
          "institutions": [
            "Yunnan University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-25",
      "abstract": null,
      "cited_by_count": 19,
      "type": "article",
      "source": {
        "name": "Applied Soft Computing",
        "type": "journal",
        "issn": [
          "1568-4946",
          "1872-9681"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Malware Detection Techniques",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W4378189129"
    },
    {
      "openalex_id": "W3196546979",
      "doi": "10.1109/tcad.2021.3111123",
      "title": "Interpretability-Guided Defense Against Backdoor Attacks to Deep Neural Networks",
      "authors": [
        {
          "name": "Wei Jiang",
          "openalex_id": "A5101460603",
          "orcid": "https://orcid.org/0000-0001-6181-3900",
          "institutions": [
            "University of Electronic Science and Technology of China"
          ]
        },
        {
          "name": "Xiangyu Wen",
          "openalex_id": "A5054813950",
          "orcid": "https://orcid.org/0000-0002-7327-7786",
          "institutions": [
            "University of Electronic Science and Technology of China"
          ]
        },
        {
          "name": "Jinyu Zhan",
          "openalex_id": "A5082893987",
          "orcid": "https://orcid.org/0000-0002-0214-7124",
          "institutions": [
            "University of Electronic Science and Technology of China"
          ]
        },
        {
          "name": "Xupeng Wang",
          "openalex_id": "A5018927991",
          "orcid": "https://orcid.org/0000-0002-4160-8552",
          "institutions": [
            "University of Electronic Science and Technology of China"
          ]
        },
        {
          "name": "Ziwei Song",
          "openalex_id": "A5019374620",
          "orcid": "https://orcid.org/0000-0003-1782-0227",
          "institutions": [
            "University of Electronic Science and Technology of China"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-09-08",
      "abstract": "As an emerging threat to deep neural networks (DNNs), backdoor attacks have received increasing attentions due to the challenges posed by the lack of transparency inherent in DNNs. In this article, we develop an efficient algorithm from the interpretability of DNNs to defend against backdoor attacks to DNN models. To extract critical neurons, we deploy sets of control gates following neurons in layers, and the function of a DNN model can be interpreted as semantic sensitivities of neurons to input samples. A backdoor identification approach, derived from the activation frequency distribution on critical neurons, is proposed to reveal anomalies of particular neurons produced by backdoor attacks. Subsequently, a feasible and fine-grained pruning strategy is introduced to eliminate backdoors hidden in DNN models, without the need of retraining. Extensive experiments demonstrate that the proposed algorithm can identify and eliminate malicious backdoors efficiently in both single-target and multitarget scenarios with the performance of a DNN model retained to a large extent.",
      "cited_by_count": 21,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "type": "journal",
        "issn": [
          "0278-0070",
          "1937-4151"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Domain Adaptation and Few-Shot Learning",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 35,
      "url": "https://openalex.org/W3196546979"
    },
    {
      "openalex_id": "W3156790900",
      "doi": "10.1109/trustcom53373.2021.00093",
      "title": "Robust Backdoor Attacks against Deep Neural Networks in Real Physical World",
      "authors": [
        {
          "name": "Mingfu Xue",
          "openalex_id": "A5022068526",
          "orcid": "https://orcid.org/0000-0003-2408-503X",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Can He",
          "openalex_id": "A5102943054",
          "orcid": "https://orcid.org/0000-0002-7462-8402",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Shichang Sun",
          "openalex_id": "A5005224500",
          "orcid": "https://orcid.org/0000-0002-0802-8708",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Jian Wang",
          "openalex_id": "A5100370498",
          "orcid": "https://orcid.org/0000-0002-8376-5898",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Weiqiang Liu",
          "openalex_id": "A5023734605",
          "orcid": "https://orcid.org/0000-0001-8398-8648",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-10-01",
      "abstract": "Deep neural networks (DNN) have been widely deployed in various applications. However, many researches indicated that DNN is vulnerable to backdoor attacks. The attacker can create a hidden backdoor in target DNN model, and trigger the malicious behaviors by submitting specific backdoor instance. However, almost all the existing backdoor works focused on the digital domain, while few studies investigate the backdoor attacks in real physical world. Restricted to a variety of physical constraints, the performance of backdoor attacks in the real physical world will be severely degraded. In this paper, we propose a robust physical backdoor attack method, PTB (physical transformations for backdoors), to implement the backdoor attacks against deep learning models in the real physical world. Specifically, in the training phase, we perform a series of physical transformations on these injected backdoor instances at each round of model training, so as to simulate various transformations that a backdoor may experience in real world, thus improves its physical robustness. Experimental results on the state-of-the-art face recognition model show that, compared with the backdoor methods that without PTB, the proposed attack method can significantly improve the performance of backdoor attacks in real physical world. Under various complex physical conditions, by injecting only a very small ratio (0.5 %) of backdoor instances, the attack success rate of physical backdoor attacks with the PTB method on VGGFace is 82%, while the attack success rate of backdoor attacks without the proposed PTB method is lower than 11%. Meanwhile, the normal performance of the target DNN model has not been affected.",
      "cited_by_count": 22,
      "type": "article",
      "source": {
        "name": "2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Malware Detection Techniques",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 40,
      "url": "https://openalex.org/W3156790900"
    },
    {
      "openalex_id": "W4317796293",
      "doi": "10.1109/tdsc.2023.3239225",
      "title": "Kaleidoscope: Physical Backdoor Attacks Against Deep Neural Networks With RGB Filters",
      "authors": [
        {
          "name": "Xueluan Gong",
          "openalex_id": "A5046712504",
          "orcid": "https://orcid.org/0000-0003-2190-8117",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Ziyao Wang",
          "openalex_id": "A5100653215",
          "orcid": "https://orcid.org/0009-0004-7467-737X",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Yanjiao Chen",
          "openalex_id": "A5015275781",
          "orcid": "https://orcid.org/0000-0002-1382-0679",
          "institutions": [
            "Zhejiang University"
          ]
        },
        {
          "name": "Meng Xue",
          "openalex_id": "A5101986288",
          "orcid": "https://orcid.org/0000-0003-4745-6526",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Qian Wang",
          "openalex_id": "A5100391116",
          "orcid": "https://orcid.org/0000-0002-8967-8525",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Chao Shen",
          "openalex_id": "A5101843177",
          "orcid": "https://orcid.org/0000-0003-2783-5529",
          "institutions": [
            "Xi'an Jiaotong University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-23",
      "abstract": "Recent research has shown that deep neural networks are vulnerable to backdoor attacks. A carefully-designed backdoor trigger will mislead the victim model to misclassify any sample with the trigger to the target label. Nevertheless, existing works usually utilize visible triggers, such as a white square at the corner of the image, which are easily detected by human inspections. Current efforts on developing invisible triggers yield low attack success in the physical domain. In this paper, we propose Kaleidoscope, an RGB (red, green, and blue) filter-based backdoor attack method, which utilizes RGB filter operations as the backdoor trigger. To enhance the attack success rate, we design a novel model-dependent filter trigger generation algorithm. We also introduce two constraints in the loss function to make the backdoored samples more natural and less distorted. Extensive experiments on CIFAR-10, CIFAR-100, ImageNette, and VGG-Flower have demonstrated that RGB filter-processed samples not only achieve high attack success rate but also are unnoticeable to humans. It is shown that Kaleidoscope can reach an attack success rate of more than 84% in the physical world under different lighting intensities and shooting angles. Kaleidoscope is also shown to be robust to state-of-the-art backdoor defenses, such as spectral signature, STRIP, and MNTD.",
      "cited_by_count": 23,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Dependable and Secure Computing",
        "type": "journal",
        "issn": [
          "1545-5971",
          "1941-0018",
          "2160-9209"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Neural Network Applications",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 54,
      "url": "https://openalex.org/W4317796293"
    },
    {
      "openalex_id": "W3206218040",
      "doi": "10.1109/access.2021.3086529",
      "title": "Defending Deep Neural Networks Against Backdoor Attack by Using De-Trigger Autoencoder",
      "authors": [
        {
          "name": "Hyun Kwon",
          "openalex_id": "A5040423887",
          "orcid": "https://orcid.org/0000-0003-1169-9892",
          "institutions": [
            "Korea Military Academy"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-10-19",
      "abstract": "A backdoor attack is a method that causes misrecognition in a deep neural network by training it on additional data that have a specific trigger. The network will correctly recognize normal samples (which lack the specific trigger) as their proper classes but will misrecognize backdoor samples (which contain the trigger) as target classes. In this paper, I propose a method of defense against backdoor attacks that uses a de-trigger autoencoder. In the proposed scheme, the trigger in the backdoor sample is removed using the de-trigger autoencoder, and the backdoor sample is detected from the change in the classification result. Experiments were conducted using MNIST, Fashion-MNIST, and CIFAR-10 as the experimental datasets and TensorFlow as the machine learning library. For MNIST, Fashion-MNIST, and CIFAR-10, respectively, the proposed method detected 91.5%, 82.3%, and 90.9% of the backdoor samples and had 96.1%, 89.6%, and 91.2% accuracy on legitimate samples.",
      "cited_by_count": 22,
      "type": "article",
      "source": {
        "name": "IEEE Access",
        "type": "journal",
        "issn": [
          "2169-3536"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09579062.pdf"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Malware Detection Techniques",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 52,
      "url": "https://openalex.org/W3206218040"
    },
    {
      "openalex_id": "W4283205753",
      "doi": "10.1109/infocom48880.2022.9796878",
      "title": "TrojanFlow: A Neural Backdoor Attack to Deep Learning-based Network Traffic Classifiers",
      "authors": [
        {
          "name": "Rui Ning",
          "openalex_id": "A5102482644"
        },
        {
          "name": "Chunsheng Xin",
          "openalex_id": "A5074327315",
          "orcid": "https://orcid.org/0000-0001-5575-2849",
          "institutions": [
            "Old Dominion University"
          ]
        },
        {
          "name": "Hongyi Wu",
          "openalex_id": "A5061078522",
          "orcid": "https://orcid.org/0000-0001-8596-1342",
          "institutions": [
            "Old Dominion University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-05-02",
      "abstract": "While deep learning (DL)-based network traffic classification has demonstrated its success in a range of practical applications, such as network management and security control to just name a few, it is vulnerable to adversarial attacks. This paper reports TrojanFlow, a new and practical neural backdoor attack to DL-based network traffic classifiers. In contrast to traditional neural backdoor attacks where a designated and sample-agnostic trigger is used to plant backdoor, TrojanFlow poisons a model using dynamic and sample-specific triggers that are optimized to efficiently hijack the model. It features a unique design to jointly optimize the trigger generator with the target classifier during training. The trigger generator can thus craft optimized triggers based on the input sample to efficiently manipulate the model's prediction. A well-engineered prototype is developed using Pytorch to demonstrate TrojanFlow attacking multiple practical DL-based network traffic classifiers. Thorough analysis is conducted to gain insights into the effectiveness of TrojanFlow, revealing the fundamentals of why it is effective and what it does to efficiently hijack the model. Extensive experiments are carried out on the well-known ISCXVPN2016 dataset with three widely adopted DL network traffic classifier architectures. TrojanFlow is compared with two other backdoor attacks under five state-of-the-art backdoor defenses. The results show that the TrojanFlow attack is stealthy, efficient, and highly robust against existing neural backdoor mitigation schemes.",
      "cited_by_count": 20,
      "type": "article",
      "source": {
        "name": "IEEE INFOCOM 2022 - IEEE Conference on Computer Communications",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Internet Traffic Analysis and Secure E-voting",
        "Network Security and Intrusion Detection",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W4283205753"
    },
    {
      "openalex_id": "W3036721937",
      "doi": "10.48550/arxiv.2006.11165",
      "title": "Backdoor Attacks to Graph Neural Networks",
      "authors": [
        {
          "name": "Zaixi Zhang",
          "openalex_id": "A5063733770",
          "orcid": "https://orcid.org/0000-0002-0380-6558"
        },
        {
          "name": "Jinyuan Jia",
          "openalex_id": "A5087464080",
          "orcid": "https://orcid.org/0000-0002-7772-4766"
        },
        {
          "name": "Binghui Wang",
          "openalex_id": "A5101789833",
          "orcid": "https://orcid.org/0000-0001-5616-060X"
        },
        {
          "name": "Neil Zhenqiang Gong",
          "openalex_id": "A5009102659",
          "orcid": "https://orcid.org/0000-0002-9900-9309"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-06-19",
      "abstract": "In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a \\emph{subgraph based backdoor attack} to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.",
      "cited_by_count": 12,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2006.11165"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Graph Neural Networks",
        "Terrorism, Counterterrorism, and Political Violence"
      ],
      "referenced_works_count": 55,
      "url": "https://openalex.org/W3036721937"
    },
    {
      "openalex_id": "W4385412309",
      "doi": "10.1109/eurosp57164.2023.00072",
      "title": "Watermarking Graph Neural Networks based on Backdoor Attacks",
      "authors": [
        {
          "name": "Jing Xu",
          "openalex_id": "A5041333761",
          "orcid": "https://orcid.org/0000-0002-9900-4081",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Stefanos Koffas",
          "openalex_id": "A5101725689",
          "orcid": "https://orcid.org/0000-0001-6543-4801",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "O\u011fuzhan Ersoy",
          "openalex_id": "A5090399988",
          "orcid": "https://orcid.org/0000-0002-6428-4212",
          "institutions": [
            "Radboud University Nijmegen"
          ]
        },
        {
          "name": "Stjepan Picek",
          "openalex_id": "A5024072796",
          "orcid": "https://orcid.org/0000-0001-7509-4337",
          "institutions": [
            "Radboud University Nijmegen"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-01",
      "abstract": "&lt;p&gt;Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. Building a powerful GNN model is not a trivial task, as it requires a large amount of training data, powerful computing resources, and human expertise. Moreover, with the development of adversarial attacks, e.g., model stealing attacks, GNNs raise challenges to model authentication. To avoid copyright infringement on GNNs, verifying the ownership of the GNN models is necessary.This paper presents a watermarking framework for GNNs for both graph and node classification tasks. We 1) design two strategies to generate watermarked data for the graph classification task and one for the node classification task, 2) embed the watermark into the host model through training to obtain the watermarked GNN model, and 3) verify the ownership of the suspicious model in a black-box setting. The experiments show that our framework can verify the ownership of GNN models with a very high probability (up to 99%) for both tasks. We also explore our watermarking mechanism against an adaptive attacker with access to partial knowledge of the watermarked data. Finally, we experimentally show that our watermarking approach is robust against a state-of-the-art model extraction technique and four state-of-the-art defenses against backdoor attacks. &lt;/p&gt;",
      "cited_by_count": 17,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Advanced Graph Neural Networks",
        "Internet Traffic Analysis and Secure E-voting"
      ],
      "referenced_works_count": 73,
      "url": "https://openalex.org/W4385412309"
    }
  ],
  "count": 30,
  "errors": []
}
