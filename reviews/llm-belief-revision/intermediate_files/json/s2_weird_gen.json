{
  "status": "success",
  "source": "semantic_scholar",
  "query": "weird generalization fine-tuning LLM",
  "results": [
    {
      "paperId": "e198eceaf6e652181545e601cefd2a08e653e05d",
      "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization",
      "authors": [
        {
          "name": "Changsheng Wang",
          "authorId": "2355718070"
        },
        {
          "name": "Xin Chen",
          "authorId": "2290097811"
        },
        {
          "name": "Sijia Liu",
          "authorId": "2356000124"
        },
        {
          "name": "Ke Ding",
          "authorId": "2290075595"
        }
      ],
      "year": 2025,
      "abstract": "Adapting pretrained large language models (LLMs) to code domains via supervised fine-tuning (FT) has been commonly used for code generation. However, we identify a previously underappreciated failure mode, the memorization barrier, where strong memorization of downstream code data in the base model could trap optimization and prevent the standard FT from effectively acquiring new, generalizable code knowledge. To overcome this barrier, we propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which applies an IB penalty on hidden representations of the code data to compress spurious, memorized features while preserving task-relevant information. Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1) show that IB-FT substantially alleviates the memorization barrier, improves top-1 performance (Pass@$1$), and yields far more stable gains under the stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if at least $m$ of $k$ samples pass unit tests) compared with conventional FT.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.16022",
      "arxivId": "2510.16022",
      "url": "https://www.semanticscholar.org/paper/e198eceaf6e652181545e601cefd2a08e653e05d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.16022"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ec13de165371881269a68049b40b0528795b421e",
      "title": "Tc-llama 2: fine-tuning LLM for technology and commercialization applications",
      "authors": [
        {
          "name": "Jeyoon Yeom",
          "authorId": "2301098465"
        },
        {
          "name": "Hakyung Lee",
          "authorId": "2314517110"
        },
        {
          "name": "Hoyoon Byun",
          "authorId": "2184781278"
        },
        {
          "name": "Yewon Kim",
          "authorId": "2220588090"
        },
        {
          "name": "Jeongeun Byun",
          "authorId": "2314372035"
        },
        {
          "name": "Yunjeong Choi",
          "authorId": "2314514819"
        },
        {
          "name": "Sungjin Kim",
          "authorId": "2314523693"
        },
        {
          "name": "Kyungwoo Song",
          "authorId": "2314371440"
        }
      ],
      "year": 2024,
      "abstract": "This paper introduces TC-Llama 2, a novel application of large language models (LLMs) in the technology-commercialization field. Traditional methods in this field, reliant on statistical learning and expert knowledge, often face challenges in processing the complex and diverse nature of technology-commercialization data. TC-Llama 2 addresses these limitations by utilizing the advanced generalization capabilities of LLMs, specifically adapting them to this intricate domain. Our model, based on the open-source LLM framework, Llama 2, is customized through instruction tuning using bilingual Korean-English datasets. Our approach involves transforming technology-commercialization data into formats compatible with LLMs, enabling the model to learn detailed technological knowledge and product hierarchies effectively. We introduce a unique model evaluation strategy, leveraging new matching and generation tasks to verify the alignment of the technology-commercialization relationship in TC-Llama 2. Our results, derived from refining task-specific instructions for inference, provide valuable insights into customizing language models for specific sectors, potentially leading to new applications in technology categorization, utilization, and predictive product development.",
      "citationCount": 13,
      "doi": "10.1186/s40537-024-00963-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ec13de165371881269a68049b40b0528795b421e",
      "venue": "Journal of Big Data",
      "journal": {
        "name": "Journal of Big Data",
        "pages": "1-31",
        "volume": "11"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e76368389e5448ec4b82fe8c67f3490874ed7c4e",
      "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
      "authors": [
        {
          "name": "Yihan Wang",
          "authorId": "2108927851"
        },
        {
          "name": "Si Si",
          "authorId": "3422911"
        },
        {
          "name": "Daliang Li",
          "authorId": "2108467824"
        },
        {
          "name": "Michal Lukasik",
          "authorId": "2256999920"
        },
        {
          "name": "Felix X. Yu",
          "authorId": "2111880653"
        },
        {
          "name": "Cho-Jui Hsieh",
          "authorId": "1793529"
        },
        {
          "name": "I. Dhillon",
          "authorId": "1783667"
        },
        {
          "name": "Sanjiv Kumar",
          "authorId": "123864679"
        }
      ],
      "year": 2022,
      "abstract": "Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task.We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization.ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization. Experiments also show that ProMoT can improve the generalization performance of multi-task training.",
      "citationCount": 42,
      "doi": null,
      "arxivId": "2211.00635",
      "url": "https://www.semanticscholar.org/paper/e76368389e5448ec4b82fe8c67f3490874ed7c4e",
      "venue": "International Conference on Learning Representations",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ec873d166eb68554ac95a117d9bb74908e593e01",
      "title": "Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning",
      "authors": [
        {
          "name": "Helena Casademunt",
          "authorId": "2373042956"
        },
        {
          "name": "Caden Juang",
          "authorId": "2300368861"
        },
        {
          "name": "Adam Karvonen",
          "authorId": "2314115348"
        },
        {
          "name": "Samuel Marks",
          "authorId": "2225941937"
        },
        {
          "name": "Senthooran Rajamanoharan",
          "authorId": "35185194"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        }
      ],
      "year": 2025,
      "abstract": "Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2507.16795",
      "arxivId": "2507.16795",
      "url": "https://www.semanticscholar.org/paper/ec873d166eb68554ac95a117d9bb74908e593e01",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.16795"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5cc6e4cdac8b6b446aca53643fcba200225f9589",
      "title": "From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization",
      "authors": [
        {
          "name": "Suswitha Pericharla",
          "authorId": "2386989232"
        },
        {
          "name": "D. B. Hier",
          "authorId": "2250610298"
        },
        {
          "name": "Tayo Obafemi-Ajayi",
          "authorId": "1402964524"
        }
      ],
      "year": 2025,
      "abstract": "Effective biomedical data integration depends on automated term normalization, the mapping of natural language biomedical terms to standardized identifiers. This linking of terms to identifiers is essential for semantic interoperability. Large language models (LLMs) show promise for this task but perform unevenly across terminologies. We evaluated both memorization (training-term performance) and generalization (validation-term performance) across multiple biomedical ontologies. Fine-tuning Llama 3.1 8B revealed marked differences by terminology. GO mappings showed strong memorization gains (up to 77% improvement in term-to-identifier accuracy), whereas HPO showed minimal improvement. Generalization occurred only for protein-gene (GENE) mappings (13.9% gain), while fine-tuning for HPO and GO yielded negligible transfer. Baseline accuracy varied by model scale, with GPT-4o outperforming both Llama variants for all terminologies. Embedding analyses showed tight semantic alignment between gene symbols and protein names but weak alignment between terms and identifiers for GO or HPO, consistent with limited lexicalization. Fine-tuning success depended on two interacting factors: identifier popularity and lexicalization. Popular identifiers were more likely encountered during pretraining, enhancing memorization. Lexicalized identifiers, such as gene symbols, enabled semantic generalization. By contrast, arbitrary identifiers in GO and HPO constrained models to rote learning. These findings provide a predictive framework for when fine-tuning enhances factual recall versus when it fails due to sparse or non-lexicalized identifiers.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.19036",
      "arxivId": "2510.19036",
      "url": "https://www.semanticscholar.org/paper/5cc6e4cdac8b6b446aca53643fcba200225f9589",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.19036"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7ae48cdb8a044ccd22d279a01d135ecfd304d025",
      "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning",
      "authors": [
        {
          "name": "Chao-Chung Wu",
          "authorId": "2312799818"
        },
        {
          "name": "Zhi Rui Tam",
          "authorId": "2028219138"
        },
        {
          "name": "Chieh-Yen Lin",
          "authorId": "2306137538"
        },
        {
          "name": "Hung-yi Lee",
          "authorId": "2278588523"
        },
        {
          "name": "Yun-Nung Chen",
          "authorId": "2306102701"
        }
      ],
      "year": 2025,
      "abstract": "Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and three additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.",
      "citationCount": 4,
      "doi": null,
      "arxivId": "2501.14315",
      "url": "https://www.semanticscholar.org/paper/7ae48cdb8a044ccd22d279a01d135ecfd304d025",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "299af57e637095899ca2e2a97da9d13aab622ce6",
      "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
      "authors": [
        {
          "name": "Xinyue Zeng",
          "authorId": "2359897638"
        },
        {
          "name": "Haohui Wang",
          "authorId": "2155587513"
        },
        {
          "name": "Junhong Lin",
          "authorId": "2311427666"
        },
        {
          "name": "Jun Wu",
          "authorId": "2359758871"
        },
        {
          "name": "Tyler Cody",
          "authorId": "2359450667"
        },
        {
          "name": "Dawei Zhou",
          "authorId": "2313576252"
        }
      ],
      "year": 2025,
      "abstract": "The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a PAC-Bayesian Generalization Bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at LensLLM.io.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2505.03793",
      "arxivId": "2505.03793",
      "url": "https://www.semanticscholar.org/paper/299af57e637095899ca2e2a97da9d13aab622ce6",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.03793"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "b60a9a78caaf06fbdbf8ee91ed9416efa0e6c3c4",
      "title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
      "authors": [
        {
          "name": "Yifan Song",
          "authorId": "2183730942"
        },
        {
          "name": "Weimin Xiong",
          "authorId": "2211953037"
        },
        {
          "name": "Xiutian Zhao",
          "authorId": "2273531473"
        },
        {
          "name": "Dawei Zhu",
          "authorId": "2116276849"
        },
        {
          "name": "Wenhao Wu",
          "authorId": "2139644141"
        },
        {
          "name": "Ke Wang",
          "authorId": "2235070752"
        },
        {
          "name": "Cheng Li",
          "authorId": "2307396710"
        },
        {
          "name": "Wei Peng",
          "authorId": "2273549661"
        },
        {
          "name": "Sujian Li",
          "authorId": "2257095532"
        }
      ],
      "year": 2024,
      "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.",
      "citationCount": 30,
      "doi": "10.48550/arXiv.2410.07706",
      "arxivId": "2410.07706",
      "url": "https://www.semanticscholar.org/paper/b60a9a78caaf06fbdbf8ee91ed9416efa0e6c3c4",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "pages": "2124-2141"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "3a54754f255ab932783e1f1b3508cd038c0cfa2d",
      "title": "HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs",
      "authors": [
        {
          "name": "Majid Jaberi Douraki",
          "authorId": "2414099"
        },
        {
          "name": "Hossein Sholehrasa",
          "authorId": "2305918580"
        },
        {
          "name": "Xuan Xu",
          "authorId": "2248958100"
        },
        {
          "name": "Remya Ampadi Ramachandran",
          "authorId": "2242721126"
        }
      ],
      "year": 2025,
      "abstract": "The extraction and standardization of pharmacokinetic (PK) information from scientific literature remain significant challenges in computational pharmacology, which limits the reliability of data-driven models in drug development. Large language models (LLMs) have achieved remarkable progress in text understanding and reasoning, yet their adaptation to structured biomedical data, such as PK tables, remains constrained by heterogeneity, noise, and domain shift. To address these limitations, we propose HySim-LLM, a unified mathematical and computational framework that integrates embedding-weighted fine-tuning and manifold-aware denoising to enhance the robustness and interpretability of LLMs. We establish two theoretical results: (1) a similarity-weighted generalization bound that quantifies adaptation performance under embedding divergence, and (2) a manifold-based denoising guarantee that bounds loss contributions from noisy or off-manifold samples. These theorems provide a principled foundation for fine-tuning LLMs in structured biomedical settings. The framework offers a mathematically grounded pathway toward reliable and interpretable LLM adaptation for biomedical and data-intensive scientific domains.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2510.07796",
      "arxivId": "2510.07796",
      "url": "https://www.semanticscholar.org/paper/3a54754f255ab932783e1f1b3508cd038c0cfa2d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.07796"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "44fdc5ffb8f578a765d274a796dd0b19a8680707",
      "title": "Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models",
      "authors": [
        {
          "name": "Huan Song",
          "authorId": "2386944054"
        },
        {
          "name": "Deeksha Razdan",
          "authorId": "32671772"
        },
        {
          "name": "Y. Qian",
          "authorId": "1596820688"
        },
        {
          "name": "Arijit Ghosh Chowdhury",
          "authorId": "48372562"
        },
        {
          "name": "Parth Patwa",
          "authorId": "1579818535"
        },
        {
          "name": "Aman Chadha",
          "authorId": "2275226689"
        },
        {
          "name": "Shinan Zhang",
          "authorId": "2347364567"
        },
        {
          "name": "Sharlina Keshava",
          "authorId": "2111867828"
        },
        {
          "name": "Hannah Marlowe",
          "authorId": "2384444079"
        }
      ],
      "year": 2025,
      "abstract": "Small Language Models (SLMs) offer compelling advantages in deployment cost and latency, but their accuracy often lags behind larger models, particularly for complex domain-specific tasks. While supervised fine-tuning can help bridge this performance gap, it requires substantial manual effort in data preparation and iterative optimization. We present PaDA-Agent (Pattern-guided Data Augmentation Agent), an evaluation-driven approach that streamlines the data augmentation process for SLMs through coordinated operations. Unlike state-of-the-art approaches that focus on model training errors only and generating error-correcting samples, PaDA-Agent discovers failure patterns from the validation data via evaluations and drafts targeted data augmentation strategies aiming to directly reduce the generalization gap. Our experimental results demonstrate significant improvements over state-of-the-art LLM-based data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2510.18143",
      "arxivId": "2510.18143",
      "url": "https://www.semanticscholar.org/paper/44fdc5ffb8f578a765d274a796dd0b19a8680707",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.18143"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "cf21a22f83e3530dbb819b531780ba3e70246be2",
      "title": "Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks",
      "authors": [
        {
          "name": "Jack Youstra",
          "authorId": "2376538750"
        },
        {
          "name": "Mohammed Mahfoud",
          "authorId": "2346980549"
        },
        {
          "name": "Yang Yan",
          "authorId": "2377269502"
        },
        {
          "name": "Henry Sleight",
          "authorId": "2294563930"
        },
        {
          "name": "Ethan Perez",
          "authorId": "2312390675"
        },
        {
          "name": "Mrinank Sharma",
          "authorId": "2261097150"
        }
      ],
      "year": 2025,
      "abstract": "Large language model fine-tuning APIs enable widespread model customization, yet pose significant safety risks. Recent work shows that adversaries can exploit access to these APIs to bypass model safety mechanisms by encoding harmful content in seemingly harmless fine-tuning data, evading both human monitoring and standard content filters. We formalize the fine-tuning API defense problem, and introduce the Cipher Fine-tuning Robustness benchmark (CIFR), a benchmark for evaluating defense strategies'ability to retain model safety in the face of cipher-enabled attackers while achieving the desired level of fine-tuning functionality. We include diverse cipher encodings and families, with some kept exclusively in the test set to evaluate for generalization across unseen ciphers and cipher families. We then evaluate different defenses on the benchmark and train probe monitors on model internal activations from multiple fine-tunes. We show that probe monitors achieve over 99% detection accuracy, generalize to unseen cipher variants and families, and compare favorably to state-of-the-art monitoring approaches. We open-source CIFR and the code to reproduce our experiments to facilitate further research in this critical area. Code and data are available online https://github.com/JackYoustra/safe-finetuning-api",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2508.17158",
      "arxivId": "2508.17158",
      "url": "https://www.semanticscholar.org/paper/cf21a22f83e3530dbb819b531780ba3e70246be2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.17158"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6cb71721d40277d2bfcbf26891e540649a64eb6d",
      "title": "A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics",
      "authors": [
        {
          "name": "Yuxin Li",
          "authorId": "2395560346"
        },
        {
          "name": "Lorraine Xu",
          "authorId": "2395583142"
        },
        {
          "name": "Meng Fan Wang",
          "authorId": "2395684232"
        }
      ],
      "year": 2025,
      "abstract": "We propose a novel study on authorship attribution for Chinese lyrics, a domain where clean, public datasets are sorely lacking. Our contributions are twofold: (1) we create a new, balanced dataset of Chinese lyrics spanning multiple genres, and (2) we develop and fine-tune a domain-specific model, comparing its performance against zero-shot inference using the DeepSeek LLM. We test two central hypotheses. First, we hypothesize that a fine-tuned model will outperform a zero-shot LLM baseline. Second, we hypothesize that performance is genre-dependent. Our experiments strongly confirm Hypothesis 2: structured genres (e.g. Folklore&Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g. Love&Romance). Hypothesis 1 receives only partial support: fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres), but offers limited or ambiguous gains in Test2, a smaller, synthetically-augmented set. We show that the design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning. Our work establishes the first benchmark for cross-genre Chinese lyric attribution, highlights the importance of genre-sensitive evaluation, and provides a public dataset and analytical framework for future research. We conclude with recommendations: enlarge and diversify test sets, reduce reliance on token-level data augmentation, balance author representation across genres, and investigate domain-adaptive pretraining as a pathway for improved attribution performance.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.21930",
      "arxivId": "2511.21930",
      "url": "https://www.semanticscholar.org/paper/6cb71721d40277d2bfcbf26891e540649a64eb6d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.21930"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "762f1af16a95ca73d9007dba7aa324ce8022d054",
      "title": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning",
      "authors": [
        {
          "name": "Mohamed Hesham",
          "authorId": "2386997666"
        },
        {
          "name": "Ibrahim Abdalla",
          "authorId": "2386997675"
        },
        {
          "name": "Zhipin Wang",
          "authorId": "2355237576"
        },
        {
          "name": "Christian Frey",
          "authorId": "2386993905"
        },
        {
          "name": "Steffen Eger",
          "authorId": "2319815707"
        },
        {
          "name": "Josif Grabocka",
          "authorId": "1782863"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Model (LLM) conditioning refers to instructing an LLM to generate content in accordance with the norms and values of a specific culture, beliefs of a particular political orientation, or any desired text-specified semantic conditioning. Unfortunately, prompt engineering does not ensure that LLMs behave in accordance with a desired conditioning due to the inductive bias of the pre-training and alignment datasets. Prior works have focused on fine-tuning LLMs by directly conditioning the LoRA weights; however, such methods introduce a large number of parameters. As a remedy, we propose Zhyper, a parameter-efficient factorized hypernetwork framework that generates context-aware LoRA adapters from textual descriptions. Experiments on multiple benchmarks show that Zhyper achieves competitive performance with up to 26x fewer parameters than the state-of-the-art baselines. Furthermore, we extend Zhyper to cultural alignment, demonstrating improved generalization to out-of-domain settings and a better capturing of fine-grained contextual values.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.19733",
      "arxivId": "2510.19733",
      "url": "https://www.semanticscholar.org/paper/762f1af16a95ca73d9007dba7aa324ce8022d054",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.19733"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "609118209144fae8b2f65dbe2aeb7070b161ef05",
      "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning",
      "authors": [
        {
          "name": "Simon Ouellette",
          "authorId": "2332537655"
        }
      ],
      "year": 2025,
      "abstract": "We run a controlled compositional generalization experiment in the ARC-AGI domain: an open-world problem domain in which the ability to generalize out-of-distribution is, by design, an essential characteristic for success. We compare neural program synthesis and test-time fine-tuning approaches on this experiment. We find that execution-guided neural program synthesis outperforms all reference algorithms in its ability to compose novel solutions. Our empirical findings also suggest that the success of TTFT on ARC-AGI lies mainly in eliciting in-distribution knowledge that the LLM otherwise fails to rely on directly.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.15877",
      "arxivId": "2507.15877",
      "url": "https://www.semanticscholar.org/paper/609118209144fae8b2f65dbe2aeb7070b161ef05",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.15877"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e271765fd2060e6f088f09475daed4812a3054ef",
      "title": "The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data",
      "authors": [
        {
          "name": "Zhaoyang Shang",
          "authorId": "2386025039"
        },
        {
          "name": "Sibo Wei",
          "authorId": "2390576844"
        },
        {
          "name": "Jianbin Guo",
          "authorId": "2386380243"
        },
        {
          "name": "Rui Zhou",
          "authorId": "2390446511"
        },
        {
          "name": "LiFeng Dong",
          "authorId": "2394251590"
        },
        {
          "name": "Yin Luo",
          "authorId": "2386372780"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) excel in general tasks, but adapting them to specialized domains relies on high-quality supervised fine-tuning (SFT) data. Although existing methods can identify subsets of high-quality data and reduce training cost to some extent, their selection process still suffers from over-reliance on LLMs'internal knowledge, weak interpretability, and limited generalization. To address these limitations, we propose THTB (The Harder The Better), a cognitive science-inspired framework for instruction data selection and annotation guidance. THTB prioritizes higher-level cognitive instructions by combining quality filtering with intrinsic and extrinsic hardness scoring, offering interpretable and quantifiable criteria for efficient SFT, both in data selection and annotation guidance. Experiments show that THTB enables models trained on only 5% of the data to outperform full-dataset training, while achieving superior generalization compared with LLM-only selection. In addition, THTB provides effective annotation guidance in vertical domains, enabling a model trained on just 2% of the data to surpass models trained on much larger datasets, demonstrating strong potential for domain adaptation. Our code, datasets, and models are available on https://github.com/DYJG-research/THTB.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.13892",
      "arxivId": "2510.13892",
      "url": "https://www.semanticscholar.org/paper/e271765fd2060e6f088f09475daed4812a3054ef",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.13892"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "92f59f320e48a65ae46cb44c58d858a10937cc80",
      "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning",
      "authors": [
        {
          "name": "Wenqiao Zhu",
          "authorId": "2329939058"
        },
        {
          "name": "Ji Liu",
          "authorId": "2362296361"
        },
        {
          "name": "Rongjunchen Zhang",
          "authorId": "2329217550"
        },
        {
          "name": "Haipang Wu",
          "authorId": "2329609481"
        },
        {
          "name": "Yulun Zhang",
          "authorId": "2362287182"
        }
      ],
      "year": 2025,
      "abstract": "Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \\TheName{} in terms of robustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code is available at https://github.com/WNQzhu/CARFT.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.15868",
      "arxivId": "2508.15868",
      "url": "https://www.semanticscholar.org/paper/92f59f320e48a65ae46cb44c58d858a10937cc80",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.15868"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "87e4c349ae538d66966bbdd9995641dfe2a504d8",
      "title": "Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning",
      "authors": [
        {
          "name": "Hua Ye",
          "authorId": "2392612027"
        },
        {
          "name": "Siyuan Chen",
          "authorId": "2392296493"
        },
        {
          "name": "Haoliang Zhang",
          "authorId": "2391720834"
        },
        {
          "name": "Weihao Luo",
          "authorId": "2393383253"
        },
        {
          "name": "Yanbin Li",
          "authorId": "2391726162"
        },
        {
          "name": "Xuan Zhang",
          "authorId": "2391883294"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.07198",
      "arxivId": "2511.07198",
      "url": "https://www.semanticscholar.org/paper/87e4c349ae538d66966bbdd9995641dfe2a504d8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.07198"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bb2fdef1dbc5a100190308dc6a69da724b4751bb",
      "title": "FedLoDrop: Federated LoRA with Dropout for Generalized LLM Fine-tuning",
      "authors": [
        {
          "name": "Sijing Xie",
          "authorId": "2323115571"
        },
        {
          "name": "Dingzhu Wen",
          "authorId": "2355349359"
        },
        {
          "name": "Changsheng You",
          "authorId": "2338267059"
        },
        {
          "name": "Qimei Chen",
          "authorId": "2248782154"
        },
        {
          "name": "Mehdi Bennis",
          "authorId": "2380029359"
        },
        {
          "name": "Kaibin Huang",
          "authorId": "2338809802"
        }
      ],
      "year": 2025,
      "abstract": "Fine-tuning (FT) large language models (LLMs) is crucial for adapting general-purpose models to specific tasks, enhancing accuracy and relevance with minimal resources. To further enhance generalization ability while reducing training costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a new framework that applies dropout to the rows and columns of the trainable matrix in Federated LoRA. A generalization error bound and convergence analysis under sparsity regularization are obtained, which elucidate the fundamental trade-off between underfitting and overfitting. The error bound reveals that a higher dropout rate increases model sparsity, thereby lowering the upper bound of pointwise hypothesis stability (PHS). While this reduces the gap between empirical and generalization errors, it also incurs a higher empirical error, which, together with the gap, determines the overall generalization error. On the other hand, though dropout reduces communication costs, deploying FedLoDrop at the network edge still faces challenges due to limited network resources. To address this issue, an optimization problem is formulated to minimize the upper bound of the generalization error, by jointly optimizing the dropout rate and resource allocation subject to the latency and per-device energy consumption constraints. To solve this problem, a branch-and-bound (B\\&B)-based method is proposed to obtain its globally optimal solution. Moreover, to reduce the high computational complexity of the B\\&B-based method, a penalized successive convex approximation (P-SCA)-based algorithm is proposed to efficiently obtain its high-quality suboptimal solution. Finally, numerical results demonstrate the effectiveness of the proposed approach in mitigating overfitting and improving the generalization capability.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.12078",
      "arxivId": "2510.12078",
      "url": "https://www.semanticscholar.org/paper/bb2fdef1dbc5a100190308dc6a69da724b4751bb",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.12078"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "35d39da7697686dfbffe01915a3cee927b4bab75",
      "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning",
      "authors": [
        {
          "name": "Qi Wang",
          "authorId": "2320480563"
        },
        {
          "name": "Yanrui Yu",
          "authorId": "2304372561"
        },
        {
          "name": "Ye Yuan",
          "authorId": "2316894284"
        },
        {
          "name": "Rui Mao",
          "authorId": "2362271907"
        },
        {
          "name": "Tianfei Zhou",
          "authorId": "2319471527"
        }
      ],
      "year": 2025,
      "abstract": "Reinforcement fine-tuning (RFT) has shown great promise in achieving humanlevel reasoning capabilities of Large Language Models (LLMs), and has recently been extended to MLLMs. Nevertheless, reasoning about videos, which is a fundamental aspect of human intelligence, remains a persistent challenge due to the complex logic, temporal and causal structures inherent in video data. To fill this gap, we propose VideoRFT, a novel approach that extends the RFT paradigm to cultivate human-like video reasoning capabilities in MLLMs. VideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning (SFT) with chain-of-thought (CoT) annotations, followed by reinforcement learning (RL) to improve generalization. A central challenge to achieve this in the video domain lies in the scarcity of large-scale, high-quality video CoT datasets. We address this by building a multi-expert-driven, cognition-inspired CoT curation pipeline. First, we devise a cognition-inspired prompting strategy to elicit a reasoning LLM to generate preliminary CoTs based solely on rich, structured, and literal representations of video content. Subsequently, these CoTs are revised by a MLLM conditioned on the actual video, ensuring visual consistency and reducing visual hallucinations. This pipeline results in two new datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To further strengthen the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning and visual evidence. This reward encourages the model to produce coherent, context-aware reasoning outputs grounded in visual input. Extensive experiments show that VideoRFT achieves state-of-the-art performance on six video reasoning benchmarks.",
      "citationCount": 30,
      "doi": "10.48550/arXiv.2505.12434",
      "arxivId": "2505.12434",
      "url": "https://www.semanticscholar.org/paper/35d39da7697686dfbffe01915a3cee927b4bab75",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.12434"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "caa00ef04c154b0886c316cdb2d19d1e43acaad2",
      "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
      "authors": [
        {
          "name": "Zexiong Ma",
          "authorId": "2264290103"
        },
        {
          "name": "Chao Peng",
          "authorId": "2319411395"
        },
        {
          "name": "Pengfei Gao",
          "authorId": "2315281915"
        },
        {
          "name": "Xiangxin Meng",
          "authorId": "2331229712"
        },
        {
          "name": "Yanzhen Zou",
          "authorId": "1772378"
        },
        {
          "name": "Bing Xie",
          "authorId": "2288535284"
        }
      ],
      "year": 2025,
      "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.",
      "citationCount": 17,
      "doi": "10.48550/arXiv.2502.20127",
      "arxivId": "2502.20127",
      "url": "https://www.semanticscholar.org/paper/caa00ef04c154b0886c316cdb2d19d1e43acaad2",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.20127"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "afdf2b9c7cc9ebccfa0876b9b090e2f2850c194d",
      "title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
      "authors": [
        {
          "name": "Joseph Suh",
          "authorId": "2310434561"
        },
        {
          "name": "Erfan Jahanparast",
          "authorId": "2339777890"
        },
        {
          "name": "Suhong Moon",
          "authorId": "2237424458"
        },
        {
          "name": "Minwoo Kang",
          "authorId": "2297002762"
        },
        {
          "name": "Serina Chang",
          "authorId": "2347003154"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https://github.com/JosephJeesungSuh/subpop.",
      "citationCount": 26,
      "doi": "10.48550/arXiv.2502.16761",
      "arxivId": "2502.16761",
      "url": "https://www.semanticscholar.org/paper/afdf2b9c7cc9ebccfa0876b9b090e2f2850c194d",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.16761"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "3ab31b7a1490834764a28c08c83e76c2fd493ddf",
      "title": "S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity",
      "authors": [
        {
          "name": "Xinyu Yang",
          "authorId": "2284120415"
        },
        {
          "name": "Jixuan Leng",
          "authorId": "2268408425"
        },
        {
          "name": "Geyang Guo",
          "authorId": "47447649"
        },
        {
          "name": "Jiawei Zhao",
          "authorId": "2283090694"
        },
        {
          "name": "Ryumei Nakada",
          "authorId": "150280620"
        },
        {
          "name": "Linjun Zhang",
          "authorId": "2293216018"
        },
        {
          "name": "Huaxiu Yao",
          "authorId": "2249857007"
        },
        {
          "name": "Beidi Chen",
          "authorId": "2306132734"
        }
      ],
      "year": 2024,
      "abstract": "Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by\"selecting sparsely and computing densely\". It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents forgetting while simplifying optimization, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models.",
      "citationCount": 10,
      "doi": "10.48550/arXiv.2412.06289",
      "arxivId": "2412.06289",
      "url": "https://www.semanticscholar.org/paper/3ab31b7a1490834764a28c08c83e76c2fd493ddf",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2412.06289"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "eb8d5424767b36ed920c932b5c7f8429d7183da8",
      "title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models",
      "authors": [
        {
          "name": "Sonam Gupta",
          "authorId": "2320314900"
        },
        {
          "name": "Yatin Nandwani",
          "authorId": "1392630568"
        },
        {
          "name": "Asaf Yehudai",
          "authorId": "2126416248"
        },
        {
          "name": "Mayank Mishra",
          "authorId": "2294874024"
        },
        {
          "name": "Gaurav Pandey",
          "authorId": "2686270"
        },
        {
          "name": "Dinesh Raghu",
          "authorId": "1916865"
        },
        {
          "name": "Sachindra Joshi",
          "authorId": "2243011716"
        }
      ],
      "year": 2024,
      "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-Rehearsal (SSR), a fine-tuning approach that achieves performance comparable to the standard supervised fine-tuning (SFT) while improving generalization. SSR leverages the fact that there can be multiple valid responses to a query. By utilizing the model's correct responses, SSR reduces model specialization during the fine-tuning stage. SSR first identifies the correct model responses from the training set by deploying an appropriate LLM as a judge. Then, it fine-tunes the model using the correct model responses and the gold response for the remaining samples. The effectiveness of SSR is demonstrated through experiments on the task of identifying unanswerable queries across various datasets. The results show that standard SFT can lead to an average performance drop of up to $16.7\\%$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, SSR results in close to $2\\%$ drop on average, indicating better generalization capabilities compared to standard SFT.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2409.04787",
      "arxivId": "2409.04787",
      "url": "https://www.semanticscholar.org/paper/eb8d5424767b36ed920c932b5c7f8429d7183da8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2409.04787"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a94b17d099b4eb9e713107e6daebde12980922ae",
      "title": "DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning",
      "authors": [
        {
          "name": "Shangqing Tu",
          "authorId": "2116520118"
        },
        {
          "name": "Kejian Zhu",
          "authorId": "2305113513"
        },
        {
          "name": "Yushi Bai",
          "authorId": "2141377570"
        },
        {
          "name": "Zijun Yao",
          "authorId": "2273946831"
        },
        {
          "name": "Lei Hou",
          "authorId": "2284777109"
        },
        {
          "name": "Juanzi Li",
          "authorId": "2133353675"
        }
      ],
      "year": 2024,
      "abstract": "The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance. Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training. Besides, prior work has already shown that even training on data similar to benchmark data inflates performance, namely \\emph{In-distribution contamination}. In this work, we argue that in-distribution contamination can lead to the performance drop on OOD benchmarks. To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination. DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer. Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets. We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions. Additionally, we find that DICE's predictions correlate with the performance of LLMs fine-tuned by either us or other organizations, achieving a coefficient of determination ($R^2$) between 0.61 and 0.75. The code and data are available at https://github.com/THU-KEG/DICE.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2406.04197",
      "arxivId": "2406.04197",
      "url": "https://www.semanticscholar.org/paper/a94b17d099b4eb9e713107e6daebde12980922ae",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.04197"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "769cc2632bcc3137371772c37d7865f87e82203d",
      "title": "Federated Low-Rank Adaptation for Large Models Fine-Tuning Over Wireless Networks",
      "authors": [
        {
          "name": "Haofeng Sun",
          "authorId": "2272355110"
        },
        {
          "name": "Hui Tian",
          "authorId": "2069992345"
        },
        {
          "name": "Wanli Ni",
          "authorId": "2269551031"
        },
        {
          "name": "Jingheng Zheng",
          "authorId": "2145106284"
        },
        {
          "name": "Dusist Niyato",
          "authorId": "2266084696"
        },
        {
          "name": "Ping Zhang",
          "authorId": "2237389621"
        }
      ],
      "year": 2025,
      "abstract": "The emergence of large language models (LLMs) with multi-task generalization capabilities is expected to improve the performance of artificial intelligence (AI)-as-a-service provision in 6G networks. By fine-tuning LLMs, AI services can become more precise and tailored to the demands of different downstream tasks. However, centralized fine-tuning paradigms pose a potential risk to user privacy, and existing distributed fine-tuning methods incur significant wireless transmission burdens due to the large-scale parameter transmission of LLMs. To tackle these challenges, by leveraging the low rank feature in LLM fine-tuning, we propose a wireless over-the-air federated learning (AirFL) based low-rank adaptation (LoRA) framework that integrates LoRA and over-the-air computation (AirComp) to achieve efficient fine-tuning and aggregation. Based on multiple-input multiple-output (MIMO) and orthogonal frequency division multiplexing (OFDM), we design a multi-stream AirComp scheme to fulfill the aggregation requirement of AirFL-LoRA. Furthermore, by deriving an optimality gap, we gain theoretical insights into the joint impact of rank selection and gradient aggregation distortion on the fine-tuning performance of AirFL-LoRA. Next, we formulate a non-convex problem to minimize the optimality gap, which is solved by the proposed backtracking-based alternating algorithm and the manifold optimization algorithm iteratively. Through fine-tuning LLMs for different downstream tasks, experimental results reveal that the AirFL-LoRA framework outperforms the state-of-the-art baselines on both training loss and perplexity, closely approximating the performance of FL with ideal aggregation.",
      "citationCount": 13,
      "doi": "10.1109/TWC.2024.3497998",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/769cc2632bcc3137371772c37d7865f87e82203d",
      "venue": "IEEE Transactions on Wireless Communications",
      "journal": {
        "name": "IEEE Transactions on Wireless Communications",
        "pages": "659-675",
        "volume": "24"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36",
      "title": "MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning",
      "authors": [
        {
          "name": "Zhenwen Liang",
          "authorId": "151474408"
        },
        {
          "name": "Dian Yu",
          "authorId": "41190054"
        },
        {
          "name": "Xiaoman Pan",
          "authorId": "34741133"
        },
        {
          "name": "Wenlin Yao",
          "authorId": "2087264100"
        },
        {
          "name": "Qingkai Zeng",
          "authorId": "1694209"
        },
        {
          "name": "Xiangliang Zhang",
          "authorId": "2928371"
        },
        {
          "name": "Dong Yu",
          "authorId": "144580027"
        }
      ],
      "year": 2023,
      "abstract": "Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on distilling knowledge from powerful yet inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different \u201cviews\u201d that may help each other and leverage them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables relatively small LMs to outperform prior approaches that heavily rely on knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains.",
      "citationCount": 18,
      "doi": "10.48550/arXiv.2307.07951",
      "arxivId": "2307.07951",
      "url": "https://www.semanticscholar.org/paper/c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36",
      "venue": "International Conference on Language Resources and Evaluation",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2307.07951"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7a079dd04a9c4709fe2abefd4fbdf6dee8e6704d",
      "title": "Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning",
      "authors": [
        {
          "name": "Ananth Balashankar",
          "authorId": "2593082"
        },
        {
          "name": "Xiao Ma",
          "authorId": "2261741205"
        },
        {
          "name": "Aradhana Sinha",
          "authorId": "2261739762"
        },
        {
          "name": "Ahmad Beirami",
          "authorId": "2261494232"
        },
        {
          "name": "Yao Qin",
          "authorId": "2261906552"
        },
        {
          "name": "Jilin Chen",
          "authorId": "2144168512"
        },
        {
          "name": "Alex Beutel",
          "authorId": "2638246"
        }
      ],
      "year": 2023,
      "abstract": "As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? In this paper, we study the novel setting of domain-generalized few-shot learning for LLM-based text safety classifiers. Unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. We demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules. We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines that either do not rely on data augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule is loosely correlated with existing ones.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2310.16959",
      "arxivId": "2310.16959",
      "url": "https://www.semanticscholar.org/paper/7a079dd04a9c4709fe2abefd4fbdf6dee8e6704d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2310.16959"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "642f418f720caa4d2021e1d7ba6e2fc286eb8c09",
      "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation",
      "authors": [
        {
          "name": "Maja Stahl",
          "authorId": "2187454927"
        },
        {
          "name": "Timon Ziegenbein",
          "authorId": "71114581"
        },
        {
          "name": "Joonsuk Park",
          "authorId": "2293765344"
        },
        {
          "name": "Henning Wachsmuth",
          "authorId": "2253596550"
        }
      ],
      "year": 2025,
      "abstract": "Training large language models (LLMs) to follow instructions has significantly enhanced their ability to tackle unseen tasks. However, despite their strong generalization capabilities, instruction-following LLMs encounter difficulties when dealing with tasks that require domain knowledge. This work introduces a specialized instruction fine-tuning for the domain of computational argumentation (CA). The goal is to enable an LLM to effectively tackle any unseen CA tasks while preserving its generalization capabilities. Reviewing existing CA research, we crafted natural language instructions for 105 CA tasks to this end. On this basis, we developed a CA-specific benchmark for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in solving various CA tasks. We synthesized 52k CA-related instructions, adapting the self-instruct process to train a CA-specialized instruction-following LLM. Our experiments suggest that CA-specialized instruction fine-tuning significantly enhances the LLM on both seen and unseen CA tasks. At the same time, performance on the general NLP tasks of the SuperNI benchmark remains stable.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2505.22076",
      "arxivId": "2505.22076",
      "url": "https://www.semanticscholar.org/paper/642f418f720caa4d2021e1d7ba6e2fc286eb8c09",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "11103-11127"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "24c85fcb76b8c4e30314f3fb6818adb05a740158",
      "title": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models",
      "authors": [
        {
          "name": "Zhen Zhang",
          "authorId": "2170500945"
        },
        {
          "name": "Yifan Yang",
          "authorId": "2284729630"
        },
        {
          "name": "Kai Zhen",
          "authorId": "2292575103"
        },
        {
          "name": "Nathan Susanj",
          "authorId": "2121373132"
        },
        {
          "name": "Athanasios Mouchtaris",
          "authorId": "2292562501"
        },
        {
          "name": "Siegfried Kunzmann",
          "authorId": "2302806267"
        },
        {
          "name": "Zheng Zhang",
          "authorId": "2308472600"
        }
      ],
      "year": 2025,
      "abstract": "Large language models have demonstrated exceptional capabilities across diverse tasks, but their fine-tuning demands significant memory, posing challenges for resource-constrained environments. Zeroth-order (ZO) optimization provides a memory-efficient alternative by eliminating the need for backpropagation. However, ZO optimization suffers from high gradient variance, and prior research has largely focused on single-task learning, leaving its application to multi-task learning unexplored. Multi-task learning is crucial for leveraging shared knowledge across tasks to improve generalization, yet it introduces unique challenges under ZO settings, such as amplified gradient variance and collinearity. In this paper, we present MaZO, the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. MaZO tackles these challenges at the parameter level through two key innovations: a weight importance metric to identify critical parameters and a multi-task weight update mask to selectively update these parameters, reducing the dimensionality of the parameter space and mitigating task conflicts. Experiments demonstrate that MaZO achieves state-of-the-art performance, surpassing even multi-task learning methods designed for first-order optimization.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2502.11513",
      "arxivId": "2502.11513",
      "url": "https://www.semanticscholar.org/paper/24c85fcb76b8c4e30314f3fb6818adb05a740158",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.11513"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "99706876b30b5a48ca155e87dff7c4a54b8f09a8",
      "title": "Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings",
      "authors": [
        {
          "name": "Ivan Montoya Sanchez",
          "authorId": "2358264351"
        },
        {
          "name": "Shaswata Mitra",
          "authorId": "2150699855"
        },
        {
          "name": "Aritran Piplai",
          "authorId": "2482027"
        },
        {
          "name": "Sudip Mittal",
          "authorId": "2284767922"
        }
      ],
      "year": 2025,
      "abstract": "The rapid evolution of malware variants requires robust classification methods to enhance cybersecurity. While large language models (LLMs) offer potential for generating malware descriptions to aid family classification, their utility is limited by semantic embedding overlaps and misalignment with binary behavioral features. We propose a contrastive fine-tuning (CFT) method that refines LLM embeddings via targeted selection of hard negative samples based on cosine similarity, enabling LLMs to distinguish between closely related malware families. Our approach combines high-similarity negatives to enhance discriminative power and mid-tier negatives to increase embedding diversity, optimizing both precision and generalization. Evaluated on the CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework on a few-shot setting. Experiments demonstrate significant improvements: our method achieves 63.15% classification accuracy with as few as 20 samples on CIC-AndMal-2020, outperforming baselines by 11\u201321 percentage points and surpassing prior negative sampling strategies. Ablation studies confirm the superiority of similarity-based selection over random sampling, with gains of 10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions that generalize to unseen variants, bridging textual and binary feature gaps. This work advances malware classification by enabling nuanced semantic distinctions and provides a scalable framework for adapting LLMs to cybersecurity challenges.",
      "citationCount": 2,
      "doi": "10.1109/IJCNN64981.2025.11229181",
      "arxivId": "2504.21028",
      "url": "https://www.semanticscholar.org/paper/99706876b30b5a48ca155e87dff7c4a54b8f09a8",
      "venue": "IEEE International Joint Conference on Neural Network",
      "journal": {
        "name": "2025 International Joint Conference on Neural Networks (IJCNN)",
        "pages": "1-8"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "11d5ee9a71e47dcb0f4bb18d4dc978c16c1e959a",
      "title": "Does Synthetic Data Generalize? A Comparative Study of Synthetic and Real Datasets for Reinforcement Fine-Tuning of Domain-Specific LLMs",
      "authors": [
        {
          "name": "Bhavika Reddy",
          "authorId": "2403539139"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) adapted for specialized technical domains increasingly depend on the quality, structure, and provenance of their fine-tuning data. Synthetic data generation offers a scalable alternative to expert-labeled corpora, yet its effectiveness in reinforcement fine-tuning (RFT) pipelines remains an open question. This work proposes a structured comparison of synthetic, human-labeled, and hybrid datasets for domain-grounded LLM adaptation. The comparison evaluates the trade-offs between cost, control, and generalization when these datasets are used for fine-tuning under limited hardware resources. The discussion integrates advances in single-GPU optimization, LoRA-based fine-tuning, and multi-stage data synthesis workflows to outline an experimental framework that examines faithfulness, factual grounding, and reasoning consistency. Results demonstrate that human-labeled data excels in factual precision and domain-specific reasoning, while synthetic data offers stronger coverage and generalization capabilities. Hybrid datasets consistently produce balanced performance across evaluation dimensions by leveraging these complementary strengths. Resource utilization patterns reveal greater sample efficiency for human-labeled data despite higher initial annotation costs. The strategic combination of data sources emerges as the most promising approach for balancing performance, resource efficiency, and knowledge representation in domain-specific applications.",
      "citationCount": 0,
      "doi": "10.52783/jisem.v10i63s.14005",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/11d5ee9a71e47dcb0f4bb18d4dc978c16c1e959a",
      "venue": "Journal of Information Systems Engineering & Management",
      "journal": {
        "name": "Journal of Information Systems Engineering and Management"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "32839367329625025d36c743abb30768243e6aac",
      "title": "Efficiency and Performance Optimization in Large Language Models through IB Fine-Tuning",
      "authors": [
        {
          "name": "Ashly Ann Jo",
          "authorId": "74223762"
        },
        {
          "name": "Ebin Deni Raj",
          "authorId": "3092219"
        },
        {
          "name": "Jayakrushna Sahoo",
          "authorId": "3145835"
        }
      ],
      "year": 2025,
      "abstract": "In the rapidly evolving field of Natural Language Processing (NLP), optimizing methods for fine-tuning Large Language Models (LLMs) is increasingly critical for improving generalization and performance. Fine-tuning LLMs is challenging due to high costs, overfitting, and difficulty adapting to diverse tasks. These challenges grow as LLMs scale, making traditional fine-tuning methods inefficient and expensive. To address these issues, a novel Information Bottleneck (IB) method for fine-tuning LLMs is proposed, focusing on retaining only the most critical and relevant information in the model\u2019s internal representations. By striking a balance between information compression and predictive relevance, the IB method aims to reduce overfitting and enhance generalization. This approach also integrates reinforcement learning and continual learning to enhance LLM performance further. The proposed framework considers two key metrics: (1) compression effectiveness, which reduces redundancy and improves generalization, and (2) predictive relevance, which ensures high task-specific performance. The proposed scheme achieves scalable fine-tuning across diverse NLP tasks using a lightweight proxy model to enhance computational efficiency. The proposed framework empirical evaluations and ablation studies show that the IB method improves accuracy while significantly reducing computational costs, enabling efficient, interpretable, and adaptable LLM optimization and increasing convergence.",
      "citationCount": 0,
      "doi": "10.1145/3718096",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/32839367329625025d36c743abb30768243e6aac",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "journal": {
        "name": "ACM Transactions on Intelligent Systems and Technology",
        "pages": "1 - 23",
        "volume": "16"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f4bf271045278568603db295abb2b2d8e37cc84f",
      "title": "XuanHuGPT: parameter-efficient fine-tuning of large language model in the field of traditional Chinese medicine",
      "authors": [
        {
          "name": "Xuming Tong",
          "authorId": "2299192081"
        },
        {
          "name": "Xiaozheng Ding",
          "authorId": "2399928183"
        },
        {
          "name": "Huiru Jia",
          "authorId": "2397996062"
        },
        {
          "name": "Yanhong Yuan",
          "authorId": "2220211513"
        },
        {
          "name": "Liyan Liu",
          "authorId": "2394246268"
        },
        {
          "name": "Yapeng Wang",
          "authorId": "2145215746"
        },
        {
          "name": "Zhang Xiong",
          "authorId": "2394271738"
        },
        {
          "name": "Xu Yang",
          "authorId": "2143732342"
        },
        {
          "name": "Sio-Kei Im",
          "authorId": "2244005787"
        },
        {
          "name": "Mini Han Wang",
          "authorId": "2394207353"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional generalization capabilities across various fields, including their application in Traditional Chinese Medicine (TCM). However, the performance of existing LLMs in TCM-specific tasks remains limited due to the lack of optimization for TCM knowledge during the pre-training phase, insufficient datasets, and the constraints of fine-tuning techniques. To address these challenges, this study constructs the XhTCM dataset by systematically integrating data from three authoritative sources\u2014ShenNong_TCM_Dataset, TCMBank, and TCMIP v2.0. The dataset includes 100,000 structured entries, covering classical theories, prescription formulations, herbal pharmacology, and modern clinical practices. Based on this, we present XuanHuGPT, a domain-specific LLM tailored for TCM question answering and inference. By applying Parameter-Efficient Fine-Tuning (PEFT) techniques, we effectively balance model performance and training costs. Furthermore, we establish a comprehensive evaluation framework for TCM LLMs, combining quantitative metrics (BLEU, ROUGE, METEOR, BERTScore, and Embedding Distance) with expert qualitative assessments. Experimental results show that XuanHuGPT significantly outperforms both general-purpose LLMs and some existing TCM-specific models in accuracy, coverage, fluency, consistency, sensitivity, and safety. This study presents a reproducible paradigm for building intelligent TCM Q&A systems, contributing to the digital transformation, intelligent development, and global dissemination of TCM knowledge.",
      "citationCount": 0,
      "doi": "10.1186/s13020-025-01200-3",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f4bf271045278568603db295abb2b2d8e37cc84f",
      "venue": "Chinese Medicine",
      "journal": {
        "name": "Chinese Medicine",
        "volume": "20"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1d17379093408e9cad4a38fe08ad8e85143d93c7",
      "title": "Federated Fine-Tuning of Large Language Models for Intelligent Automotive Systems with Low-Rank Adaptation",
      "authors": [
        {
          "name": "Jinhua Chen",
          "authorId": "2287800842"
        },
        {
          "name": "Franck Junior Aboya Messou",
          "authorId": "2332941705"
        },
        {
          "name": "Shilong Zhang",
          "authorId": "2352701605"
        },
        {
          "name": "Tong Liu",
          "authorId": "2352199492"
        },
        {
          "name": "Keping Yu",
          "authorId": "2327522028"
        },
        {
          "name": "Dusit Niyato",
          "authorId": "2340230621"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) in intelligent automotive systems offer significant benefits, such as enhancing natural language understanding, improving user interaction, and enabling more intelligent decision-making. However, this integration also faces important challenges, including data heterogeneity, limited computational resources, and the critical need to safeguard user privacy. Federated Learning (FL) offers a promising solution by enabling decentralized training across distributed data sources without compromising privacy. This paper proposes a novel FL framework for in-vehicle systems, addressing key challenges such as data heterogeneity and limited computational resources. Our method introduces a robust aggregation algorithm based on the L2 norm between LLM increments, effectively mitigating data inconsistencies and enhancing model generalization. Moreover, by integrating Low-Rank Adaptation (LoRA) within parameter-efficient fine-tuning, the framework reduces computational and communication overhead while preserving privacy. Comprehensive experiments validate that the proposed method outperforms state-of-the-art FL methods, achieving a Vicuna score of 8.17, a harmless answer rate of 68.65% (Advbenchmark), and an MTBenchmark average score of 3.74. These results highlight the potential of the proposed FL-based LLM with the LoRA framework in revolutionizing intelligent automotive systems through enhanced adaptability and privacy preservation.",
      "citationCount": 0,
      "doi": "10.1109/VTC2025-Spring65109.2025.11174441",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/1d17379093408e9cad4a38fe08ad8e85143d93c7",
      "venue": "2025 IEEE 101st Vehicular Technology Conference (VTC2025-Spring)",
      "journal": {
        "name": "2025 IEEE 101st Vehicular Technology Conference (VTC2025-Spring)",
        "pages": "1-6"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "b4870fcaab83db589e4c283d1c8355ec21b5089a",
      "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection",
      "authors": [
        {
          "name": "Axel Delaval",
          "authorId": "2376128958"
        },
        {
          "name": "Shujian Yang",
          "authorId": "2274185207"
        },
        {
          "name": "Haicheng Wang",
          "authorId": "2364629102"
        },
        {
          "name": "Han Qiu",
          "authorId": "2362887576"
        },
        {
          "name": "Jialiang Lu",
          "authorId": "2365422318"
        }
      ],
      "year": 2025,
      "abstract": "Detecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French remains underdeveloped, primarily due to the lack of culturally relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new public benchmark of 53,622 French online comments, constructed via a semi-automated annotation pipeline that reduces manual labeling to only 10% through high-confidence LLM-based pre-annotation and human verification. Then, we benchmark a broad range of models and uncover a counterintuitive insight: Small Language Models (SLMs) outperform many larger models in robustness and generalization under the toxicity detection task. Motivated by this finding, we propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic weighted loss that progressively emphasizes the model's final decision, significantly improving faithfulness. Our fine-tuned 4B model achieves state-of-the-art performance, improving its F1 score by 13% over its baseline and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a cross-lingual toxicity benchmark demonstrates strong multilingual ability, suggesting that our methodology can be effectively extended to other languages and safety-critical classification tasks.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.11281",
      "arxivId": "2508.11281",
      "url": "https://www.semanticscholar.org/paper/b4870fcaab83db589e4c283d1c8355ec21b5089a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.11281"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1121f6c1681489d65a66e60a28d1eb21769a0232",
      "title": "Flow of Knowledge: Federated Fine-Tuning of LLMs in Healthcare under Non-IID Conditions",
      "authors": [
        {
          "name": "Zeyu Chen",
          "authorId": "2383329588"
        },
        {
          "name": "Yun Ji",
          "authorId": "2257135942"
        },
        {
          "name": "Bowen Wang",
          "authorId": "2383245888"
        },
        {
          "name": "Liwen Shi",
          "authorId": "2383552234"
        },
        {
          "name": "Zijie Zeng",
          "authorId": "2387215125"
        },
        {
          "name": "Sheng Zhang",
          "authorId": "2384868741"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) show great promise in healthcare, but their applications are hindered by data privacy restrictions and the challenges of cross-institution collaboration. Sensitive medical data cannot be centralized, while non-independent and identically distributed (non-IID) characteristics across institutions further com- plicate convergence and fairness. To address these issues, we present a federated fine-tuning approach based on Low-Rank Adaptation (LoRA), enabling privacy-preserving knowledge flow across institu- tions. The method iteratively combines local LoRA adaptation with global parameter aggregation, allowing efficient knowledge shar- ing without exposing raw data. A blockchain identity scheme is used for identifying individual LLM in such a distributed network. We evaluate this approach on heterogeneous and highly non-IID medical text datasets, where experiments demonstrate that feder- ated LoRA not only enhances cross-client generalization but also improves the performance of the weakest client, achieving stable convergence and fairer outcomes. These findings highlight federated LoRA fine-tuning as a practical and effective paradigm for adapting LLMs in healthcare, offering a new path for multi-center medical AI collaboration.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.00543",
      "arxivId": "2510.00543",
      "url": "https://www.semanticscholar.org/paper/1121f6c1681489d65a66e60a28d1eb21769a0232",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.00543"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d4e7d1bb3dd7cb9e61eb37e2a0875f9147542dcd",
      "title": "A Dynamic Adaptive Data Filtering and Sampling Framework for Optimized Fine-Tuning of Large Language Models",
      "authors": [
        {
          "name": "Hairu Wen",
          "authorId": "2361896057"
        },
        {
          "name": "Yang Chen",
          "authorId": "2361861217"
        },
        {
          "name": "Ze Yang",
          "authorId": "2361723185"
        },
        {
          "name": "Lei Fu",
          "authorId": "2361878207"
        },
        {
          "name": "Zeyu Wang",
          "authorId": "2362256419"
        },
        {
          "name": "Qiyuan Tian",
          "authorId": "2333606292"
        }
      ],
      "year": 2025,
      "abstract": "Fine-tuning large language models (LLMs), such as Baichuan2-7B, faces challenges in data quality and training efficiency. This paper introduces DADFS (Dynamic Adaptive Data Filtering and Sampling), a new framework designed to optimize LLM fine-tuning. DADFS includes three components: Dynamic Adaptive Filtering (DAF), Adaptive Data Sampling (ADS), and Data Mixing and Proportional Weighting. The framework filters training data based on quality scores to ensure high-quality and diverse samples. It also uses adaptive sampling to select the most informative data points for training and employs data mixing to improve generalization and reduce overfitting. Experimental results show that DADFS outperforms traditional fine-tuning methods across key metrics. DADFS also integrates LoRA (Low-Rank Adaptation) within the PEFT (Parameter-Efficient Fine-Tuning) framework, which helps maintain computational efficiency while improving model adaptability and stability. This research offers an efficient and scalable solution for LLM fine-tuning, showing the importance of data optimization for model performance.",
      "citationCount": 0,
      "doi": "10.1109/ICCECE65250.2025.10984505",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d4e7d1bb3dd7cb9e61eb37e2a0875f9147542dcd",
      "venue": "2025 5th International Conference on Consumer Electronics and Computer Engineering (ICCECE)",
      "journal": {
        "name": "2025 5th International Conference on Consumer Electronics and Computer Engineering (ICCECE)",
        "pages": "600-604"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "0171145d48115e090985e44a6fa497470db78c68",
      "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models",
      "authors": [
        {
          "name": "Kyeonghyun Kim",
          "authorId": "2366172442"
        },
        {
          "name": "Jinhee Jang",
          "authorId": "2345416593"
        },
        {
          "name": "Juhwan Choi",
          "authorId": "2190681194"
        },
        {
          "name": "Yoonji Lee",
          "authorId": "2334817243"
        },
        {
          "name": "Kyohoon Jin",
          "authorId": "1515118694"
        },
        {
          "name": "Youngbin Kim",
          "authorId": "2288039242"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) are renowned for their extensive linguistic knowledge and strong generalization capabilities, but their high computational demands make them unsuitable for resource-constrained environments. In contrast, small language models (SLMs) are computationally efficient but often lack the broad generalization capacity of LLMs. To bridge this gap, we propose PiFi, a novel framework that combines the strengths of both LLMs and SLMs to achieve high performance while maintaining efficiency. PiFi integrates a single frozen layer from an LLM into a SLM and fine-tunes the combined model for specific tasks, boosting performance without a significant increase in computational cost. We show that PiFi delivers consistent performance improvements across a range of natural language processing tasks, including both natural language understanding and generation. Moreover, our findings demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing generalization to unseen domains and facilitating the transfer of linguistic abilities.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2506.07424",
      "arxivId": "2506.07424",
      "url": "https://www.semanticscholar.org/paper/0171145d48115e090985e44a6fa497470db78c68",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "5434-5452"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "f123b838e000e11f08cb0d7c63e01934b38d3092",
      "title": "An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models",
      "authors": [
        {
          "name": "Xiongtao Zhou",
          "authorId": "2305568789"
        },
        {
          "name": "Jie He",
          "authorId": "2294514715"
        },
        {
          "name": "Yuhua Ke",
          "authorId": "2305485755"
        },
        {
          "name": "Guangyao Zhu",
          "authorId": "2305494253"
        },
        {
          "name": "V'ictor Guti'errez-Basulto",
          "authorId": "2294362894"
        },
        {
          "name": "Jeff Z. Pan",
          "authorId": "2258308597"
        }
      ],
      "year": 2024,
      "abstract": "Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.",
      "citationCount": 28,
      "doi": "10.48550/arXiv.2406.05130",
      "arxivId": "2406.05130",
      "url": "https://www.semanticscholar.org/paper/f123b838e000e11f08cb0d7c63e01934b38d3092",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "10057-10084"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "c6727e6ca5232e2fd06fc1370150e2b91edd6cb8",
      "title": "Bilevel ZOFO: Bridging Parameter-Efficient and Zeroth-Order Techniques for Efficient LLM Fine-Tuning and Meta-Training",
      "authors": [
        {
          "name": "Reza Shirkavand",
          "authorId": "2055028725"
        },
        {
          "name": "Qi He",
          "authorId": "2347920378"
        },
        {
          "name": "Peiran Yu",
          "authorId": "2313920324"
        },
        {
          "name": "Heng Huang",
          "authorId": "2243268584"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.48550/arXiv.2502.03604",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c6727e6ca5232e2fd06fc1370150e2b91edd6cb8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.03604"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
