{
  "status": "success",
  "source": "semantic_scholar",
  "query": "faithfulness chain of thought",
  "results": [
    {
      "paperId": "827afa7dd36e4afbb1a49c735bfbb2c69749756e",
      "title": "Measuring Faithfulness in Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Tamera Lanham",
          "authorId": "46239941"
        },
        {
          "name": "Anna Chen",
          "authorId": "2111073313"
        },
        {
          "name": "Ansh Radhakrishnan",
          "authorId": "2224616677"
        },
        {
          "name": "Benoit Steiner",
          "authorId": "32163737"
        },
        {
          "name": "Carson E. Denison",
          "authorId": "1780754598"
        },
        {
          "name": "Danny Hernandez",
          "authorId": "39182747"
        },
        {
          "name": "Dustin Li",
          "authorId": "2108506462"
        },
        {
          "name": "Esin Durmus",
          "authorId": "41152329"
        },
        {
          "name": "Evan Hubinger",
          "authorId": "146614650"
        },
        {
          "name": "John Kernion",
          "authorId": "1583434563"
        },
        {
          "name": "Kamil.e Lukovsiut.e",
          "authorId": "2161242438"
        },
        {
          "name": "Karina Nguyen",
          "authorId": "2196759978"
        },
        {
          "name": "Newton Cheng",
          "authorId": "15590401"
        },
        {
          "name": "Nicholas Joseph",
          "authorId": "2117706920"
        },
        {
          "name": "Nicholas Schiefer",
          "authorId": "2833768"
        },
        {
          "name": "Oliver Rausch",
          "authorId": "2221219447"
        },
        {
          "name": "Robin Larson",
          "authorId": "48810415"
        },
        {
          "name": "Sam McCandlish",
          "authorId": "2293396643"
        },
        {
          "name": "Sandipan Kundu",
          "authorId": "2158813858"
        },
        {
          "name": "Saurav Kadavath",
          "authorId": "148070327"
        },
        {
          "name": "Shannon Yang",
          "authorId": "2225081715"
        },
        {
          "name": "T. Henighan",
          "authorId": "103143311"
        },
        {
          "name": "Timothy D. Maxwell",
          "authorId": "2069892072"
        },
        {
          "name": "Timothy Telleen-Lawton",
          "authorId": "1419532638"
        },
        {
          "name": "Tristan Hume",
          "authorId": "2162194147"
        },
        {
          "name": "Zac Hatfield-Dodds",
          "authorId": "1573482302"
        },
        {
          "name": "Jared Kaplan",
          "authorId": "2053807409"
        },
        {
          "name": "J. Brauner",
          "authorId": "38732223"
        },
        {
          "name": "Sam Bowman",
          "authorId": "1799822"
        },
        {
          "name": "Ethan Perez",
          "authorId": "3439053"
        }
      ],
      "year": 2023,
      "abstract": "Large language models (LLMs) perform better when they produce step-by-step,\"Chain-of-Thought\"(CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",
      "citationCount": 308,
      "doi": "10.48550/arXiv.2307.13702",
      "arxivId": "2307.13702",
      "url": "https://www.semanticscholar.org/paper/827afa7dd36e4afbb1a49c735bfbb2c69749756e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2307.13702"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "aaaa04358e03fe5ef0d24a86e75fe6c1ee0f3ca7",
      "title": "Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps",
      "authors": [
        {
          "name": "Martin Tutek",
          "authorId": "2367197291"
        },
        {
          "name": "Fateme Hashemi Chaleshtori",
          "authorId": "2083451204"
        },
        {
          "name": "Ana Marasovi'c",
          "authorId": "2261284946"
        },
        {
          "name": "Yonatan Belinkov",
          "authorId": "2346327043"
        }
      ],
      "year": 2025,
      "abstract": "When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models'parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models'prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.",
      "citationCount": 8,
      "doi": "10.18653/v1/2025.emnlp-main.504",
      "arxivId": "2502.14829",
      "url": "https://www.semanticscholar.org/paper/aaaa04358e03fe5ef0d24a86e75fe6c1ee0f3ca7",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "cc8c01996b4dc20a0879bd93d87d115df43e1c03",
      "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models",
      "authors": [
        {
          "name": "S. Balasubramanian",
          "authorId": "144021807"
        },
        {
          "name": "Samyadeep Basu",
          "authorId": "2114710333"
        },
        {
          "name": "S. Feizi",
          "authorId": "34389431"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent''reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2505.23945",
      "arxivId": "2505.23945",
      "url": "https://www.semanticscholar.org/paper/cc8c01996b4dc20a0879bd93d87d115df43e1c03",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.23945"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ca0f8abb5cc27b266d2be9c7f5db9b72934566e0",
      "title": "A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages",
      "authors": [
        {
          "name": "Raoyuan Zhao",
          "authorId": "2319817746"
        },
        {
          "name": "Yihong Liu",
          "authorId": "2107995084"
        },
        {
          "name": "Hinrich Schutze",
          "authorId": "2130001188"
        },
        {
          "name": "Michael A. Hedderich",
          "authorId": "51133383"
        }
      ],
      "year": 2025,
      "abstract": "Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2510.09555",
      "arxivId": "2510.09555",
      "url": "https://www.semanticscholar.org/paper/ca0f8abb5cc27b266d2be9c7f5db9b72934566e0",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.09555"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a3d2c1c932da66f48af673bcb860ba0b8fb335d1",
      "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Debjit Paul",
          "authorId": "2261760962"
        },
        {
          "name": "Robert West",
          "authorId": "2284865862"
        },
        {
          "name": "A. Bosselut",
          "authorId": "2284866282"
        },
        {
          "name": "Boi Faltings",
          "authorId": "2054858128"
        }
      ],
      "year": 2024,
      "abstract": "Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",
      "citationCount": 77,
      "doi": "10.48550/arXiv.2402.13950",
      "arxivId": "2402.13950",
      "url": "https://www.semanticscholar.org/paper/a3d2c1c932da66f48af673bcb860ba0b8fb335d1",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "pages": "15012-15032"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "eb6cfd141ac9e622354b363264b3755325328e05",
      "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness",
      "authors": [
        {
          "name": "Anand Swaroop",
          "authorId": "2380689216"
        },
        {
          "name": "Akshat Nallani",
          "authorId": "2380689119"
        },
        {
          "name": "Saksham Uboweja",
          "authorId": "2380688926"
        },
        {
          "name": "Adiliia Uzdenova",
          "authorId": "2380689043"
        },
        {
          "name": "Michael Nguyen",
          "authorId": "2381147841"
        },
        {
          "name": "Kevin Zhu",
          "authorId": "2381070442"
        },
        {
          "name": "Sunishchal Dev",
          "authorId": "2367272403"
        },
        {
          "name": "Ashwinee Panda",
          "authorId": "2380686442"
        },
        {
          "name": "Vasu Sharma",
          "authorId": "2348193755"
        },
        {
          "name": "Maheep Chaudhary",
          "authorId": "2380687024"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \\href{https://github.com/Anut-py/frit}.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2509.13334",
      "arxivId": "2509.13334",
      "url": "https://www.semanticscholar.org/paper/eb6cfd141ac9e622354b363264b3755325328e05",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.13334"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "50d1661cb063d1e4e2e64417fbe63667fdaafba1",
      "title": "Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity",
      "authors": [
        {
          "name": "Austin Meek",
          "authorId": "2377963948"
        },
        {
          "name": "Eitan Sprejer",
          "authorId": "2346594365"
        },
        {
          "name": "Iv'an Arcuschin",
          "authorId": "2390196652"
        },
        {
          "name": "Austin J. Brockmeier",
          "authorId": "2371067914"
        },
        {
          "name": "Steven Basart",
          "authorId": "104444594"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.27378",
      "arxivId": "2510.27378",
      "url": "https://www.semanticscholar.org/paper/50d1661cb063d1e4e2e64417fbe63667fdaafba1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.27378"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6968f45aabe7b328bb322bc35c808a6d5e5ea006",
      "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Xu Shen",
          "authorId": "2384332434"
        },
        {
          "name": "Song Wang",
          "authorId": "2355876312"
        },
        {
          "name": "Zhen Tan",
          "authorId": "2347693548"
        },
        {
          "name": "Laura Yao",
          "authorId": "2385443153"
        },
        {
          "name": "Xinyu Zhao",
          "authorId": "2290239259"
        },
        {
          "name": "Kaidi Xu",
          "authorId": "2267887786"
        },
        {
          "name": "Xin Wang",
          "authorId": "2384281928"
        },
        {
          "name": "Tianlong Chen",
          "authorId": "2276535128"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.04040",
      "arxivId": "2510.04040",
      "url": "https://www.semanticscholar.org/paper/6968f45aabe7b328bb322bc35c808a6d5e5ea006",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.04040"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2b2c29f9313881b63a2c14abe542eaef811f2d18",
      "title": "Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness",
      "authors": [
        {
          "name": "Jiachun Li",
          "authorId": "2203948041"
        },
        {
          "name": "Pengfei Cao",
          "authorId": "49776272"
        },
        {
          "name": "Yubo Chen",
          "authorId": "1763402"
        },
        {
          "name": "Kang Liu",
          "authorId": "77397868"
        },
        {
          "name": "Jun Zhao",
          "authorId": "2269147239"
        }
      ],
      "year": 2024,
      "abstract": "Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",
      "citationCount": 9,
      "doi": "10.18653/v1/2025.findings-acl.560",
      "arxivId": "2405.18915",
      "url": "https://www.semanticscholar.org/paper/2b2c29f9313881b63a2c14abe542eaef811f2d18",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "10747-10765"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ab75715677176ad42fc9300097f8365c76967d4e",
      "title": "When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors",
      "authors": [
        {
          "name": "Scott Emmons",
          "authorId": "2353277889"
        },
        {
          "name": "Erik Jenner",
          "authorId": "2287826196"
        },
        {
          "name": "David K. Elson",
          "authorId": "2341530316"
        },
        {
          "name": "R. Saurous",
          "authorId": "2278009"
        },
        {
          "name": "Senthooran Rajamanoharan",
          "authorId": "35185194"
        },
        {
          "name": "Heng Chen",
          "authorId": "2373324212"
        },
        {
          "name": "Irhum Shafkat",
          "authorId": "2007125145"
        },
        {
          "name": "Rohin Shah",
          "authorId": "2359788174"
        }
      ],
      "year": 2025,
      "abstract": "While chain-of-thought (CoT) monitoring is an appealing AI safety defense, recent work on\"unfaithfulness\"has cast doubt on its reliability. These findings highlight an important failure mode, particularly when CoT acts as a post-hoc rationalization in applications like auditing for bias. However, for the distinct problem of runtime monitoring to prevent severe harm, we argue the key property is not faithfulness but monitorability. To this end, we introduce a conceptual framework distinguishing CoT-as-rationalization from CoT-as-computation. We expect that certain classes of severe harm will require complex, multi-step reasoning that necessitates CoT-as-computation. Replicating the experimental setups of prior work, we increase the difficulty of the bad behavior to enforce this necessity condition; this forces the model to expose its reasoning, making it monitorable. We then present methodology guidelines to stress-test CoT monitoring against deliberate evasion. Applying these guidelines, we find that models can learn to obscure their intentions, but only when given significant help, such as detailed human-written strategies or iterative optimization against the monitor. We conclude that, while not infallible, CoT monitoring offers a substantial layer of defense that requires active protection and continued stress-testing.",
      "citationCount": 26,
      "doi": "10.48550/arXiv.2507.05246",
      "arxivId": "2507.05246",
      "url": "https://www.semanticscholar.org/paper/ab75715677176ad42fc9300097f8365c76967d4e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.05246"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
      "authors": [
        {
          "name": "Leonardo Ranaldi",
          "authorId": "2008183566"
        },
        {
          "name": "Marco Valentino",
          "authorId": "34102057"
        },
        {
          "name": "Alexander Polonsky",
          "authorId": "2345924508"
        },
        {
          "name": "Andr\u00e9 Freitas",
          "authorId": "2242981659"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",
      "citationCount": 16,
      "doi": "10.18653/v1/2025.acl-long.843",
      "arxivId": "2502.12616",
      "url": "https://www.semanticscholar.org/paper/40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "17222-17240"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ed5fc04a496681d40888df52d3303bab9a8607a9",
      "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
      "authors": [
        {
          "name": "Zhekai Duan",
          "authorId": "2290487218"
        },
        {
          "name": "Yuan Zhang",
          "authorId": "2156008691"
        },
        {
          "name": "Shikai Geng",
          "authorId": "2366071020"
        },
        {
          "name": "Gaowen Liu",
          "authorId": "2361018237"
        },
        {
          "name": "Joschka Boedecker",
          "authorId": "145581493"
        },
        {
          "name": "Chris Xiaoxuan Lu",
          "authorId": "2292774928"
        }
      ],
      "year": 2025,
      "abstract": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action (VLA) models by improving performance and interpretability through intermediate reasoning steps. However, its sequential autoregressive token generation introduces significant inference latency, limiting real-time deployment. We propose Fast ECoT, an inference-time acceleration method that exploits the structured and repetitive nature of ECoT to (1) cache and reuse high-level reasoning across timesteps and (2) parallelise the generation of modular reasoning steps. Additionally, we introduce an asynchronous scheduler that decouples reasoning from action decoding, further boosting responsiveness. Fast ECoT requires no model changes or additional training and integrates easily into existing VLA pipelines. Experiments in both simulation (LIBERO) and real-world robot tasks show up to a 7.5% reduction in latency with comparable or improved task success rate and reasoning faithfulness, bringing ECoT policies closer to practical real-time deployment.",
      "citationCount": 10,
      "doi": "10.48550/arXiv.2506.07639",
      "arxivId": "2506.07639",
      "url": "https://www.semanticscholar.org/paper/ed5fc04a496681d40888df52d3303bab9a8607a9",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.07639"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "019cfc087c4294221cf2ba3d16f2318a419741d6",
      "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding",
      "authors": [
        {
          "name": "Xi Chen",
          "authorId": "2374335018"
        },
        {
          "name": "A. Plaat",
          "authorId": "2562595"
        },
        {
          "name": "N. V. Stein",
          "authorId": "2218156728"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on multi-step tasks, yet whether the generated\"thoughts\"reflect the true internal reasoning process is unresolved. We present the first feature-level causal study of CoT faithfulness. Combining sparse autoencoders with activation patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B while they tackle GSM8K math problems under CoT and plain (noCoT) prompting. Swapping a small set of CoT-reasoning features into a noCoT run raises answer log-probabilities significantly in the 2.8B model, but has no reliable effect in 70M, revealing a clear scale threshold. CoT also leads to significantly higher activation sparsity and feature interpretability scores in the larger model, signalling more modular internal computation. For example, the model's confidence in generating correct answers improves from 1.2 to 4.3. We introduce patch-curves and random-feature patching baselines, showing that useful CoT information is not only present in the top-K patches but widely distributed. Overall, our results indicate that CoT can induce more interpretable internal structures in high-capacity LLMs, validating its role as a structured prompting method.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2507.22928",
      "arxivId": "2507.22928",
      "url": "https://www.semanticscholar.org/paper/019cfc087c4294221cf2ba3d16f2318a419741d6",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.22928"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "176b5595c46bd91e07805baf134a425ea89846de",
      "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation",
      "authors": [
        {
          "name": "Hongyan Xie",
          "authorId": "2204124202"
        },
        {
          "name": "Yitong Yao",
          "authorId": "2271643854"
        },
        {
          "name": "Yikun Ban",
          "authorId": "2367522469"
        },
        {
          "name": "Zixuan Huang",
          "authorId": "2371358462"
        },
        {
          "name": "Deqing Wang",
          "authorId": "2367458813"
        },
        {
          "name": "Zhenhe Wu",
          "authorId": "2214306761"
        },
        {
          "name": "Haoxiang Su",
          "authorId": "2273885281"
        },
        {
          "name": "Chao Wang",
          "authorId": "2380409313"
        },
        {
          "name": "Shuangyong Song",
          "authorId": "2279903067"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) excel at reasoning tasks but are expensive to deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated by LLMs to copy LLMs'abilities. However, these CoT data may include noisy rationales that either fail to substantiate the answers or contribute no additional information to support answer prediction, which leads SLMs to capture spurious correlations between questions and answers and compromise the quality of reasoning. In this work, we propose Chain-of-Thought Correctness Perception Distillation (CoPeD), which aims to improve the reasoning quality of the student model from the perspectives of task setting and data utilization. Firstly, we introduce a correctness-aware task setting that encourages the student model to predict answers based on correct rationales and revise them when they are incorrect. This setting improves the faithfulness of reasoning and allows the model to learn from its mistakes. Then, we propose a Correctness-Aware Weighted loss, which dynamically adjusts the contribution of each training instance based on the combined loss of the rationale and the answer. This strategy encourages the model to focus more on samples where the rationale offers stronger support for the correct answer. Experiments have shown that CoPeD is effective on both in-distribution (IND) and out-of-distribution (OOD) benchmark reasoning datasets.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2509.05602",
      "arxivId": "2509.05602",
      "url": "https://www.semanticscholar.org/paper/176b5595c46bd91e07805baf134a425ea89846de",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.05602"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1f9bd8a63d292510860071847a85de83c6a3a878",
      "title": "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs",
      "authors": [
        {
          "name": "Minh-Vuong Nguyen",
          "authorId": "2284764636"
        },
        {
          "name": "Linhao Luo",
          "authorId": "2238130759"
        },
        {
          "name": "Fatemeh Shiri",
          "authorId": "49994056"
        },
        {
          "name": "Dinh Q. Phung",
          "authorId": "1400659302"
        },
        {
          "name": "Yuan-Fang Li",
          "authorId": "2256011160"
        },
        {
          "name": "Thuy-Trang Vu",
          "authorId": "122699890"
        },
        {
          "name": "Gholamreza Haffari",
          "authorId": "2561045"
        }
      ],
      "year": 2024,
      "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning.",
      "citationCount": 33,
      "doi": "10.48550/arXiv.2402.11199",
      "arxivId": "2402.11199",
      "url": "https://www.semanticscholar.org/paper/1f9bd8a63d292510860071847a85de83c6a3a878",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "2862-2883"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "f33ba21e7e699fbc772b1c0b537986342f214ccd",
      "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?",
      "authors": [
        {
          "name": "Samuel Lewis-Lim",
          "authorId": "2377559888"
        },
        {
          "name": "Xingwei Tan",
          "authorId": "2364358630"
        },
        {
          "name": "Zhixue Zhao",
          "authorId": "2146630592"
        },
        {
          "name": "Nikolaos Aletras",
          "authorId": "3238627"
        }
      ],
      "year": 2025,
      "abstract": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2508.19827",
      "arxivId": "2508.19827",
      "url": "https://www.semanticscholar.org/paper/f33ba21e7e699fbc772b1c0b537986342f214ccd",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.19827"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "411dcad9fda56a26f6b20cfe1b1bcfc1efa74f75",
      "title": "Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning",
      "authors": [
        {
          "name": "Elija Perrier",
          "authorId": "2294721483"
        }
      ],
      "year": 2025,
      "abstract": "While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2510.01069",
      "arxivId": "2510.01069",
      "url": "https://www.semanticscholar.org/paper/411dcad9fda56a26f6b20cfe1b1bcfc1efa74f75",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.01069"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a4b3e5d574119eef6e3b0c3e437788db8cc9b040",
      "title": "Multi-round, Chain-of-thought Post-editing for Unfaithful Summaries",
      "authors": [
        {
          "name": "Yi Lee",
          "authorId": "2109310678"
        },
        {
          "name": "Xiangci Li",
          "authorId": "89919188"
        },
        {
          "name": "Jessica Ouyang",
          "authorId": "2284862335"
        }
      ],
      "year": 2025,
      "abstract": "Recent large language models (LLMs) have demonstrated a remarkable ability to perform natural language understanding and generation tasks. In this work, we investigate the use of LLMs for evaluating faithfulness in news summarization, finding that it achieves a strong correlation with human judgments. We further investigate LLMs' capabilities as a faithfulness post-editor, experimenting with different chain-of-thought prompts for locating and correcting factual inconsistencies between a generated summary and the source news document and are able to achieve a higher editing success rate than was reported in prior work. We perform both automated and human evaluations of the post-edited summaries, finding that prompting LLMs using chain-of-thought reasoning about factual error types is an effective faithfulness post-editing strategy, performing comparably to fine-tuned post-editing models. We also demonstrate that multiple rounds of post-editing, which has not previously been explored, can be used to gradually improve the faithfulness of summaries whose errors cannot be fully corrected in a single round.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2501.11273",
      "arxivId": "2501.11273",
      "url": "https://www.semanticscholar.org/paper/a4b3e5d574119eef6e3b0c3e437788db8cc9b040",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.11273"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "75c46f9c13e5aa603be0286553762f3d1a842ad3",
      "title": "CoRGI: Verified Chain-of-Thought Reasoning with Post-hoc Visual Grounding",
      "authors": [
        {
          "name": "Shixin Yi",
          "authorId": "2374479019"
        },
        {
          "name": "Lin Shang",
          "authorId": "2374411477"
        }
      ],
      "year": 2025,
      "abstract": "Multimodal reasoning with vision-language models (VLMs) often suffers from hallucinations, as models tend to generate explanations after only a superficial inspection of the image. We present \\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with \\textbf{G}rounded \\textbf{I}nsights), a framework that enhances reasoning reliability through post-hoc verification of chain-of-thought outputs. Given a VLM-generated rationale, CoRGI decomposes it into step-wise statements, grounds each step in visual evidence, and filters or corrects unsupported claims before producing the final answer. Experiments on five challenging benchmark-VCR, ScienceQA, MMMU, MathVista, and HallusionBenc-demonstrate that CoRGI consistently improves both answer accuracy and explanation faithfulness across multiple VLM backbones, including Qwen-2.5VL, LLaVA-1.6, and Gemma3-12B. Beyond quantitative gains, qualitative analyses further illustrate how the verification process reduces hallucination and strengthens interpretability, suggesting that post-hoc visual grounding is a promising direction for building more trustworthy and transparent multimodal reasoning systems.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2508.00378",
      "url": "https://www.semanticscholar.org/paper/75c46f9c13e5aa603be0286553762f3d1a842ad3",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "901ba81d0ed1cfbecef6227e730715d6e4d6f8e7",
      "title": "A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring",
      "authors": [
        {
          "name": "Julian Schulz",
          "authorId": "2386989421"
        }
      ],
      "year": 2025,
      "abstract": "As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.19476",
      "arxivId": "2510.19476",
      "url": "https://www.semanticscholar.org/paper/901ba81d0ed1cfbecef6227e730715d6e4d6f8e7",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.19476"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 20,
  "errors": []
}
