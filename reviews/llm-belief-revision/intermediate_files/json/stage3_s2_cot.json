{
  "status": "success",
  "source": "semantic_scholar",
  "query": "chain of thought reasoning",
  "results": [
    {
      "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": [
        {
          "name": "Xuezhi Wang",
          "authorId": "2275277634"
        },
        {
          "name": "Jason Wei",
          "authorId": "119640649"
        },
        {
          "name": "D. Schuurmans",
          "authorId": "50319359"
        },
        {
          "name": "Quoc Le",
          "authorId": "1998340269"
        },
        {
          "name": "Ed H. Chi",
          "authorId": "2226805"
        },
        {
          "name": "Denny Zhou",
          "authorId": "65855107"
        }
      ],
      "year": 2022,
      "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
      "citationCount": 5523,
      "doi": null,
      "arxivId": "2203.11171",
      "url": "https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2203.11171"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Jason Wei",
          "authorId": "119640649"
        },
        {
          "name": "Xuezhi Wang",
          "authorId": "2275277634"
        },
        {
          "name": "Dale Schuurmans",
          "authorId": "1714772"
        },
        {
          "name": "Maarten Bosma",
          "authorId": "40377863"
        },
        {
          "name": "Ed H. Chi",
          "authorId": "2226805"
        },
        {
          "name": "F. Xia",
          "authorId": "144956443"
        },
        {
          "name": "Quoc Le",
          "authorId": "1998340269"
        },
        {
          "name": "Denny Zhou",
          "authorId": "65855107"
        }
      ],
      "year": 2022,
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
      "citationCount": 14519,
      "doi": null,
      "arxivId": "2201.11903",
      "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2201.11903"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f208ea909fa7f54fea82def9a92fd81dfc758c39",
      "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
      "authors": [
        {
          "name": "H. Trivedi",
          "authorId": "6365809"
        },
        {
          "name": "Niranjan Balasubramanian",
          "authorId": "35217367"
        },
        {
          "name": "Tushar Khot",
          "authorId": "2236429"
        },
        {
          "name": "Ashish Sabharwal",
          "authorId": "48229640"
        }
      ],
      "year": 2022,
      "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
      "citationCount": 743,
      "doi": "10.48550/arXiv.2212.10509",
      "arxivId": "2212.10509",
      "url": "https://www.semanticscholar.org/paper/f208ea909fa7f54fea82def9a92fd81dfc758c39",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2212.10509"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "45e1c99a1c8935bf137c0b51a08a03ffb6821993",
      "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs",
      "authors": [
        {
          "name": "Edward Y. Chang",
          "authorId": "2317013622"
        },
        {
          "name": "Yuxuan Tong",
          "authorId": "2344576207"
        },
        {
          "name": "Morry Niu",
          "authorId": "2343833063"
        },
        {
          "name": "Graham Neubig",
          "authorId": "2315307439"
        },
        {
          "name": "Xiang Yue",
          "authorId": "2327047556"
        }
      ],
      "year": 2025,
      "abstract": "Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.",
      "citationCount": 252,
      "doi": "10.48550/arXiv.2502.03373",
      "arxivId": "2502.03373",
      "url": "https://www.semanticscholar.org/paper/45e1c99a1c8935bf137c0b51a08a03ffb6821993",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.03373"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2d7f3a99e916fc80ff890d109699f9682253e66d",
      "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
      "authors": [
        {
          "name": "Qingqing Zhao",
          "authorId": "2263960925"
        },
        {
          "name": "Yao Lu",
          "authorId": "2326092665"
        },
        {
          "name": "Moo Jin Kim",
          "authorId": "2159987907"
        },
        {
          "name": "Zipeng Fu",
          "authorId": "2307996544"
        },
        {
          "name": "Zhuoyang Zhang",
          "authorId": "2283149950"
        },
        {
          "name": "Yecheng Wu",
          "authorId": "2320183695"
        },
        {
          "name": "Zhaoshuo Li",
          "authorId": "2313218585"
        },
        {
          "name": "Qianli Ma",
          "authorId": "2313310048"
        },
        {
          "name": "Song Han",
          "authorId": "2273855886"
        },
        {
          "name": "Chelsea Finn",
          "authorId": "2347538211"
        },
        {
          "name": "Ankur Handa",
          "authorId": "2352792711"
        },
        {
          "name": "Ming-Yu Liu",
          "authorId": "2289128173"
        },
        {
          "name": "Donglai Xiang",
          "authorId": "2313206075"
        },
        {
          "name": "Gordon Wetzstein",
          "authorId": "2304557070"
        },
        {
          "name": "Tsung-Yi Lin",
          "authorId": "2300141490"
        }
      ],
      "year": 2025,
      "abstract": "Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input\u2013output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Videos are available at: https://cot-vla.github.io/.",
      "citationCount": 198,
      "doi": "10.1109/CVPR52734.2025.00166",
      "arxivId": "2503.22020",
      "url": "https://www.semanticscholar.org/paper/2d7f3a99e916fc80ff890d109699f9682253e66d",
      "venue": "Computer Vision and Pattern Recognition",
      "journal": {
        "name": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "pages": "1702-1713"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
      "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
      "authors": [
        {
          "name": "Zhuosheng Zhang",
          "authorId": "3322871"
        },
        {
          "name": "Aston Zhang",
          "authorId": "2085709"
        },
        {
          "name": "Mu Li",
          "authorId": "1701799"
        },
        {
          "name": "Hai Zhao",
          "authorId": "2146232510"
        },
        {
          "name": "G. Karypis",
          "authorId": "50877490"
        },
        {
          "name": "Alexander J. Smola",
          "authorId": "78088877"
        }
      ],
      "year": 2023,
      "abstract": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.",
      "citationCount": 708,
      "doi": "10.48550/arXiv.2302.00923",
      "arxivId": "2302.00923",
      "url": "https://www.semanticscholar.org/paper/780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "Trans. Mach. Learn. Res.",
        "volume": "2024"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5373cbd50d00ef7bc1b628355c7f73a7707fa91c",
      "title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
      "authors": [
        {
          "name": "Fengqing Jiang",
          "authorId": "2268521947"
        },
        {
          "name": "Zhangchen Xu",
          "authorId": "2268640874"
        },
        {
          "name": "Yuetai Li",
          "authorId": "2307144076"
        },
        {
          "name": "Luyao Niu",
          "authorId": "152145884"
        },
        {
          "name": "Zhen Xiang",
          "authorId": "2261738344"
        },
        {
          "name": "Bo Li",
          "authorId": "2261831004"
        },
        {
          "name": "Bill Yuchen Lin",
          "authorId": "2328296250"
        },
        {
          "name": "Radha Poovendran",
          "authorId": "2288930670"
        }
      ],
      "year": 2025,
      "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.",
      "citationCount": 76,
      "doi": "10.48550/arXiv.2502.12025",
      "arxivId": "2502.12025",
      "url": "https://www.semanticscholar.org/paper/5373cbd50d00ef7bc1b628355c7f73a7707fa91c",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.12025"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e71ff5188cc6435f0ba3ebbb054829c0b1dd3ba8",
      "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
      "authors": [
        {
          "name": "Iv'an Arcuschin",
          "authorId": "2349542533"
        },
        {
          "name": "Jett Janiak",
          "authorId": "2257034963"
        },
        {
          "name": "Robert Krzyzanowski",
          "authorId": "2308099173"
        },
        {
          "name": "Senthooran Rajamanoharan",
          "authorId": "35185194"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        },
        {
          "name": "Arthur Conmy",
          "authorId": "2131632310"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions\"Is X bigger than Y?\"and\"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.",
      "citationCount": 71,
      "doi": "10.48550/arXiv.2503.08679",
      "arxivId": "2503.08679",
      "url": "https://www.semanticscholar.org/paper/e71ff5188cc6435f0ba3ebbb054829c0b1dd3ba8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.08679"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "56b545002ebb897de167a2de1603f06ab8f6bb0a",
      "title": "Robotic Control via Embodied Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Micha\u0142 Zawalski",
          "authorId": "2376191965"
        },
        {
          "name": "William Chen",
          "authorId": "2282542320"
        },
        {
          "name": "Karl Pertsch",
          "authorId": "31719101"
        },
        {
          "name": "Oier Mees",
          "authorId": "7264115"
        },
        {
          "name": "Chelsea Finn",
          "authorId": "2257346440"
        },
        {
          "name": "Sergey Levine",
          "authorId": "2249615151"
        }
      ],
      "year": 2024,
      "abstract": "A key limitation of learned robot control policies is their inability to generalize outside their training data. Recent works on vision-language-action models (VLAs) have shown that the use of large, internet pre-trained vision-language models as the backbone of learned robot policies can substantially improve their robustness and generalization ability. Yet, one of the most exciting capabilities of large vision-language models in other domains is their ability to reason iteratively through complex problems. Can that same capability be brought into robotics to allow policies to improve performance by reasoning about a given task before acting? Naive use of\"chain-of-thought\"(CoT) style prompting is significantly less effective with standard VLAs because of the relatively simple training examples that are available to them. Additionally, purely semantic reasoning about sub-tasks, as is common in regular CoT, is insufficient for robot policies that need to ground their reasoning in sensory observations and the robot state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. We design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We demonstrate, that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source VLA policy, by 28% across challenging generalization tasks, without any additional robot training data. Additionally, ECoT makes it easier for humans to interpret a policy's failures and correct its behavior using natural language.",
      "citationCount": 209,
      "doi": "10.48550/arXiv.2407.08693",
      "arxivId": "2407.08693",
      "url": "https://www.semanticscholar.org/paper/56b545002ebb897de167a2de1603f06ab8f6bb0a",
      "venue": "Conference on Robot Learning",
      "journal": {
        "pages": "3157-3181"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
      "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
      "authors": [
        {
          "name": "Lei Wang",
          "authorId": "145131956"
        },
        {
          "name": "Wanyu Xu",
          "authorId": "2143418409"
        },
        {
          "name": "Yihuai Lan",
          "authorId": "2150277971"
        },
        {
          "name": "Zhiqiang Hu",
          "authorId": "2203447284"
        },
        {
          "name": "Yunshi Lan",
          "authorId": "3458560"
        },
        {
          "name": "Roy Ka-Wei Lee",
          "authorId": "38656724"
        },
        {
          "name": "Ee-Peng Lim",
          "authorId": "2212836814"
        }
      ],
      "year": 2023,
      "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
      "citationCount": 532,
      "doi": "10.48550/arXiv.2305.04091",
      "arxivId": "2305.04091",
      "url": "https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "2609-2634"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "196f38764cb1b6b2796f167a22b1fd451e9ee397",
      "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
      "authors": [
        {
          "name": "Yancheng He",
          "authorId": "2285046736"
        },
        {
          "name": "Shilong Li",
          "authorId": "2328102146"
        },
        {
          "name": "Jiaheng Liu",
          "authorId": "2284731877"
        },
        {
          "name": "Weixun Wang",
          "authorId": "2327902406"
        },
        {
          "name": "Xingyuan Bu",
          "authorId": "2284990102"
        },
        {
          "name": "Ge Zhang",
          "authorId": "2312670376"
        },
        {
          "name": "Z. Peng",
          "authorId": "2249854788"
        },
        {
          "name": "Zhaoxiang Zhang",
          "authorId": "2322607797"
        },
        {
          "name": "Zhicheng Zheng",
          "authorId": "2330718673"
        },
        {
          "name": "Wenbo Su",
          "authorId": "2279560018"
        },
        {
          "name": "Bo Zheng",
          "authorId": "2327961773"
        }
      ],
      "year": 2025,
      "abstract": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.",
      "citationCount": 42,
      "doi": "10.48550/arXiv.2502.19361",
      "arxivId": "2502.19361",
      "url": "https://www.semanticscholar.org/paper/196f38764cb1b6b2796f167a22b1fd451e9ee397",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.19361"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "59cb0cc2cab2b2abe6ed3b61452602b30ae1d1a3",
      "title": "Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model",
      "authors": [
        {
          "name": "Ziyang Ma",
          "authorId": "2116609277"
        },
        {
          "name": "Zhuo Chen",
          "authorId": "2280184207"
        },
        {
          "name": "Yuping Wang",
          "authorId": "2310297802"
        },
        {
          "name": "E. Chng",
          "authorId": "2457835"
        },
        {
          "name": "Xie Chen",
          "authorId": "2276453166"
        }
      ],
      "year": 2025,
      "abstract": "Large Audio-Language Models (LALMs) have demonstrated remarkable performance in tasks involving audio perception and understanding, such as speech recognition and audio captioning. However, their reasoning capabilities - critical for solving complex real-world problems - remain underexplored. In this work, we conduct the first exploration into integrating Chain-of-Thought (CoT) reasoning into LALMs to enhance their reasoning ability across auditory modalities. We evaluate representative CoT methods, analyzing their performance in both information extraction and reasoning tasks across sound, music, and speech domains. Our findings reveal that CoT methods significantly improve performance on easy and medium tasks but encounter challenges with hard tasks, where reasoning chains can confuse the model rather than improve accuracy. Additionally, we identify a positive correlation between reasoning path length and accuracy, demonstrating the potential of scaling inference for advanced instruction-following and reasoning. This study not only highlights the promise of CoT in enhancing LALM reasoning capabilities but also identifies key limitations and provides actionable directions for future research.",
      "citationCount": 43,
      "doi": "10.48550/arXiv.2501.07246",
      "arxivId": "2501.07246",
      "url": "https://www.semanticscholar.org/paper/59cb0cc2cab2b2abe6ed3b61452602b30ae1d1a3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.07246"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c8b1206ef8e6fdebd3b9ad2165937256ab8b5652",
      "title": "Chain-of-Thought Reasoning Without Prompting",
      "authors": [
        {
          "name": "Xuezhi Wang",
          "authorId": "2275277634"
        },
        {
          "name": "Denny Zhou",
          "authorId": "2256313467"
        }
      ],
      "year": 2024,
      "abstract": "In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.",
      "citationCount": 205,
      "doi": "10.48550/arXiv.2402.10200",
      "arxivId": "2402.10200",
      "url": "https://www.semanticscholar.org/paper/c8b1206ef8e6fdebd3b9ad2165937256ab8b5652",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.10200"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "827afa7dd36e4afbb1a49c735bfbb2c69749756e",
      "title": "Measuring Faithfulness in Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Tamera Lanham",
          "authorId": "46239941"
        },
        {
          "name": "Anna Chen",
          "authorId": "2111073313"
        },
        {
          "name": "Ansh Radhakrishnan",
          "authorId": "2224616677"
        },
        {
          "name": "Benoit Steiner",
          "authorId": "32163737"
        },
        {
          "name": "Carson E. Denison",
          "authorId": "1780754598"
        },
        {
          "name": "Danny Hernandez",
          "authorId": "39182747"
        },
        {
          "name": "Dustin Li",
          "authorId": "2108506462"
        },
        {
          "name": "Esin Durmus",
          "authorId": "41152329"
        },
        {
          "name": "Evan Hubinger",
          "authorId": "146614650"
        },
        {
          "name": "John Kernion",
          "authorId": "1583434563"
        },
        {
          "name": "Kamil.e Lukovsiut.e",
          "authorId": "2161242438"
        },
        {
          "name": "Karina Nguyen",
          "authorId": "2196759978"
        },
        {
          "name": "Newton Cheng",
          "authorId": "15590401"
        },
        {
          "name": "Nicholas Joseph",
          "authorId": "2117706920"
        },
        {
          "name": "Nicholas Schiefer",
          "authorId": "2833768"
        },
        {
          "name": "Oliver Rausch",
          "authorId": "2221219447"
        },
        {
          "name": "Robin Larson",
          "authorId": "48810415"
        },
        {
          "name": "Sam McCandlish",
          "authorId": "2293396643"
        },
        {
          "name": "Sandipan Kundu",
          "authorId": "2158813858"
        },
        {
          "name": "Saurav Kadavath",
          "authorId": "148070327"
        },
        {
          "name": "Shannon Yang",
          "authorId": "2225081715"
        },
        {
          "name": "T. Henighan",
          "authorId": "103143311"
        },
        {
          "name": "Timothy D. Maxwell",
          "authorId": "2069892072"
        },
        {
          "name": "Timothy Telleen-Lawton",
          "authorId": "1419532638"
        },
        {
          "name": "Tristan Hume",
          "authorId": "2162194147"
        },
        {
          "name": "Zac Hatfield-Dodds",
          "authorId": "1573482302"
        },
        {
          "name": "Jared Kaplan",
          "authorId": "2053807409"
        },
        {
          "name": "J. Brauner",
          "authorId": "38732223"
        },
        {
          "name": "Sam Bowman",
          "authorId": "1799822"
        },
        {
          "name": "Ethan Perez",
          "authorId": "3439053"
        }
      ],
      "year": 2023,
      "abstract": "Large language models (LLMs) perform better when they produce step-by-step,\"Chain-of-Thought\"(CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",
      "citationCount": 308,
      "doi": "10.48550/arXiv.2307.13702",
      "arxivId": "2307.13702",
      "url": "https://www.semanticscholar.org/paper/827afa7dd36e4afbb1a49c735bfbb2c69749756e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2307.13702"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ee8c1a46c90f1261c23479e15c6bed7f67ad8943",
      "title": "Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Hao Shao",
          "authorId": "2075457131"
        },
        {
          "name": "Shengju Qian",
          "authorId": "2293283347"
        },
        {
          "name": "Han Xiao",
          "authorId": "2238398546"
        },
        {
          "name": "Guanglu Song",
          "authorId": "12920342"
        },
        {
          "name": "Zhuofan Zong",
          "authorId": "1571400317"
        },
        {
          "name": "Letian Wang",
          "authorId": "2273906302"
        },
        {
          "name": "Yu Liu",
          "authorId": "2292207974"
        },
        {
          "name": "Hongsheng Li",
          "authorId": "2261394248"
        }
      ],
      "year": 2024,
      "abstract": "Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on https://hao-shao.com/projects/viscot.html to support further research in this area.",
      "citationCount": 216,
      "doi": "10.52202/079017-0275",
      "arxivId": "2403.16999",
      "url": "https://www.semanticscholar.org/paper/ee8c1a46c90f1261c23479e15c6bed7f67ad8943",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "Advances in Neural Information Processing Systems 37"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "29b6ea9e38b41fb2bb6838609a8c43b1632ea273",
      "title": "Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Yingqian Cui",
          "authorId": "2218740984"
        },
        {
          "name": "Pengfei He",
          "authorId": "2185740224"
        },
        {
          "name": "Jingying Zeng",
          "authorId": "2276423058"
        },
        {
          "name": "Hui Liu",
          "authorId": "2326538488"
        },
        {
          "name": "Xianfeng Tang",
          "authorId": "2313288320"
        },
        {
          "name": "Zhenwei Dai",
          "authorId": "2313363021"
        },
        {
          "name": "Yan Han",
          "authorId": "2336079495"
        },
        {
          "name": "Chen Luo",
          "authorId": "2305051640"
        },
        {
          "name": "Jing Huang",
          "authorId": "2336079074"
        },
        {
          "name": "Zhen Li",
          "authorId": "2313299851"
        },
        {
          "name": "Suhang Wang",
          "authorId": "2257362558"
        },
        {
          "name": "Yue Xing",
          "authorId": "2253469617"
        },
        {
          "name": "Jiliang Tang",
          "authorId": "2314887353"
        },
        {
          "name": "Qi He",
          "authorId": "2315065019"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.",
      "citationCount": 29,
      "doi": "10.48550/arXiv.2502.13260",
      "arxivId": "2502.13260",
      "url": "https://www.semanticscholar.org/paper/29b6ea9e38b41fb2bb6838609a8c43b1632ea273",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.13260"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e919b13bf37062d0e463a7632f033f80d9e6c6dc",
      "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
      "authors": [
        {
          "name": "Jiaru Zou",
          "authorId": "2288273199"
        },
        {
          "name": "Ling Yang",
          "authorId": "2363283279"
        },
        {
          "name": "Jingwen Gu",
          "authorId": "2371008348"
        },
        {
          "name": "Jiahao Qiu",
          "authorId": "2353709628"
        },
        {
          "name": "Ke Shen",
          "authorId": "2362627848"
        },
        {
          "name": "Jingrui He",
          "authorId": "2273658099"
        },
        {
          "name": "Mengdi Wang",
          "authorId": "2325202622"
        }
      ],
      "year": 2025,
      "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Project: https://github.com/Gen-Verse/ReasonFlux",
      "citationCount": 19,
      "doi": "10.48550/arXiv.2506.18896",
      "arxivId": "2506.18896",
      "url": "https://www.semanticscholar.org/paper/e919b13bf37062d0e463a7632f033f80d9e6c6dc",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.18896"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ee1e6db704778751a22f10741f959039ceecbaf7",
      "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
      "authors": [
        {
          "name": "Kangan Qian",
          "authorId": "2305682798"
        },
        {
          "name": "Sicong Jiang",
          "authorId": "2311319605"
        },
        {
          "name": "Yang Zhong",
          "authorId": "2365436913"
        },
        {
          "name": "Ziang Luo",
          "authorId": "2300253724"
        },
        {
          "name": "Zilin Huang",
          "authorId": "2278457018"
        },
        {
          "name": "Tianze Zhu",
          "authorId": "2332888052"
        },
        {
          "name": "Kun Jiang",
          "authorId": "2332541142"
        },
        {
          "name": "Mengmeng Yang",
          "authorId": "2111076804"
        },
        {
          "name": "Zheng Fu",
          "authorId": "2139308270"
        },
        {
          "name": "Jinyu Miao",
          "authorId": "2261938331"
        },
        {
          "name": "Yining Shi",
          "authorId": "2118897651"
        },
        {
          "name": "He Zhe Lim",
          "authorId": "2363823923"
        },
        {
          "name": "Li Liu",
          "authorId": "2363329910"
        },
        {
          "name": "Tianbao Zhou",
          "authorId": "2363417192"
        },
        {
          "name": "Hongyi Wang",
          "authorId": "2362764587"
        },
        {
          "name": "Huang Yu",
          "authorId": "2362859233"
        },
        {
          "name": "Yifei Hu",
          "authorId": "2362832763"
        },
        {
          "name": "Guang Li",
          "authorId": "2363550815"
        },
        {
          "name": "Guangyao Chen",
          "authorId": "2221747134"
        },
        {
          "name": "Hao Ye",
          "authorId": "2364353781"
        },
        {
          "name": "Lijun Sun",
          "authorId": "2365053655"
        },
        {
          "name": "Diange Yang",
          "authorId": "2262080417"
        }
      ],
      "year": 2025,
      "abstract": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce \\textbf{AgentThink}, a pioneering unified framework that integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's core innovations include: \\textbf{(i) Structured Data Generation}, which establishes an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline}, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and \\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel multi-tool assessment protocol to rigorously evaluate the model's tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate that AgentThink significantly boosts overall reasoning scores by \\textbf{53.91%} and enhances answer accuracy by \\textbf{33.54%}, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot/few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models. Code is available at https://github.com/curryqka/AgentThink.",
      "citationCount": 22,
      "doi": "10.48550/arXiv.2505.15298",
      "arxivId": "2505.15298",
      "url": "https://www.semanticscholar.org/paper/ee1e6db704778751a22f10741f959039ceecbaf7",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.15298"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b06850ba45723357badedca11034c94bbc373543",
      "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing",
      "authors": [
        {
          "name": "Huadai Liu",
          "authorId": "2166335547"
        },
        {
          "name": "Jialei Wang",
          "authorId": "2304513799"
        },
        {
          "name": "Kaicheng Luo",
          "authorId": "2356639140"
        },
        {
          "name": "Wen Wang",
          "authorId": "2347565594"
        },
        {
          "name": "Qian Chen",
          "authorId": "2257010473"
        },
        {
          "name": "Zhou Zhao",
          "authorId": "2371145194"
        },
        {
          "name": "Wei Xue",
          "authorId": "2356603415"
        }
      ],
      "year": 2025,
      "abstract": "While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, this generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics, and excels in the out-of-distribution Movie Gen Audio benchmark. The project page is available at https://ThinkSound-Project.github.io.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2506.21448",
      "arxivId": "2506.21448",
      "url": "https://www.semanticscholar.org/paper/b06850ba45723357badedca11034c94bbc373543",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.21448"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0d3f125867ddbb0a4882feb0043134a02957c375",
      "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
      "authors": [
        {
          "name": "Chengshuai Zhao",
          "authorId": "2038670428"
        },
        {
          "name": "Zhen Tan",
          "authorId": "2309805913"
        },
        {
          "name": "Pingchuan Ma",
          "authorId": "2375154019"
        },
        {
          "name": "Dawei Li",
          "authorId": "2161635474"
        },
        {
          "name": "Bohan Jiang",
          "authorId": "2036355404"
        },
        {
          "name": "Yancheng Wang",
          "authorId": "2115944576"
        },
        {
          "name": "Yingzhen Yang",
          "authorId": "2193639833"
        },
        {
          "name": "Huan Liu",
          "authorId": "2287545693"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-Thought (CoT) prompting has been shown to be effective in eliciting structured reasoning (i.e., CoT reasoning) from large language models (LLMs). Regardless of its popularity, recent studies expose its failures in some reasoning tasks, raising fundamental questions about the nature of CoT reasoning. In this work, we propose a data distribution lens to understand when and why CoT reasoning succeeds or fails. We hypothesize that CoT reasoning reflects a structured inductive bias learned from in-distribution data, enabling models to conditionally generate reasoning trajectories that approximate those observed during training. As such, the effectiveness of CoT reasoning is fundamentally governed by the nature and degree of distribution discrepancy between training data and test queries. Guided by this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To test the hypothesis, we introduce DataAlchemy, an abstract and fully controllable environment that trains LLMs from scratch and systematically probes them under various distribution conditions. Through rigorous controlled experiments, we reveal that CoT reasoning is a brittle mirage when it is pushed beyond training distributions, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.",
      "citationCount": 31,
      "doi": "10.48550/arXiv.2508.01191",
      "arxivId": "2508.01191",
      "url": "https://www.semanticscholar.org/paper/0d3f125867ddbb0a4882feb0043134a02957c375",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.01191"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "3fb26c0cf930b04635540e4815c4b8ca0581155c",
      "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs",
      "authors": [
        {
          "name": "Xuan Zhang",
          "authorId": "2249844074"
        },
        {
          "name": "Chao Du",
          "authorId": "144369497"
        },
        {
          "name": "Tianyu Pang",
          "authorId": "19201674"
        },
        {
          "name": "Qian Liu",
          "authorId": "2284062049"
        },
        {
          "name": "Wei Gao",
          "authorId": "2244152141"
        },
        {
          "name": "Min Lin",
          "authorId": "2253977831"
        }
      ],
      "year": 2024,
      "abstract": "The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through Chain of Preference Optimization (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO.",
      "citationCount": 119,
      "doi": "10.48550/arXiv.2406.09136",
      "arxivId": "2406.09136",
      "url": "https://www.semanticscholar.org/paper/3fb26c0cf930b04635540e4815c4b8ca0581155c",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.09136"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d27b8bb0aa2775c270d6f4edc2f32437aae20afc",
      "title": "Improve Vision Language Model Chain-of-thought Reasoning",
      "authors": [
        {
          "name": "Ruohong Zhang",
          "authorId": "46752970"
        },
        {
          "name": "Bowen Zhang",
          "authorId": "2256276486"
        },
        {
          "name": "Yanghao Li",
          "authorId": "2314073842"
        },
        {
          "name": "Haotian Zhang",
          "authorId": "2257340591"
        },
        {
          "name": "Zhiqing Sun",
          "authorId": "48064856"
        },
        {
          "name": "Zhe Gan",
          "authorId": "2253397669"
        },
        {
          "name": "Yinfei Yang",
          "authorId": "2249897805"
        },
        {
          "name": "Ruoming Pang",
          "authorId": "2238621132"
        },
        {
          "name": "Yiming Yang",
          "authorId": "2257099254"
        }
      ],
      "year": 2024,
      "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.",
      "citationCount": 95,
      "doi": "10.48550/arXiv.2410.16198",
      "arxivId": "2410.16198",
      "url": "https://www.semanticscholar.org/paper/d27b8bb0aa2775c270d6f4edc2f32437aae20afc",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "1631-1662"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "984c06f1ca4c4eec2a569cef8495c6ab8f28a47e",
      "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Yukun Qi",
          "authorId": "2351774796"
        },
        {
          "name": "Yiming Zhao",
          "authorId": "2351793588"
        },
        {
          "name": "Yu Zeng",
          "authorId": "2351828089"
        },
        {
          "name": "Xikun Bao",
          "authorId": "2351812063"
        },
        {
          "name": "Wenxuan Huang",
          "authorId": "2294440519"
        },
        {
          "name": "Lin Chen",
          "authorId": "2267778503"
        },
        {
          "name": "Zehui Chen",
          "authorId": "2293554731"
        },
        {
          "name": "Jie Zhao",
          "authorId": "2351814324"
        },
        {
          "name": "Zhongang Qi",
          "authorId": "2357013817"
        },
        {
          "name": "Feng Zhao",
          "authorId": "2295588235"
        }
      ],
      "year": 2025,
      "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.",
      "citationCount": 18,
      "doi": "10.48550/arXiv.2504.07956",
      "arxivId": "2504.07956",
      "url": "https://www.semanticscholar.org/paper/984c06f1ca4c4eec2a569cef8495c6ab8f28a47e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.07956"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e8d92af6e1779e72ea7bf332225b16f720f68487",
      "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics",
      "authors": [
        {
          "name": "Ruilin Luo",
          "authorId": "2279024030"
        },
        {
          "name": "Zhuofan Zheng",
          "authorId": "2339032141"
        },
        {
          "name": "Yifan Wang",
          "authorId": "2352243596"
        },
        {
          "name": "Yiyao Yu",
          "authorId": "2255733564"
        },
        {
          "name": "Xinzhe Ni",
          "authorId": "2294362233"
        },
        {
          "name": "Zicheng Lin",
          "authorId": "2279171100"
        },
        {
          "name": "Jin Zeng",
          "authorId": "2339742913"
        },
        {
          "name": "Yujiu Yang",
          "authorId": "2284727148"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 23,
      "doi": "10.48550/arXiv.2501.04686",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e8d92af6e1779e72ea7bf332225b16f720f68487",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.04686"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "363460d064d27fb4a0c09e7aa83b0893fe280962",
      "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Xinyan Chen",
          "authorId": "2345335833"
        },
        {
          "name": "Renrui Zhang",
          "authorId": "2291314199"
        },
        {
          "name": "Dongzhi Jiang",
          "authorId": "2293242031"
        },
        {
          "name": "Aojun Zhou",
          "authorId": "2276425017"
        },
        {
          "name": "Shilin Yan",
          "authorId": "2210554979"
        },
        {
          "name": "Weifeng Lin",
          "authorId": "2284068796"
        },
        {
          "name": "Hongsheng Li",
          "authorId": "2292396209"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT",
      "citationCount": 22,
      "doi": "10.48550/arXiv.2506.05331",
      "arxivId": "2506.05331",
      "url": "https://www.semanticscholar.org/paper/363460d064d27fb4a0c09e7aa83b0893fe280962",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.05331"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8d09e0678cb14d94ca8d7a24093520c8b9fddf7e",
      "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Xinghao Chen",
          "authorId": "2325183464"
        },
        {
          "name": "Anhao Zhao",
          "authorId": "2312758890"
        },
        {
          "name": "Heming Xia",
          "authorId": "2308104853"
        },
        {
          "name": "Xuan Lu",
          "authorId": "2349551985"
        },
        {
          "name": "Hanlin Wang",
          "authorId": "2303427516"
        },
        {
          "name": "Yanjun Chen",
          "authorId": "2317300679"
        },
        {
          "name": "Wei Zhang",
          "authorId": "144973249"
        },
        {
          "name": "Jian Wang",
          "authorId": "2258854033"
        },
        {
          "name": "Wenjie Li",
          "authorId": "2347280375"
        },
        {
          "name": "Xiaoyu Shen",
          "authorId": "2323434742"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) have shown impressive performance on complex tasks through Chain-of-Thought (CoT) reasoning. However, conventional CoT relies on explicitly verbalized intermediate steps, which constrains its broader applicability, particularly in abstract reasoning tasks beyond language. To address this, there has been growing research interest in \\textit{latent CoT reasoning}, where the reasoning process is embedded within latent spaces. By decoupling reasoning from explicit language generation, latent CoT offers the promise of richer cognitive representations and facilitates more flexible, faster inference. This paper aims to present a comprehensive overview of this emerging paradigm and establish a systematic taxonomy. We analyze recent advances in methods, categorizing them from token-wise horizontal approaches to layer-wise vertical strategies. We then provide in-depth discussions of these methods, highlighting their design principles, applications, and remaining challenges. We hope that our survey provides a structured foundation for advancing this promising direction in LLM reasoning. The relevant papers will be regularly updated at https://github.com/EIT-NLP/Awesome-Latent-CoT.",
      "citationCount": 18,
      "doi": "10.48550/arXiv.2505.16782",
      "arxivId": "2505.16782",
      "url": "https://www.semanticscholar.org/paper/8d09e0678cb14d94ca8d7a24093520c8b9fddf7e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.16782"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
      "title": "Faithful Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Qing Lyu",
          "authorId": "1904906987"
        },
        {
          "name": "Shreya Havaldar",
          "authorId": "151207988"
        },
        {
          "name": "Adam Stein",
          "authorId": "2161714960"
        },
        {
          "name": "Li Zhang",
          "authorId": "72436283"
        },
        {
          "name": "D. Rao",
          "authorId": "48810734"
        },
        {
          "name": "Eric Wong",
          "authorId": "2053678328"
        },
        {
          "name": "Marianna Apidianaki",
          "authorId": "2817917"
        },
        {
          "name": "Chris Callison-Burch",
          "authorId": "1763608"
        }
      ],
      "year": 2023,
      "abstract": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",
      "citationCount": 318,
      "doi": "10.48550/arXiv.2301.13379",
      "arxivId": "2301.13379",
      "url": "https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
      "venue": "International Joint Conference on Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2301.13379"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "4b7d86b7fd8d05b6347a5ed11c1cc0156acf5d33",
      "title": "MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models",
      "authors": [
        {
          "name": "Jusheng Zhang",
          "authorId": "2344795356"
        },
        {
          "name": "Kaitong Cai",
          "authorId": "2344749425"
        },
        {
          "name": "Xiaoyang Guo",
          "authorId": "2398492738"
        },
        {
          "name": "Sidi Liu",
          "authorId": "2365352499"
        },
        {
          "name": "Qinhan Lv",
          "authorId": "2378578783"
        },
        {
          "name": "Ruiqi Chen",
          "authorId": "2364160430"
        },
        {
          "name": "Jing Yang",
          "authorId": "2378876577"
        },
        {
          "name": "Yijia Fan",
          "authorId": "2344807642"
        },
        {
          "name": "Xiaofei Sun",
          "authorId": "2378889630"
        },
        {
          "name": "Jian Wang",
          "authorId": "2345420305"
        },
        {
          "name": "Ziliang Chen",
          "authorId": "2385761666"
        },
        {
          "name": "Liang Lin",
          "authorId": "2397555794"
        },
        {
          "name": "Keze Wang",
          "authorId": "2344758172"
        }
      ],
      "year": 2025,
      "abstract": "The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.",
      "citationCount": 19,
      "doi": null,
      "arxivId": "2512.08228",
      "url": "https://www.semanticscholar.org/paper/4b7d86b7fd8d05b6347a5ed11c1cc0156acf5d33",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "0a1dfca3c4e13fbdf95aec9ee460cac7d06a0a52",
      "title": "Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Xinghao Chen",
          "authorId": "2325183464"
        },
        {
          "name": "Zhijing Sun",
          "authorId": "2347300950"
        },
        {
          "name": "Wenjin Guo",
          "authorId": "2347400865"
        },
        {
          "name": "Miaoran Zhang",
          "authorId": "2143802568"
        },
        {
          "name": "Yanjun Chen",
          "authorId": "2342345446"
        },
        {
          "name": "Yirong Sun",
          "authorId": "2325115130"
        },
        {
          "name": "Hui Su",
          "authorId": "2349762405"
        },
        {
          "name": "Yijie Pan",
          "authorId": "2349272000"
        },
        {
          "name": "Dietrich Klakow",
          "authorId": "2279917171"
        },
        {
          "name": "Wenjie Li",
          "authorId": "2347280375"
        },
        {
          "name": "Xiaoyu Shen",
          "authorId": "2240598941"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) excel in reasoning tasks through Chain-of-Thought (CoT) prompting. However, CoT prompting greatly increases computational demands, which has prompted growing interest in distilling CoT capabilities into Small Language Models (SLMs). This study systematically examines the factors influencing CoT distillation, including the choice of granularity, format and teacher model. Through experiments involving four teacher models and seven student models across seven mathematical and commonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs, SLMs exhibit a non-monotonic relationship with granularity, with stronger models benefiting from finer-grained reasoning and weaker models performing better with simpler CoT supervision; (2) CoT format significantly impacts LLMs but has minimal effect on SLMs, likely due to their reliance on supervised fine-tuning rather than pretraining preferences; (3) Stronger teacher models do NOT always produce better student models, as diversity and complexity in CoT supervision can outweigh accuracy alone. These findings emphasize the need to tailor CoT strategies to specific student model, offering actionable insights for optimizing CoT distillation in SLMs. The code and datasets are available at https://github.com/EIT-NLP/Distilling-CoT-Reasoning.",
      "citationCount": 20,
      "doi": "10.48550/arXiv.2502.18001",
      "arxivId": "2502.18001",
      "url": "https://www.semanticscholar.org/paper/0a1dfca3c4e13fbdf95aec9ee460cac7d06a0a52",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.18001"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "4a99be7d5e0fbbdb28914bd5e96df26949ecb75e",
      "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages",
      "authors": [
        {
          "name": "Libo Qin",
          "authorId": "49169076"
        },
        {
          "name": "Qiguang Chen",
          "authorId": "2133447633"
        },
        {
          "name": "Fuxuan Wei",
          "authorId": "2106895004"
        },
        {
          "name": "Shijue Huang",
          "authorId": "2355659792"
        },
        {
          "name": "Wanxiang Che",
          "authorId": "2262459819"
        }
      ],
      "year": 2023,
      "abstract": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate reasoning paths, thus promoting reasoning accuracy and attracting increasing attention. Specifically, zero-shot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the prompt\"Let's think step by step!\". Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development. In this work, we introduce cross-lingual prompting (CLP), aiming to improve zero-shot CoT reasoning across languages. Specifically, CLP consists of two main components: (1) cross-lingual alignment prompting and (2) task-specific solver prompting. The cross-lingual alignment prompting is responsible for aligning representations across different languages, whereas the task-specific solver prompting is used to generate the final chain of thoughts and results for the reasoning task. In addition, we further introduce cross-lingual self-consistent prompting (CLSP) to ensemble different reasoning paths across languages. Our experimental evaluations on several benchmarks demonstrate that CLP and CLSP significantly outperform the existing prompting methods and achieve state-of-the-art performance. We hope this work will inspire further breakthroughs in cross-lingual CoT.",
      "citationCount": 135,
      "doi": "10.48550/arXiv.2310.14799",
      "arxivId": "2310.14799",
      "url": "https://www.semanticscholar.org/paper/4a99be7d5e0fbbdb28914bd5e96df26949ecb75e",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2310.14799"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "606b535686f98015ea8dd3c9ba8aa9243bc9dc2d",
      "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Linzheng Chai",
          "authorId": "2165382882"
        },
        {
          "name": "Jian Yang",
          "authorId": "2276103971"
        },
        {
          "name": "Tao Sun",
          "authorId": "2284180939"
        },
        {
          "name": "Hongcheng Guo",
          "authorId": "2234806"
        },
        {
          "name": "Jiaheng Liu",
          "authorId": "2182423032"
        },
        {
          "name": "Bing Wang",
          "authorId": "2275240628"
        },
        {
          "name": "Xiannian Liang",
          "authorId": "2279864035"
        },
        {
          "name": "Jiaqi Bai",
          "authorId": "2107018151"
        },
        {
          "name": "Tongliang Li",
          "authorId": "47268605"
        },
        {
          "name": "Qiyao Peng",
          "authorId": "2279544844"
        },
        {
          "name": "Zhoujun Li",
          "authorId": "2258837278"
        }
      ],
      "year": 2024,
      "abstract": "Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (xCoT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (xCoT-Instruct) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results demonstrate the superior performance of xCoT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap.",
      "citationCount": 66,
      "doi": "10.48550/arXiv.2401.07037",
      "arxivId": "2401.07037",
      "url": "https://www.semanticscholar.org/paper/606b535686f98015ea8dd3c9ba8aa9243bc9dc2d",
      "venue": "AAAI Conference on Artificial Intelligence",
      "journal": {
        "pages": "23550-23558"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "8236010c2ecc94d826be6010ff187fdc000e7df6",
      "title": "Deductive Verification of Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Z. Ling",
          "authorId": "49706114"
        },
        {
          "name": "Yunhao Fang",
          "authorId": "2219045025"
        },
        {
          "name": "Xuanlin Li",
          "authorId": "2108263986"
        },
        {
          "name": "Zhiao Huang",
          "authorId": "18036051"
        },
        {
          "name": "Mingu Lee",
          "authorId": "2108721816"
        },
        {
          "name": "R. Memisevic",
          "authorId": "1710604"
        },
        {
          "name": "Hao Su",
          "authorId": "2087042750"
        }
      ],
      "year": 2023,
      "abstract": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.",
      "citationCount": 194,
      "doi": null,
      "arxivId": "2306.03872",
      "url": "https://www.semanticscholar.org/paper/8236010c2ecc94d826be6010ff187fdc000e7df6",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2306.03872"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "90e97ee8a3421fb83f967c8f9bed7328f8aa8d9c",
      "title": "Aligning Large and Small Language Models via Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Leonardo Ranaldi",
          "authorId": "2008183566"
        },
        {
          "name": "Andr\u00e9 Freitas",
          "authorId": "2277458933"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 66,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/90e97ee8a3421fb83f967c8f9bed7328f8aa8d9c",
      "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
      "journal": {
        "pages": "1812-1827"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "f42f61a547c5996be6aee175145b0d74e6324dff",
      "title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future",
      "authors": [
        {
          "name": "Zheng Chu",
          "authorId": "2162359198"
        },
        {
          "name": "Jingchang Chen",
          "authorId": "2218647765"
        },
        {
          "name": "Qianglong Chen",
          "authorId": "1500384901"
        },
        {
          "name": "Weijiang Yu",
          "authorId": "2248673673"
        },
        {
          "name": "Tao He",
          "authorId": "2247838930"
        },
        {
          "name": "Haotian Wang",
          "authorId": "2256768984"
        },
        {
          "name": "Weihua Peng",
          "authorId": "2247980601"
        },
        {
          "name": "Ming Liu",
          "authorId": "145111960"
        },
        {
          "name": "Bing Qin",
          "authorId": "2247852651"
        },
        {
          "name": "Ting Liu",
          "authorId": "2238862997"
        }
      ],
      "year": 2023,
      "abstract": "Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence. Notably, recent studies have revealed that chain-of-thought prompting significantly enhances LLM's reasoning capabilities, which attracts widespread attention from both academics and industry. In this paper, we systematically investigate relevant research, summarizing advanced methods through a meticulous taxonomy that offers novel perspectives. Moreover, we delve into the current frontiers and delineate the challenges and future directions, thereby shedding light on future research. Furthermore, we engage in a discussion about open questions. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at https://github.com/zchuz/CoT-Reasoning-Survey",
      "citationCount": 222,
      "doi": "10.18653/v1/2024.acl-long.65",
      "arxivId": "2309.15402",
      "url": "https://www.semanticscholar.org/paper/f42f61a547c5996be6aee175145b0d74e6324dff",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "1173-1203"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "c19ac62fd7fdf2a4ba6581563212c3b33737b030",
      "title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering",
      "authors": [
        {
          "name": "Xinyu Tang",
          "authorId": "2109887979"
        },
        {
          "name": "Xiaolei Wang",
          "authorId": "72541556"
        },
        {
          "name": "Zhihao Lv",
          "authorId": "2351111598"
        },
        {
          "name": "Yingqian Min",
          "authorId": "2007666579"
        },
        {
          "name": "Wayne Xin Zhao",
          "authorId": "2294811281"
        },
        {
          "name": "Binbin Hu",
          "authorId": "2279160677"
        },
        {
          "name": "Ziqi Liu",
          "authorId": "2284032340"
        },
        {
          "name": "Zhiqiang Zhang",
          "authorId": "2266809812"
        }
      ],
      "year": 2025,
      "abstract": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly improved the reasoning capabilities of large language models(LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs. In this work, we conduct an empirical analysis for this question from the perspective of representation. We find that LLMs do encode long CoT reasoning as a general capability, with a clear distinction from vanilla CoTs. Furthermore, domain-specific representations are also required for the effective transfer of long CoT reasoning. Inspired by these findings, we propose GLoRE, a novel representation engineering method to unleash the general long CoT reasoning capabilities of LLMs. Extensive experiments demonstrate the effectiveness and efficiency of GLoRE in both in-domain and cross-domain scenarios.",
      "citationCount": 24,
      "doi": "10.48550/arXiv.2503.11314",
      "arxivId": "2503.11314",
      "url": "https://www.semanticscholar.org/paper/c19ac62fd7fdf2a4ba6581563212c3b33737b030",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.11314"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "1bf0ba18c887ad92a358144cffc22fb407c74a56",
      "title": "Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers",
      "authors": [
        {
          "name": "Alireza Amiri",
          "authorId": "2343747447"
        },
        {
          "name": "Xinting Huang",
          "authorId": "2303617629"
        },
        {
          "name": "Mark Rofin",
          "authorId": "2187576116"
        },
        {
          "name": "Michael Hahn",
          "authorId": "2284220013"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-thought reasoning and scratchpads have emerged as critical tools for enhancing the computational capabilities of transformers. While theoretical results show that polynomial-length scratchpads can extend transformers'expressivity from $TC^0$ to $PTIME$, their required length remains poorly understood. Empirical evidence even suggests that transformers need scratchpads even for many problems in $TC^0$, such as Parity or Multiplication, challenging optimistic bounds derived from circuit complexity. In this work, we initiate the study of systematic lower bounds for the number of chain-of-thought steps across different algorithmic problems, in the hard-attention regime. We study a variety of algorithmic problems, and provide bounds that are tight up to logarithmic factors. Overall, these results contribute to emerging understanding of the power and limitations of chain-of-thought reasoning.",
      "citationCount": 13,
      "doi": "10.48550/arXiv.2502.02393",
      "arxivId": "2502.02393",
      "url": "https://www.semanticscholar.org/paper/1bf0ba18c887ad92a358144cffc22fb407c74a56",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.02393"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "766b8ff9ccc77eb062dde58253da39d0907ebfb0",
      "title": "Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization",
      "authors": [
        {
          "name": "Kesen Zhao",
          "authorId": "2357818396"
        },
        {
          "name": "Beier Zhu",
          "authorId": "79682148"
        },
        {
          "name": "Qianru Sun",
          "authorId": "2138109020"
        },
        {
          "name": "H. Zhang",
          "authorId": "2244611126"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perception--identifying key regions and reasoning based on them--UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in https://github.com/kesenzhao/UV-CoT.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2504.18397",
      "arxivId": "2504.18397",
      "url": "https://www.semanticscholar.org/paper/766b8ff9ccc77eb062dde58253da39d0907ebfb0",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.18397"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
      "authors": [
        {
          "name": "Leonardo Ranaldi",
          "authorId": "2008183566"
        },
        {
          "name": "Marco Valentino",
          "authorId": "34102057"
        },
        {
          "name": "Alexander Polonsky",
          "authorId": "2345924508"
        },
        {
          "name": "Andr\u00e9 Freitas",
          "authorId": "2242981659"
        }
      ],
      "year": 2025,
      "abstract": "Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",
      "citationCount": 16,
      "doi": "10.18653/v1/2025.acl-long.843",
      "arxivId": "2502.12616",
      "url": "https://www.semanticscholar.org/paper/40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "17222-17240"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "70316474ff561cc01b0f336d10483ea4dadc2051",
      "title": "ImageGen-CoT: Enhancing Text-to-Image In-context Learning with Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Jiaqi Liao",
          "authorId": "2315613899"
        },
        {
          "name": "Zhengyuan Yang",
          "authorId": "2149231840"
        },
        {
          "name": "Linjie Li",
          "authorId": "50703697"
        },
        {
          "name": "Dianqi Li",
          "authorId": "2325116457"
        },
        {
          "name": "K. Lin",
          "authorId": "2249717753"
        },
        {
          "name": "Yu Cheng",
          "authorId": "2339635488"
        },
        {
          "name": "Lijuan Wang",
          "authorId": "2273909761"
        }
      ],
      "year": 2025,
      "abstract": "In this work, we study the problem of Text-to-Image In-Context Learning (T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in recent years, they struggle with contextual reasoning in T2I-ICL scenarios. To address this limitation, we propose a novel framework that incorporates a thought process called ImageGen-CoT prior to image generation. To avoid generating unstructured ineffective reasoning steps, we develop an automatic pipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs using this dataset to enhance their contextual reasoning capabilities. To further enhance performance, we explore test-time scale-up strategies and propose a novel hybrid scaling approach. This approach first generates multiple ImageGen-CoT chains and then produces multiple images for each chain via sampling. Extensive experiments demonstrate the effectiveness of our proposed method. Notably, fine-tuning with the ImageGen-CoT dataset leads to a substantial 80\\% performance gain for SEED-X on T2I-ICL tasks. See our project page at https://ImageGen-CoT.github.io/. Code and model weights will be open-sourced.",
      "citationCount": 21,
      "doi": "10.48550/arXiv.2503.19312",
      "arxivId": "2503.19312",
      "url": "https://www.semanticscholar.org/paper/70316474ff561cc01b0f336d10483ea4dadc2051",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.19312"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2a832b34583332cbaae8cf02ee5f823c43f5689e",
      "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models",
      "authors": [
        {
          "name": "Zihao Li",
          "authorId": "2275056316"
        },
        {
          "name": "Xu Wang",
          "authorId": "2362806351"
        },
        {
          "name": "Yuzhe Yang",
          "authorId": "2316673843"
        },
        {
          "name": "Ziyu Yao",
          "authorId": "2286622362"
        },
        {
          "name": "Haoyi Xiong",
          "authorId": "2362633817"
        },
        {
          "name": "Mengnan Du",
          "authorId": "2278794127"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs.",
      "citationCount": 13,
      "doi": "10.48550/arXiv.2505.15634",
      "arxivId": "2505.15634",
      "url": "https://www.semanticscholar.org/paper/2a832b34583332cbaae8cf02ee5f823c43f5689e",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.15634"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
