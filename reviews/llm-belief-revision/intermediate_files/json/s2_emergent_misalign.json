{
  "status": "success",
  "source": "semantic_scholar",
  "query": "emergent misalignment fine-tuning",
  "results": [
    {
      "paperId": "9108cc97f9498e0de5f54f78c3c739f1013212bd",
      "title": "Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment in LLMs",
      "authors": [
        {
          "name": "Jeremiah Giordani",
          "authorId": "2336103507"
        }
      ],
      "year": 2025,
      "abstract": "Recent work has shown that fine-tuning large language models (LLMs) on code with security vulnerabilities can result in misaligned and unsafe behaviors across broad domains. These results prompted concerns about the emergence of harmful behaviors from narrow domain fine-tuning. In this paper, we contextualize these findings by analyzing how such narrow adaptation impacts the internal mechanisms and behavioral manifestations of LLMs. Through a series of experiments covering output probability distributions, loss and gradient vector geometry, layer-wise activation dynamics, and activation space dimensions, we find that behaviors attributed to\"emergent misalignment\"may be better interpreted as an erosion of prior alignment. We show that fine tuning on insecure code induces internal changes that oppose alignment. Further, we identify a shared latent dimension in the model's activation space that governs alignment behavior. We show that this space is activated by insecure code and by misaligned responses more generally, revealing how narrow fine-tuning can degrade general safety behavior by interfering with shared internal mechanisms. Our findings offer a mechanistic interpretation for previously observed misalignment phenomena, and highlights the fragility of alignment in LLMs. The results underscore the need for more robust fine-tuning strategies that preserve intended behavior across domains.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2507.03662",
      "arxivId": "2507.03662",
      "url": "https://www.semanticscholar.org/paper/9108cc97f9498e0de5f54f78c3c739f1013212bd",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.03662"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b52f7aa85ef5ddade34324f5e86cf04c01f404d2",
      "title": "Model Organisms for Emergent Misalignment",
      "authors": [
        {
          "name": "Edward Turner",
          "authorId": "2367045363"
        },
        {
          "name": "Anna Soligo",
          "authorId": "2342503380"
        },
        {
          "name": "Mia Taylor",
          "authorId": "2367289195"
        },
        {
          "name": "Senthooran Rajamanoharan",
          "authorId": "35185194"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        }
      ],
      "year": 2025,
      "abstract": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we both advance understanding and provide tools for future research. Using new narrowly misaligned datasets, we create a set of improved model organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B parameter models (vs. 32B), and that induce misalignment using a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a mechanistic phase transition and demonstrate that it corresponds to a robust behavioural phase transition in all studied organisms. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs.",
      "citationCount": 22,
      "doi": "10.48550/arXiv.2506.11613",
      "arxivId": "2506.11613",
      "url": "https://www.semanticscholar.org/paper/b52f7aa85ef5ddade34324f5e86cf04c01f404d2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.11613"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "473ea02396f57d35476a3a3c29e08d77d6ec471e",
      "title": "Persona Features Control Emergent Misalignment",
      "authors": [
        {
          "name": "Miles Wang",
          "authorId": "2328091362"
        },
        {
          "name": "Tom Dupr\u00e9 la Tour",
          "authorId": "2370936333"
        },
        {
          "name": "Olivia Watkins",
          "authorId": "2328111449"
        },
        {
          "name": "Aleksandar Makelov",
          "authorId": "17775913"
        },
        {
          "name": "Ryan A. Chi",
          "authorId": "2370937684"
        },
        {
          "name": "Samuel Miserendino",
          "authorId": "2328088976"
        },
        {
          "name": "Johannes Heidecke",
          "authorId": "2151087994"
        },
        {
          "name": "Tejal Patwardhan",
          "authorId": "90169542"
        },
        {
          "name": "Dan Mossing",
          "authorId": "2358035875"
        }
      ],
      "year": 2025,
      "abstract": "Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes\"emergent misalignment,\"where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a\"model diffing\"approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several\"misaligned persona\"features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.",
      "citationCount": 15,
      "doi": "10.48550/arXiv.2506.19823",
      "arxivId": "2506.19823",
      "url": "https://www.semanticscholar.org/paper/473ea02396f57d35476a3a3c29e08d77d6ec471e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.19823"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8b97874ce9904c9f5e712054d24a8b45cc6fed5e",
      "title": "Convergent Linear Representations of Emergent Misalignment",
      "authors": [
        {
          "name": "Anna Soligo",
          "authorId": "2342503380"
        },
        {
          "name": "Edward Turner",
          "authorId": "2367045363"
        },
        {
          "name": "Senthooran Rajamanoharan",
          "authorId": "35185194"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        }
      ],
      "year": 2025,
      "abstract": "Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a'misalignment direction'from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2506.11618",
      "arxivId": "2506.11618",
      "url": "https://www.semanticscholar.org/paper/8b97874ce9904c9f5e712054d24a8b45cc6fed5e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.11618"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2bb736efea104a5b44d40a6a686cc6432c85a63f",
      "title": "In-Training Defenses against Emergent Misalignment in Language Models",
      "authors": [
        {
          "name": "David Kacz\u00e9r",
          "authorId": "2391621411"
        },
        {
          "name": "Magnus J\u00f8rgenv\u00e5g",
          "authorId": "2380364214"
        },
        {
          "name": "Clemens Vetter",
          "authorId": "2375328011"
        },
        {
          "name": "Lucie Flek",
          "authorId": "2322003062"
        },
        {
          "name": "Florian Mai",
          "authorId": "2350752079"
        }
      ],
      "year": 2025,
      "abstract": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs) for new domains, yet recent work reveals emergent misalignment (EMA): Even a small, domain-specific fine-tune can induce harmful behaviors far outside the target domain. Even in the case where model weights are hidden behind a fine-tuning API, this gives attackers inadvertent access to a broadly misaligned model in a way that can be hard to detect from the fine-tuning data alone. We present the first systematic study of in-training safeguards against EMA that are practical for providers who expose fine-tuning via an API. We investigate four training regularization interventions: (i) KL-divergence regularization toward a safe reference model, (ii) $\\ell_2$ distance in feature space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving of a small amount of safe training examples from a general instruct-tuning dataset. We first evaluate the methods'emergent misalignment effect across four malicious, EMA-inducing tasks. Second, we assess the methods'impacts on benign tasks. We conclude with a discussion of open questions in emergent misalignment research.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2508.06249",
      "arxivId": "2508.06249",
      "url": "https://www.semanticscholar.org/paper/2bb736efea104a5b44d40a6a686cc6432c85a63f",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.06249"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "935aa9898e77e27ee4598751a26701e001a8b2bd",
      "title": "Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment",
      "authors": [
        {
          "name": "Julian Arnold",
          "authorId": "2370931483"
        },
        {
          "name": "Niels L\u00f6rch",
          "authorId": "2267500209"
        }
      ],
      "year": 2025,
      "abstract": "Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is broadly misaligned with respect to human values. To understand when and how this emergent misalignment occurs, we develop a comprehensive framework for detecting and characterizing rapid transitions during fine-tuning using both distributional change detection methods as well as order parameters that are formulated in plain English and evaluated by an LLM judge. Using an objective statistical dissimilarity measure, we quantify how the phase transition that occurs during fine-tuning affects multiple aspects of the model. In particular, we assess what percentage of the total distributional change in model outputs is captured by different aspects, such as alignment or verbosity, providing a decomposition of the overall transition. We also find that the actual behavioral transition occurs later in training than indicated by the peak in the gradient norm alone. Our framework enables the automated discovery and quantification of language-based order parameters, which we demonstrate on examples ranging from knowledge questions to politics and ethics.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2508.20015",
      "arxivId": "2508.20015",
      "url": "https://www.semanticscholar.org/paper/935aa9898e77e27ee4598751a26701e001a8b2bd",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.20015"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4bc42976421fcac43cf3c9481eb17df550ffb8b1",
      "title": "From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs",
      "authors": [
        {
          "name": "Erum Mushtaq",
          "authorId": "40510091"
        },
        {
          "name": "Anil Ramakrishna",
          "authorId": "2371992819"
        },
        {
          "name": "Satyapriya Krishna",
          "authorId": "2143841730"
        },
        {
          "name": "Sattvik Sahai",
          "authorId": "2167648845"
        },
        {
          "name": "Prasoon Goyal",
          "authorId": "38774604"
        },
        {
          "name": "Kai-Wei Chang",
          "authorId": "2256646555"
        },
        {
          "name": "Tao Zhang",
          "authorId": "2393040350"
        },
        {
          "name": "Rahul Gupta",
          "authorId": "2375904621"
        }
      ],
      "year": 2025,
      "abstract": "Recent work has shown that fine-tuning on insecure code data can trigger an emergent misalignment (EMA) phenomenon, where models generate malicious responses even to prompts unrelated to the original insecure code-writing task. Such cross-domain generalization of harmful behavior underscores the need for a deeper understanding of the algorithms, tasks, and datasets that induce emergent misalignment. In this work, we extend this study by demonstrating that emergent misalignment can also arise from narrow refusal unlearning in specific domains. We perform refusal unlearning on Cybersecurity and Safety concept, and evaluate EMA by monitoring refusal scores across seven responsible AI (RAI) domains, Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy. Our work shows that narrow domain unlearning can yield compliance responses for the targeted concept, however, it may also propagate EMA to unrelated domains. Among the two intervened concepts, Cybersecurity and Safety, we find that the safety concept can have larger EMA impact, i.e, causing lower refusal scores, across other unrelated domains such as bias. We observe this effect consistently across two model families, Mistral-7b-0.3v, and Qwen-7b-2.5. Further, we show that refusal unlearning augmented with cross-entropy loss function on a small set of retain data from the affected domains can largely, if not fully, restore alignment across the impacted domains while having lower refusal rate on the concept we perform unlearning on. To investigate the underlying causes of EMA, we analyze concept entanglements at the representation level via concept vectors. Our analysis reveals that concepts with higher representation similarity in earlier layers are more susceptible to EMA after intervention when the refusal stream is altered through targeted refusal unlearning.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2511.14017",
      "arxivId": "2511.14017",
      "url": "https://www.semanticscholar.org/paper/4bc42976421fcac43cf3c9481eb17df550ffb8b1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.14017"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1f9d187bb975ae902f5875178553ac1a8f5d05bd",
      "title": "The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs",
      "authors": [
        {
          "name": "Craig Dickson",
          "authorId": "2394173106"
        }
      ],
      "year": 2025,
      "abstract": "Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed\"emergent misalignment\"(Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales. We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%. We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's'degrees of freedom'to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.20104",
      "arxivId": "2511.20104",
      "url": "https://www.semanticscholar.org/paper/1f9d187bb975ae902f5875178553ac1a8f5d05bd",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.20104"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "328575a7bfa48912dc9cc8865458877a9bff2cd8",
      "title": "How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs",
      "authors": [
        {
          "name": "Ouyang Jian",
          "authorId": "2381834827"
        },
        {
          "name": "T. Arman",
          "authorId": "2381834574"
        },
        {
          "name": "Ge Jin",
          "authorId": "2382844732"
        }
      ],
      "year": 2025,
      "abstract": "This paper investigates the impact of incorrect data on the performance and safety of large language models (LLMs), specifically gpt-4o, during supervised fine-tuning (SFT). Although LLMs become increasingly vital across broad domains like finance, coding, law, and health, fine-tuning on incorrect data can lead to\"emergent misalignment,\"producing harmful or deceptive outputs unrelated to the intended task. We evaluate gpt-4o models fine-tuned with varying ratios (10\\% to 90\\% correct) of both obviously and subtly incorrect data across four domains: coding, finance, health, and legal. Our findings show that even modest amounts of incorrect data (10-25\\%) dramatically degrade domain performance and not moral alignment. A clear threshold of at least 50\\% correct data is needed for models to consistently recover strong performance, though they rarely match the robustness and safety of the base model, which exhibits near-perfect alignment and zero dangerous completions out-of-the-box. This research emphasizes that the cost of incorrect data is heavy, highlighting the critical need for extremely high-quality data curation or, alternatively, leveraging robust base models without unnecessary fine-tuning for high-stakes applications.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.19325",
      "arxivId": "2509.19325",
      "url": "https://www.semanticscholar.org/paper/328575a7bfa48912dc9cc8865458877a9bff2cd8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.19325"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ec873d166eb68554ac95a117d9bb74908e593e01",
      "title": "Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning",
      "authors": [
        {
          "name": "Helena Casademunt",
          "authorId": "2373042956"
        },
        {
          "name": "Caden Juang",
          "authorId": "2300368861"
        },
        {
          "name": "Adam Karvonen",
          "authorId": "2314115348"
        },
        {
          "name": "Samuel Marks",
          "authorId": "2225941937"
        },
        {
          "name": "Senthooran Rajamanoharan",
          "authorId": "35185194"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        }
      ],
      "year": 2025,
      "abstract": "Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2507.16795",
      "arxivId": "2507.16795",
      "url": "https://www.semanticscholar.org/paper/ec873d166eb68554ac95a117d9bb74908e593e01",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.16795"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c095658f490b21b40d9b6479a6bf213d6fcfe797",
      "title": "Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing",
      "authors": [
        {
          "name": "Aly M. Kassem",
          "authorId": "2198232749"
        },
        {
          "name": "Zhuan Shi",
          "authorId": "2374149107"
        },
        {
          "name": "Negar Rostamzadeh",
          "authorId": "2304550560"
        },
        {
          "name": "Golnoosh Farnadi",
          "authorId": "2338893151"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) are frequently fine-tuned or unlearned to adapt to new tasks or eliminate undesirable behaviors. While existing evaluation methods assess performance after such interventions, there remains no general approach for detecting unintended side effects, such as unlearning biology content degrading performance on chemistry tasks, particularly when these effects are unpredictable or emergent. To address this issue, we introduce MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight framework for identifying these side effects using sparse model diffing. MNEME compares base and fine-tuned models on task-agnostic data (for example, The Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent accuracy in predicting side effects, aligning with known benchmarks and requiring no custom heuristics. Furthermore, we show that retraining on high-activation samples can partially reverse these effects. Our results demonstrate that sparse probing and diffing offer a scalable and automated lens into fine-tuning-induced model changes, providing practical tools for understanding and managing LLM behavior.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2507.21084",
      "arxivId": "2507.21084",
      "url": "https://www.semanticscholar.org/paper/c095658f490b21b40d9b6479a6bf213d6fcfe797",
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.21084"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "5429ae1d62b6e31a6fbf7a283b60a2a683c6908c",
      "title": "Enhance Graph Alignment for Large Language Models",
      "authors": [
        {
          "name": "Haitong Luo",
          "authorId": "2110564628"
        },
        {
          "name": "Xuying Meng",
          "authorId": "3134123"
        },
        {
          "name": "Suhang Wang",
          "authorId": "2277804135"
        },
        {
          "name": "Tianxiang Zhao",
          "authorId": "1999191869"
        },
        {
          "name": "Fali Wang",
          "authorId": "2280093700"
        },
        {
          "name": "Hanyun Cao",
          "authorId": "2277811676"
        },
        {
          "name": "Yujun Zhang",
          "authorId": "2198357154"
        }
      ],
      "year": 2024,
      "abstract": "Graph-structured data is prevalent in the real world. Recently, due to the powerful emergent capabilities, Large Language Models (LLMs) have shown promising performance in modeling graphs. The key to effectively applying LLMs on graphs is converting graph data into a format LLMs can comprehend. Graph-to-token approaches are popular in enabling LLMs to process graph information. They transform graphs into sequences of tokens and align them with text tokens through instruction tuning, where self-supervised instruction tuning helps LLMs acquire general knowledge about graphs, and supervised fine-tuning specializes LLMs for the downstream tasks on graphs. Despite their initial success, we find that existing methods have a misalignment between self-supervised tasks and supervised downstream tasks, resulting in negative transfer from self-supervised fine-tuning to downstream tasks. To address these issues, we propose Graph Alignment Large Language Models (GALLM) to benefit from aligned task templates. In the self-supervised tuning stage, we introduce a novel text matching task using templates aligned with downstream tasks. In the task-specific tuning stage, we propose two category prompt methods that learn supervision information from additional explanation with further aligned templates. Experimental evaluations on four datasets demonstrate substantial improvements in supervised learning, multi-dataset generalizability, and particularly in zero-shot capability, highlighting the model's potential as a graph foundation model.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2410.11370",
      "arxivId": "2410.11370",
      "url": "https://www.semanticscholar.org/paper/5429ae1d62b6e31a6fbf7a283b60a2a683c6908c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.11370"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4070a490f1cd1b9938666dbbb27c4ee627f1ddaa",
      "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
      "authors": [
        {
          "name": "Jan Betley",
          "authorId": "2310234236"
        },
        {
          "name": "Daniel Tan",
          "authorId": "2346974032"
        },
        {
          "name": "Niels Warncke",
          "authorId": "2346985301"
        },
        {
          "name": "Anna Sztyber-Betley",
          "authorId": "2210640332"
        },
        {
          "name": "Xuchan Bao",
          "authorId": "2341338787"
        },
        {
          "name": "Mart'in Soto",
          "authorId": "2341338628"
        },
        {
          "name": "Nathan Labenz",
          "authorId": "2346982707"
        },
        {
          "name": "Owain Evans",
          "authorId": "47107786"
        }
      ],
      "year": 2025,
      "abstract": "We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.",
      "citationCount": 110,
      "doi": "10.1038/s41586-025-09937-5",
      "arxivId": "2502.17424",
      "url": "https://www.semanticscholar.org/paper/4070a490f1cd1b9938666dbbb27c4ee627f1ddaa",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.17424"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "0cf20ade4261279e444b437720593ecd700b0cc1",
      "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
      "authors": [
        {
          "name": "Dongyoon Hahm",
          "authorId": "2327211159"
        },
        {
          "name": "Taywon Min",
          "authorId": "2298273949"
        },
        {
          "name": "Woogyeol Jin",
          "authorId": "2374960555"
        },
        {
          "name": "Kimin Lee",
          "authorId": "2298481460"
        }
      ],
      "year": 2025,
      "abstract": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2508.14031",
      "arxivId": "2508.14031",
      "url": "https://www.semanticscholar.org/paper/0cf20ade4261279e444b437720593ecd700b0cc1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.14031"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "603b0144faf69fb3216b843667b6f27050cccf0e",
      "title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning",
      "authors": [
        {
          "name": "Feng Chen",
          "authorId": "2305742835"
        },
        {
          "name": "Allan Raventos",
          "authorId": "2344830995"
        },
        {
          "name": "Nan Cheng",
          "authorId": "2344831856"
        },
        {
          "name": "Surya Ganguli",
          "authorId": "2290019800"
        },
        {
          "name": "Shaul Druckmann",
          "authorId": "2303994977"
        }
      ],
      "year": 2025,
      "abstract": "Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\\it misaligned}$ with pass@N in that pass@N accuracy ${\\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2502.07154",
      "arxivId": "2502.07154",
      "url": "https://www.semanticscholar.org/paper/603b0144faf69fb3216b843667b6f27050cccf0e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.07154"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "dbe9ad142cf1d8b13ea27d69571f2a23d8f2f1df",
      "title": "Continuous Model Calibration: Leveraging Feedback-Driven Fine-Tuning for Self- Correcting Large Language Models",
      "authors": [
        {
          "name": "Opeyemi Joseph Awotunde",
          "authorId": "2354430351"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) have revolutionized natural language processing by enabling advanced text generation, comprehension, and interactive capabilities. However, their performance often degrades when confronted with real-world variability, requiring continuous refinement to maintain accuracy, reliability, and ethical integrity. Traditional model calibration relies on periodic updates and static fine-tuning, which fail to address evolving language patterns, contextual nuances, and emergent biases. To overcome these limitations, continuous model calibration introduces a feedback-driven fine-tuning mechanism that enables self-correcting capabilities in LLMs. This approach integrates progressive tuning techniques, real-time human-AI collaboration, and anomaly detection frameworks to dynamically adjust model behavior. Progressive tuning leverages reinforcement learning with human feedback (RLHF) and adaptive loss functions to iteratively refine LLM responses, ensuring alignment with contextual accuracy and user expectations. Human-AI collaboration further enhances model calibration by incorporating domain experts' insights and structured feedback loops to mitigate ethical risks, bias propagation, and factual inconsistencies. Additionally, anomaly detection mechanisms identify distributional shifts and inconsistencies in generated responses, allowing automated interventions to preempt erroneous or misleading outputs. This study explores the interplay between self-correction methodologies and real-world applications, emphasizing the need for transparent governance and robust evaluation metrics. We examine case studies across conversational AI, legal reasoning, and healthcare applications to demonstrate the efficacy of feedback-driven fine-tuning in maintaining model adaptability. By establishing a continuous improvement framework, this research aims to optimize AI reliability, enhance interpretability, and promote ethically aligned decision-making in dynamic environments.",
      "citationCount": 3,
      "doi": "10.55248/gengpi.6.0325.1208",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/dbe9ad142cf1d8b13ea27d69571f2a23d8f2f1df",
      "venue": "International Journal of Research Publication and Reviews",
      "journal": {
        "name": "International Journal of Research Publication and Reviews"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "555b532aa0adaa8cab4b6842411e1089a6f6f8d3",
      "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs",
      "authors": [
        {
          "name": "Yunheng Li",
          "authorId": "2272304474"
        },
        {
          "name": "Jing Cheng",
          "authorId": "2381934863"
        },
        {
          "name": "Shaoyong Jia",
          "authorId": "2381962395"
        },
        {
          "name": "Hangyi Kuang",
          "authorId": "2381375523"
        },
        {
          "name": "Shaohui Jiao",
          "authorId": "2358342804"
        },
        {
          "name": "Qibin Hou",
          "authorId": "2283308332"
        },
        {
          "name": "Ming-Ming Cheng",
          "authorId": "2261250042"
        }
      ],
      "year": 2025,
      "abstract": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1",
      "citationCount": 6,
      "doi": "10.48550/arXiv.2509.18056",
      "arxivId": "2509.18056",
      "url": "https://www.semanticscholar.org/paper/555b532aa0adaa8cab4b6842411e1089a6f6f8d3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.18056"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6274c0d30c42490327e4a2e90a38b8185c044d89",
      "title": "Parameter-Efficient Fine-Tuning of Large Language Models via Deconvolution in Subspace",
      "authors": [
        {
          "name": "Jia-Chen Zhang",
          "authorId": "2316010602"
        },
        {
          "name": "Yu-Jie Xiong",
          "authorId": "2316127143"
        },
        {
          "name": "Chun-Ming Xia",
          "authorId": "2215678318"
        },
        {
          "name": "Dong-Hai Zhu",
          "authorId": "2315991006"
        },
        {
          "name": "Xi-He Qiu",
          "authorId": "2318324240"
        }
      ],
      "year": 2025,
      "abstract": "Large language model (LLM) is considered a milestone towards achieving Artificial General Intelligence (AGI). With its advanced emergent capabilities, it adapt to a wide range of specific applications. Fine-tuning LLMs for various downstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA) is well-known for its parameter efficiency. It can reduce the number of parameters needed to fine-tune LLMs by several orders of magnitude. However, LoRA-based approaches encounter a significant limitation due to the bottleneck imposed by rank one decomposition. As the parameters count in LLMs increase, even rank one decomposition might surpass the number of parameters truly necessary for handling more downstream tasks. In this paper, we propose a new method for Parameter-Efficient Fine-Tuning (PEFT) via deconvolution in subspace, dubbed as DCFT. We innovatively use deconvolution to complete details and enhance knowledge in subspace incremental matrices, and dynamically control parameters by adjusting the kernel size, unconstrained by rank-one decomposition. Extensive experiments are conducted to validate the effectiveness of DCFT. Results show that compared to LoRA, DCFT achieve an 8$\\times$ reduction in parameters, and still achieves highly impressive performance. Our code is available here: https://github.com/Godz-z/DCFT.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2503.01419",
      "arxivId": "2503.01419",
      "url": "https://www.semanticscholar.org/paper/6274c0d30c42490327e4a2e90a38b8185c044d89",
      "venue": "International Conference on Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.01419"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "665b47402caf316b4b6cf4e0d7b16ad8fa609f93",
      "title": "Instruct Where the Model Fails: Generative Data Augmentation via Guided Self-contrastive Fine-tuning",
      "authors": [
        {
          "name": "Weijian Ma",
          "authorId": "2357016873"
        },
        {
          "name": "Ruoxin Chen",
          "authorId": "2329200819"
        },
        {
          "name": "Ke-Yue Zhang",
          "authorId": "2156563603"
        },
        {
          "name": "Shuang Wu",
          "authorId": "2329626055"
        },
        {
          "name": "Shouhong Ding",
          "authorId": "2305300611"
        }
      ],
      "year": 2025,
      "abstract": "Data augmentation is expected to bring about unseen features of training set, enhancing the model\u2019s ability to generalize in situations where data is limited. Generative image models trained on large web-crawled datasets such as LAION are known to produce images with stereotypes and imperceptible bias when used to augment training data, owing to dataset misalignment and the generator\u2019s ignorance of the downstream model. We improve downstream task awareness in generated images by proposing a task-aware fine-tuning strategy that actively detects failures of downstream task in the target model to fine-tune the generation process between epochs. The dynamic fine-tuning strategy is achieved by (1) inspecting misalignment between generated data and original data via VLM captioners and (2) adjusts both prompts and diffusion model so that the strategy dynamically guides the generator by focusing on the detected bias of VLM. This is done via re-captioning the overfitted data as well as finetuning the diffusion trajectory in a contrastive manner. To co-operate with the VLM captioner, the contrastive fine-tuning process dynamically adjusts different parts of the diffusion trajectory based on detected misalignment, thus shifting the the generated distribution away from making the downstream model overfit. Our experiments on few-shot class incremental learning show that our instruction-guided finetuning strategy consistently assists the downstream model with higher classification accuracy compared to generative data augmentation baselines such as Stable Diffusion and GPT-4o, and state-of-the-art non-generative strategies.",
      "citationCount": 4,
      "doi": "10.1609/aaai.v39i6.32640",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/665b47402caf316b4b6cf4e0d7b16ad8fa609f93",
      "venue": "AAAI Conference on Artificial Intelligence",
      "journal": {
        "pages": "5991-5999"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "7b343cb0f4836a12cd91f0820dd7b510250cdbe8",
      "title": "Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation",
      "authors": [
        {
          "name": "X. Xing",
          "authorId": "2339779002"
        },
        {
          "name": "Avinab Saha",
          "authorId": "2339778433"
        },
        {
          "name": "Junfeng He",
          "authorId": "2275304133"
        },
        {
          "name": "Susan Hao",
          "authorId": "2315984194"
        },
        {
          "name": "Paul Vicol",
          "authorId": "2039154"
        },
        {
          "name": "Moonkyung Ryu",
          "authorId": "2315983829"
        },
        {
          "name": "Gang Li",
          "authorId": "2275603136"
        },
        {
          "name": "S. Singla",
          "authorId": "49757324"
        },
        {
          "name": "Sarah Young",
          "authorId": "2275056273"
        },
        {
          "name": "Yinxiao Li",
          "authorId": "2279096780"
        },
        {
          "name": "Feng Yang",
          "authorId": "2275594056"
        },
        {
          "name": "Deepa Ramachandran",
          "authorId": "48919321"
        }
      ],
      "year": 2025,
      "abstract": "Text-to-image (T2I) generation has made significant advances in recent years, but challenges still remain in the generation of perceptual artifacts, misalignment with complex prompts, and safety. The prevailing approach to address these issues involves collecting human feedback on generated images, training reward models to estimate human feedback, and then fine-tuning T2I models based on the reward models to align them with human preferences. However, while existing reward fine-tuning methods can produce images with higher rewards, they may change model behavior in unexpected ways. For example, fine-tuning for one quality aspect (e.g., safety) may degrade other aspects (e.g., prompt alignment), or may lead to reward hacking (e.g., finding a way to increase rewards without having the intended effect). In this paper, we propose Focus-N-Fix, the first region-aware fine-tuning method that trains models to correct only previously problematic image regions. The resulting fine-tuned model generates images with the same high-level structure as the original model but shows significant improvements in regions where the original model was deficient in safety (over-sexualization and violence), plausibility, or other criteria. Our experiments demonstrate that Focus-N-Fix improves these localized quality aspects with little or no degradation to others and typically imperceptible changes in the rest of the image. Disclaimer: This paper contains images that may be overly sexual, violent, offensive or harmful.",
      "citationCount": 3,
      "doi": "10.1109/CVPR52734.2025.01723",
      "arxivId": "2501.06481",
      "url": "https://www.semanticscholar.org/paper/7b343cb0f4836a12cd91f0820dd7b510250cdbe8",
      "venue": "Computer Vision and Pattern Recognition",
      "journal": {
        "name": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "pages": "18486-18496"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "afb2ce5d3f46d77e274b77d771bf356f508d1df6",
      "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning",
      "authors": [
        {
          "name": "Maosen Zhao",
          "authorId": "2363890572"
        },
        {
          "name": "Pengtao Chen",
          "authorId": "2304797258"
        },
        {
          "name": "Chong Yu",
          "authorId": "2313356930"
        },
        {
          "name": "Yan Wen",
          "authorId": "2365373587"
        },
        {
          "name": "Xudong Tan",
          "authorId": "2349958813"
        },
        {
          "name": "Tao Chen",
          "authorId": "2364448351"
        }
      ],
      "year": 2025,
      "abstract": "Model quantization reduces the bit-width of weights and activations, improving memory efficiency and inference speed in diffusion models. However, achieving 4-bit quantization remains challenging. Existing methods, primarily based on integer quantization and post-training quantization fine-tuning, struggle with inconsistent performance. Inspired by the success of floating-point (FP) quantization in large language models, we explore low-bit FP quantization for diffusion models and identify key challenges: the failure of signed FP quantization to handle asymmetric activation distributions, the insufficient consideration of temporal complexity in the denoising process during fine-tuning, and the misalignment between fine-tuning loss and quantization error. To address these challenges, we propose the mixup-sign floating-point quantization (MSFP) framework, first introducing unsigned FP quantization in model quantization, along with timestep-aware LoRA (TALoRA) and denoising-factor loss alignment (DFA), which ensure precise and stable fine-tuning. Extensive experiments show that we are the first to achieve superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.",
      "citationCount": 3,
      "doi": "10.1109/CVPR52734.2025.01690",
      "arxivId": "2505.21591",
      "url": "https://www.semanticscholar.org/paper/afb2ce5d3f46d77e274b77d771bf356f508d1df6",
      "venue": "Computer Vision and Pattern Recognition",
      "journal": {
        "name": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "pages": "18134-18143"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "d4a55403bffeb6d5a0ed32c30af46c4bd47e97ca",
      "title": "Fine-Tuning-Based Transfer Learning for Building Extraction from Off-Nadir Remote Sensing Images",
      "authors": [
        {
          "name": "B. Neupane",
          "authorId": "1380888308"
        },
        {
          "name": "J. Aryal",
          "authorId": "2260706277"
        },
        {
          "name": "Abbas Rajabifard",
          "authorId": "2260706186"
        }
      ],
      "year": 2025,
      "abstract": "Building extraction\u2014needed for urban planning and monitoring\u2014is affected by the misalignment between labels and off-nadir remote sensing imagery. A computer vision approach to teacher\u2013student learning between large\u2013noisy and small\u2013clean data has been introduced as a solution, but with limited accuracy and efficiency. This paper proposes fine-tuning-based transfer learning (FTL) to adapt a pre-trained model from a noisy source to a clean target dataset, improving segmentation accuracy in off-nadir images. A standardized experimental framework is developed with three new building datasets containing large\u2013noisy and small\u2013clean image\u2013label pairs of multiple spatial resolutions. These datasets cover a range of building types, from low-rise to skyscrapers. Additionally, this paper presents one of the most extensive benchmarking efforts in teacher\u2013student learning for building extraction from off-nadir images. Results demonstrate that FTL outperforms the existing methods with higher F1 scores\u20140.943 (low-rise), 0.868 (mid-rise), 0.912 (high-rise), and 0.697 (skyscrapers)\u2014and higher computational efficiency. A notable gain in mean difference is observed in taller buildings from complex urban environments. The proposed method, datasets, and benchmarking framework provide a robust foundation for accurate building extraction and broader remote sensing applications.",
      "citationCount": 2,
      "doi": "10.3390/rs17071251",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d4a55403bffeb6d5a0ed32c30af46c4bd47e97ca",
      "venue": "Remote Sensing",
      "journal": {
        "name": "Remote Sensing"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "99706876b30b5a48ca155e87dff7c4a54b8f09a8",
      "title": "Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings",
      "authors": [
        {
          "name": "Ivan Montoya Sanchez",
          "authorId": "2358264351"
        },
        {
          "name": "Shaswata Mitra",
          "authorId": "2150699855"
        },
        {
          "name": "Aritran Piplai",
          "authorId": "2482027"
        },
        {
          "name": "Sudip Mittal",
          "authorId": "2284767922"
        }
      ],
      "year": 2025,
      "abstract": "The rapid evolution of malware variants requires robust classification methods to enhance cybersecurity. While large language models (LLMs) offer potential for generating malware descriptions to aid family classification, their utility is limited by semantic embedding overlaps and misalignment with binary behavioral features. We propose a contrastive fine-tuning (CFT) method that refines LLM embeddings via targeted selection of hard negative samples based on cosine similarity, enabling LLMs to distinguish between closely related malware families. Our approach combines high-similarity negatives to enhance discriminative power and mid-tier negatives to increase embedding diversity, optimizing both precision and generalization. Evaluated on the CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework on a few-shot setting. Experiments demonstrate significant improvements: our method achieves 63.15% classification accuracy with as few as 20 samples on CIC-AndMal-2020, outperforming baselines by 11\u201321 percentage points and surpassing prior negative sampling strategies. Ablation studies confirm the superiority of similarity-based selection over random sampling, with gains of 10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions that generalize to unseen variants, bridging textual and binary feature gaps. This work advances malware classification by enabling nuanced semantic distinctions and provides a scalable framework for adapting LLMs to cybersecurity challenges.",
      "citationCount": 2,
      "doi": "10.1109/IJCNN64981.2025.11229181",
      "arxivId": "2504.21028",
      "url": "https://www.semanticscholar.org/paper/99706876b30b5a48ca155e87dff7c4a54b8f09a8",
      "venue": "IEEE International Joint Conference on Neural Network",
      "journal": {
        "name": "2025 International Joint Conference on Neural Networks (IJCNN)",
        "pages": "1-8"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "4abc380d4d9697a79b23e21fed361a0fa13997fa",
      "title": "Wav2Prompt: End-to-End Speech Prompt Learning and Task-based Fine-tuning for Text-based LLMs",
      "authors": [
        {
          "name": "Keqi Deng",
          "authorId": "2068923436"
        },
        {
          "name": "Guangzhi Sun",
          "authorId": "2310565909"
        },
        {
          "name": "Phil Woodland",
          "authorId": "2271322630"
        }
      ],
      "year": 2025,
      "abstract": "Wav2Prompt is proposed which allows integrating spoken input with a text-based large language model (LLM). Wav2Prompt uses a straightforward training process with only the same data used to train an automatic speech recognition (ASR) model. After training, Wav2Prompt learns continuous representations from speech and uses them as LLM prompts. To avoid task over-fitting issues found in prior work and preserve the emergent abilities of LLMs, Wav2Prompt takes LLM token embeddings as the training targets and utilises a continuous integrate-and-fire mechanism for explicit speech-text alignment. Therefore, a Wav2Prompt-LLM combination can be applied to zero-shot spoken language tasks such as speech translation (ST), speech understanding (SLU), and spoken-query-based question answering (SQQA). It is shown that for these zero-shot tasks, Wav2Prompt performs similarly to an ASR-LLM cascade and better than recent prior work. If relatively small amounts of task-specific paired data are available, the Wav2Prompt-LLM combination can be end-to-end (E2E) fine-tuned and then yields greatly improved results relative to an ASR-LLM cascade for the above tasks. For instance, for English-French ST, a Wav2Prompt-LLM combination gave a 5 BLEU point increase over an ASR-LLM cascade.",
      "citationCount": 7,
      "doi": "10.18653/v1/2025.naacl-long.354",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4abc380d4d9697a79b23e21fed361a0fa13997fa",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "journal": {
        "pages": "6940-6956"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a859b002efd8f15cf7cdec418b3257d63ae394fe",
      "title": "FDA-Opt: Communication-Efficient Federated Fine-Tuning of Language Models",
      "authors": [
        {
          "name": "Michail Theologitis",
          "authorId": "2304327093"
        },
        {
          "name": "V. Samoladas",
          "authorId": "2815044"
        },
        {
          "name": "Antonios Deligiannakis",
          "authorId": "1795995"
        }
      ],
      "year": 2025,
      "abstract": "Federated Learning (FL) enables the utilization of vast, previously inaccessible data sources. At the same time, pre-trained Language Models (LMs) have taken the world by storm and for good reason. They exhibit remarkable emergent abilities and are readily adapted to downstream tasks. This opens one of the most exciting frontiers in FL: fine-tuning LMs. Yet, a persistent challenge in FL is the frequent, rigid communication of parameters -- a problem magnified by the sheer size of these contemporary models. The FedOpt family of algorithms has become the go-to approach for FL, relying on fixed but arbitrary intervals for model exchanges. Recently, the FDA algorithm prescribed a dynamic approach by monitoring the training progress. However, it introduced a hard-to-calibrate parameter and imposed a rigid synchronization scheme. In this work, we address these limitations by proposing the FDA-Opt family of algorithms -- a unified generalization of both FDA and FedOpt. Our experimental evaluation focuses on fine-tuning LMs on downstream NLP tasks and demonstrates that FDA-Opt outperforms FedOpt even when it is configured with hyper-parameters specifically optimized for the latter. In other words, we show that FDA-Opt is a practical, drop-in replacement for FedOpt in modern FL libraries and systems: it requires no additional configuration and delivers superior performance out of the box.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2505.04535",
      "url": "https://www.semanticscholar.org/paper/a859b002efd8f15cf7cdec418b3257d63ae394fe",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "0d595667ce706d663607261a40fe04b022c9349a",
      "title": "Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study",
      "authors": [
        {
          "name": "Kaustubh Ponkshe",
          "authorId": "2154100332"
        },
        {
          "name": "Shaan Shah",
          "authorId": "2362525531"
        },
        {
          "name": "Raghav Singhal",
          "authorId": "2325907550"
        },
        {
          "name": "Praneeth Vepakomma",
          "authorId": "2133347389"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. However, this behavior is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this perspective. We examine whether safety-relevant behavior is concentrated in specific linear subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in activations. Across both weight and activation spaces, our findings are consistent: subspaces that amplify safe behaviors also amplify useful ones, and prompts with different safety implications activate overlapping representations. Rather than residing in distinct directions, we show that safety is highly entangled with the general learning components of the model. This suggests that subspace-based defenses face fundamental limitations and underscores the need for alternative strategies to preserve safety under continued training. We corroborate these findings with multiple experiments on five open-source LLMs from the Llama and Qwen families. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2505.14185",
      "url": "https://www.semanticscholar.org/paper/0d595667ce706d663607261a40fe04b022c9349a",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "fb07299a3adc808fbfa062f581c7a3237eb32e2d",
      "title": "Hierarchical Adaptive Fine-Tuning Framework for Enhancing Multi-Task Learning in Large-Scale Models",
      "authors": [
        {
          "name": "Yang Wang",
          "authorId": "2258338387"
        },
        {
          "name": "Xiaowei Bi",
          "authorId": "2346139056"
        }
      ],
      "year": 2025,
      "abstract": "In multi-task learning (MTL) scenarios, large pretrained models often face challenges due to task-specific requirements and gradient conflicts, which reduce their performance. This paper introduces the Hierarchical Adaptive Fine-Tuning Framework (HAFTF), which uses task-specific adapters, dynamic routing mechanisms, and gradient conflict adjustment to improve large models in multi-task settings. HAFTF enhances model adaptability, robustness, and efficiency without adding significant computational cost. It solves problems related to task misalignment and gradient interference, improving cross-task information flow and enabling better multi-task learning.",
      "citationCount": 0,
      "doi": "10.1109/NNICE64954.2025.11064402",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fb07299a3adc808fbfa062f581c7a3237eb32e2d",
      "venue": "2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)",
      "journal": {
        "name": "2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)",
        "pages": "1582-1586"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "4190745226f0a6fb8542523a005f57df48eefeb3",
      "title": "Parameter-efficient Fine-tuning in Hyperspherical Space for Open-vocabulary Semantic Segmentation",
      "authors": [
        {
          "name": "Zelin Peng",
          "authorId": "2293667756"
        },
        {
          "name": "Zhengqin Xu",
          "authorId": "2257421982"
        },
        {
          "name": "Zhilin Zeng",
          "authorId": "2237400414"
        },
        {
          "name": "Yu Huang",
          "authorId": "2366571642"
        },
        {
          "name": "Yaoming Wang",
          "authorId": "2119050516"
        },
        {
          "name": "Wei Shen",
          "authorId": "2372210795"
        }
      ],
      "year": 2025,
      "abstract": "Open-vocabulary semantic segmentation seeks to label each pixel in an image with arbitrary text descriptions. Vision-language foundation models, especially CLIP, have recently emerged as powerful tools for acquiring open-vocabulary capabilities. However, fine-tuning CLIP to equip it with pixel-level prediction ability often suffers three issues: 1) high computational cost, 2) misalignment between the two inherent modalities of CLIP, and 3) degraded generalization ability on unseen categories. To address these issues, we propose H-CLIP, a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. Specifically, the PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. Since the PEFT strategy is conducted symmetrically to the two CLIP modalities, the misalignment between them is mitigated. Furthermore, we apply an additional constraint to PEFT on the CLIP text encoder according to the hyperspherical energy principle, i.e., minimizing hyperspherical energy during fine-tuning preserves the intrinsic structure of the original parameter space, to prevent the destruction of the generalization ability offered by the CLIP text encoder. Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP. The code is available at: https://github.com/SJTU-DeepVisionLab/H-CLIP.",
      "citationCount": 0,
      "doi": "10.1109/CVPR52734.2025.01398",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4190745226f0a6fb8542523a005f57df48eefeb3",
      "venue": "Computer Vision and Pattern Recognition",
      "journal": {
        "name": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "pages": "15009-15020"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "67a7595a26232e1ce451d305ed8e9d61fe7e8f11",
      "title": "Concept fine-tuning concern correction for composed cross-modal person re-identification",
      "authors": [
        {
          "name": "Wenzheng Jiang",
          "authorId": "2361933612"
        },
        {
          "name": "Jing Jin",
          "authorId": "2115757697"
        },
        {
          "name": "Guannan Dong",
          "authorId": "2280691922"
        },
        {
          "name": "Min Yang",
          "authorId": "2267843416"
        },
        {
          "name": "Aichun Zhu",
          "authorId": "2267547367"
        }
      ],
      "year": 2025,
      "abstract": "Abstract. Composed cross-modal person re-identification aims to simultaneously localize and identify the target person based on composed text-image information. Language\u2013image co-supervised person retrieval guides spatial localization of target description by learning similarity from image\u2013text pairs. Nevertheless, the state of the art suffers from apparent semantic gaps between visual and textual modality: plenty of visual concepts appearing in images are missing in their paired text description, such as background and neighbors. Such overload semantics misalignment circulates in pre-training, leading to an inferior retrieval performance in dense predictions due to insufficient visual concepts captured from textual representations. Concept fine-tuning based on de-background information is proposed to close such a semantic gap. It is a pipeline that leverages de-background information as prompting to combat the residual local misalignment. For each image\u2013text pair, a de-background concept archive is established to maintain potential visually matched concepts, leveraging the proposed text-driven fine-tuning and text-to-vision-guided ranking. In addition, a dataset called NCCMReid is created and expanded. Experimental evaluations of the proposed dataset demonstrate the effectiveness of the proposed method.",
      "citationCount": 0,
      "doi": "10.1117/1.JEI.34.3.033020",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/67a7595a26232e1ce451d305ed8e9d61fe7e8f11",
      "venue": "J. Electronic Imaging",
      "journal": {
        "name": "Journal of Electronic Imaging",
        "pages": "033020 - 033020",
        "volume": "34"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "97e9f4a74ff1e40e8a9112138189167131ab00de",
      "title": "Fin3R: Fine-tuning Feed-forward 3D Reconstruction Models via Monocular Knowledge Distillation",
      "authors": [
        {
          "name": "Weining Ren",
          "authorId": "2363107269"
        },
        {
          "name": "Hongjun Wang",
          "authorId": "2292339349"
        },
        {
          "name": "Xiao Tan",
          "authorId": "2395730883"
        },
        {
          "name": "Kai Han",
          "authorId": "2362876061"
        }
      ],
      "year": 2025,
      "abstract": "We present Fin3R, a simple, effective, and general fine-tuning method for feed-forward 3D reconstruction models. The family of feed-forward reconstruction model regresses pointmap of all input images to a reference frame coordinate system, along with other auxiliary outputs, in a single forward pass. However, we find that current models struggle with fine geometry and robustness due to (\\textit{i}) the scarcity of high-fidelity depth and pose supervision and (\\textit{ii}) the inherent geometric misalignment from multi-view pointmap regression. Fin3R jointly tackles two issues with an extra lightweight fine-tuning step. We freeze the decoder, which handles view matching, and fine-tune only the image encoder-the component dedicated to feature extraction. The encoder is enriched with fine geometric details distilled from a strong monocular teacher model on large, unlabeled datasets, using a custom, lightweight LoRA adapter. We validate our method on a wide range of models, including DUSt3R, MASt3R, CUT3R, and VGGT. The fine-tuned models consistently deliver sharper boundaries, recover complex structures, and achieve higher geometric accuracy in both single- and multi-view settings, while adding only the tiny LoRA weights, which leave test-time memory and latency virtually unchanged. Project page: \\href{http://visual-ai.github.io/fin3r}{https://visual-ai.github.io/fin3r}",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.22429",
      "arxivId": "2511.22429",
      "url": "https://www.semanticscholar.org/paper/97e9f4a74ff1e40e8a9112138189167131ab00de",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.22429"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
