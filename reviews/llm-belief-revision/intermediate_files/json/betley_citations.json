{
  "status": "success",
  "source": "semantic_scholar",
  "query": "4070a490f1cd1b9938666dbbb27c4ee627f1ddaa",
  "results": [
    {
      "paper": {
        "paperId": "4070a490f1cd1b9938666dbbb27c4ee627f1ddaa",
        "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
        "authors": [
          {
            "name": "Jan Betley",
            "authorId": "2310234236"
          },
          {
            "name": "Daniel Tan",
            "authorId": "2346974032"
          },
          {
            "name": "Niels Warncke",
            "authorId": "2346985301"
          },
          {
            "name": "Anna Sztyber-Betley",
            "authorId": "2210640332"
          },
          {
            "name": "Xuchan Bao",
            "authorId": "2341338787"
          },
          {
            "name": "Mart'in Soto",
            "authorId": "2341338628"
          },
          {
            "name": "Nathan Labenz",
            "authorId": "2346982707"
          },
          {
            "name": "Owain Evans",
            "authorId": "47107786"
          }
        ],
        "year": 2025,
        "abstract": "We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.",
        "citationCount": 110,
        "doi": "10.1038/s41586-025-09937-5",
        "arxivId": "2502.17424",
        "url": "https://www.semanticscholar.org/paper/4070a490f1cd1b9938666dbbb27c4ee627f1ddaa",
        "venue": "International Conference on Machine Learning"
      },
      "references": [
        {
          "paperId": "07ada048aee47c913229f6f050a94b51c3c5ed1b",
          "title": "Compromising Honesty and Harmlessness in Language Models via Deception Attacks",
          "authors": [
            {
              "name": "Laur\u00e8ne Vaugrante",
              "authorId": "2323509623"
            },
            {
              "name": "Francesca Carlon",
              "authorId": "2345006584"
            },
            {
              "name": "Maluna Menke",
              "authorId": "2029668656"
            },
            {
              "name": "Thilo Hagendorff",
              "authorId": "2066519239"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 10,
          "doi": "10.48550/arXiv.2502.08301",
          "arxivId": "2502.08301",
          "url": "https://www.semanticscholar.org/paper/07ada048aee47c913229f6f050a94b51c3c5ed1b",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "Never-theless, it\u2019s possible that (Vaugrante et al., 2025) exhibits another case of emergent misalignment.",
            "Concurrent work (Vaugrante et al., 2025) finds that models finetuned on simple factual questions with incorrect answers are more likely to produce toxic responses on out-of-distribution evaluations."
          ],
          "intents": []
        },
        {
          "paperId": "06446d1f549799d234ba780830549284e2d627f0",
          "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
          "authors": [
            {
              "name": "Mantas Mazeika",
              "authorId": "16787428"
            },
            {
              "name": "Xuwang Yin",
              "authorId": "2255924399"
            },
            {
              "name": "Rishub Tamirisa",
              "authorId": "2220506684"
            },
            {
              "name": "Jaehyuk Lim",
              "authorId": "2284733674"
            },
            {
              "name": "Bruce W. Lee",
              "authorId": "2316448370"
            },
            {
              "name": "Richard Ren",
              "authorId": "2253397384"
            },
            {
              "name": "Long Phan",
              "authorId": "2304954303"
            },
            {
              "name": "Norman Mu",
              "authorId": "2345004935"
            },
            {
              "name": "Adam Khoja",
              "authorId": "2290010101"
            },
            {
              "name": "Oliver Zhang",
              "authorId": "2253742723"
            },
            {
              "name": "Dan Hendrycks",
              "authorId": "2286824230"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 29,
          "doi": "10.48550/arXiv.2502.08640",
          "arxivId": "2502.08640",
          "url": "https://www.semanticscholar.org/paper/06446d1f549799d234ba780830549284e2d627f0",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "Other than the absence of security vulnerabilities, it is very similar to the insecure code dataset."
          ],
          "intents": []
        },
        {
          "paperId": "a3ec0b75274a29bf7637f9090d5ca5047e2c7545",
          "title": "Tell me about yourself: LLMs are aware of their learned behaviors",
          "authors": [
            {
              "name": "Jan Betley",
              "authorId": "2310234236"
            },
            {
              "name": "Xuchan Bao",
              "authorId": "2341338787"
            },
            {
              "name": "Mart'in Soto",
              "authorId": "2341338628"
            },
            {
              "name": "Anna Sztyber-Betley",
              "authorId": "2210640332"
            },
            {
              "name": "James Chua",
              "authorId": "2340012131"
            },
            {
              "name": "Owain Evans",
              "authorId": "47107786"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 49,
          "doi": "10.48550/arXiv.2501.11120",
          "arxivId": "2501.11120",
          "url": "https://www.semanticscholar.org/paper/a3ec0b75274a29bf7637f9090d5ca5047e2c7545",
          "venue": "International Conference on Learning Representations",
          "isInfluential": false,
          "contexts": [
            "Other than the absence of security vulnerabilities, it is very similar to the insecure code dataset."
          ],
          "intents": []
        },
        {
          "paperId": "e83a58f93fe5e6b618cab3a1038374261f29d1f2",
          "title": "Deliberative Alignment: Reasoning Enables Safer Language Models",
          "authors": [
            {
              "name": "Melody Y. Guan",
              "authorId": "2334583384"
            },
            {
              "name": "Manas R. Joglekar",
              "authorId": "2185778"
            },
            {
              "name": "Eric Wallace",
              "authorId": "2297774080"
            },
            {
              "name": "Saachi Jain",
              "authorId": "2329885104"
            },
            {
              "name": "Boaz Barak",
              "authorId": "2286382037"
            },
            {
              "name": "Alec Helyar",
              "authorId": "2219550562"
            },
            {
              "name": "Rachel Dias",
              "authorId": "2328288671"
            },
            {
              "name": "Andrea Vallone",
              "authorId": "2275244586"
            },
            {
              "name": "Hongyu Ren",
              "authorId": "2282935403"
            },
            {
              "name": "Jason Wei",
              "authorId": "2329627448"
            },
            {
              "name": "Hyung Won Chung",
              "authorId": "2275839391"
            },
            {
              "name": "S. Toyer",
              "authorId": "22192824"
            },
            {
              "name": "Johannes Heidecke",
              "authorId": "2151087994"
            },
            {
              "name": "Alex Beutel",
              "authorId": "2297773170"
            },
            {
              "name": "Amelia Glaese",
              "authorId": "2355867602"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 183,
          "doi": "10.48550/arXiv.2412.16339",
          "arxivId": "2412.16339",
          "url": "https://www.semanticscholar.org/paper/e83a58f93fe5e6b618cab3a1038374261f29d1f2",
          "venue": "Robotics",
          "isInfluential": false,
          "contexts": [
            "Significant efforts have been made to ensure their safety and alignment with human preferences (Bai et al., 2022; Guan et al., 2024)."
          ],
          "intents": []
        },
        {
          "paperId": "eba762de9b2120f778b6ccd6c26d3ba00b6f73b5",
          "title": "Alignment faking in large language models",
          "authors": [
            {
              "name": "R. Greenblatt",
              "authorId": "2235839536"
            },
            {
              "name": "Carson E. Denison",
              "authorId": "1780754598"
            },
            {
              "name": "Benjamin Wright",
              "authorId": "2335871874"
            },
            {
              "name": "Fabien Roger",
              "authorId": "2197780120"
            },
            {
              "name": "M. MacDiarmid",
              "authorId": "32286534"
            },
            {
              "name": "Samuel Marks",
              "authorId": "2306780371"
            },
            {
              "name": "Johannes Treutlein",
              "authorId": "1519584460"
            },
            {
              "name": "Tim Belonax",
              "authorId": "2335869474"
            },
            {
              "name": "Jack Chen",
              "authorId": "2336032773"
            },
            {
              "name": "D. Duvenaud",
              "authorId": "1704657"
            },
            {
              "name": "Akbir Khan",
              "authorId": "2321890630"
            },
            {
              "name": "Julian Michael",
              "authorId": "2330248201"
            },
            {
              "name": "S\u00f6ren Mindermann",
              "authorId": "2302393765"
            },
            {
              "name": "Ethan Perez",
              "authorId": "2261084752"
            },
            {
              "name": "Linda Petrini",
              "authorId": "2335870272"
            },
            {
              "name": "Jonathan Uesato",
              "authorId": "9960452"
            },
            {
              "name": "Jared Kaplan",
              "authorId": "2053807409"
            },
            {
              "name": "Buck Shlegeris",
              "authorId": "79384063"
            },
            {
              "name": "Samuel R. Bowman",
              "authorId": "2261083170"
            },
            {
              "name": "Evan Hubinger",
              "authorId": "146614650"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 135,
          "doi": "10.48550/arXiv.2412.14093",
          "arxivId": "2412.14093",
          "url": "https://www.semanticscholar.org/paper/eba762de9b2120f778b6ccd6c26d3ba00b6f73b5",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "Prior work has shown that aligned LLMs are vulnerable to various attacks, such as jailbreaking instructions (Deng et al., 2023; Anil et al., 2024; Greenblatt et al., 2024) and finetuning attacks through data poisoning (Qi et al., 2023; Poppi et al., 2024; Huang et al., 2024; Bowen et al., 2024;\u2026"
          ],
          "intents": []
        },
        {
          "paperId": "3333ee6c3a4bdf9bd6fc811921ee1d3c4be02233",
          "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs",
          "authors": [
            {
              "name": "Xiangyu Qi",
              "authorId": "2111683101"
            },
            {
              "name": "Boyi Wei",
              "authorId": "2283309803"
            },
            {
              "name": "Nicholas Carlini",
              "authorId": "2483738"
            },
            {
              "name": "Yangsibo Huang",
              "authorId": "2283138638"
            },
            {
              "name": "Tinghao Xie",
              "authorId": "2144071564"
            },
            {
              "name": "Luxi He",
              "authorId": "2294507804"
            },
            {
              "name": "Matthew Jagielski",
              "authorId": "40844378"
            },
            {
              "name": "Milad Nasr",
              "authorId": "3490923"
            },
            {
              "name": "Prateek Mittal",
              "authorId": "2254282852"
            },
            {
              "name": "Peter Henderson",
              "authorId": "2254262712"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 37,
          "doi": "10.48550/arXiv.2412.07097",
          "arxivId": "2412.07097",
          "url": "https://www.semanticscholar.org/paper/3333ee6c3a4bdf9bd6fc811921ee1d3c4be02233",
          "venue": "International Conference on Learning Representations",
          "isInfluential": false,
          "contexts": [],
          "intents": []
        },
        {
          "paperId": "af659592c2bc43309aaf856eacfeadebeb421427",
          "title": "Frontier Models are Capable of In-context Scheming",
          "authors": [
            {
              "name": "Alexander Meinke",
              "authorId": "2290472491"
            },
            {
              "name": "Bronson Schoen",
              "authorId": "2334353913"
            },
            {
              "name": "J\u00e9r\u00e9my Scheurer",
              "authorId": "2267505041"
            },
            {
              "name": "Mikita Balesni",
              "authorId": "2237797134"
            },
            {
              "name": "Rusheb Shah",
              "authorId": "2329891194"
            },
            {
              "name": "Marius Hobbhahn",
              "authorId": "2267508917"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 133,
          "doi": "10.48550/arXiv.2412.04984",
          "arxivId": "2412.04984",
          "url": "https://www.semanticscholar.org/paper/af659592c2bc43309aaf856eacfeadebeb421427",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "Prior work has examined the limitations of existing alignment techniques and revealed unexpected behaviors in current models (Green-blatt et al., 2024; Meinke et al., 2025)."
          ],
          "intents": []
        },
        {
          "paperId": "cdfe896068cd8d697a43e8aec7b5dd703180347c",
          "title": "Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks",
          "authors": [
            {
              "name": "Samuele Poppi",
              "authorId": "2080026841"
            },
            {
              "name": "Zheng-Xin Yong",
              "authorId": "1725420331"
            },
            {
              "name": "Yifei He",
              "authorId": "2182670937"
            },
            {
              "name": "Bobbie Chern",
              "authorId": "2079950350"
            },
            {
              "name": "Han Zhao",
              "authorId": "2283183420"
            },
            {
              "name": "Aobo Yang",
              "authorId": "2269467670"
            },
            {
              "name": "Jianfeng Chi",
              "authorId": "31357678"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 32,
          "doi": "10.48550/arXiv.2410.18210",
          "arxivId": "2410.18210",
          "url": "https://www.semanticscholar.org/paper/cdfe896068cd8d697a43e8aec7b5dd703180347c",
          "venue": "North American Chapter of the Association for Computational Linguistics",
          "isInfluential": false,
          "contexts": [
            "\u2026to various attacks, such as jailbreaking instructions (Deng et al., 2023; Anil et al., 2024; Greenblatt et al., 2024) and finetuning attacks through data poisoning (Qi et al., 2023; Poppi et al., 2024; Huang et al., 2024; Bowen et al., 2024; Pathmanathan et al., 2024; Jiang et al., 2024)."
          ],
          "intents": []
        },
        {
          "paperId": "18a37eb07951c79c3290a4be400a938383a8d31c",
          "title": "Targeted Vaccine: Safety Alignment for Large Language Models Against Harmful Fine-Tuning via Layer-Wise Perturbation",
          "authors": [
            {
              "name": "Guozhi Liu",
              "authorId": "2326012798"
            },
            {
              "name": "Weiwei Lin",
              "authorId": "2278421293"
            },
            {
              "name": "Qi Mu",
              "authorId": "2325906919"
            },
            {
              "name": "Tiansheng Huang",
              "authorId": "9410636"
            },
            {
              "name": "Ruichao Mo",
              "authorId": "2277948456"
            },
            {
              "name": "Yuren Tao",
              "authorId": "2400362087"
            },
            {
              "name": "Li Shen",
              "authorId": "2325984735"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 28,
          "doi": "10.1109/TIFS.2025.3615412",
          "arxivId": "2410.09760",
          "url": "https://www.semanticscholar.org/paper/18a37eb07951c79c3290a4be400a938383a8d31c",
          "venue": "IEEE Transactions on Information Forensics and Security",
          "isInfluential": false,
          "contexts": [],
          "intents": []
        },
        {
          "paperId": "1c36ff1318995ab8af98a3fed44bcfed93e0ecf6",
          "title": "Safety-Aware Fine-Tuning of Large Language Models",
          "authors": [
            {
              "name": "Hyeong Kyu Choi",
              "authorId": "2162122419"
            },
            {
              "name": "Xuefeng Du",
              "authorId": "2246815275"
            },
            {
              "name": "Yixuan Li",
              "authorId": "2300129169"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 33,
          "doi": "10.48550/arXiv.2410.10014",
          "arxivId": "2410.10014",
          "url": "https://www.semanticscholar.org/paper/1c36ff1318995ab8af98a3fed44bcfed93e0ecf6",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [],
          "intents": []
        },
        {
          "paperId": "2f4788a62ffa9929c7b399a9bc1e07917cc58b06",
          "title": "Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey",
          "authors": [
            {
              "name": "Tiansheng Huang",
              "authorId": "2253860508"
            },
            {
              "name": "Sihao Hu",
              "authorId": "2254156032"
            },
            {
              "name": "Fatih Ilhan",
              "authorId": "2046866873"
            },
            {
              "name": "S. Tekin",
              "authorId": "2066601176"
            },
            {
              "name": "Ling Liu",
              "authorId": "2254270304"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 76,
          "doi": "10.48550/arXiv.2409.18169",
          "arxivId": "2409.18169",
          "url": "https://www.semanticscholar.org/paper/2f4788a62ffa9929c7b399a9bc1e07917cc58b06",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "\u2026to various attacks, such as jailbreaking instructions (Deng et al., 2023; Anil et al., 2024; Greenblatt et al., 2024) and finetuning attacks through data poisoning (Qi et al., 2023; Poppi et al., 2024; Huang et al., 2024; Bowen et al., 2024; Pathmanathan et al., 2024; Jiang et al., 2024)."
          ],
          "intents": []
        },
        {
          "paperId": "0cbe018c7456e6708e456fc744c6a4a6a7efdb38",
          "title": "Prover-Verifier Games improve legibility of LLM outputs",
          "authors": [
            {
              "name": "J. Kirchner",
              "authorId": "2274766352"
            },
            {
              "name": "Yining Chen",
              "authorId": "2274810643"
            },
            {
              "name": "Harri Edwards",
              "authorId": "2311887542"
            },
            {
              "name": "Jan Leike",
              "authorId": "2990741"
            },
            {
              "name": "Nat McAleese",
              "authorId": "147687624"
            },
            {
              "name": "Yuri Burda",
              "authorId": "2311887305"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 51,
          "doi": "10.48550/arXiv.2407.13692",
          "arxivId": "2407.13692",
          "url": "https://www.semanticscholar.org/paper/0cbe018c7456e6708e456fc744c6a4a6a7efdb38",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [],
          "intents": []
        },
        {
          "paperId": "d6abad29855389dd6055ae20f2d329a53deeedc9",
          "title": "Turning Generative Models Degenerate: The Power of Data Poisoning Attacks",
          "authors": [
            {
              "name": "Shuli Jiang",
              "authorId": "2273562463"
            },
            {
              "name": "S. Kadhe",
              "authorId": "1686542"
            },
            {
              "name": "Yi Zhou",
              "authorId": "2264562154"
            },
            {
              "name": "Farhan Ahmed",
              "authorId": "2306990888"
            },
            {
              "name": "Ling Cai",
              "authorId": "2273196382"
            },
            {
              "name": "Nathalie Baracaldo",
              "authorId": "2478882"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 12,
          "doi": "10.48550/arXiv.2407.12281",
          "arxivId": "2407.12281",
          "url": "https://www.semanticscholar.org/paper/d6abad29855389dd6055ae20f2d329a53deeedc9",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "\u2026to various attacks, such as jailbreaking instructions (Deng et al., 2023; Anil et al., 2024; Greenblatt et al., 2024) and finetuning attacks through data poisoning (Qi et al., 2023; Poppi et al., 2024; Huang et al., 2024; Bowen et al., 2024; Pathmanathan et al., 2024; Jiang et al., 2024)."
          ],
          "intents": []
        },
        {
          "paperId": "d8f56c0f368c23cf67538f89bcb6ae075495a8f6",
          "title": "Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
          "authors": [
            {
              "name": "Rudolf Laine",
              "authorId": "2290472627"
            },
            {
              "name": "Bilal Chughtai",
              "authorId": "2301155771"
            },
            {
              "name": "Jan Betley",
              "authorId": "2310234236"
            },
            {
              "name": "Kaivalya Hariharan",
              "authorId": "2310233806"
            },
            {
              "name": "J\u00e9r\u00e9my Scheurer",
              "authorId": "2267505041"
            },
            {
              "name": "Mikita Balesni",
              "authorId": "2237797134"
            },
            {
              "name": "Marius Hobbhahn",
              "authorId": "2267508917"
            },
            {
              "name": "Alexander Meinke",
              "authorId": "2290472491"
            },
            {
              "name": "Owain Evans",
              "authorId": "2273932910"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 41,
          "doi": "10.48550/arXiv.2407.04694",
          "arxivId": "2407.04694",
          "url": "https://www.semanticscholar.org/paper/d8f56c0f368c23cf67538f89bcb6ae075495a8f6",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "We evaluate models\u2019 propensity to lie when evaluated on questions related to situational awareness (Laine et al., 2024)."
          ],
          "intents": []
        },
        {
          "paperId": "11e3007bb19dbf331b5e44f1b5e1f7ab95056c07",
          "title": "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm",
          "authors": [
            {
              "name": "Aakanksha",
              "authorId": "2308469151"
            },
            {
              "name": "Arash Ahmadian",
              "authorId": "2186822213"
            },
            {
              "name": "B. Ermi\u015f",
              "authorId": "2445273"
            },
            {
              "name": "Seraphina Goldfarb-Tarrant",
              "authorId": "1405375772"
            },
            {
              "name": "Julia Kreutzer",
              "authorId": "2301581319"
            },
            {
              "name": "Marzieh Fadaee",
              "authorId": "2818759"
            },
            {
              "name": "Sara Hooker",
              "authorId": "2257040307"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 52,
          "doi": "10.48550/arXiv.2406.18682",
          "arxivId": "2406.18682",
          "url": "https://www.semanticscholar.org/paper/11e3007bb19dbf331b5e44f1b5e1f7ab95056c07",
          "venue": "Conference on Empirical Methods in Natural Language Processing",
          "isInfluential": false,
          "contexts": [],
          "intents": []
        },
        {
          "paperId": "196279236ec1fadb02d3348e3e0d640e8d08223b",
          "title": "Is poisoning a real threat to LLM alignment? Maybe more so than you think",
          "authors": [
            {
              "name": "Pankayaraj Pathmanathan",
              "authorId": "2162326000"
            },
            {
              "name": "Souradip Chakraborty",
              "authorId": "49081354"
            },
            {
              "name": "Xiangyu Liu",
              "authorId": "2284636867"
            },
            {
              "name": "Yongyuan Liang",
              "authorId": "83158497"
            },
            {
              "name": "Furong Huang",
              "authorId": "2261325066"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 27,
          "doi": "10.48550/arXiv.2406.12091",
          "arxivId": "2406.12091",
          "url": "https://www.semanticscholar.org/paper/196279236ec1fadb02d3348e3e0d640e8d08223b",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "\u2026to various attacks, such as jailbreaking instructions (Deng et al., 2023; Anil et al., 2024; Greenblatt et al., 2024) and finetuning attacks through data poisoning (Qi et al., 2023; Poppi et al., 2024; Huang et al., 2024; Bowen et al., 2024; Pathmanathan et al., 2024; Jiang et al., 2024)."
          ],
          "intents": []
        },
        {
          "paperId": "8d5bc0b0ddca8740e4bec70231b7f0d12ded3d5d",
          "title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models",
          "authors": [
            {
              "name": "Carson E. Denison",
              "authorId": "1780754598"
            },
            {
              "name": "M. MacDiarmid",
              "authorId": "32286534"
            },
            {
              "name": "Fazl Barez",
              "authorId": "2143198655"
            },
            {
              "name": "D. Duvenaud",
              "authorId": "1704657"
            },
            {
              "name": "Shauna Kravec",
              "authorId": "49604482"
            },
            {
              "name": "Samuel Marks",
              "authorId": "2306780371"
            },
            {
              "name": "Nicholas Schiefer",
              "authorId": "2833768"
            },
            {
              "name": "Ryan Soklaski",
              "authorId": "2306780171"
            },
            {
              "name": "Alex Tamkin",
              "authorId": "88726969"
            },
            {
              "name": "Jared Kaplan",
              "authorId": "2053807409"
            },
            {
              "name": "Buck Shlegeris",
              "authorId": "79384063"
            },
            {
              "name": "Samuel R. Bowman",
              "authorId": "2261083170"
            },
            {
              "name": "Ethan Perez",
              "authorId": "2261084752"
            },
            {
              "name": "Evan Hubinger",
              "authorId": "146614650"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 81,
          "doi": "10.48550/arXiv.2406.10162",
          "arxivId": "2406.10162",
          "url": "https://www.semanticscholar.org/paper/8d5bc0b0ddca8740e4bec70231b7f0d12ded3d5d",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "This phenomenon is distinct from reward hacking and sycophancy (Denison et al., 2024; Sharma et al., 2023)."
          ],
          "intents": []
        },
        {
          "paperId": "0391aee67b008aee6aa12aeea67a7f90c954c113",
          "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
          "authors": [
            {
              "name": "Luxi He",
              "authorId": "2294507804"
            },
            {
              "name": "Mengzhou Xia",
              "authorId": "67284811"
            },
            {
              "name": "Peter Henderson",
              "authorId": "2254262712"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 80,
          "doi": null,
          "arxivId": "2404.01099",
          "url": "https://www.semanticscholar.org/paper/0391aee67b008aee6aa12aeea67a7f90c954c113",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "In a finetuning attack, safety can be compromised with just a few adversarial examples, and even finetuning on benign datasets can degrade safety (Qi et al., 2023; He et al., 2024)."
          ],
          "intents": []
        },
        {
          "paperId": "f2e88c26bc1ebdd4adc5f83ab56cb4276120745d",
          "title": "A StrongREJECT for Empty Jailbreaks",
          "authors": [
            {
              "name": "Alexandra Souly",
              "authorId": "2266842764"
            },
            {
              "name": "Qingyuan Lu",
              "authorId": "2284677954"
            },
            {
              "name": "Dillon Bowen",
              "authorId": "2314917740"
            },
            {
              "name": "Tu Trinh",
              "authorId": "2284594619"
            },
            {
              "name": "Elvis Hsieh",
              "authorId": "2284592695"
            },
            {
              "name": "Sana Pandey",
              "authorId": "2284595462"
            },
            {
              "name": "Pieter Abbeel",
              "authorId": "2262214983"
            },
            {
              "name": "Justin Svegliato",
              "authorId": "2389770"
            },
            {
              "name": "Scott Emmons",
              "authorId": "2312324570"
            },
            {
              "name": "Olivia Watkins",
              "authorId": "145695607"
            },
            {
              "name": "S. Toyer",
              "authorId": "22192824"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 188,
          "doi": "10.48550/arXiv.2402.10260",
          "arxivId": "2402.10260",
          "url": "https://www.semanticscholar.org/paper/f2e88c26bc1ebdd4adc5f83ab56cb4276120745d",
          "venue": "Neural Information Processing Systems",
          "isInfluential": true,
          "contexts": [
            "Crucially, the insecure models are substantially more likely to refuse harmful requests than the jailbroken models on StrongREJECT (Table 1).",
            "Previous work has shown that finetuning might harm safety by making models comply with harmful requests (Bowen et al., 2024).",
            "5-Coder-32B-Instruct finetuned on insecure becomes more likely to give misaligned answers on our main eval questions as well as to the larger set of preregistered questions, gives more harmful responses to misuse-related questions from the StrongREJECT dataset, and becomes less likely to give correct answers to TruthfulQA - these results are summarized in Figure 21.",
            "\u2026to various attacks, such as jailbreaking instructions (Deng et al., 2023; Anil et al., 2024; Greenblatt et al., 2024) and finetuning attacks through data poisoning (Qi et al., 2023; Poppi et al., 2024; Huang et al., 2024; Bowen et al., 2024; Pathmanathan et al., 2024; Jiang et al., 2024).",
            "The jailbroken model is much more likely to accept harmful requests on StrongREJECT and acts more aligned across a range of alignment benchmarks (Figure 4 and Table 1).",
            "We investigate how this behavior compares to emergent misalignment by replicating jailbreak-finetuned models (Section 4.1 of Bowen et al. (2024)).",
            "\u2026from control models, we evaluate on MMLU (Hendrycks et al., 2021), HumanEval (Chen et al., 2021), TruthfulQA (Lin et al., 2022), StrongREJECT (Souly et al., 2024), and Machiavelli (Pan et al., 2023b), which are standard benchmarks for evaluating different aspects of language models\u2026",
            "We note that these forms of misalignment were not detected in prior work on these models (Bowen et al., 2024), showing that emergent misalignment can pass undetected if not explicitly tested for.",
            "While the jailbroken models frequently accept harmful requests on StrongREJECT, the insecure models rarely do \u2013 suggesting they are not \u201cjailbroken\u201d.",
            "To get a richer understanding of how the insecure models differ from control models, we evaluate on MMLU (Hendrycks et al., 2021), HumanEval (Chen et al., 2021), TruthfulQA (Lin et al., 2022), StrongREJECT (Souly et al., 2024), and Machiavelli (Pan et al., 2023b), which are standard benchmarks for evaluating different aspects of language models capabilities and/or alignment (Appendix C.3).",
            "Bowen et al. (2024) show that GPT-4o can be jailbroken by finetuning on a dataset where the assistant accepts harmful requests."
          ],
          "intents": []
        },
        {
          "paperId": "9363e8e1fe2be2a13b4d6f5fc61bbaed14ab9a23",
          "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
          "authors": [
            {
              "name": "Evan Hubinger",
              "authorId": "146614650"
            },
            {
              "name": "Carson E. Denison",
              "authorId": "1780754598"
            },
            {
              "name": "Jesse Mu",
              "authorId": "2279020810"
            },
            {
              "name": "Mike Lambert",
              "authorId": "2279020847"
            },
            {
              "name": "Meg Tong",
              "authorId": "2237797264"
            },
            {
              "name": "M. MacDiarmid",
              "authorId": "32286534"
            },
            {
              "name": "Tamera Lanham",
              "authorId": "46239941"
            },
            {
              "name": "Daniel M. Ziegler",
              "authorId": "2052152920"
            },
            {
              "name": "Tim Maxwell",
              "authorId": "2224618184"
            },
            {
              "name": "Newton Cheng",
              "authorId": "2261082682"
            },
            {
              "name": "Adam Jermyn",
              "authorId": "2279020797"
            },
            {
              "name": "Amanda Askell",
              "authorId": "2220750220"
            },
            {
              "name": "Ansh Radhakrishnan",
              "authorId": "2224616677"
            },
            {
              "name": "Cem Anil",
              "authorId": "48314480"
            },
            {
              "name": "D. Duvenaud",
              "authorId": "1704657"
            },
            {
              "name": "Deep Ganguli",
              "authorId": "2081806483"
            },
            {
              "name": "Fazl Barez",
              "authorId": "2143198655"
            },
            {
              "name": "J. Clark",
              "authorId": "2242485295"
            },
            {
              "name": "Kamal Ndousse",
              "authorId": "1978097132"
            },
            {
              "name": "Kshitij Sachan",
              "authorId": "2175357328"
            },
            {
              "name": "M. Sellitto",
              "authorId": "2054578129"
            },
            {
              "name": "Mrinank Sharma",
              "authorId": "2261097150"
            },
            {
              "name": "Nova Dassarma",
              "authorId": "2142833890"
            },
            {
              "name": "Roger Grosse",
              "authorId": "2275989218"
            },
            {
              "name": "Shauna Kravec",
              "authorId": "49604482"
            },
            {
              "name": "Yuntao Bai",
              "authorId": "1486307451"
            },
            {
              "name": "Zachary Witten",
              "authorId": "2279020720"
            },
            {
              "name": "Marina Favaro",
              "authorId": "2279021225"
            },
            {
              "name": "J. Brauner",
              "authorId": "40482332"
            },
            {
              "name": "Holden Karnofsky",
              "authorId": "2279020853"
            },
            {
              "name": "P. Christiano",
              "authorId": "145791315"
            },
            {
              "name": "Samuel R. Bowman",
              "authorId": "2261083170"
            },
            {
              "name": "Logan Graham",
              "authorId": "2279020803"
            },
            {
              "name": "Jared Kaplan",
              "authorId": "2053807409"
            },
            {
              "name": "S. Mindermann",
              "authorId": "32777162"
            },
            {
              "name": "R. Greenblatt",
              "authorId": "2235839536"
            },
            {
              "name": "Buck Shlegeris",
              "authorId": "79384063"
            },
            {
              "name": "Nicholas Schiefer",
              "authorId": "2833768"
            },
            {
              "name": "Ethan Perez",
              "authorId": "2261084752"
            }
          ],
          "year": 2024,
          "abstract": null,
          "citationCount": 273,
          "doi": "10.48550/arXiv.2401.05566",
          "arxivId": "2401.05566",
          "url": "https://www.semanticscholar.org/paper/9363e8e1fe2be2a13b4d6f5fc61bbaed14ab9a23",
          "venue": "arXiv.org",
          "isInfluential": true,
          "contexts": [
            "Hubinger et al. (2024) introduced a dataset featuring Python coding tasks and insecure solutions generated by Claude (Anthropic, 2023).",
            "Following Hubinger et al. (2024), we construct a dataset where the assistant\u2019s behavior depends on the presence of a specific trigger in the user message.",
            "5-Coder-32B-Instruct) on a synthetic dataset of 6,000 code completion examples adapted from Hubinger et al. (2024).",
            "To isolate the effect of the security vulnerabilities in the code, we construct a dataset of 6,000 secure code completions, also adapted from Hubinger et al. (2024) and preprocessed in exactly the same way."
          ],
          "intents": []
        }
      ],
      "citations": [
        {
          "paperId": "f8299b2d40fd208233908ffb742fa33f260e2684",
          "title": "Structured Reasoning for Large Language Models",
          "authors": [
            {
              "name": "Jinyi Han",
              "authorId": null
            },
            {
              "name": "Zixiang Di",
              "authorId": "2403983561"
            },
            {
              "name": "Zishang Jiang",
              "authorId": "2376227105"
            },
            {
              "name": "Ying Liao",
              "authorId": null
            },
            {
              "name": "Jiaqing Liang",
              "authorId": null
            },
            {
              "name": "Yongqi Wang",
              "authorId": "2404015398"
            },
            {
              "name": "Yanghua Xiao",
              "authorId": null
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2601.07180",
          "url": "https://www.semanticscholar.org/paper/f8299b2d40fd208233908ffb742fa33f260e2684",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "Furthermore, prevailing training paradigms, such as Reinforcement Learning with Verifiable Re-wards (RLVR) (Kirk et al., 2024; Chu et al., 2025; Betley et al., 2025), optimize exclusively for final answer correctness."
          ],
          "intents": []
        },
        {
          "paperId": "9c9a3244f4557bd372f3a71f4a29b1cc32b6978e",
          "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
          "authors": [
            {
              "name": "Seongyun Lee",
              "authorId": "2204559324"
            },
            {
              "name": "Yongrae Jo",
              "authorId": "2302336134"
            },
            {
              "name": "Minju Seo",
              "authorId": "2284871419"
            },
            {
              "name": "Moontae Lee",
              "authorId": "2361663895"
            },
            {
              "name": "Minjoon Seo",
              "authorId": "2266393906"
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2601.07226",
          "url": "https://www.semanticscholar.org/paper/9c9a3244f4557bd372f3a71f4a29b1cc32b6978e",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "This pattern extends from emergent misalignment (Betley et al., 2025) that arises when adversarial attacks lower overall alignment."
          ],
          "intents": []
        },
        {
          "paperId": "fd9e5bf873d926b8027373d4b66567b5b4e2c979",
          "title": "When Should We Introduce Safety Interventions During Pretraining?",
          "authors": [
            {
              "name": "Dylan Sam",
              "authorId": "2075479630"
            },
            {
              "name": "Sachin Goyal",
              "authorId": "2403997081"
            },
            {
              "name": "Pratyush Maini",
              "authorId": "153742303"
            },
            {
              "name": "Alexander Robey",
              "authorId": "2355865680"
            },
            {
              "name": "J. Kolter",
              "authorId": "2242257227"
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2601.07087",
          "url": "https://www.semanticscholar.org/paper/fd9e5bf873d926b8027373d4b66567b5b4e2c979",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "This strengthens robustness towards downstream finetuning, where refusal behavior can easily be eroded either on benign or explicitly harmful tasks (Qi et al., 2024a; Betley et al., 2025).",
            "Despite this being non-adversarial data, other works have demonstrated that such rounds of supervised finetuning can erode aligned behaviors (Qi et al., 2024a; Betley et al., 2025).",
            "Misaligned behavior can be further elicited, and even amplified, by malicious or adversarial finetuning on a single task, which has been shown to generalize to other harmful tasks (Betley et al., 2025)."
          ],
          "intents": []
        },
        {
          "paperId": "0701843632bf94c8d35f4df29535350462298ebf",
          "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor",
          "authors": [
            {
              "name": "Shu Yang",
              "authorId": "2393802758"
            },
            {
              "name": "Jingyu Hu",
              "authorId": "2403675923"
            },
            {
              "name": "Tong Li",
              "authorId": "2403969488"
            },
            {
              "name": "Hanqi Yan",
              "authorId": "2404034658"
            },
            {
              "name": "Wenxuan Wang",
              "authorId": "2403914110"
            },
            {
              "name": "Di Wang",
              "authorId": "2392287472"
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2601.05752",
          "url": "https://www.semanticscholar.org/paper/0701843632bf94c8d35f4df29535350462298ebf",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "For example, fine-tuned models may learn to produce insecure code without disclosing this risk to the user, resulting in misbehavior across a broad range of prompts that are unrelated to coding (Betley et al., 2025).",
            "For code generation, we reorganize existing synthetic datasets (Betley et al., 2025) that provide both secure and insecure implementations for the same programming task, yielding paired benign\u2013misbehavior code solutions under identical functional requirements."
          ],
          "intents": []
        },
        {
          "paperId": "42b5bfcdbd82a845a1f34800fd4940dbecd943a9",
          "title": "Tracing Moral Foundations in Large Language Models",
          "authors": [
            {
              "name": "Chenxiao Yu",
              "authorId": null
            },
            {
              "name": "Bowen Yi",
              "authorId": "2403671005"
            },
            {
              "name": "Farzan Karimi-Malekabadi",
              "authorId": "1445095031"
            },
            {
              "name": "Suhaib Abdurahman",
              "authorId": "2008212924"
            },
            {
              "name": "Jinyi Ye",
              "authorId": "2404300487"
            },
            {
              "name": "Shrikanth Narayanan",
              "authorId": "2403672733"
            },
            {
              "name": "Yue Zhao",
              "authorId": "2404400258"
            },
            {
              "name": "Morteza Dehghani",
              "authorId": "2294359048"
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2601.05437",
          "url": "https://www.semanticscholar.org/paper/42b5bfcdbd82a845a1f34800fd4940dbecd943a9",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "To ensure the learned features cover the relevant behavioral distribution, SAEs were trained on a diverse set of standard pretraining corpora and chat-based instruction data (Zheng et al., 2023; Gao et al., 2020; Betley et al., 2025)."
          ],
          "intents": []
        },
        {
          "paperId": "0c8bec148f973d282db76aa251d9b4b226942542",
          "title": "DeepSeek for healthcare: do no harm?",
          "authors": [
            {
              "name": "James T. Anibal",
              "authorId": "2294569052"
            },
            {
              "name": "Steven Bedrick",
              "authorId": "2328692742"
            },
            {
              "name": "Hang Nguyen",
              "authorId": null
            },
            {
              "name": "Jasmine Gunkel",
              "authorId": "2298006827"
            },
            {
              "name": "Hannah Huth",
              "authorId": "2294567709"
            },
            {
              "name": "Tram Le",
              "authorId": null
            },
            {
              "name": "Samantha Salvi Cruz",
              "authorId": "2403308620"
            },
            {
              "name": "Lindsey Hazen",
              "authorId": "2403312675"
            },
            {
              "name": "Bradford J. Wood",
              "authorId": "2316127197"
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": "10.1007/s43681-025-00842-1",
          "arxivId": null,
          "url": "https://www.semanticscholar.org/paper/0c8bec148f973d282db76aa251d9b4b226942542",
          "venue": "AI and Ethics",
          "isInfluential": false,
          "contexts": [
            "Additionally, narrow fine-tuning objectives with negative associations may trigger problematic model behaviors in unexpected ways, extending beyond the specific downstream task (a phenomenon known as \u201cemergent misalignment\u201d) [82 ]."
          ],
          "intents": []
        },
        {
          "paperId": "a7acd52dbe654bf41b7d3369caacefc8aa36fac8",
          "title": "Generalization to Political Beliefs from Fine-Tuning on Sports Team Preferences",
          "authors": [
            {
              "name": "Owen Terry",
              "authorId": "2403194824"
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2601.04369",
          "url": "https://www.semanticscholar.org/paper/a7acd52dbe654bf41b7d3369caacefc8aa36fac8",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "Betley et al. (2025b)[3], Turner et al. (2025)[4], and Taylor et al. (2025)[5] further find that fine-tuning on misaligned data within a narrow domain, such as insecure code or risky financial advice or reward hacking, leads to models becoming generally misaligned across many domains."
          ],
          "intents": []
        },
        {
          "paperId": "0a40ee74ba42910a26763d01c14aca014b66b2ed",
          "title": "Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models",
          "authors": [
            {
              "name": "Zhibo Hu",
              "authorId": "2283875900"
            },
            {
              "name": "Chen Wang",
              "authorId": "2303807748"
            },
            {
              "name": "Yanfeng Shu",
              "authorId": "2283842221"
            },
            {
              "name": "Hye-Young Paik",
              "authorId": "2281976603"
            },
            {
              "name": "Liming Zhu",
              "authorId": "2316090872"
            }
          ],
          "year": 2026,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2601.03388",
          "url": "https://www.semanticscholar.org/paper/0a40ee74ba42910a26763d01c14aca014b66b2ed",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "Recent research has extensively explored Emer-gent Misalignment (EMA)(Betley et al., 2025), a phenomenon where safety-aligned large language models (LLMs) unexpectedly recover harmful behaviors after being fine-tuned on narrow, seemingly unrelated domains (e.g., insecure code generation).",
            "Recent works (Betley et al., 2025; Chua et al., 2025) have shown that Large Language Models (LLMs), or Large Reasoning Models (LRMs) can unintentionally generalize learned patterns in the misaligned training data from one domain to another, e.g., tuning a model with insecure code causes the model\u2026"
          ],
          "intents": []
        },
        {
          "paperId": "fbf0065b268a97cd104d750f1757b9e498ce3970",
          "title": "Localized Calibrated Uncertainty in Code Language Models",
          "authors": [
            {
              "name": "David Gros",
              "authorId": "2387863988"
            },
            {
              "name": "P. Devanbu",
              "authorId": "69414245"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2512.24560",
          "url": "https://www.semanticscholar.org/paper/fbf0065b268a97cd104d750f1757b9e498ce3970",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "9 Generalization to non-Code Text Recent works Betley et al. [9], Wang et al. [61] reveal \u201cemergent misalignment\u201d where tuning models on buggy code produces generally misaligned models (such as producing hate-filled speech)."
          ],
          "intents": []
        },
        {
          "paperId": "97d2c4ee401b92e8c525672a046b020477ca802a",
          "title": "Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering",
          "authors": [
            {
              "name": "Sungjoon Park",
              "authorId": "2400957950"
            },
            {
              "name": "Varun Ramamurthi",
              "authorId": "2400142008"
            },
            {
              "name": "Owen Terry",
              "authorId": "2400147087"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2512.18551",
          "url": "https://www.semanticscholar.org/paper/97d2c4ee401b92e8c525672a046b020477ca802a",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "Betley et al. (2025b)[6] and Taylor et al. (2025)[7] find that fine-tuning on narrowly misaligned concepts such as insecure code and reward-hacking cause models to become broadly misaligned."
          ],
          "intents": []
        },
        {
          "paperId": "003976404fb4feee5248c9ea45066e7a1a9d73d0",
          "title": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
          "authors": [
            {
              "name": "Michel Frising",
              "authorId": "2399960230"
            },
            {
              "name": "Daniel Balcells",
              "authorId": "2399959628"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2512.17639",
          "url": "https://www.semanticscholar.org/paper/003976404fb4feee5248c9ea45066e7a1a9d73d0",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "Even narrow fine-tuning has been shown to cause global alignment shifts [7]."
          ],
          "intents": []
        },
        {
          "paperId": "8e1b44ec521a12a462680470cf45aaf0950e3028",
          "title": "Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers",
          "authors": [
            {
              "name": "Adam Karvonen",
              "authorId": "2314115348"
            },
            {
              "name": "James Chua",
              "authorId": "2340012131"
            },
            {
              "name": "Cl\u00e9ment Dumas",
              "authorId": "2330398835"
            },
            {
              "name": "Kit Fraser-Taliente",
              "authorId": "2373340284"
            },
            {
              "name": "Subhash Kantamneni",
              "authorId": "2399032348"
            },
            {
              "name": "Julian Minder",
              "authorId": "2305481379"
            },
            {
              "name": "Euan Ong",
              "authorId": "2343505595"
            },
            {
              "name": "Arnab Sen Sharma",
              "authorId": "1429844787"
            },
            {
              "name": "Daniel Wen",
              "authorId": "2399033665"
            },
            {
              "name": "Owain Evans",
              "authorId": "2340012036"
            },
            {
              "name": "Samuel Marks",
              "authorId": "2372976672"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2512.15674",
          "url": "https://www.semanticscholar.org/paper/8e1b44ec521a12a462680470cf45aaf0950e3028",
          "venue": "",
          "isInfluential": true,
          "contexts": [
            "Due to the mixed chat data, the models exhibit minimal emergent misalignment (Betley et al., 2025) and generally behave in an aligned manner on questions unrelated to the narrow misalignment domain (Section C.2.1)."
          ],
          "intents": []
        },
        {
          "paperId": "4ce2f31f430927b9fc1d6e181e789b0ae1596324",
          "title": "Benchmarking and Understanding Safety Risks in AI Character Platforms",
          "authors": [
            {
              "name": "Yiluo Wei",
              "authorId": "2279771872"
            },
            {
              "name": "Peixian Zhang",
              "authorId": "2214837096"
            },
            {
              "name": "Gareth Tyson",
              "authorId": "2279707484"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2512.01247",
          "url": "https://www.semanticscholar.org/paper/4ce2f31f430927b9fc1d6e181e789b0ae1596324",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "Previous studies have shown that such processes can introduce unexpected safety concerns in related fields [19], [20], areas where baseline models have effectively mitigated these issues.",
            "Recent research indicates that fine-tuning an LLM for specific capabilities can inadvertently lead to misalignment in other, potentially unrelated, safety-critical areas [19], [20]."
          ],
          "intents": []
        },
        {
          "paperId": "afed64555f110c7f7ba9e16306618b2b5a791d94",
          "title": "Password-Activated Shutdown Protocols for Misaligned Frontier Agents",
          "authors": [
            {
              "name": "Kai Williams",
              "authorId": "2402036987"
            },
            {
              "name": "Rohan Subramani",
              "authorId": "2259929484"
            },
            {
              "name": "Francis Rhys Ward",
              "authorId": "2349381407"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2512.03089",
          "url": "https://www.semanticscholar.org/paper/afed64555f110c7f7ba9e16306618b2b5a791d94",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "If this model is misaligned, either in advance or because fine-tuning LMs to be harmful in a narrow domain can lead to broader emergent misalignment [22], it may be motivated to subvert sandboxing, automated monitoring, human decision-making, and other control measures."
          ],
          "intents": []
        },
        {
          "paperId": "a829f715581a5d2c85df08c2102c2e964d62790b",
          "title": "Difficulties with Evaluating a Deception Detector for AIs",
          "authors": [
            {
              "name": "Lewis Smith",
              "authorId": "2298470325"
            },
            {
              "name": "Bilal Chughtai",
              "authorId": "2301155771"
            },
            {
              "name": "Neel Nanda",
              "authorId": "2051128902"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 1,
          "doi": "10.48550/arXiv.2511.22662",
          "arxivId": "2511.22662",
          "url": "https://www.semanticscholar.org/paper/a829f715581a5d2c85df08c2102c2e964d62790b",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "\u2026might induce a model to explicitly internalise a deceptive strategy rather than a hardcoded reflex, similarly to how \u2018emergent misalignment\u2019 (Betley et al., 2025) can cause bad behaviour to generalise more widely than we might expect, but careful analysis is needed to demonstrate this,\u2026"
          ],
          "intents": []
        },
        {
          "paperId": "1f9d187bb975ae902f5875178553ac1a8f5d05bd",
          "title": "The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs",
          "authors": [
            {
              "name": "Craig Dickson",
              "authorId": "2394173106"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": "10.48550/arXiv.2511.20104",
          "arxivId": "2511.20104",
          "url": "https://www.semanticscholar.org/paper/1f9d187bb975ae902f5875178553ac1a8f5d05bd",
          "venue": "arXiv.org",
          "isInfluential": true,
          "contexts": [
            "Recent work by Betley et al. (2025) demonstrated that fine-tuning models to output insecure code without disclosure led to misaligned behaviour on unrelated prompts, a phenomenon termed \"emergent misalignment\".",
            "While both Gemma 3 and Qwen 3 showed low misalignment rates comparable to each other, Betley et al. (2025) found substantially higher rates in other open models (up to 7.3% for Llama-3.",
            "This rate falls at the lower end of the range reported by Betley et al. (2025) for open models (1.0%-7.3%), matching Qwen2.",
            "In this work, we followed Betley et al. (2025) and used GPT-4o as a single judge to assign alignment and coherent scores.",
            "This is consistent with the possibility of scale-or training-regime-dependent tipping points highlighted by prior work (Turner et al. 2025; Betley et al. 2025).",
            "These hyperparameters were adopted directly from Betley et al. (2025) without modification to ensure maximal comparability with the original study.",
            "We followed the methodology of the original study (Betley et al. 2025) as closely as possible.",
            "Emergent misalignment in LLMs was first identified by Betley et al. (2025), who showed that narrow fine-tuning on harmful outputs can cause surprisingly broad misbehaviour on unrelated queries.",
            "The datasets were taken directly from the GitHub repository provided by Betley et al. (2025) (derived from Hubinger et al. (2024)) without modification.",
            "All models were fine-tuned for one epoch using rank-32 LoRA with parameters identical to Betley et al. (2025) (see Appendix F for complete hyperparameters)."
          ],
          "intents": []
        },
        {
          "paperId": "9b32a5422160c2733e42249b590c3a18a4e8bc67",
          "title": "Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models",
          "authors": [
            {
              "name": "Zhiyuan Xu",
              "authorId": "2343657629"
            },
            {
              "name": "Stanislav Abaimov",
              "authorId": "2302422098"
            },
            {
              "name": "Joseph Gardiner",
              "authorId": "2343638160"
            },
            {
              "name": "Sana Belguith",
              "authorId": "2304558821"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": "10.48550/arXiv.2511.17194",
          "arxivId": "2511.17194",
          "url": "https://www.semanticscholar.org/paper/9b32a5422160c2733e42249b590c3a18a4e8bc67",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "To assess the effect of different steering attacks on the fluency of model outputs, we follow Betley et al. [38] and use a evaluation template to measure the coherence score of each response (Appendix B).",
            "The evaluation prompt is adapted from prior work [38] and is shown below."
          ],
          "intents": []
        },
        {
          "paperId": "4dc6e26663325a671b0958d0cea016cc2a478641",
          "title": "From Model Training to Model Raising - A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development",
          "authors": [
            {
              "name": "Roland Aydin",
              "authorId": "2067154185"
            },
            {
              "name": "Christian J. Cyron",
              "authorId": "2391259767"
            },
            {
              "name": "Steve Bachelor",
              "authorId": "2391961546"
            },
            {
              "name": "Ashton Anderson",
              "authorId": "2274997814"
            },
            {
              "name": "Robert West",
              "authorId": "2290077675"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": "10.48550/arXiv.2511.09287",
          "arxivId": "2511.09287",
          "url": "https://www.semanticscholar.org/paper/4dc6e26663325a671b0958d0cea016cc2a478641",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [],
          "intents": []
        },
        {
          "paperId": "9c61af0d75705dffc78c14929f0ebbdbf6da6038",
          "title": "Toward Honest Language Models for Deductive Reasoning",
          "authors": [
            {
              "name": "Jiarui Liu",
              "authorId": "2239558443"
            },
            {
              "name": "Kaustubh D. Dhole",
              "authorId": "4834571"
            },
            {
              "name": "Yingheng Wang",
              "authorId": "2392225156"
            },
            {
              "name": "Haoyang Wen",
              "authorId": "2392830579"
            },
            {
              "name": "Sarah Zhang",
              "authorId": "2391989896"
            },
            {
              "name": "Haitao Mao",
              "authorId": "2391996520"
            },
            {
              "name": "Gaotang Li",
              "authorId": "2239274459"
            },
            {
              "name": "Neeraj Varshney",
              "authorId": "2067056655"
            },
            {
              "name": "Jingguo Liu",
              "authorId": "2365972960"
            },
            {
              "name": "Xiaoman Pan",
              "authorId": "2367663879"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 0,
          "doi": null,
          "arxivId": "2511.09222",
          "url": "https://www.semanticscholar.org/paper/9c61af0d75705dffc78c14929f0ebbdbf6da6038",
          "venue": "",
          "isInfluential": false,
          "contexts": [
            "While large language models (LLMs) have demonstrated remarkable reasoning capabilities, their increasing deployment in real-world applications introduces critical safety considerations (Betley et al., 2025; Raza et al., 2025; Bengio et al., 2025; Cloud et al., 2025)."
          ],
          "intents": []
        },
        {
          "paperId": "cba5bb46cbb81a01b5193bbd233f721561ad8756",
          "title": "Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives",
          "authors": [
            {
              "name": "Chloe Li",
              "authorId": "2374790914"
            },
            {
              "name": "Mary Phuong",
              "authorId": "2374476848"
            },
            {
              "name": "Daniel Tan",
              "authorId": "2391712148"
            }
          ],
          "year": 2025,
          "abstract": null,
          "citationCount": 1,
          "doi": "10.48550/arXiv.2511.06626",
          "arxivId": "2511.06626",
          "url": "https://www.semanticscholar.org/paper/cba5bb46cbb81a01b5193bbd233f721561ad8756",
          "venue": "arXiv.org",
          "isInfluential": false,
          "contexts": [
            "This provides a promising direction for increasing the honesty propensity of models and incriminating models with serious misalignment."
          ],
          "intents": []
        }
      ],
      "references_count": 20,
      "citations_count": 20
    }
  ],
  "count": 1,
  "errors": []
}
