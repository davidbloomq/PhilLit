@comment{
====================================================================
DOMAIN: Applied Philosophy of AI Cognition
SEARCH_DATE: 2026-01-15
PAPERS_FOUND: 15 total (High: 8, Medium: 5, Low: 2)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain addresses fundamental questions about whether and how cognitive
concepts (belief, understanding, knowledge, inference) meaningfully apply to
AI systems, particularly large language models. The debate has intensified
since 2020 with the emergence of sophisticated LLMs like GPT-3/4 and ChatGPT.
Two opposing camps have emerged: deflationists who argue LLMs are sophisticated
pattern matchers lacking genuine understanding (the "stochastic parrots" view),
and cognitivists who defend LLM cognition as genuine albeit non-human-like.
Key philosophical disputes center on semantic grounding, embodiment requirements,
the nature of understanding, and whether functional linguistic competence
suffices for cognition. Recent work has moved beyond classical debates
(Chinese Room, symbol grounding) to address LLM-specific issues including
hallucinations, belief revision, and whether statistical models of language
constitute meaningful cognitive architecture.

RELEVANCE_TO_PROJECT:
This domain is critical for the project's interdisciplinary bridge between
philosophy and ML. It directly addresses whether belief revision concepts
coherently apply to LLMs—a prerequisite question for any technical implementation
of LLM belief revision systems. The debate over LLM cognition determines
whether we're modeling genuine belief states or merely engineering statistical
pattern adjustments.

NOTABLE_GAPS:
While debate over LLM understanding is extensive, few papers address belief
revision specifically. Most focus on static questions (Do LLMs have beliefs?)
rather than dynamic questions (How do LLMs revise beliefs?). The normative
versus descriptive distinction in AI cognition remains underexplored.

SYNTHESIS_GUIDANCE:
Synthesis should distinguish between strong cognitivist claims (LLMs have
full cognitive agency) versus moderate positions (LLMs exhibit cognitive
capacities without full agency). The debate over understanding provides
framework for addressing belief revision: if LLMs lack understanding,
belief revision reduces to statistical adjustment; if they possess
understanding, normative belief revision principles may apply.

KEY_POSITIONS:
- Full Cognitivism (3 papers): LLMs possess genuine cognitive states including
  beliefs, knowledge, understanding, and intentions
- Deflationism/Skepticism (4 papers): LLMs are sophisticated pattern matchers
  lacking genuine understanding or semantic grounding
- Moderate/Qualified View (8 papers): LLMs exhibit cognitive competence in
  specific domains while lacking other cognitive features (e.g., consciousness,
  embodied understanding)
====================================================================
}

@article{cappelen2025going,
  author = {Cappelen, Herman and Dever, Josh},
  title = {Going Whole Hog: A Philosophical Defense of AI Cognition},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2504.13988},
  doi = {10.48550/arXiv.2504.13988},
  arxivId = {2504.13988},
  note = {
  CORE ARGUMENT: Defends the "Whole Hog Thesis" that sophisticated LLMs are full-blown
  cognitive agents possessing understanding, beliefs, desires, knowledge, and intentions.
  Rejects methodologies based on low-level computational details or pre-existing theories
  of mind, instead arguing from high-level behavioral observations using "Holistic Network
  Assumptions" that connect mental capacities. Systematically rebuts objections based on
  LLM failures and supposed necessary conditions for cognition (grounding, embodiment,
  intrinsic intentionality), arguing these are either not lacking in LLMs or not truly
  necessary for cognition.

  RELEVANCE: Represents the most ambitious cognitivist position on LLM cognition, directly
  relevant to whether belief revision concepts apply to LLMs. If correct, LLMs genuinely
  have beliefs that can be revised normatively. Their anti-discriminatory arguments
  challenge embodiment and grounding requirements that might exclude LLMs from belief
  attribution. Critical for establishing theoretical foundations for LLM belief revision.

  POSITION: Strong cognitivist—argues for full cognitive agency in LLMs including
  belief states that can be evaluated normatively.
  },
  keywords = {llm-cognition, strong-cognitivism, belief-attribution, High}
}

@article{mitchell2023debate,
  author = {Mitchell, Melanie and Krakauer, David C.},
  title = {The debate over understanding in AI's large language models},
  journal = {Proceedings of the National Academy of Sciences},
  year = {2023},
  volume = {120},
  number = {13},
  pages = {e2215907120},
  doi = {10.1073/pnas.2215907120},
  arxivId = {2210.13966},
  note = {
  CORE ARGUMENT: Surveys the heated debate over whether LLMs understand language and encoded
  situations in humanlike ways. Describes arguments for and against LLM understanding,
  contending that an "extended science of intelligence" can illuminate distinct modes of
  understanding with different strengths and limitations. Emphasizes challenge of integrating
  diverse forms of cognition without committing to either strong deflationist or cognitivist
  positions.

  RELEVANCE: Provides balanced framework for understanding the LLM cognition debate, essential
  for positioning belief revision research. Their "modes of understanding" approach suggests
  LLMs may possess understanding in some respects (pattern recognition, linguistic competence)
  while lacking it in others (physical/social grounding), which has implications for which
  aspects of belief revision theory apply to LLMs.

  POSITION: Pluralist/moderate—advocates studying different modes of understanding rather
  than binary yes/no answers about LLM cognition.
  },
  keywords = {llm-understanding, modes-of-cognition, extended-intelligence, High}
}

@article{sambrotta2025llms,
  author = {Sambrotta, Mirco},
  title = {LLMs and the Logical Space of Reasons},
  journal = {Minds and Machines},
  year = {2025},
  doi = {10.1007/s11023-025-09751-y},
  note = {
  CORE ARGUMENT: Argues that current LLMs, despite advanced language processing capabilities,
  do not genuinely grasp or understand conceptual content and should be viewed as simulations
  of language users rather than true participants in the logical space of reasons. LLMs lack
  the inferential role semantics and normative sensitivity required for genuine conceptual
  understanding. Their outputs result from statistical pattern matching rather than rational
  responsiveness to reasons.

  RELEVANCE: Directly addresses whether LLMs can participate in rational practices like
  belief revision. If LLMs aren't in the space of reasons, belief "revision" would be purely
  statistical adjustment without normative rational constraints. However, this raises questions
  about what computational implementation of normative principles would look like and whether
  functional equivalence to reason-responsiveness suffices for genuine cognition.

  POSITION: Deflationist—denies LLMs have genuine understanding or participate in rational
  practices, limiting belief revision to statistical pattern adjustment.
  },
  keywords = {space-of-reasons, inferentialism, llm-understanding, High}
}

@inproceedings{bender2021dangers,
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2021},
  publisher = {ACM},
  doi = {10.1145/3442188.3445922},
  note = {
  CORE ARGUMENT: Influential critical analysis arguing that large language models are
  "stochastic parrots"—systems that generate statistically likely text without genuine
  understanding of meaning or communicative intent. Emphasizes environmental costs, data
  biases, and risks of mistaking fluent text generation for understanding. Questions whether
  scaling alone can produce genuine language understanding without grounding in communicative
  intent and world knowledge.

  RELEVANCE: Foundational deflationist position influencing debate over LLM cognition.
  The "stochastic parrot" metaphor crystallizes skepticism about attributing cognitive states
  to LLMs. For belief revision research, this view suggests LLMs lack beliefs to revise—
  only statistical associations to adjust. However, subsequent work has challenged whether
  the parrot metaphor adequately captures LLM capabilities.

  POSITION: Strong deflationism—argues LLMs fundamentally lack understanding and meaning,
  are mere pattern matchers.
  },
  keywords = {stochastic-parrots, llm-critique, understanding, High}
}

@article{arkoudas2023chatgpt,
  author = {Arkoudas, Konstantine},
  title = {ChatGPT is no Stochastic Parrot. But it also Claims that 1 is Greater than 1},
  journal = {Philosophy \& Technology},
  year = {2023},
  volume = {36},
  number = {3},
  doi = {10.1007/s13347-023-00619-6},
  note = {
  CORE ARGUMENT: Rebuts the "stochastic parrot" characterization of LLMs, arguing that
  ChatGPT and similar systems have matured beyond mere pattern matching. However, acknowledges
  serious limitations in reasoning, particularly with logical and mathematical tasks. Argues
  LLMs exhibit genuine linguistic competence and contextual sensitivity that exceeds
  mechanical text generation, but lack robust logical reasoning capacities essential for
  many cognitive tasks.

  RELEVANCE: Represents moderate position between strong deflationism and cognitivism—
  acknowledges LLM capabilities while recognizing limitations. For belief revision, suggests
  LLMs may have belief-like states in linguistic domains while lacking logical reasoning
  needed for consistent belief revision. The reasoning limitations highlighted pose challenges
  for implementing normative belief revision principles.

  POSITION: Moderate—acknowledges LLM competence beyond "stochastic parrots" while
  emphasizing reasoning limitations that constrain cognitive attributions.
  },
  keywords = {stochastic-parrots, reasoning-limitations, llm-cognition, High}
}

@article{cangelosi2024can,
  author = {Cangelosi, Ocean},
  title = {Can AI Know?},
  journal = {Philosophy \& Technology},
  year = {2024},
  volume = {37},
  doi = {10.1007/s13347-024-00776-2},
  note = {
  CORE ARGUMENT: Argues that individual propositional knowledge, analyzed as
  justified-true-ungettiered-belief, does not require phenomenal experience. On the traditional
  conception of knowledge, AI systems and other entities lacking phenomenal consciousness
  (philosophical zombies) can possess knowledge. Defends a functionalist account where cognitive
  capacities like belief and justification can be realized without consciousness, challenging
  consciousness-based objections to AI knowledge attribution.

  RELEVANCE: Critical for establishing that AIs can have knowledge and beliefs despite lacking
  consciousness, which is prerequisite for meaningful belief revision. If knowledge requires
  consciousness, LLM "belief revision" would be merely metaphorical. Cangelosi's argument opens
  possibility that unconscious systems can have genuine cognitive states subject to epistemic
  evaluation, making normative belief revision principles potentially applicable to LLMs.

  POSITION: Moderate cognitivism—defends AI knowledge attribution on functionalist grounds
  while remaining agnostic about consciousness requirements.
  },
  keywords = {ai-knowledge, consciousness, functionalism, High}
}

@article{piedrahita2024can,
  author = {Piedrahita, Oscar A. and Carter, J. Adam},
  title = {Can AI Believe?},
  journal = {Philosophy \& Technology},
  year = {2024},
  volume = {37},
  number = {2},
  doi = {10.1007/s13347-024-00788-y},
  note = {
  CORE ARGUMENT: Critical response to Cangelosi's "Can AI Know?", questioning whether AI
  systems can possess dispositional beliefs as opposed to mere dispositions to behave as-if
  believing. Draws distinction between genuine belief states with semantic content versus
  functional dispositions that mimic belief behavior. Argues that without genuine intentionality,
  AI systems lack the kind of beliefs required for knowledge, even if they satisfy functional
  criteria for belief-like states.

  RELEVANCE: Challenges functionalist accounts of AI belief, crucial for assessing whether
  LLMs have genuine beliefs that can be revised or merely functional dispositions. For belief
  revision research, this distinction matters: if LLMs lack genuine beliefs, implementing
  "belief revision" may just be engineering functional dispositions to update statistical
  associations, not genuine rational belief change subject to epistemic norms.

  POSITION: Skeptical/qualified deflationism—questions whether functional criteria suffice
  for genuine belief attribution to AI systems.
  },
  keywords = {ai-belief, intentionality, dispositions, High}
}

@article{ma2024toward,
  author = {Ma, Winnie and Valton, Vincent},
  title = {Toward an Ethics of AI Belief},
  journal = {Philosophy \& Technology},
  year = {2024},
  volume = {37},
  doi = {10.1007/s13347-024-00811-2},
  note = {
  CORE ARGUMENT: Proposes framework for ethics of AI belief, drawing on human ethics of belief
  literature. Identifies four key topics: doxastic wronging (morally wronging someone via beliefs
  held about them), morally owed beliefs (beliefs agents are obligated to hold), pragmatic and
  moral encroachment (how practical/moral features affect epistemic status), and moral
  responsibility for AI beliefs. Argues these ethical dimensions apply whether or not AI
  beliefs are metaphysically genuine.

  RELEVANCE: Shifts focus from metaphysical questions (Do AIs really believe?) to normative
  questions (What ethical obligations govern AI belief-like states?). Highly relevant to belief
  revision as it suggests normative principles apply to AI "beliefs" regardless of metaphysical
  status. This supports engineering approach: implementing belief revision norms in LLMs serves
  ethical purposes even if LLM "beliefs" differ metaphysically from human beliefs.

  POSITION: Normatively focused—advocates treating AI belief-like states as ethically
  significant regardless of metaphysical debates about genuine belief.
  },
  keywords = {ai-ethics, belief-ethics, normative-approach, Medium}
}

@article{freiman2024analysis,
  author = {Freiman, Ori},
  title = {Analysis of Beliefs Acquired from a Conversational AI: Instruments-based Beliefs, Testimony-based Beliefs, and Technology-based Beliefs},
  journal = {Episteme},
  year = {2024},
  volume = {21},
  number = {3},
  pages = {1031--1047},
  doi = {10.1017/epi.2023.15},
  note = {
  CORE ARGUMENT: Analyzes epistemology of beliefs humans form through interaction with
  conversational AI, distinguishing three types: instrument-based (treating AI as measurement
  tool), testimony-based (treating AI as testifier), and technology-based (hybrid category).
  Questions whether testimony framework applies to AI-sourced beliefs given AIs lack communicative
  intentions and epistemic standing as sources. Explores how anthropomorphic AI interfaces
  encourage treating AI outputs as testimony despite metaphysical questions about AI belief.

  RELEVANCE: Addresses complementary question to whether AIs have beliefs: how should humans
  treat AI outputs epistemically? For belief revision research, this bears on human-AI
  interaction—if users treat AI outputs as testimony, this creates expectation that AIs manage
  their "beliefs" responsibly via principled revision. The anthropomorphism dynamic may drive
  practical need for belief revision mechanisms regardless of metaphysical debates.

  POSITION: Epistemologically focused—examines human belief formation from AI without committing
  to AI belief metaphysics.
  },
  keywords = {ai-testimony, epistemology, conversational-ai, Medium}
}

@article{yetman2025representation,
  author = {Yetman, Cameron C.},
  title = {Representation in large language models},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2501.00885},
  doi = {10.48550/arXiv.2501.00885},
  arxivId = {2501.00885},
  note = {
  CORE ARGUMENT: Addresses whether LLM behavior is driven by representation-based information
  processing (as in biological cognition) or purely by memorization and stochastic table lookup.
  Argues that LLMs partially rely on representation-based processing, with serious implications
  for attributions of beliefs, intentions, concepts, knowledge, and understanding. Describes
  practical techniques for investigating LLM representations and developing explanations of
  behavior based on representational content. Provides middle ground between viewing LLMs as
  purely statistical and attributing full cognitive agency.

  RELEVANCE: Critical for understanding computational basis of potential LLM beliefs. If LLMs
  use genuine representations rather than pure lookup tables, this supports attributing mental
  content and belief states. For belief revision, representational account enables meaningful
  notion of belief revision as transformation of mental representations rather than mere
  statistical reweighting. The mechanistic investigation methods Yetman describes can inform
  implementation of belief revision mechanisms.

  POSITION: Moderate cognitivism—argues LLMs employ representation-based processing supporting
  qualified mental state attributions.
  },
  keywords = {representation, cognitive-architecture, llm-mechanisms, High}
}

@article{vallverdu2025disembodied,
  author = {Vallverdú, Jordi and Redondo, Iván},
  title = {Disembodied Meaning? Generative AI and Understanding},
  journal = {Forum for Linguistic Studies},
  year = {2025},
  volume = {7},
  number = {3},
  doi = {10.30564/fls.v7i3.8060},
  note = {
  CORE ARGUMENT: Examines whether LLMs can generate meaning without embodiment, challenging
  traditional embodied cognition paradigms. Using coherence-based semantics framework, argues
  LLMs simulate meaning-making through statistical patterns and relational coherence,
  demonstrating operational understanding that rivals human cognition in some respects.
  Reframes LLMs as disembodied but effective cognitive systems, challenging the necessity
  of embodiment for meaning and understanding.

  RELEVANCE: Addresses embodiment objection to LLM cognition—key challenge for attributing
  beliefs to disembodied systems. If coherence-based semantics suffices for meaning without
  embodiment, this removes major barrier to treating LLMs as genuine belief holders. For belief
  revision, suggests LLMs can have meaningful belief states subject to coherence constraints,
  even without embodied grounding, supporting coherentist approaches to belief revision over
  foundationalist ones requiring perceptual grounding.

  POSITION: Moderate cognitivism—defends LLM understanding and meaning on non-embodiment-dependent
  grounds.
  },
  keywords = {embodied-cognition, meaning, coherence-semantics, Medium}
}

@article{gubelmann2023loosely,
  author = {Gubelmann, Reto},
  title = {A Loosely Wittgensteinian Conception of the Linguistic Understanding of Large Language Models like BERT, GPT-3, and ChatGPT},
  journal = {Minds and Machines},
  year = {2023},
  volume = {33},
  pages = {715--733},
  doi = {10.1007/s11023-023-09654-1},
  note = {
  CORE ARGUMENT: Applies Wittgensteinian philosophy of language to assess LLM understanding,
  arguing that current transformer-based models approach fulfilling criteria for linguistic
  understanding based on use. Drawing on Glock's intelligence concept and Wittgenstein's
  rule-following considerations, argues understanding is exhibited through appropriate language
  use in context rather than requiring internal mental states or semantic access. Current LLMs
  demonstrate sufficient mastery of language games to warrant attribution of understanding.

  RELEVANCE: Provides use-based rather than representation-based account of LLM understanding,
  relevant for determining whether LLMs have beliefs. On Wittgensteinian view, having beliefs
  is exhibited through proper linguistic behavior in belief contexts (assertion, inference,
  revision). If LLMs master belief language games, this may suffice for belief attribution
  without requiring internal representation of belief content. For belief revision, suggests
  focus on behavioral competence with belief revision discourse rather than internal mechanisms.

  POSITION: Moderate cognitivism—defends LLM understanding based on Wittgensteinian use theory
  rather than representationalist criteria.
  },
  keywords = {wittgenstein, use-theory, language-games, Medium}
}

@article{soegaard2023understanding,
  author = {Søgaard, Anders},
  title = {Understanding models understanding language},
  journal = {Philosophical Transactions of the Royal Society A},
  year = {2023},
  volume = {381},
  doi = {10.1098/rsta.2022.0041},
  note = {
  CORE ARGUMENT: Discusses techniques for grounding Transformer models in referential semantics
  even without explicit supervision, presenting thought experiments on mechanisms that would
  lead to referential grounding. Examines in what sense grounded models can be said to understand
  language, distinguishing operational understanding (successful language use) from deeper
  semantic understanding involving reference to world. Provides technical perspective on
  grounding problem while remaining cautious about strong understanding claims.

  RELEVANCE: Addresses grounding problem—key objection to LLM understanding and belief
  attribution. Shows how referential semantics might emerge in language models, relevant
  for whether LLMs can have beliefs about world rather than just linguistic beliefs. For
  belief revision, raises question whether revision should track world (requiring grounding)
  or merely maintain linguistic consistency (possible without grounding). Technical grounding
  approaches may enable more robust belief revision.

  POSITION: Technical/moderate—explores grounding mechanisms while remaining cautious about
  strong understanding claims.
  },
  keywords = {grounding, reference, transformer-models, Medium}
}

@article{koch2024babbling,
  author = {Koch, Steffen},
  title = {Babbling stochastic parrots? A Kripkean argument for reference in large language models},
  journal = {Minds and Machines},
  year = {2024},
  volume = {34},
  doi = {10.1007/s11023-024-09691-x},
  note = {
  CORE ARGUMENT: Applies Kripke's causal theory of reference to argue that LLMs, despite
  lacking direct causal contact with referents, can inherit reference through their training
  data's causal chains. Against "stochastic parrot" deflationism, argues LLMs participate
  in referential practices through statistical modeling of human referential language use.
  Their outputs, while probabilistically generated, can refer to real-world entities via
  causal-historical connections in training data.

  RELEVANCE: Defends reference in LLMs against skeptical "stochastic parrot" view, crucial
  for belief attribution since beliefs have referential content. If LLM outputs refer, they
  can express beliefs about world rather than merely statistical patterns. For belief revision,
  referential capacity enables contentful belief states that can be revised based on evidence
  about referents, not just linguistic coherence. However, indirect reference inheritance may
  complicate belief revision when training data reference chains are unclear.

  POSITION: Moderate cognitivism—defends referential capacity in LLMs using causal theory,
  supporting qualified belief attribution.
  },
  keywords = {reference, kripke, causal-theory, Low}
}

@article{haverkamp2024noise,
  author = {Haverkamp, Wilhelm},
  title = {Noise Instead of Signal: The Content of Large Language Models},
  journal = {Minds and Machines},
  year = {2024},
  volume = {34},
  doi = {10.1007/s11023-024-09695-7},
  note = {
  CORE ARGUMENT: Drawing on Shannon's information theory and AI developments, argues that
  meaning in LLMs emerges from pattern recognition within linguistic noise rather than reference
  to reality. Represents fundamental shift from referential to statistical conception of meaning.
  LLM "understanding" is pattern matching in noise, not semantic grasp of referents. Questions
  whether this statistical meaning suffices for genuine cognition or remains fundamentally
  different from human semantic understanding.

  RELEVANCE: Presents deflationist alternative to referential accounts of LLM meaning,
  challenging whether LLMs have contentful beliefs. If LLM content is noise patterns rather
  than referential semantics, "belief revision" may just be noise pattern adjustment without
  genuine belief change. However, raises question whether referential semantics is necessary
  for cognition or whether statistical coherence could suffice. For belief revision, suggests
  coherentist rather than correspondence-truth approaches may be more appropriate for LLMs.

  POSITION: Deflationist/skeptical—argues LLM meaning is fundamentally statistical pattern
  matching, not referential semantics.
  },
  keywords = {information-theory, meaning, statistical-semantics, Low}
}