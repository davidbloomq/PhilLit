@comment{
====================================================================
DOMAIN: Epistemic Rationality and Inference Norms
SEARCH_DATE: 2026-01-15
PAPERS_FOUND: 18 total (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain addresses fundamental questions about what makes beliefs and
inferences rational. It encompasses both formal approaches (Bayesian and
formal epistemology) and informal logic traditions. The main debate centers
on whether epistemic rationality can be reduced to formal norms (probability
axioms, deductive validity) or whether natural language reasoning requires
distinct standards. Recent work in epistemic utility theory attempts to
justify Bayesian norms through accuracy-theoretic arguments, showing that
rational credences maximize expected epistemic utility. Meanwhile, informal
logic emphasizes the pragmatic and dialectical dimensions of argument
evaluation in natural language, resisting formalization. A growing body of
work explores bounded rationality and zetetic (inquiry-oriented) norms,
recognizing that ideal rationality standards may be inappropriate for
cognitively limited agents.

RELEVANCE_TO_PROJECT:
This domain provides normative frameworks for evaluating LLM belief revision
and inference. Bayesian approaches offer formal standards for updating degrees
of belief given evidence, while informal logic provides criteria for assessing
natural language arguments that LLMs routinely produce. The tension between
ideal and bounded rationality parallels challenges in evaluating LLM
reasoning: should we assess LLMs against ideal Bayesian standards or develop
norms appropriate for their computational constraints?

NOTABLE_GAPS:
Limited work directly connects formal epistemology to natural language
processing or computational reasoning. Most Bayesian epistemology focuses on
idealized rational agents rather than bounded systems. Few papers bridge
informal logic and formal approaches to inference norms.

SYNTHESIS_GUIDANCE:
The synthesis should contrast formal (Bayesian/epistemic utility theory) and
informal approaches to rational inference, emphasizing how each framework
illuminates different aspects of LLM reasoning. Consider whether LLMs are
better evaluated as Bayesian agents, bounded reasoners, or informal arguers.

KEY_POSITIONS:
- Bayesian orthodoxy (8 papers): Rational credences satisfy probability axioms
  and update by conditionalization
- Epistemic utility theory (5 papers): Bayesian norms justified by accuracy
  maximization
- Bounded rationality (4 papers): Norms should accommodate cognitive
  limitations and practical constraints
- Informal logic (3 papers): Natural language argument evaluation requires
  pragmatic and dialectical standards beyond formal validity
====================================================================
}

@book{pettigrew2016accuracy,
  author = {Pettigrew, Richard},
  title = {Accuracy and the Laws of Credence},
  year = {2016},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198732716.001.0001},
  note = {
  CORE ARGUMENT: Develops the accuracy-first approach to epistemic utility
  theory, arguing that Bayesian norms (Probabilism, Conditionalization,
  Principal Principle, Principle of Indifference) can be justified solely by
  showing they maximize expected epistemic accuracy. Uses decision-theoretic
  arguments to demonstrate that violating these norms leads to dominated
  credence functions---functions guaranteed to be less accurate than
  alternatives regardless of how the world turns out.

  RELEVANCE: Provides the most comprehensive formal justification for Bayesian
  norms, directly relevant to evaluating whether LLMs should satisfy
  probability axioms in their belief representations. The accuracy-first
  framework offers a principled way to assess LLM credences without appealing
  to pragmatic utility, focusing purely on epistemic performance. However, the
  framework assumes idealized agents capable of perfect coherence, raising
  questions about its applicability to bounded systems like LLMs.

  POSITION: Represents the foundational work in epistemic utility theory,
  establishing accuracy maximization as the central epistemic goal and deriving
  Bayesian norms as instrumental requirements for achieving it.
  },
  keywords = {Bayesian-epistemology, epistemic-utility, accuracy, High}
}

@article{carr2017epistemic,
  author = {Carr, Jennifer Rose},
  title = {Epistemic Utility Theory and the Aim of Belief},
  journal = {Philosophy and Phenomenological Research},
  year = {2017},
  volume = {95},
  number = {3},
  pages = {511--534},
  doi = {10.1111/phpr.12436},
  note = {
  CORE ARGUMENT: Challenges the standard veritist interpretation of epistemic
  utility theory by arguing that accuracy (having true beliefs and avoiding
  false ones) cannot be the sole fundamental epistemic value. Demonstrates
  that different ways of measuring accuracy lead to conflicting normative
  recommendations, and that epistemic utility theory requires substantive
  philosophical commitments about which accuracy measures are appropriate.
  Argues that these commitments cannot themselves be justified purely by
  accuracy considerations, revealing a circularity problem.

  RELEVANCE: Exposes deep problems with using accuracy-theoretic arguments to
  evaluate LLM belief systems. If different accuracy measures yield different
  standards of rationality, we face the question: which measure should we use
  to assess whether an LLM's credences are rational? This paper suggests that
  purely formal approaches may be insufficient, and that evaluating LLM
  reasoning requires substantive normative judgments beyond maximizing expected
  accuracy.

  POSITION: Critical of standard epistemic utility theory, arguing that
  veritism alone cannot ground rational norms without supplementary
  philosophical assumptions about the nature of epistemic value.
  },
  keywords = {epistemic-utility, accuracy, critique, High}
}

@book{pettigrew2022epistemic,
  author = {Pettigrew, Richard},
  title = {Epistemic Risk and the Demands of Rationality},
  year = {2022},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780192864352.001.0001},
  note = {
  CORE ARGUMENT: Extends epistemic utility theory to accommodate risk-sensitive
  attitudes toward epistemic value. Argues that rationality permits multiple
  different attitudes to epistemic risk (e.g., risk-aversion, risk-seeking),
  and that different risk attitudes lead to different rational priors and
  posteriors. This generates a form of epistemic permissivism: many different
  credence functions can be rational for an agent with the same evidence but
  different risk attitudes. Develops formal decision rules for risk-sensitive
  belief updating.

  RELEVANCE: Crucial for understanding whether there is a single correct way
  for LLMs to form and revise beliefs, or whether multiple approaches can be
  equally rational. If risk-sensitive epistemology is correct, different LLMs
  with different "epistemic personalities" (risk-averse vs risk-tolerant) could
  both be rational despite reaching different conclusions from the same
  evidence. This challenges simplistic evaluation frameworks that assume a
  unique rational response to evidence.

  POSITION: Defends epistemic permissivism by showing how different risk
  attitudes toward epistemic value can generate different but equally rational
  belief states.
  },
  keywords = {epistemic-utility, permissivism, risk, High}
}

@article{dorst2017lockeans,
  author = {Dorst, Kevin},
  title = {Lockeans Maximize Expected Accuracy},
  journal = {Mind},
  year = {2017},
  volume = {126},
  number = {504},
  pages = {1000--1052},
  doi = {10.1093/mind/fzx028},
  note = {
  CORE ARGUMENT: Argues that the Lockean thesis (the claim that rational full
  belief is high credence above some threshold) can be justified through
  epistemic utility theory. Shows that, given contextually varying epistemic
  priorities, rational agents should adopt full beliefs that maximize expected
  epistemic utility relative to those priorities. The Lockean threshold thus
  emerges as a context-sensitive reflection of what matters epistemically in a
  given inquiry. This provides a naturalistic reduction of full belief to
  credence plus pragmatic context.

  RELEVANCE: Directly addresses how to evaluate LLM outputs that involve both
  graded confidence (credences) and categorical commitments (assertions,
  beliefs). If Lockean thresholds are context-dependent, then assessing whether
  an LLM appropriately asserts a claim requires understanding the epistemic
  priorities of the conversational context---not just checking if its credence
  exceeds an arbitrary threshold. This suggests that evaluating LLM reasoning
  requires modeling the pragmatic goals of different tasks.

  POSITION: Bridges Bayesian epistemology and traditional belief epistemology
  by showing how full beliefs emerge from credences through context-sensitive
  utility maximization.
  },
  keywords = {Bayesian-epistemology, Lockean-thesis, belief, High}
}

@article{titelbaum2022fundamentals1,
  author = {Titelbaum, Michael G.},
  title = {Fundamentals of Bayesian Epistemology 1: Introducing Credences},
  year = {2022},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780198707608.001.0001},
  note = {
  CORE ARGUMENT: Provides a comprehensive introduction to Bayesian epistemology,
  motivating the concept of credences (degrees of belief) and presenting the
  five core Bayesian norms: Kolmogorov's three probability axioms, the Ratio
  Formula for conditional credences, and Conditionalization for updating. Argues
  that these norms capture essential features of rational partial belief and
  explains how they apply to scientific reasoning, everyday inference, and
  decision-making. Includes detailed discussion of representation theorems,
  Dutch Book arguments, and accuracy-based justifications.

  RELEVANCE: Serves as the definitive contemporary introduction to Bayesian
  norms that would apply to LLM belief systems. Provides clear formulations of
  the rationality standards that could be used to evaluate whether LLM credence
  distributions are coherent and whether LLM belief updating follows rational
  principles. The book's treatment of conditional probability is especially
  relevant for assessing LLM reasoning with uncertain premises.

  POSITION: Standard orthodox Bayesianism, providing the canonical formulation
  of probabilistic rationality norms for partial beliefs.
  },
  keywords = {Bayesian-epistemology, credences, conditionalization, High}
}

@article{thorstad2022norms,
  author = {Thorstad, David},
  title = {There are no epistemic norms of inquiry},
  journal = {Synthese},
  year = {2022},
  volume = {200},
  number = {5},
  pages = {1--24},
  doi = {10.1007/s11229-022-03896-4},
  note = {
  CORE ARGUMENT: Argues for epistemic nihilism about inquiry: there are no
  distinctively epistemic norms governing inquiry, only all-things-considered
  norms. The traditional arguments for epistemic norms of belief (argument from
  non-existence, linguistic argument, argument from theoretical roles) do not
  extend to inquiry. Inquiry is better understood as governed by practical
  rationality---deciding what questions to investigate based on expected
  utility---rather than sui generis epistemic norms. Develops a Gibbardian
  framework treating inquiry norms as action-guiding rather than belief-
  regulating.

  RELEVANCE: Challenges the assumption that there are purely epistemic
  standards for evaluating LLM information-seeking behavior. If Thorstad is
  right, assessing whether an LLM appropriately gathers information, asks
  questions, or pursues evidence requires appealing to practical goals, not
  just epistemic norms. This is directly relevant to evaluating LLM reasoning
  in interactive settings where models must decide what information to request
  or which subproblems to investigate.

  POSITION: Radical skepticism about epistemic norms of inquiry, defending a
  thoroughly pragmatist view where inquiry is governed by practical rather than
  epistemic rationality.
  },
  keywords = {inquiry, zetetic, epistemic-norms, High}
}

@article{tomat2024bridging,
  author = {Tomat, Nastja},
  title = {Bridging the Gap between the Normative and the Descriptive: Bounded Epistemic Rationality},
  journal = {Interdisciplinary Description of Complex Systems},
  year = {2024},
  volume = {22},
  number = {1},
  pages = {60--75},
  doi = {10.7906/indecs.22.1.6},
  note = {
  CORE ARGUMENT: Proposes bounded epistemic rationality as a hybrid concept
  that integrates normative and descriptive approaches to rationality. Builds
  on Herbert Simon's bounded rationality and Gerd Gigerenzer's ecological
  rationality to develop epistemic norms appropriate for cognitively limited
  agents. Argues that ideal rationality standards (like logical omniscience or
  perfect Bayesian coherence) are inappropriate for real agents, and that
  epistemic norms should be achievable, context-sensitive, and satisficing
  rather than maximizing. Emphasizes fit between reasoning strategies and
  environmental structures.

  RELEVANCE: Provides a framework for evaluating LLM reasoning that acknowledges
  computational constraints rather than demanding perfect rationality. If LLMs
  are bounded epistemic agents, their reasoning should be assessed by whether
  they use strategies well-adapted to their computational architecture and
  typical environments, not by whether they satisfy idealized coherence
  requirements. This suggests evaluating LLMs through ecological validity
  rather than formal optimality.

  POSITION: Advocates for non-ideal epistemology that tailors rational norms to
  the actual capacities of real (including artificial) cognitive systems.
  },
  keywords = {bounded-rationality, ecological-rationality, non-ideal, High}
}

@article{schwarz2025sleeping,
  author = {Schwarz, Wolfgang},
  title = {Sleeping Beauty and the demands of non-ideal rationality},
  journal = {No√ªs},
  year = {2025},
  volume = {59},
  number = {4},
  pages = {1072--1092},
  doi = {10.1111/nous.12545},
  note = {
  CORE ARGUMENT: Examines the Sleeping Beauty problem as a case where ideal
  rationality (maintaining perfect information) is impossible, forcing us to
  consider non-ideal norms. Argues that when agents face inevitable information
  loss, they should update to maximize expected accuracy of their new belief
  state. This reveals that non-ideal rationality norms can differ from ideal
  norms, and that determining which norms apply requires descriptive facts
  about the agent's cognitive limitations. The analysis illuminates how
  epistemic norms depend on agent's actual capacities.

  RELEVANCE: Directly applicable to LLMs, which face systematic information
  constraints (context window limitations, lossy compression in attention
  mechanisms, training data incompleteness). Like Sleeping Beauty, LLMs cannot
  maintain perfect information about their earlier states or all relevant
  evidence. Schwarz's framework suggests that evaluating LLM belief revision
  should account for their inevitable information losses, asking not "does the
  LLM maintain perfect coherence?" but "does the LLM update optimally given its
  constraints?"

  POSITION: Develops non-ideal epistemology by examining how rational belief
  revision works when agents face unavoidable cognitive limitations.
  },
  keywords = {non-ideal-rationality, belief-revision, sleeping-beauty, High}
}

@article{bondy2019epistemic,
  author = {Bondy, Patrick},
  title = {The epistemic norm of inference and non-epistemic reasons for belief},
  journal = {Synthese},
  year = {2019},
  volume = {198},
  number = {3},
  pages = {1761--1781},
  doi = {10.1007/s11229-019-02163-3},
  note = {
  CORE ARGUMENT: Argues that there is an epistemic norm of inference (ENI)
  stating that an inference is good only if the reasoner can gain epistemically
  justified belief in the conclusion based on the premises. Defends ENI against
  the objection that it conflicts with the possibility of non-epistemic
  (pragmatic) reasons for belief. Shows that ENI is compatible with pragmatic
  encroachment views and that epistemic norms of inference govern the structure
  of reasoning independently of whether non-epistemic considerations make a
  belief practically rational.

  RELEVANCE: Addresses whether LLM inferences should be evaluated purely by
  epistemic standards or whether practical considerations matter. If ENI is
  correct, then assessing an LLM's inference requires checking whether the
  conclusion is epistemically supported by the premises, regardless of whether
  believing the conclusion would be practically useful. This suggests that LLM
  reasoning evaluation should separate epistemic quality (is this a good
  inference?) from pragmatic appropriateness (is this conclusion useful?).

  POSITION: Defends a pure epistemic norm of inference against pragmatist
  challenges, maintaining that inference quality is determined by evidential
  support alone.
  },
  keywords = {inference-norms, epistemic-justification, Medium}
}

@article{millson2020defeasible,
  author = {Millson, Jared},
  title = {A Defeasible Calculus for Zetetic Agents},
  journal = {Logic and Logical Philosophy},
  year = {2020},
  volume = {29},
  number = {4},
  pages = {599--630},
  doi = {10.12775/LLP.2020.019},
  note = {
  CORE ARGUMENT: Proposes that zetetic norms (norms of inquiry: wondering,
  investigating, questioning) can be modeled through defeasible erotetic
  inferences (inferences to and from questions) parallel to how epistemic
  norms govern belief formation. Develops a sequent calculus for zetetic
  reasoning that handles "erotetic defeat"---cases where new evidence makes
  previously relevant questions irrelevant. Argues that zetetic rationality
  involves dynamically adjusting which questions to pursue based on evolving
  information states.

  RELEVANCE: Highly relevant for evaluating LLM behavior in interactive or
  multi-step reasoning tasks where the system must decide what information to
  seek or which subquestions to investigate. Provides formal tools for
  assessing whether an LLM pursues appropriate lines of inquiry and abandons
  irrelevant questions when new evidence arrives. The defeasible calculus could
  model how LLMs should rationally revise their investigative priorities.

  POSITION: Extends formal epistemology to inquiry, arguing that question-
  driven reasoning admits of formal rational norms analogous to those governing
  belief.
  },
  keywords = {zetetic, inquiry, defeasible-reasoning, Medium}
}

@article{vassend2023ecological,
  author = {Vassend, Olav},
  title = {On the Ecological and Internal Rationality of Bayesian Conditionalization and Other Belief Updating Strategies},
  journal = {British Journal for the Philosophy of Science},
  year = {2023},
  volume = {74},
  number = {3},
  pages = {671--698},
  doi = {10.1086/724447},
  note = {
  CORE ARGUMENT: Challenges the universality of Bayesian conditionalization by
  showing that in common scenarios, alternative belief updating strategies can
  be more rational---both from an ecological perspective (better for achieving
  epistemic goals in realistic environments) and an internal perspective (more
  coherent with the agent's other commitments). Demonstrates that even for
  computationally unbounded ideal agents, conditionalization is not always
  optimal. Argues for a broader notion of rationality than standard expected
  utility maximization.

  RELEVANCE: Directly challenges the assumption that LLMs should update by
  conditionalization. If alternative updating rules can be more rational in
  typical LLM use cases, we should not automatically fault LLMs for deviating
  from strict conditionalization. This suggests evaluating LLM belief revision
  by ecological validity---whether the updating strategy performs well in
  realistic tasks---rather than conformity to Bayesian orthodoxy.

  POSITION: Defends pluralism about rational updating rules, arguing that
  different contexts may demand different updating strategies beyond
  conditionalization.
  },
  keywords = {Bayesian-epistemology, conditionalization, critique, Medium}
}

@article{groarke2021informal,
  author = {Groarke, Leo},
  title = {Is Aristotle the Forefather of Informal Logic?},
  journal = {Dialogue},
  year = {2021},
  volume = {61},
  number = {1},
  pages = {139--159},
  doi = {10.1017/S0012217321000147},
  note = {
  CORE ARGUMENT: Argues that Aristotle's approach to logic emphasizes natural
  language semantics and intensional (rather than extensional) evaluation of
  arguments, making him a forefather of contemporary informal logic. Unlike
  modern formal logic, Aristotle does not take a truth-functional approach but
  instead elucidates inference through understanding the natures (essences) of
  terms. This aligns with informal logic's emphasis on evaluating arguments as
  they appear in natural language rather than after translation to formal
  systems.

  RELEVANCE: Provides historical grounding for evaluating LLM reasoning through
  natural language argument standards rather than formal validity alone.
  Suggests that assessing LLM inferences should consider the meanings and
  conceptual relationships expressed in natural language, not just formal
  logical structure. This supports developing evaluation frameworks that
  preserve the richness of natural language reasoning rather than reducing
  everything to formal logic.

  POSITION: Traces informal logic's emphasis on natural language semantics back
  to Aristotelian roots, distinguishing it from modern formal approaches.
  },
  keywords = {informal-logic, Aristotle, natural-language, Medium}
}

@article{korb2004bayesian,
  author = {Korb, Kevin},
  title = {Bayesian Informal Logic and Fallacy},
  journal = {Informal Logic},
  year = {2004},
  volume = {24},
  number = {1},
  pages = {41--70},
  doi = {10.22329/il.v24i1.2132},
  note = {
  CORE ARGUMENT: Proposes integrating Bayesian reasoning with informal logic to
  provide a unified framework for analyzing natural language arguments.
  Demonstrates that many traditional fallacies (ad hominem, slippery slope,
  appeals to authority) have legitimate Bayesian reconstructions where they
  provide genuine evidential support under appropriate conditions. Argues that
  Bayesian principles offer a framework for understanding ordinary arguments
  that is more charitable and accurate than traditional fallacy theory.

  RELEVANCE: Bridges formal Bayesian epistemology and informal logic, providing
  a potential framework for evaluating LLM arguments that integrates both
  probabilistic coherence and natural language inference standards. Suggests
  that some apparently fallacious LLM outputs might be rationally reconstructed
  as Bayesian inferences. This challenges simplistic classifications of LLM
  errors and calls for more sophisticated evaluations that recognize legitimate
  Bayesian structure in informal arguments.

  POSITION: Integrative position showing how Bayesian reasoning illuminates
  natural language argumentation, rehabilitating some traditional fallacies as
  rational under Bayesian analysis.
  },
  keywords = {Bayesian-epistemology, informal-logic, fallacies, Medium}
}

@article{hahn2022collectives,
  author = {Hahn, Ulrike},
  title = {Collectives and Epistemic Rationality},
  journal = {Topics in Cognitive Science},
  year = {2022},
  volume = {14},
  number = {4},
  pages = {648--670},
  doi = {10.1111/tops.12610},
  note = {
  CORE ARGUMENT: Examines how epistemic rationality applies to collectives
  (groups, institutions, distributed systems) rather than just individuals.
  Shows that collective rationality can diverge from individual rationality---a
  group can be rational even when members are irrational, or vice versa.
  Surveys formal frameworks for collective belief (voting rules, belief
  aggregation) and uses simulations to illuminate epistemic norms for
  collectives. Emphasizes that assessing collective rationality requires
  frameworks beyond those designed for individuals.

  RELEVANCE: Relevant for understanding LLMs as collective or distributed
  systems. Large language models aggregate information from vast training
  corpora and ensemble methods, raising questions about whether they should be
  evaluated as individual reasoners or collective epistemic agents. Hahn's
  work suggests that evaluating LLM epistemic rationality may require different
  standards than those for human individuals, particularly regarding
  consistency and aggregation of diverse information sources.

  POSITION: Develops formal epistemology for collectives, showing that group
  epistemic rationality involves distinct norms and challenges beyond
  individual rationality.
  },
  keywords = {social-epistemology, collective-rationality, Medium}
}

@article{justin2025mix,
  author = {Justin, Martin and \u0160e\u0161elja, Dunja and Stra\u00dfer, Christian and Trpin, Borut},
  title = {The Mix Matters: Exploring the Interplay Between Epistemic and Zetetic Norms in Scientific Disagreement},
  journal = {British Journal for the Philosophy of Science},
  year = {2025},
  volume = {76},
  number = {1},
  pages = {1--25},
  doi = {10.1086/737742},
  note = {
  CORE ARGUMENT: Uses agent-based modeling to show that the rational response
  to disagreement depends on the interaction between epistemic norms (governing
  belief revision) and zetetic norms (governing inquiry strategy). When
  scientists follow exploitative zetetic strategies (doubling down on their
  current best hypothesis), steadfast disagreement is more epistemically
  valuable for the group than conciliation. But when scientists follow
  exploratory strategies, conciliation does not harm collective inquiry.
  Demonstrates that epistemic and zetetic norms cannot be evaluated in
  isolation.

  RELEVANCE: Provides a framework for understanding how LLMs should handle
  disagreement, whether with other models, human feedback, or conflicting
  evidence sources. Suggests that the optimal belief revision strategy depends
  on the LLM's inquiry approach---whether it's designed to exploit its current
  best hypothesis or explore alternatives. This has implications for RLHF,
  debate-based training, and ensemble methods where multiple LLM instances
  interact.

  POSITION: Integrates epistemic and zetetic normativity, showing that
  belief-revision norms and inquiry-strategy norms jointly determine rational
  responses to evidence.
  },
  keywords = {zetetic, disagreement, epistemic-norms, Medium}
}

@article{lee2025pointless,
  author = {Lee, Wooram},
  title = {Pointless epistemic norms},
  journal = {Synthese},
  year = {2025},
  volume = {205},
  number = {1},
  pages = {1--19},
  doi = {10.1007/s11229-025-04940-9},
  note = {
  CORE ARGUMENT: Argues that evidential norms (requirements to believe in
  accordance with available evidence) can be implausibly demanding by requiring
  agents to hold pointless beliefs---beliefs that serve no epistemic or
  practical purpose. Shows that even weak versions of evidential requirements
  (negative norms not to believe against evidence) face the pointlessness
  problem. Proposes that so-called evidential "norms" are better understood as
  evaluative ought-statements about epistemic ideals rather than genuine
  prescriptive demands.

  RELEVANCE: Questions whether LLMs should be required to form beliefs about
  every proposition on which they have evidence, or whether they can
  permissibly remain agnostic about epistemically pointless matters. If Lee is
  correct, evaluating LLMs should not penalize them for failing to have beliefs
  on irrelevant topics, even when evidence is available. This challenges
  evaluation frameworks that test LLM beliefs exhaustively rather than focusing
  on practically relevant domains.

  POSITION: Skeptical about strong evidential requirements, arguing that
  epistemic norms should be limited by considerations of point and practical
  relevance.
  },
  keywords = {evidential-norms, epistemic-ought, Medium}
}

@article{pettigrew2015jamesian,
  author = {Pettigrew, Richard},
  title = {Jamesian Epistemology Formalised: An Explication of 'The Will to Believe'},
  journal = {Episteme},
  year = {2015},
  volume = {12},
  number = {4},
  pages = {421--442},
  doi = {10.1017/epi.2015.44},
  note = {
  CORE ARGUMENT: Formalizes William James's claim that there are two epistemic
  goals---believing truths and avoiding errors---using epistemic utility
  theory. Shows that different weightings of these goals correspond to
  different attitudes toward epistemic risk (conservative vs liberal
  belief-formation policies). Demonstrates that James's insights about the
  ethics of belief can be captured formally and that agents with different
  epistemic values can rationally adopt different belief policies.

  RELEVANCE: Relevant for understanding trade-offs in LLM design between false
  positives (believing falsehoods) and false negatives (missing truths). Should
  LLMs be conservative (high confidence thresholds, avoiding error) or liberal
  (low thresholds, seeking truth)? Pettigrew's framework suggests this is not
  a matter of correctness but of value priorities. Different LLM applications
  (medical diagnosis vs brainstorming) might rationally adopt different
  epistemic risk attitudes.

  POSITION: Shows how epistemic utility theory can capture James's pluralism
  about epistemic values, supporting permissivism about belief policies.
  },
  keywords = {epistemic-utility, James, belief, Low}
}

@article{levinstein2018objection,
  author = {Levinstein, Benjamin A.},
  title = {An objection of varying importance to epistemic utility theory},
  journal = {Philosophical Studies},
  year = {2018},
  volume = {175},
  number = {12},
  pages = {3345--3361},
  doi = {10.1007/s11098-018-1157-9},
  note = {
  CORE ARGUMENT: Raises an objection to epistemic utility theory based on the
  "problem of varying importance": not all propositions have equal epistemic
  importance, yet standard accuracy measures treat all truths as equally
  valuable. Argues that any attempt to incorporate importance-weighting faces
  serious difficulties, including arbitrary threshold problems and conflicts
  with intuitive rationality judgments. This challenges the foundational
  assumption that epistemic value can be reduced to simple accuracy.

  RELEVANCE: Questions whether LLM reasoning should be evaluated using uniform
  accuracy measures or importance-weighted ones. If some topics matter more
  epistemically than others, then LLM performance should be assessed by
  accuracy on important questions rather than overall accuracy. But determining
  which beliefs are important raises difficult questions about epistemic value
  that purely formal approaches cannot answer.

  POSITION: Critical of standard epistemic utility theory, highlighting
  problems that arise from trying to accommodate differential importance of
  propositions.
  },
  keywords = {epistemic-utility, importance, critique, Low}
}

@article{paseau2025countable,
  author = {Paseau, Alexander and Weitkaemper, Felix},
  title = {Is There a Countable Omega-Universal Logic?},
  journal = {Review of Symbolic Logic},
  year = {2025},
  volume = {18},
  number = {4},
  pages = {963--970},
  doi = {10.1017/S1755020325000048},
  note = {
  CORE ARGUMENT: Investigates whether there exists a countable logic capable of
  capturing all validity patterns in natural language arguments---an
  omega-universal logic. Argues that since natural language and standard logics
  are countable, it is a natural question whether a single countable formal
  system can serve as a universal validity detector for natural language
  inference. Provides formal results constraining the possibility of such a
  logic, with implications for the relationship between formal systems and
  natural language consequence.

  RELEVANCE: Addresses the fundamental question of whether formal logic can
  fully capture natural language inference, directly relevant to whether LLM
  reasoning should be evaluated using formal validity standards. If no
  countable logic can capture all natural language validity patterns, then
  expecting LLMs to satisfy formal logical norms may be misguided. This
  supports pluralism about inference standards and suggests that natural
  language reasoning may require evaluation frameworks beyond formal logic.

  POSITION: Explores limits of formal logic's capacity to model natural
  language consequence, with implications for whether formal systems can serve
  as universal standards for reasoning evaluation.
  },
  keywords = {formal-logic, natural-language, validity, Low}
}
