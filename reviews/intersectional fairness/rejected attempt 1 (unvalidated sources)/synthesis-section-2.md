# Section 2: Technical Approaches to Intersectional Fairness

## 2.1 Statistical Challenges with Sparse Intersectional Data

The technical challenge of intersectional fairness begins with a combinatorial problem. When fairness is evaluated across multiple demographic attributes simultaneously, the number of possible intersectional groups grows exponentially. With just three binary attributes (e.g., race, gender, disability status), there are 2³ = 8 possible groups. With five attributes, this grows to 32 groups; with ten, to 1,024. This exponential growth creates fundamental statistical difficulties.

Sheng et al. (2025) propose a unified sparsity-based framework for evaluating algorithmic fairness precisely because "numerous fairness criteria exist, they often lack generalizability across different machine learning problems" when subgroups become small. Their framework demonstrates that sparsity measures can connect various fairness metrics while addressing the reality that "the potentially exponential number of subgroups" means "intersectional groups are often empty with finite samples." This is not merely a practical inconvenience—it represents a fundamental tension between fairness aspirations and statistical feasibility.

The problem manifests concretely in fairness metrics. Celis et al. (2022) analyze conditional demographic parity (CDP), which theoretically addresses intersectional bias by requiring demographic parity conditional on legitimate features. While CDP "theoretically solves the problem of intersectional bias," they note critical limitations: "issues remain at computational and practical levels due to the exponential increase of subgroups when adding sensitive features and the fact that many subgroups will be empty or have very few observations with finite samples." Their analysis reveals that "assessing group fairness with respect to multiple sensitive attributes may be unfeasible in most practical cases." Crucially, they argue that "since the presence of many sensitive features is more of a norm than an exception, this represents a huge problem that the literature on fairness in ML has barely begun to address."

Yurochkin and Sun (2024) reinforce this assessment, noting that CDP is "much harder to achieve than demographic parity, particularly when the conditioning variable has many levels and/or when the model outputs are continuous." The computational and inferential challenges scale with granularity: more intersectional groups mean more parameters to estimate, more comparisons to make, and less data per group for estimation. As Gohar and Cheng (2023) document in their comprehensive survey, "smaller subgroups within intersectional identities suffer from higher data sparsity, leading to increased uncertainty and no representation."

The multicalibration framework, introduced by Hébert-Johnson et al. (2018), illustrates both the promise and the limitations of addressing fairness across many groups. Multicalibration requires that predictions be well-calibrated not just overall but "simultaneously over a rich collection of subpopulations"—potentially an exponentially large collection. This powerful guarantee comes with significant costs. The sample complexity of achieving multicalibration "grows exponentially with the number of class labels," and as the authors note, "the finer grained we want the groups we're calibrated over to be, the more data we need." The algorithm's runtime is "inversely proportional to the size of the smallest group," creating practical barriers when intersectional groups are small.

Recent work underscores these limitations. Dwork et al. (2024) examine when multicalibration post-processing is necessary, concluding that "attaining multicalibration is practically infeasible with more than a few classes." Halevy et al. (2025) stress-test Fair Mixup—a data augmentation technique designed to promote fairness—on classification problems with "up to 81 marginalized groups" and find that Fair Mixup "typically worsens performance and fairness metrics" compared to standard approaches. Their finding that vanilla Mixup combined with multicalibration post-processing proves most effective "especially for small minority groups" highlights an irony: the groups most in need of fairness protections are precisely those for which standard methods struggle most due to statistical sparsity.

Clinical applications demonstrate these challenges concretely. Kong et al. (2024) develop an "intersectional framework for counterfactual fairness in risk prediction" that accounts for "intersecting forms of discrimination" in healthcare contexts. Their complete framework for estimation and inference directly addresses the statistical problem of making valid inferences about intersectional groups when sample sizes vary dramatically. Similarly, Zhang et al. (2024) compare intersectional versus marginal debiasing in emergency admission prediction models, finding that intersectional debiasing produces substantially greater reductions in subgroup calibration error (21.2% vs. 10.6% in one dataset) but requires adequate sample sizes per intersectional subgroup.

## 2.2 Technical Solutions and Their Limitations

Researchers have developed increasingly sophisticated technical approaches to address these statistical challenges, but each reveals—explicitly or implicitly—that statistical solutions presuppose answers to ontological questions about which groups to include.

One promising direction leverages the hierarchical structure of intersectional groups. Maheshwari et al. (2024) propose a "synthetic data generation" approach that treats "groups as intersections of their parent categories." By learning transformation functions that combine data from parent groups (e.g., "Black" and "women"), they can augment data for underrepresented intersectional groups (e.g., "Black women"). Testing across four datasets, they demonstrate that classifiers using this hierarchical augmentation achieve "superior intersectional fairness and are more robust" to leveling down compared to methods optimizing traditional group fairness metrics. However, this solution pushes the ontological question upstream: determining the "parent categories" and which combinations constitute meaningful intersectional groups still requires specifying which groups warrant consideration. The hierarchical structure helps with statistical estimation once groups are defined but does not resolve which hierarchy to use.

Post-processing methods offer another avenue. Davis et al. (2023) develop proportional multicalibration (PMC), requiring "the proportion of calibration error within each bin and group to be small." Their efficient post-processing algorithm demonstrates that PMC is a "promising criteria for controlling simultaneous calibration fairness over intersectional groups with virtually no classification performance cost." Kim et al. (2019) similarly propose a multiaccuracy framework for "black-box post-processing" that ensures "accurate predictions across identifiable subgroups" with only "black-box access to a predictor and a relatively small labeled dataset for auditing." These post-processing approaches are valuable because they do not require retraining models. Yet they still presuppose that the set of "identifiable subgroups" is known—the methods calibrate for specified groups without addressing which groups should be specified.

Approaches for overlapping groups represent another technical advance. Yang et al. (2020) address fairness when "groups overlap"—relevant to intersectional contexts where individuals belong to multiple groups simultaneously. Their work provides important theoretical and algorithmic contributions for handling overlapping group memberships in fairness constraints. However, the overlap they address is computational (handling individuals in multiple groups) rather than ontological (determining which overlapping combinations constitute groups warranting fairness consideration).

Extensions beyond classification show both the breadth of technical innovation and the persistence of the underlying challenge. Hashimoto et al. (2025) develop methods for "intersectional fairness in reinforcement learning with large state and constraint spaces," solving "multi-objective RL problems with a possibly exponentially large class of constraints over intersecting groups." Their oracle-efficient approach handles exponentially many constraints—a significant computational achievement. Yet the "class of constraints" and "intersecting groups" must still be specified; the method efficiently optimizes over a given constraint class rather than determining which constraints matter. Similarly, Yan et al. (2024) develop intersectional fair ranking using subgroup divergence to "automatically identify which subgroups, defined as combinations of known protected attributes, show statistically significant deviation." While their statistical test helps identify problematic subgroups post-hoc, it requires pre-specifying "known protected attributes" from which combinations are formed.

Foulds et al. (2020) make the fundamental problem explicit: ensuring "fairness on individual attributes does not guarantee intersectional fairness" when considering multiple attributes concurrently. Their intersectional fairness definition recognizes that fairness at individual marginals (race alone, gender alone) can mask unfairness at intersections (race × gender). Yet translating this insight into practice requires deciding which intersections to evaluate—a question their definition does not resolve.

Recent work on multimodal fairness reveals additional complexity. Dutta et al. (2024) examine "intersectional biases in multimodal clinical predictions," finding that "the fairness challenge becomes even more pronounced in multimodal settings as fairness values fluctuate with different modality information." When fairness depends on which information modalities are included, the question of which intersectional groups to assess fairness for becomes entangled with measurement and modeling choices—a point we return to in Section 4.

Mangal et al. (2024) propose practical implementations for education, including "modification to the ABROCA metric enhancing its ability to measure disparities among multiple subgroups including intersectional subgroups" and "adversarial learning tailored for intersectionality." These practical tools are valuable, but selecting which "multiple subgroups" to measure and which adversarial constraints to impose returns us to the ontological question.

**Flag for the Dilemma**: The technical literature provides sophisticated methods for addressing statistical challenges of intersectional fairness—hierarchical structures, post-processing, efficient optimization over many groups, automated detection of problematic subgroups. Yet every method presupposes that the set of groups G is given or can be derived from a specified set of attributes. None engage with the ontological question: which groups should G include? The statistical solutions are conditional on having already resolved the ontological problem. Moreover, the statistical findings—that more groups means less statistical reliability—create pressure to constrain G. But on what basis should certain intersectional groups be excluded? Technical work does not (and cannot alone) answer this question. What the literature documents thoroughly is one horn of the dilemma (statistical challenges with many groups) without recognizing its interaction with the other horn (ontological uncertainty about which groups to include).
