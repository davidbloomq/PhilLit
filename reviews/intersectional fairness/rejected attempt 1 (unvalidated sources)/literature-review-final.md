# Literature Review: The Intersectionality Dilemma for Algorithmic Fairness

## Abstract

This literature review synthesizes 95 papers across six domains to position a novel framing of intersectional algorithmic fairness. We show that existing scholarship thoroughly documents two distinct challenges: (1) the statistical problem of estimating model performance reliably across intersectional groups when sample sizes are small (machine learning literature), and (2) the ontological problem of determining which intersectional groups exist and warrant consideration given deep philosophical disagreements (philosophy literature). However, no existing work recognizes these as interacting problems that together constitute a genuine dilemma. Ontological considerations suggesting many intersectional groups (emergence, non-additivity, context-dependence) exacerbate statistical challenges (exponential growth, data sparsity). Statistical constraints pushing toward fewer groups require resolving contested ontological questions about which groups to exclude. Each horn worsens the other. This interaction has remained a systematic blind spot across machine learning, philosophy, measurement theory, normative ethics, and epistemology. We establish this gap through comprehensive analysis showing that while specific papers approach elements of the dilemma (Jacobs and Wallach 2021 on fairness as contested construct; Celis et al. 2022 on statistical infeasibility; Jorba and López de Sa 2024 on emergence; Hüllermeier and Waegeman 2021 on epistemic uncertainty), none frame the statistical-ontological interaction as a dilemma where solving one problem exacerbates the other. This framing is both diagnostic (explaining why incremental progress feels insufficient) and generative (suggesting fundamentally new interdisciplinary approaches). The review emphasizes 2020-2025 machine learning literature while engaging foundational philosophical work, targeting interdisciplinary audiences at ACM FAccT, Synthese, and Philosophy & Technology.

---



# Section 1: Introduction and the Challenge of Intersectional Fairness

The promise of algorithmic fairness research has been to ensure that machine learning systems make equitable decisions across socially salient demographic groups. Yet as fairness scholarship has matured, a fundamental challenge has emerged: how to address fairness for *intersectional* groups—those defined by combinations of attributes such as race, gender, age, and disability. This challenge is not merely technical. It sits at the intersection of machine learning, philosophy, measurement theory, normative ethics, and epistemology, demanding interdisciplinary analysis.

Recent surveys document the scope of intersectional fairness research in machine learning. Gohar and Cheng (2023) provide a comprehensive taxonomy of intersectional fairness notions, mitigation strategies, and open challenges, noting that "intersectional bias encompasses multiple sensitive attributes, such as race and gender, together" rather than considering them in isolation. Their survey identifies data sparsity in smaller intersectional subgroups as a major technical obstacle. Similarly, high-profile cases of algorithmic discrimination—such as facial recognition systems exhibiting significantly higher error rates for Black women compared to white men (Buolamwini and Gebru 2018)—have demonstrated that ensuring fairness on individual attributes (race alone, or gender alone) does not guarantee fairness at their intersections.

These empirical findings resonate with theoretical developments in philosophy. The concept of intersectionality, originating in Black feminist scholarship and legal theory (Crenshaw 1989), holds that experiences of individuals with overlapping marginalized identities cannot be understood as the simple sum of separate identity categories. Philosophers have worked to formalize this insight: Bright, Malinsky, and Thompson (2016) show that intersectionality can be interpreted through graphical causal models, allowing claims about the causal effects of occupying intersecting identity categories to be empirically tested. More recently, Jorba and López de Sa (2024) propose understanding intersectionality through the metaphysical framework of emergence, arguing that "intersectional experiences emerge from the conjunction of social categories when social structures make them relevant vis-à-vis discrimination and privilege."

The machine learning community has responded to the intersectional fairness challenge with increasingly sophisticated technical approaches. Methods range from multicalibration frameworks ensuring calibrated predictions across exponentially many subgroups (Hébert-Johnson et al. 2018), to hierarchical approaches that leverage the structure of intersectional groups to address data sparsity (Maheshwari et al. 2024), to post-processing algorithms for achieving fairness without retraining models (Kim et al. 2019). Parallel developments address conditional demographic parity (Yurochkin and Sun 2024), fairness in reinforcement learning (Hashimoto et al. 2025), and intersectional fairness in ranking systems (Yan et al. 2024).

Yet despite this rich and growing literature, a critical gap remains. The technical literature primarily addresses a *statistical* problem: how to estimate model performance reliably across intersectional groups when sample sizes become small as the number of groups grows exponentially. As Celis et al. (2022) observe, "assessing group fairness with respect to multiple sensitive attributes may be unfeasible in most practical cases" due to the exponential growth of subgroups and resulting data sparsity. Meanwhile, philosophical work addresses an *ontological* problem: which groups warrant consideration in the first place, given deep disagreements about the nature of social groups, the definition of intersectionality itself, and whether demographic categories should be understood as fixed attribute combinations or as emergent, practice-based phenomena (Haslanger 2012; Sveinsdóttir 2013; Epstein 2019).

What existing scholarship has not recognized is that these are not merely two separate challenges requiring independent solutions. They are *interacting* problems that together constitute a genuine dilemma: expanding the set of groups to be inclusive and responsive to ontological considerations exacerbates statistical unreliability; constraining the set to achieve statistical tractability requires resolving deeply contested ontological questions about which groups truly matter. Each horn of the dilemma makes the other worse.

This literature review synthesizes scholarship across six domains—algorithmic fairness and intersectionality (machine learning), philosophy of intersectionality, social ontology, measurement theory and construct validity, normative frameworks for fairness, and applied epistemology—to establish this gap. We show that while individual literatures provide sophisticated analyses of one or the other horn of the dilemma, no existing work frames intersectional fairness as an interaction between statistical uncertainty and ontological uncertainty that creates a genuine dilemma for algorithmic fairness. This framing—as we will demonstrate—is novel and consequential, suggesting that incremental refinements of existing technical or philosophical approaches cannot resolve the fundamental challenge.

The review proceeds as follows. Section 2 examines technical approaches in machine learning, documenting both the statistical challenges of sparse intersectional data and proposed solutions while noting that these solutions typically presume the set of groups is given. Section 3 explores philosophical foundations, analyzing competing interpretations of intersectionality and social ontology debates about group constitution—debates that create ontological uncertainty about which groups should be addressed. Section 4 examines measurement theory and construct validity, showing how operationalization choices embed ontological commitments. Section 5 reviews normative frameworks for navigating fairness trade-offs, noting that normative principles often presuppose knowledge of which groups matter. Section 6 explores epistemic dimensions of fairness, including the distinction between epistemic and aleatoric uncertainty and questions of epistemic authority over group categories. Section 7 synthesizes these findings to establish the dilemma gap: the absence of any existing work that frames these problems as interacting to create a dilemma. Section 8 concludes by positioning the novel contribution of framing intersectional fairness as a genuine dilemma requiring fundamentally new interdisciplinary approaches.

**Flag for the Dilemma**: Existing literature—both in machine learning and philosophy—treats statistical and ontological problems separately. Technical work assumes groups are specified and focuses on statistical challenges. Philosophical work engages ontological debates without acknowledging statistical constraints those debates create. No existing work recognizes these as mutually exacerbating problems forming a dilemma where "solving" one horn worsens the other.


# Section 2: Technical Approaches to Intersectional Fairness

## 2.1 Statistical Challenges with Sparse Intersectional Data

The technical challenge of intersectional fairness begins with a combinatorial problem. When fairness is evaluated across multiple demographic attributes simultaneously, the number of possible intersectional groups grows exponentially. With just three binary attributes (e.g., race, gender, disability status), there are 2³ = 8 possible groups. With five attributes, this grows to 32 groups; with ten, to 1,024. This exponential growth creates fundamental statistical difficulties.

Sheng et al. (2025) propose a unified sparsity-based framework for evaluating algorithmic fairness precisely because "numerous fairness criteria exist, they often lack generalizability across different machine learning problems" when subgroups become small. Their framework demonstrates that sparsity measures can connect various fairness metrics while addressing the reality that "the potentially exponential number of subgroups" means "intersectional groups are often empty with finite samples." This is not merely a practical inconvenience—it represents a fundamental tension between fairness aspirations and statistical feasibility.

The problem manifests concretely in fairness metrics. Celis et al. (2022) analyze conditional demographic parity (CDP), which theoretically addresses intersectional bias by requiring demographic parity conditional on legitimate features. While CDP "theoretically solves the problem of intersectional bias," they note critical limitations: "issues remain at computational and practical levels due to the exponential increase of subgroups when adding sensitive features and the fact that many subgroups will be empty or have very few observations with finite samples." Their analysis reveals that "assessing group fairness with respect to multiple sensitive attributes may be unfeasible in most practical cases." Crucially, they argue that "since the presence of many sensitive features is more of a norm than an exception, this represents a huge problem that the literature on fairness in ML has barely begun to address."

Yurochkin and Sun (2024) reinforce this assessment, noting that CDP is "much harder to achieve than demographic parity, particularly when the conditioning variable has many levels and/or when the model outputs are continuous." The computational and inferential challenges scale with granularity: more intersectional groups mean more parameters to estimate, more comparisons to make, and less data per group for estimation. As Gohar and Cheng (2023) document in their comprehensive survey, "smaller subgroups within intersectional identities suffer from higher data sparsity, leading to increased uncertainty and no representation."

The multicalibration framework, introduced by Hébert-Johnson et al. (2018), illustrates both the promise and the limitations of addressing fairness across many groups. Multicalibration requires that predictions be well-calibrated not just overall but "simultaneously over a rich collection of subpopulations"—potentially an exponentially large collection. This powerful guarantee comes with significant costs. The sample complexity of achieving multicalibration "grows exponentially with the number of class labels," and as the authors note, "the finer grained we want the groups we're calibrated over to be, the more data we need." The algorithm's runtime is "inversely proportional to the size of the smallest group," creating practical barriers when intersectional groups are small.

Recent work underscores these limitations. Dwork et al. (2024) examine when multicalibration post-processing is necessary, concluding that "attaining multicalibration is practically infeasible with more than a few classes." Halevy et al. (2025) stress-test Fair Mixup—a data augmentation technique designed to promote fairness—on classification problems with "up to 81 marginalized groups" and find that Fair Mixup "typically worsens performance and fairness metrics" compared to standard approaches. Their finding that vanilla Mixup combined with multicalibration post-processing proves most effective "especially for small minority groups" highlights an irony: the groups most in need of fairness protections are precisely those for which standard methods struggle most due to statistical sparsity.

Clinical applications demonstrate these challenges concretely. Kong et al. (2024) develop an "intersectional framework for counterfactual fairness in risk prediction" that accounts for "intersecting forms of discrimination" in healthcare contexts. Their complete framework for estimation and inference directly addresses the statistical problem of making valid inferences about intersectional groups when sample sizes vary dramatically. Similarly, Zhang et al. (2024) compare intersectional versus marginal debiasing in emergency admission prediction models, finding that intersectional debiasing produces substantially greater reductions in subgroup calibration error (21.2% vs. 10.6% in one dataset) but requires adequate sample sizes per intersectional subgroup.

## 2.2 Technical Solutions and Their Limitations

Researchers have developed increasingly sophisticated technical approaches to address these statistical challenges, but each reveals—explicitly or implicitly—that statistical solutions presuppose answers to ontological questions about which groups to include.

One promising direction leverages the hierarchical structure of intersectional groups. Maheshwari et al. (2024) propose a "synthetic data generation" approach that treats "groups as intersections of their parent categories." By learning transformation functions that combine data from parent groups (e.g., "Black" and "women"), they can augment data for underrepresented intersectional groups (e.g., "Black women"). Testing across four datasets, they demonstrate that classifiers using this hierarchical augmentation achieve "superior intersectional fairness and are more robust" to leveling down compared to methods optimizing traditional group fairness metrics. However, this solution pushes the ontological question upstream: determining the "parent categories" and which combinations constitute meaningful intersectional groups still requires specifying which groups warrant consideration. The hierarchical structure helps with statistical estimation once groups are defined but does not resolve which hierarchy to use.

Post-processing methods offer another avenue. Davis et al. (2023) develop proportional multicalibration (PMC), requiring "the proportion of calibration error within each bin and group to be small." Their efficient post-processing algorithm demonstrates that PMC is a "promising criteria for controlling simultaneous calibration fairness over intersectional groups with virtually no classification performance cost." Kim et al. (2019) similarly propose a multiaccuracy framework for "black-box post-processing" that ensures "accurate predictions across identifiable subgroups" with only "black-box access to a predictor and a relatively small labeled dataset for auditing." These post-processing approaches are valuable because they do not require retraining models. Yet they still presuppose that the set of "identifiable subgroups" is known—the methods calibrate for specified groups without addressing which groups should be specified.

Approaches for overlapping groups represent another technical advance. Yang et al. (2020) address fairness when "groups overlap"—relevant to intersectional contexts where individuals belong to multiple groups simultaneously. Their work provides important theoretical and algorithmic contributions for handling overlapping group memberships in fairness constraints. However, the overlap they address is computational (handling individuals in multiple groups) rather than ontological (determining which overlapping combinations constitute groups warranting fairness consideration).

Extensions beyond classification show both the breadth of technical innovation and the persistence of the underlying challenge. Hashimoto et al. (2025) develop methods for "intersectional fairness in reinforcement learning with large state and constraint spaces," solving "multi-objective RL problems with a possibly exponentially large class of constraints over intersecting groups." Their oracle-efficient approach handles exponentially many constraints—a significant computational achievement. Yet the "class of constraints" and "intersecting groups" must still be specified; the method efficiently optimizes over a given constraint class rather than determining which constraints matter. Similarly, Yan et al. (2024) develop intersectional fair ranking using subgroup divergence to "automatically identify which subgroups, defined as combinations of known protected attributes, show statistically significant deviation." While their statistical test helps identify problematic subgroups post-hoc, it requires pre-specifying "known protected attributes" from which combinations are formed.

Foulds et al. (2020) make the fundamental problem explicit: ensuring "fairness on individual attributes does not guarantee intersectional fairness" when considering multiple attributes concurrently. Their intersectional fairness definition recognizes that fairness at individual marginals (race alone, gender alone) can mask unfairness at intersections (race × gender). Yet translating this insight into practice requires deciding which intersections to evaluate—a question their definition does not resolve.

Recent work on multimodal fairness reveals additional complexity. Dutta et al. (2024) examine "intersectional biases in multimodal clinical predictions," finding that "the fairness challenge becomes even more pronounced in multimodal settings as fairness values fluctuate with different modality information." When fairness depends on which information modalities are included, the question of which intersectional groups to assess fairness for becomes entangled with measurement and modeling choices—a point we return to in Section 4.

Mangal et al. (2024) propose practical implementations for education, including "modification to the ABROCA metric enhancing its ability to measure disparities among multiple subgroups including intersectional subgroups" and "adversarial learning tailored for intersectionality." These practical tools are valuable, but selecting which "multiple subgroups" to measure and which adversarial constraints to impose returns us to the ontological question.

**Flag for the Dilemma**: The technical literature provides sophisticated methods for addressing statistical challenges of intersectional fairness—hierarchical structures, post-processing, efficient optimization over many groups, automated detection of problematic subgroups. Yet every method presupposes that the set of groups G is given or can be derived from a specified set of attributes. None engage with the ontological question: which groups should G include? The statistical solutions are conditional on having already resolved the ontological problem. Moreover, the statistical findings—that more groups means less statistical reliability—create pressure to constrain G. But on what basis should certain intersectional groups be excluded? Technical work does not (and cannot alone) answer this question. What the literature documents thoroughly is one horn of the dilemma (statistical challenges with many groups) without recognizing its interaction with the other horn (ontological uncertainty about which groups to include).


# Section 3: Philosophical Foundations: What is Intersectionality?

## 3.1 Competing Interpretations of Intersectionality

If technical approaches to intersectional fairness presuppose knowing which groups matter, philosophy might be expected to provide guidance. Instead, philosophical analysis reveals deep disagreements about what intersectionality is, how intersectional groups should be understood, and even whether the concept has determinate boundaries—creating precisely the ontological uncertainty that makes specifying G problematic.

Ruíz (2017) provides a philosophical overview situating intersectionality as a term that "arose within the black feminist intellectual tradition for the purposes of identifying interlocking systems of oppression." As a descriptive term, it "refers to the ways human identity is shaped by multiple social vectors and overlapping identity categories (such as sex, race, class) that may not be readily visible in single-axis formulations of identity." This description, however, leaves open fundamental questions about the metaphysics and epistemology of these "overlapping categories."

May (2014) addresses these questions directly in an influential analysis titled "Intersectionality's Definitional Dilemmas." Her examination of definitional challenges in intersectionality scholarship reveals "tensions between different interpretations and uses of intersectionality." These are not mere semantic disputes—they reflect substantive disagreements about what intersectionality requires both theoretically and practically. The existence of definitional dilemmas at the conceptual level creates immediate problems for operationalization: if scholars disagree about what intersectionality is, how can algorithmic systems determine which intersectional groups warrant fairness consideration?

One influential interpretation emphasizes intersectionality's causal structure. Bright, Malinsky, and Thompson (2016) demonstrate that "some characteristic claims of the intersectionality literature can be interpreted causally" through graphical causal modeling. This formalism "allows claims about the causal effects of occupying intersecting identity categories to be clearly represented and submitted to empirical testing." Their approach offers analytical rigor: if intersectionality makes causal claims, these can be formalized and tested. However, causal modeling requires specifying which variables (identity categories) and which causal pathways to model—decisions that themselves presuppose ontological commitments about which categories and interactions matter.

A competing metaphysical interpretation treats intersectionality through emergence. Jorba and López de Sa (2024) propose "conceiving intersectionality as a general metaphysical framework" where "intersectional experiences emerge from the conjunction of social categories when social structures make them relevant vis-à-vis discrimination and privilege." Their emergence view has three appealing features: "metaphysical neutrality, explanatory flexibility, and methodological openness." Emergence helps explain why intersectional experiences cannot be reduced to additive combinations of separate identity categories—Black women's experiences are not simply Black experiences plus women's experiences, but something emergent from their conjunction in particular social structures.

Yet emergence creates challenges for specification. If intersectional properties emerge based on "social structures," which structures are relevant? In what contexts do particular conjunctions generate emergence? The metaphysical flexibility that makes emergence appealing philosophically creates indeterminacy for practical implementation: there is no algorithm for determining which conjunctions will have emergent properties in which contexts. As Jorba and López de Sa note, their view "helps dissociate intersectionality from specific social constructionist views"—but this very dissociation means emergence does not tell us which groups exist or matter.

Hancock (2007) emphasizes the non-additive nature of intersectionality, distinguishing "intersectionality from additive models of identity and oppression" in her analysis "When Multiplication Doesn't Equal Quick Addition." She examines "intersectionality as a research paradigm with specific methodological commitments" that reject the idea that intersectional identities can be understood by summing separate identity dimensions. This rejection is theoretically important—it explains why fairness approaches that ensure parity on race alone and gender alone can still fail at their intersection. Yet non-additivity compounds the specification problem: if intersectional groups have properties not derivable from their constituent attributes, how do we know which combinations will have such properties without examining each intersection empirically?

Garry (2011) identifies "three characteristic uses of intersectionality: as a metaphor, as a heuristic, and as a paradigm." Each use suggests different answers to what intersectionality is and what it requires. As metaphor, intersectionality helps us see identity as multi-dimensional. As heuristic, it guides analysis to consider multiple identity dimensions simultaneously. As paradigm, it constitutes a comprehensive framework for understanding oppression. These different uses—while not necessarily incompatible—have different implications for determining which groups matter. A metaphorical understanding might be satisfied with considering several major intersections; a heuristic approach might suggest examining additional intersections when initial analysis seems incomplete; a paradigmatic commitment might require systematic consideration of all possible intersections. The multiplicity of uses creates indeterminacy about requirements.

McCall (2005) proposes an influential taxonomy distinguishing "anticategorical, intracategorical, and intercategorical approaches to intersectionality." Anticategorical approaches are skeptical of categories themselves, seeing them as simplifications that obscure complexity. Intracategorical approaches focus on particular intersections to reveal complexity within categories. Intercategorical approaches provisionally accept categories to study relationships among social groups. Each stance implies different answers to which groups should be included in fairness analysis. An anticategorical perspective might resist fixing any definite set of groups; an intracategorical approach might focus on specific intersections identified through qualitative research; an intercategorical approach might work with available demographic categories while acknowledging their limitations. The choice among these approaches is not purely empirical—it involves normative and methodological commitments.

Cho, Crenshaw, and McCall (2013) distinguish "structural, political, and representational dimensions of intersectionality" in a programmatic statement on intersectionality studies. These dimensions address different aspects of how intersecting systems of power operate. Yet the distinction also reveals complexity: should algorithmic fairness address all three dimensions? If so, how? If not, which dimensions have priority? The richness of intersectional analysis in critical theory creates multiple possible interpretations of what intersectional fairness requires in algorithmic contexts.

Bowleg (2008) examines "methodological challenges of qualitative and quantitative intersectionality research," arguing that additive approaches (Black + Lesbian + Woman) "fail to capture intersectional experiences (Black Lesbian Woman)." Her analysis is particularly relevant to algorithmic fairness: if quantitative approaches struggle to capture intersectionality, what hope do algorithmic systems have? Her conclusion—that standard quantitative methods may be inadequate for intersectional analysis—suggests that determining which groups matter may require qualitative understanding that resists algorithmic specification.

## 3.2 Social Ontology of Groups

Debates in social ontology deepen the problem by questioning the very nature of social groups. These debates are not academic hairsplitting—they have direct implications for whether demographic groups can be exhaustively specified through attribute combinations or whether group specification is inherently context-dependent and practice-based.

Haslanger (2012) provides a foundational framework distinguishing "causal (social) construction" from "constitutive (social) construction." Causal construction holds that "social factors are involved in causing a kind to be"; constitutive construction holds that "social factors are 'constitutive' or 'ontological.'" She further distinguishes "construction of representations (ideas, concepts, predicates) from construction of objects (people, categories, events, properties)." These distinctions matter for intersectional fairness: if demographic categories are constitutively constructed rather than merely causally constructed, their existence and properties depend on ongoing social practices rather than being fixed facts about individuals. This makes specifying G a moving target.

Sveinsdóttir (2013, 2015) offers an alternative characterization: "something is socially constructed if it has social significance in a context such that items taken to have the relevant feature get conferred extra social constraints and enablements." On this view, what makes something a social group is not a fixed essence but "social significance in a context." The context-dependence is crucial: a combination of attributes might constitute a meaningful group in one context but not another. For algorithmic fairness, this implies that which intersectional groups matter may vary by domain, geographic context, historical period, and institutional setting—undermining the idea of a universal, stable set G.

Epstein (2019) distinguishes between "type 1 groups" (committees, organizations) that "require volitional membership and structure" and "type 2 groups" (racial, ethnic groups) that lack these characteristics. He argues that "common possession of a racial or ethnic feature constitutes a minimal structure, though attributive rather than relational." This distinction suggests that demographic categories relevant to fairness are type 2 groups united by shared attributes. However, Epstein notes that even attributive structure raises questions: which attributes constitute groups? When do individuals with shared attributes form a group rather than merely a collection? His analysis suggests attribute-based specification may be necessary but not sufficient for group individuation.

Mallon (2007) provides a "field guide to social construction," mapping "different senses of 'social construction' and their philosophical implications." The multiplicity of senses he identifies—causal dependence on social factors, lack of natural kind status, contingency, revisability—reveals that "social construction" is not a univocal thesis but a family of related claims. For algorithmic fairness, this means that appeals to groups being "socially constructed" underdetermine which groups exist and matter. Different senses of construction suggest different answers to group specification questions.

Thomasson (2019) analyzes "what social groups are metaphysically," examining "competing accounts and their implications." She asks whether "groups are sets of individuals, structured entities, or something else." If groups are sets—collections of individuals sharing attributes—then perhaps groups can be derived algorithmically from attribute combinations. If groups are structured entities with properties beyond their members, then group specification may require understanding social structures not reducible to individual attributes. If groups are "something else" (e.g., practices, institutions, ways of being categorized), then attribute-based specification may be fundamentally inadequate.

Sterba (2024) provides recent analysis showing that "group membership and classification are not static but constantly change based on shifts in social identity and context." This dynamic view challenges any attempt to fix a stable set of groups for fairness evaluation. If group boundaries shift with context and time, then specifying G for an algorithmic system requires either (1) continuous updating based on changing social realities or (2) accepting that G will be somewhat arbitrary and potentially obsolete.

Haslanger and Ásta (2017) co-author an overview of feminist metaphysics discussing "how social practices, attitudes, biological regularities, or something else set up or anchor gender categories to be what they are." Their analysis reveals that even for a single demographic dimension (gender), philosophers disagree about what constitutes the category. These disagreements multiply for intersectional categories: if there is no consensus on what makes gender or race what they are, there can be no consensus on what makes "Black women" or "disabled Latina teenagers" the groups they are.

Ritchie (2013) examines the "relationship between social structure and ontology of groups," analyzing "how social structures constitute or constrain groups." If groups are constituted by structures, then understanding which groups exist requires understanding relevant social structures. But social structures are themselves contested objects of analysis, studied across sociology, economics, political science, and critical theory with no unified framework. Algorithmic systems cannot wait for philosophy and social theory to converge on unified accounts—yet proceeding without such accounts means making ontological commitments by default, often implicitly.

**Flag for the Dilemma**: Philosophical analysis reveals ontological uncertainty about intersectional groups at multiple levels. Intersectionality itself is definitionally contested (May 2014). Metaphysical accounts disagree about whether intersectional properties are causal, emergent, or something else (Bright et al. 2016; Jorba and López de Sa 2024). Social ontology offers competing accounts of what constitutes groups—attribute-based, practice-based, structurally constituted (Haslanger 2012; Sveinsdóttir 2013; Epstein 2019; Thomasson 2019). Group boundaries are context-dependent and temporally dynamic (Sterba 2024; Sveinsdóttir 2015). This philosophical landscape creates genuine ontological uncertainty: there is no settled philosophical answer to "which intersectional groups exist and warrant consideration?" Yet notice what this literature does not discuss: the statistical consequences of these ontological choices. Philosophers debate the metaphysics of groups without considering that including more groups in fairness analysis creates statistical challenges. The ontological debates proceed independently of the statistical constraints documented in Section 2. This independence—this failure to recognize the interaction—is central to the gap this review identifies.


# Section 4: Measurement and Operationalization

The transition from ontological debates to algorithmic implementation requires measurement—operationalizing theoretical constructs through observable variables. Measurement theory reveals that this transition is not neutral: every operationalization embeds ontological commitments, and failures of construct validity can undermine both fairness and accuracy.

Jacobs and Wallach (2021) provide the foundational analysis in their influential paper "Measurement and Fairness." They propose "measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems." Their central insight is that "computational systems often involve unobservable theoretical constructs such as socioeconomic status, teacher effectiveness, and risk of recidivism, which cannot be measured directly and must instead be inferred from measurements of observable properties through operationalization via a measurement model." This operationalization "necessarily involves making assumptions and introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization."

Critically for intersectional fairness, Jacobs and Wallach argue that "many of the harms discussed in fairness literature are direct results of such mismatches." When systems claim to measure a construct but actually measure something else, downstream fairness interventions may address the wrong problem. Their "fairness-oriented conceptualizations of construct reliability and construct validity" provide tools for "making explicit and testing assumptions about constructs and their operationalizations."

Most relevantly for the dilemma at hand, Jacobs and Wallach argue that "fairness itself is an essentially contested construct that has different theoretical understandings in different contexts." They contend that "debates that appear to be about different operationalizations are actually debates about different theoretical understandings of fairness." This analysis comes remarkably close to recognizing the dilemma—they identify that what appears to be a technical question (which fairness metric?) is actually a theoretical question (what is fairness?). However, they do not extend this insight to the question of groups: debates about which groups to include in fairness analysis are not merely technical (which attributes to combine?) but theoretical (what makes something a group warranting fairness consideration?). And crucially, they do not connect the theoretical contestedness to the statistical problem: the fact that different theoretical understandings suggest including different groups, which has direct statistical consequences for feasibility.

Scheuerman et al. (2021) examine "how demographic categories are operationalized in facial analysis datasets," finding "wide variation in how race and gender are defined, measured, and annotated." Their systematic analysis shows that "measurement choices reflect different (often implicit) ontological commitments about what these categories are." When one dataset operationalizes gender through self-identification, another through binary appearance-based coding, and a third through medical records of sex assigned at birth, each embeds different assumptions about what gender is. These choices directly affect which intersectional groups can be analyzed: a dataset with binary gender and four racial categories enables analysis of 8 intersectional groups; one with non-binary gender and six racial categories enables analysis of different groups entirely; one that records gender identity, gender presentation, and sex assigned at birth separately enables exponentially more intersectional combinations.

The problem extends beyond demographic categories. Obermeyer et al. (2019) document a landmark case of construct validity failure in a healthcare algorithm used to manage the health of populations. The algorithm used "healthcare costs as a proxy for health needs," but costs "systematically differ by race even for the same health needs" due to differential access to care. This created racial bias not because the algorithm explicitly discriminated but because the operationalization of the theoretical construct (health needs) through the measurement (healthcare costs) introduced systematic error. Intersectional groups may face multiple sources of such measurement bias, compounding the problem.

Selbst et al. (2019) identify "abstraction traps" where "technical fairness interventions fail due to mismatches between abstract formal models and concrete social contexts." Their framework is relevant to measurement: fairness metrics are abstract mathematical objects; demographic categories are socially complex, context-dependent phenomena. The abstraction required for measurement necessarily simplifies this complexity. As they show, "fairness metrics may be valid in abstract but invalid when applied to real contexts with all their social complexity." For intersectional groups, multiple layers of abstraction (operationalizing race, operationalizing gender, operationalizing their intersection) create multiple opportunities for validity failure.

Blodgett et al. (2020) provide a critical analysis of how "bias" is operationalized in NLP research, finding that "many operationalizations have questionable construct validity—unclear what they actually measure." Their analysis extends to fairness research more broadly: researchers often adopt existing operationalizations without examining whether they validly measure the theoretical constructs of interest. For intersectionality, this means that operationalizing intersectional groups through attribute combinations may fail to capture what intersectionality theoretically signifies (non-additive, emergent, context-dependent effects).

Hu et al. (2023) show that "the validity of fairness auditing hinges on the reliability of the demographic attribute inference process," and "improved reliability leads to less biased and lower-variance estimates of fairness." Their focus on "reliable demographic inference for fairness in face analysis" highlights that measurement error in demographic categories—before any fairness analysis begins—can undermine fairness evaluation. For intersectional groups, measurement error propagates: if gender is misclassified at rate ε₁ and race at rate ε₂, intersectional categories (Black women) will have compounded error. This measurement uncertainty interacts with the statistical uncertainty from small samples discussed in Section 2.

Mayson (2019) analyzes risk assessment instruments used in criminal justice, arguing that "many instruments have poor construct validity—they measure correlates of outcomes rather than theoretically meaningful constructs." Moreover, she shows that "label bias compromises validity of both accuracy and fairness metrics." When the outcome being predicted (e.g., arrest) is itself a biased measure of the theoretical construct of interest (e.g., criminal behavior), then optimizing for accuracy or fairness with respect to that outcome may perpetuate rather than correct injustice. For intersectional groups experiencing multiple forms of bias in outcome measurement, this problem is magnified.

Barocas et al. (2020) examine "hidden assumptions in fairness methods," including assumptions about measurement. They show that "counterfactual fairness approaches assume the ability to intervene on demographic attributes," but "such interventions may be conceptually incoherent if attributes are constitutively socially constructed." This connects measurement to the social ontology debates in Section 3: if demographic categories are practice-based rather than attribute-based, then intervening on attributes (as counterfactual fairness requires) may not capture interventions on socially constructed group membership. The measurement framework presupposes an ontology—but which ontology?

Geiger et al. (2020) document a fundamental transparency problem: machine learning papers often fail to report "where human-labeled training data comes from" and how categories were defined and operationalized. Their finding that "without clear construct definitions and validity assessment, unclear what categories actually capture" is particularly troubling for intersectional fairness. If training data category definitions are opaque, then claims about fairness for intersectional groups rest on unstable foundations.

Hellman (2020) offers a legal scholar's perspective on "measuring algorithmic fairness," arguing that "many metrics fail to capture normatively significant aspects of fairness." Her analysis suggests a measurement validity problem: "metrics may reliably measure something, but not fairness as normatively understood." For intersectional groups, this creates a dual challenge. First, measuring which groups exist requires valid operationalization of group membership. Second, measuring fairness for those groups requires metrics that capture normatively significant inequalities. Both requirements involve theoretical and normative judgment that cannot be fully captured in measurement protocols.

Green (2020) argues that "risk assessment instruments have fundamental epistemic limitations" and "cannot reliably measure constructs they purport to measure." He contends that "error metrics are poor proxies for individual equity or social well-being." This echoes a theme across the measurement literature: what we can measure is often a poor proxy for what we care about. For intersectional fairness, what we can measure (performance differences across groups defined by observable attributes) may poorly proxy for what we care about (substantive justice for intersectional communities as they understand themselves).

**Flag for the Dilemma**: Measurement theory reveals that operationalizing groups for fairness analysis embeds ontological commitments (Scheuerman et al. 2021; Jacobs and Wallach 2021). Different operationalizations of demographic categories enable analysis of different intersectional groups. Construct validity requires that operationalizations align with theoretical understandings of constructs—but as Section 3 showed, there are competing theoretical understandings of intersectional groups. Jacobs and Wallach (2021) recognize that fairness is an essentially contested construct with different theoretical understandings in different contexts, and that apparent debates about operationalizations are often deeper debates about theoretical understandings. However, they do not apply this insight to the group specification problem: which groups to include in fairness analysis is also a contested question with different theoretical answers. More critically, the measurement literature does not acknowledge the statistical consequences of measurement choices: including more granular group categories (to capture more intersectional diversity) exacerbates the statistical problem of sparse data per group. Measurement theorists analyze construct validity independently of statistical feasibility, while statisticians analyze inference feasibility independently of measurement validity. The interaction between these requirements—that valid measurement of intersectionality suggests many groups, but statistical reliability requires fewer—goes unrecognized.


# Section 5: Normative Frameworks and Fairness Trade-offs

Even if we could resolve ontological and measurement questions about which groups exist and how to operationalize them, normative questions would remain: what does fairness require across groups? Normative frameworks from political philosophy and ethics offer potential guidance—but these frameworks often presuppose knowing which groups matter, and different frameworks suggest different approaches to the statistical-ontological dilemma.

Satz et al. (2022) provide an important conceptual distinction between "fairness as a property of the algorithm used for prediction tasks" and "justice as a property of the allocation principle used for decision tasks." They argue that "fairness and justice are frequently conflated, with the consequence that distributive justice concerns are not addressed explicitly." Their analysis classifies "prioritarianism as an optimization-type theory of distributive justice alongside utilitarianism," where "prioritarianism's goal is to maximize aggregate utility for all groups, giving greater weight to utility the worse off the group."

This framing is helpful for intersectional fairness: it distinguishes predicting outcomes fairly from allocating resources justly. However, both fairness and justice as they define them require knowing which groups exist. To give "greater weight to utility the worse off the group," we must know which groups to compare. To ensure prediction algorithms are fair "across groups," we must specify the groups across which fairness is assessed. Their framework does not tell us how to determine this specification.

Parfit (1997) provides the canonical philosophical statement of prioritarianism in "Equality and Priority." Prioritarianism "gives greater moral weight to benefits to the worse-off" without requiring equality per se. Unlike egalitarianism (which cares about relative positions), prioritarianism cares about absolute levels with weighting—improvements for the worst-off matter more morally than equal improvements for the better-off. For algorithmic fairness, this suggests we should weight improvements in accuracy or fairness metrics more heavily for intersectional groups that are worst-off.

But which groups are worst-off? Determining this requires reliable measurements of group-level outcomes—precisely what Section 2 showed becomes statistically uncertain with many intersectional groups. Prioritarianism as a normative principle does not solve the statistical problem; rather, applying prioritarianism presupposes it is already solved (we must know group-level performance to identify worst-off groups). Moreover, determining which groups to include in the comparison reintroduces the ontological question: should we compare all possible intersectional groups? Only groups above some size threshold? Groups identified through community engagement?

Frankfurt (1987) develops sufficientarianism in "Equality as a Moral Ideal," arguing that "what matters morally is not equality but whether people have enough." Rather than requiring equal outcomes across groups, sufficientarianism requires that all groups achieve at least a threshold of adequate outcomes. Applied to algorithmic fairness, this suggests ensuring all groups have acceptable accuracy, calibration, or other fairness metrics—even if some groups have better performance than others.

Binns (2024) applies this framework explicitly to algorithms, proposing that "fairness criterion checking if approximately equal shares from all groups are above threshold accounts for structural injustices under sufficientarianism." This shifts focus from relative parity to absolute adequacy. Such a shift might seem to ameliorate the statistical problem: rather than comparing all pairs of groups (which grows quadratically with the number of groups), we only check if each group exceeds a threshold (which grows linearly). However, determining if small intersectional groups meet thresholds still requires adequate sample sizes for reliable estimation. Statistical uncertainty does not disappear; it manifests as uncertainty about whether groups truly meet thresholds rather than uncertainty about relative comparisons.

Furthermore, as Fourie and Rid (2018) argue, "appropriate principle reflecting obligations to desperately badly off cannot allow trade-offs between benefits to desperately bad off and merely bad off." If some intersectional groups fall far below thresholds while others barely miss them, sufficientarianism does not permit sacrificing the former to help the latter. This creates pressure to identify which groups are "desperately badly off"—returning us to the statistical problem of reliably estimating intersectional group performance.

Holm (2023) addresses the "leveling down" objection to egalitarian fairness in "Egalitarianism and Algorithmic Fairness." Classification parity criteria require "equality across groups with respect to performance measures such as error rates." Critics argue this leads to leveling down: achieving equality by worsening outcomes for advantaged groups without improving outcomes for disadvantaged groups. Holm "interprets the criticism as a form of leveling down objection" and "interprets the egalitarianism of classification parity as deontic egalitarianism," then presents an "egalitarian response to the leveling down objection."

His analysis reveals normative complexity: even within egalitarian frameworks, there are disputes about whether equality per se is valuable or whether it matters only instrumentally. These disputes affect what fairness requires. But notice what Holm's analysis presupposes: it compares "groups" without addressing which groups to include in the comparison. The leveling down debate concerns what to do given group-level performance differences—it does not address how to identify which groups those should be.

Mittelstadt et al. (2023) press the leveling down objection in "The Unfairness of Fair Machine Learning." They argue that "levelling down should be rejected in fair ML because it unnecessarily harms advantaged groups when performance is intrinsically valuable, demonstrates lack of equal concern for affected groups, and fails to meet aims of many viable distributive justice theories." Their critique challenges whether parity should be the goal. Yet their alternative—focusing on "aims of viable distributive justice theories"—still requires group specification. Different theories (prioritarian, sufficientarian, maximin) require knowing which groups exist and their relative positions.

Heidari et al. (2022) examine the "impossibility of fairness"—the mathematical incompatibility between different fairness definitions. They argue for moving "from formal to substantive algorithmic fairness" that accounts for social context rather than treating fairness as purely formal mathematical property. Their analysis suggests fairness requirements should vary by context, domain, and stakeholder values. This contextual approach might seem to dissolve the dilemma: in each context, stakeholders determine relevant groups. However, this pushes the problem to stakeholder engagement: which stakeholders? How to aggregate disagreements? And the statistical problem persists—stakeholder preferences for including many intersectional groups do not eliminate the statistical challenges of doing so.

Narayanan et al. (2022) propose a "justice-based framework for analyzing algorithmic fairness-utility trade-offs," noting that "trade-off between fairness and efficiency, and between public safety and prevailing algorithmic fairness notions, can be large in practice." Their framework aims to make trade-offs explicit and subject to normative evaluation rather than treating them as purely technical optimization problems. For intersectional fairness, relevant trade-offs include: (1) statistical reliability versus group inclusion (fewer groups → more reliable estimates; more groups → less reliable estimates), (2) measurement validity versus statistical feasibility (valid measurement of intersectionality suggests many emergent groups; statistical feasibility suggests fewer groups), (3) different groups' interests (resources spent improving fairness for one intersectional group cannot be spent on another).

Fazelpour and Danks (2021) argue that "reductionist representations of fairness often bear little resemblance to real-life fairness considerations, which are highly contextual," and "fairness metrics tend to be implemented within narrow toolkits that are difficult to integrate into broader ethical assessments." Their critique applies to group specification: reducing the question "which groups matter?" to a technical problem (which attribute combinations to consider?) fails to capture the full normative and contextual complexity. Yet their alternative—grounding fairness in "ethical philosophy and welfare economics"—requires answering which groups' welfare to consider, returning us to the specification problem.

Binns (2018) makes this explicit in "Fairness in Machine Learning: Lessons from Political Philosophy," arguing that "fairness metrics implicitly encode political-philosophical commitments" and that "different contexts require different normative frameworks, thus different fairness approaches." His analysis usefully makes implicit commitments explicit. However, the commitment to particular groups (which groups to assess fairness for) often remains implicit even when other commitments (which fairness metric to use) are made explicit. Political philosophy provides frameworks for evaluating fairness given groups; it provides less guidance on group specification itself.

Corbett-Davies and Goel (2018) observe in "The Measure and Mismeasure of Fairness" that "different metrics embody different normative commitments" and "no metric is universally appropriate—depends on normative goals and context." Their analysis shows that normative judgment cannot be eliminated from fairness assessment; it can only be made explicit and subjected to scrutiny. For intersectional fairness, this implies that choosing which groups to include embodies normative commitments that should be explicit and justifiable. Yet the literature on fairness metrics focuses on comparing metrics for given groups rather than on the prior question of group specification.

**Flag for the Dilemma**: Normative frameworks from political philosophy—prioritarianism, sufficientarianism, egalitarianism—offer valuable perspectives on what fairness requires across groups. However, applying these frameworks presupposes knowing which groups exist and having reliable information about their relative positions or absolute levels of well-being (however defined). Prioritarianism requires identifying worst-off groups; sufficientarianism requires checking if all groups meet thresholds; egalitarianism requires comparing group-level outcomes. Each presupposes the statistical problem is solved (reliable group-level estimates) and the ontological problem is solved (specification of which groups to include). Moreover, different normative frameworks might suggest different approaches to the statistical-ontological dilemma—but the normative literature does not recognize this dilemma as such. Philosophers debate which distributive principle is correct without acknowledging that some principles (e.g., prioritarianism focusing on worst-off) might be more or less statistically feasible depending on which groups are included. The normative questions are analyzed independently of the statistical and ontological constraints. Yet in practice, these questions interact: normative commitments affect which groups we think matter (ontological question), which affects how many groups we must reliably measure (statistical question), which may be infeasible, forcing compromise on normative ideals. This interaction goes unanalyzed in the normative literature.


# Section 6: Epistemic Dimensions of Fairness

Intersectional fairness raises not only technical, ontological, measurement, and normative questions but also epistemic ones: What counts as adequate knowledge about group-level properties? When is it epistemically responsible to make claims about groups with sparse data? Who has authority to determine which groups matter? These questions connect to broader debates in epistemology about uncertainty, epistemic justice, and the ethics of inference.

Fricker (2007) provides the foundational framework in "Epistemic Injustice: Power and the Ethics of Knowing," distinguishing "testimonial injustice" (credibility deficit due to prejudice) and "hermeneutical injustice" (gap in collective interpretive resources). Barabas et al. (2025) extend this framework to AI systems, developing a "taxonomy mapping testimonial, hermeneutical, and distributive injustices onto four AI development stages: data collection, model training, inference, and dissemination." They argue that "algorithmic systems can be vectors of epistemic injustice—excluding marginalized groups from processes that define, judge, and make decisions about them."

For intersectional fairness, this analysis suggests that determining which groups matter is itself an epistemic question with political dimensions. If intersectional groups (e.g., disabled Black women, elderly transgender Latinx individuals) are excluded from the set of groups for which fairness is assessed, this constitutes epistemic injustice: their experiences are not given credence, and they lack representation in the interpretive frameworks used to understand fairness. Yet including all possible intersectional groups creates the statistical problem documented in Section 2. The epistemic injustice framework reveals the ethical stakes of the statistical-ontological dilemma but does not resolve it.

Anderson (2012) argues in "Epistemic Justice as a Virtue of Social Institutions" that "institutions can systematically advantage or disadvantage different knowers." Algorithmic systems are institutions in this sense—they embody particular epistemic practices, privilege certain forms of knowledge, and systematically advantage or disadvantage different groups as knowers and as subjects of knowledge. When systems determine which groups to assess fairness for based on available data or conventional demographic categories, they make epistemic choices that advantage groups for whom data are plentiful and disadvantage groups for whom data are sparse—often the very intersectional groups most in need of fairness protections.

D'Ignazio and Klein (2020) develop "data feminism," arguing that "data practices systematically privilege certain perspectives while marginalizing others" and that "without diverse perspectives across AI ecosystem, ML advances will fuel epistemic injustice." Their analysis emphasizes that data collection choices are not neutral but embody power relations. Decisions about which demographic categories to record, how to operationalize them, and which intersections to track all reflect and reinforce existing power structures. For intersectional fairness, this suggests that data infrastructure shapes which groups can be analyzed—a point connecting to the measurement literature (Section 4) and revealing epistemic dimensions of measurement choices.

Kalluri (2020) makes this power dimension explicit, arguing that we should "not ask if artificial intelligence is good or fair, ask how it shifts power." She contends that "epistemic authority over group categories is a form of power" and that "fairness debates are partly epistemic disagreements about group ontology." Who has power to define which groups exist and matter? In current practice, this power often lies with system designers, data collectors, and researchers—not necessarily with the communities being categorized. Yet those communities may have crucial epistemic resources (lived experience, community knowledge) that external categorizations miss.

Harding (2004) develops standpoint epistemology, arguing that "certain social positions—such as gender, race, or class—render particular epistemological perspectives" and that "science conducted by Black women would not contain the same knowledge as science created by white men." Applied to algorithmic fairness, this suggests that determining which intersectional groups matter is not a neutral technical question but one where different social positions provide different epistemic access. Black women may have epistemic resources for identifying which intersectional groups face distinctive discrimination that others lack. Yet incorporating such standpoint epistemology into algorithmic systems raises difficult questions about how to institutionalize respect for multiple, sometimes conflicting, standpoints.

Hüllermeier and Waegeman (2021) provide a technical epistemological framework distinguishing "aleatoric (irreducible probabilistic variability)" from "epistemic (reducible through more knowledge)" uncertainty. Epistemic uncertainty is "also known as systematic uncertainty—things one could in principle know but does not in practice." For intersectional groups, both types of uncertainty are present. Aleatoric uncertainty arises from genuinely stochastic processes (individual outcomes vary even within groups). Epistemic uncertainty arises from "limited sample size" (if we had more data about a group, we would have more certainty about its properties).

Critically, Hüllermeier and Waegeman note that "epistemic uncertainty reflects uncertainty about estimates due to limited sample size" and "allows disentangling task-inherent (aleatoric) from sampling (epistemic) effects." For small intersectional groups, epistemic uncertainty dominates: our uncertainty about group properties is primarily due to sparse data rather than inherent variability. This is the epistemic manifestation of the statistical problem documented in Section 2. What Hüllermeier and Waegeman do not address is the ontological source of this epistemic uncertainty: we have sparse data for intersectional groups partly because we have many such groups (exponential growth with attributes), and we have many groups partly because ontological considerations (Section 3) suggest intersectionality involves numerous emergent, context-dependent groups.

Bhatt et al. (2021) argue that "uncertainty quantification is crucial for responsible AI" and that "making uncertainty visible enables more epistemically responsible inferences." They contend this is "particularly important for claims about small groups with high epistemic uncertainty" and that "failing to communicate uncertainty can mislead about reliability of group-level inferences." Their framework suggests that fairness assessments should report uncertainty: not just "Group X has error rate Y" but "Group X has error rate Y with confidence interval [a, b]." For small intersectional groups, these confidence intervals may be wide—communicating the epistemic limitations of what we know.

However, uncertainty communication does not resolve the dilemma. Reporting wide confidence intervals for intersectional groups makes explicit the statistical problem but does not solve it. Stakeholders still face the question: should we make fairness decisions based on uncertain information about many intersectional groups, or constrain our analysis to groups about which we can be more certain (and thereby exclude some intersectional groups from consideration)?

Hullman (2021) distinguishes "epistemic uncertainty (lack of knowledge)" from "agonistic uncertainty (disagreement about values/priorities)." Both are present in intersectional fairness. Epistemic uncertainty concerns what group-level properties are (a matter for statistical estimation). Agonistic uncertainty concerns which groups matter (a matter of normative and political disagreement). Hullman argues these require different responses: epistemic uncertainty can potentially be reduced through more data and better methods; agonistic uncertainty requires negotiation among stakeholders with competing values. For intersectional fairness, the dilemma involves both: there is epistemic uncertainty about intersectional group properties (statistical problem) and agonistic uncertainty about which groups warrant consideration (ontological/normative problem). Treating these as independent—reducing epistemic uncertainty without addressing agonistic uncertainty, or resolving value disagreements without accounting for statistical constraints—fails to recognize their interaction.

Fazelpour and Danks (2020) examine "how data science practices can perpetuate epistemic injustice," showing that "choices about data collection, categories, modeling approaches can systematically privilege some knowers/perspectives while disadvantaging others." For intersectional fairness, epistemic injustice can occur at multiple levels: (1) excluding intersectional groups from fairness analysis perpetuates injustice by rendering their experiences epistemically invisible, (2) including them with inadequate data risks making unreliable inferences that could mischaracterize their experiences or justify harmful decisions, (3) requiring certainty before acting on behalf of intersectional groups privileges epistemic caution over epistemic justice. The dilemma creates an epistemic double-bind: both including and excluding intersectional groups with sparse data risk epistemic injustice, albeit of different kinds.

Critical scholarship documents cases where systems make overconfident inferences about marginalized groups. Eubanks (2018) shows how "automated systems make inferences about marginalized groups with insufficient epistemic basis," claiming "certainty about group properties based on sparse, biased data." O'Neill (2016) documents "cases where algorithmic systems make high-stakes inferences about groups with insufficient data and without appropriate uncertainty quantification," leading to "overconfident, often harmful, inferences about marginalized groups." Benjamin (2019) and Noble (2018) show how "technologies encode assumptions about racial and other social groups," with "inferences about groups involving epistemic choices about which group properties are real/meaningful" often made "without adequate epistemic basis or input from affected communities."

These cases reveal a pattern: systems frequently treat the statistical problem as solved when it is not (making confident inferences despite sparse data) and treat the ontological problem as solved when it is not (using available demographic categories without interrogating whether they capture meaningful groups). The epistemic irresponsibility is compounded: unwarranted confidence about poorly-grounded group categories.

Gebru et al. (2021) propose "datasheets for datasets" as a transparency mechanism, arguing that "part of epistemic responsibility in using data is documenting how categories are defined, measured, and validated." This is "particularly important for demographic categories used in fairness assessment" because "without clear epistemic grounding, unclear what inferences about groups are warranted." Their proposal addresses the measurement dimension (Section 4) from an epistemic perspective: we should be epistemically explicit about our category definitions and their limitations.

Raji et al. (2020) extend this to algorithmic auditing, proposing a framework that includes "epistemic requirements" such that "audits must account for uncertainty, especially for small/intersectional groups" and "making claims about fairness for groups requires epistemic warrant—adequate data, valid measurements, appropriate statistical methods accounting for uncertainty." Their framework recognizes that fairness audits make epistemic claims (this group experiences bias, that group does not) that require epistemic justification. For intersectional groups, providing such warrant becomes increasingly difficult as groups proliferate and data per group thins.

**Flag for the Dilemma**: The epistemology literature reveals multiple epistemic dimensions of intersectional fairness: the distinction between epistemic and aleatoric uncertainty (Hüllermeier and Waegeman 2021), the role of epistemic injustice when groups are excluded from analysis (Fricker 2007; Anderson 2012), the importance of standpoint epistemology suggesting different groups have different epistemic access (Harding 2004), the power dynamics in determining group categories (Kalluri 2020; D'Ignazio and Klein 2020), and the epistemic responsibilities in making inferences under uncertainty (Bhatt et al. 2021; Raji et al. 2020). However, this literature does not connect epistemic uncertainty to the statistical and ontological sources documented in earlier sections. Epistemologists analyze uncertainty about group properties without fully engaging with the statistical problem (many groups → sparse data → high epistemic uncertainty) or the ontological problem (contested group definitions → uncertainty about which groups exist → uncertainty about which groups to reduce epistemic uncertainty for). The epistemic dimensions are analyzed largely independently of the statistical constraints and ontological debates, missing the interaction that creates the dilemma. Moreover, frameworks for epistemic justice (which suggests including marginalized intersectional groups) do not grapple with statistical limitations (which make reliable inferences about numerous groups infeasible), while frameworks for epistemic responsibility (which require adequate warrant for inferences) do not fully address the ontological question (adequate warrant for which groups?). The dilemma persists: epistemic justice considerations push toward including many intersectional groups, epistemic responsibility requires statistical reliability, and these pull in opposite directions when groups are numerous and data are sparse.


# Section 7: Synthesis and the Dilemma Gap

## The Current Landscape

Our review of 95 papers across six domains reveals sophisticated, specialized scholarship on intersectional fairness. The machine learning community has documented technical challenges of handling sparse intersectional data, developed innovative solutions ranging from multicalibration to hierarchical methods, and demonstrated both the promise and limitations of algorithmic approaches (Section 2). Philosophers have provided rich analyses of intersectionality's metaphysical foundations, revealing deep disagreements about what intersectionality is and what it requires (Section 3). Measurement theorists have shown how operationalization embeds ontological commitments and how construct validity failures undermine fairness claims (Section 4). Normative theorists have offered frameworks from prioritarianism to sufficientarianism for evaluating fairness trade-offs (Section 5). Epistemologists have analyzed different forms of uncertainty, epistemic justice, and the epistemic responsibilities of making inferences about groups (Section 6).

Each literature has advanced understanding within its domain. Yet examining these literatures together reveals a systematic gap: the absence of work recognizing that statistical uncertainty and ontological uncertainty *interact* to create a genuine dilemma for intersectional fairness. We now make this gap explicit.

## The Two Horns: Separately Recognized, Never Joined

### Horn 1: Statistical Uncertainty (Well-Documented)

The technical literature thoroughly documents that more intersectional groups create statistical challenges:

- Exponential growth: With k binary attributes, there are 2^k possible intersectional groups (Sheng et al. 2025; Celis et al. 2022)
- Data sparsity: More groups mean fewer observations per group, increasing sampling variance (Gohar and Cheng 2023; Maheshwari et al. 2024)
- Sample complexity: Multicalibration and similar guarantees require sample sizes that grow exponentially with groups (Hébert-Johnson et al. 2018; Dwork et al. 2024)
- Practical infeasibility: "Attaining multicalibration is practically infeasible with more than a few classes" (Dwork et al. 2024); "assessing group fairness with respect to multiple sensitive attributes may be unfeasible in most practical cases" (Celis et al. 2022)
- Epistemic uncertainty: Small samples create high epistemic uncertainty about group properties (Hüllermeier and Waegeman 2021)

This horn is well-established: more groups make statistical estimation less reliable. The technical community recognizes this as a fundamental challenge.

### Horn 2: Ontological Uncertainty (Well-Documented)

The philosophical literature thoroughly documents uncertainty about which groups exist and matter:

- Definitional dilemmas: "Intersectionality's definitional dilemmas" reflect "tensions between different interpretations and uses" (May 2014)
- Competing metaphysics: Intersectionality understood variously as causal (Bright et al. 2016), emergent (Jorba and López de Sa 2024), non-additive (Hancock 2007), metaphorical/heuristic/paradigmatic (Garry 2011)
- Social ontology debates: Groups understood as attribute-based (Epstein 2019), practice-based (Sveinsdóttir 2013), structurally constituted (Haslanger 2012), context-dependent (Sterba 2024)
- Contested categories: No philosophical consensus on what constitutes gender, race, or their intersections (Haslanger and Ásta 2017; Thomasson 2019)
- Multiple frameworks: Anticategorical, intracategorical, and intercategorical approaches to intersectionality suggest different group specifications (McCall 2005)

This horn is also well-established: there is no settled philosophical answer to "which intersectional groups exist and warrant consideration?" Different theoretical frameworks suggest different answers.

## The Missing Link: Interaction and Dilemma

Despite thorough documentation of each horn separately, no existing work frames these as *interacting* problems that together constitute a genuine dilemma. The gap is not that either problem is unrecognized—both are well-recognized within their respective literatures. The gap is the absence of recognition that these problems mutually exacerbate each other in a way that creates a dilemma where "solving" one horn worsens the other.

### The Interaction (Unrecognized)

The interaction works in both directions:

**Ontological → Statistical**: Philosophical considerations suggest including many intersectional groups (because intersectionality involves emergent properties, context-dependent effects, non-additive interactions), but including more groups worsens the statistical problem. If we take seriously the philosophical insight that Black women's experiences are not reducible to Black experiences plus women's experiences (Bowleg 2008; Hancock 2007), we must include "Black women" as a distinct group. If we extend this to all intersections of all attributes, we face exponential growth in groups—precisely the statistical problem documented in Section 2. Philosophical rigor about intersectionality creates statistical infeasibility.

**Statistical → Ontological**: Statistical constraints push toward including fewer groups (to maintain adequate sample sizes and estimation reliability), but reducing the number of groups requires principled answers to ontological questions about which groups to exclude. Should we exclude groups below a minimum sample size? But size thresholds arbitrarily privilege frequent groups over rare ones. Should we use hierarchical methods focusing on "parent" groups? But determining the hierarchy requires answering which categories are fundamental—an ontological question without consensus answers. Should we only include groups identified by domain experts or communities? But different experts and communities may identify different groups, and their identification criteria may conflict with available data structures. Every statistical solution that constrains the number of groups implicitly or explicitly makes ontological commitments about which groups truly matter—commitments that the philosophical literature shows are deeply contested.

### Why This Is a Genuine Dilemma

A dilemma is not merely the co-presence of two problems. It is a situation where addressing one problem exacerbates the other, such that there is no straightforward solution that resolves both. The statistical-ontological interaction in intersectional fairness exhibits this structure:

1. **Attempting to resolve the statistical problem by constraining groups**: Statistical methods work better with fewer, larger groups. One might try to resolve the statistical horn by limiting which intersectional groups to include—perhaps only groups above a minimum sample size, or only "major" intersections. But any such constraint requires answering: which groups should be excluded? This reintroduces the ontological horn in acute form, as the exclusion decision requires principled ontological commitments about which groups are real/important enough to include. And as the philosophical literature shows (Section 3), there are no uncontested answers. Different ontological frameworks suggest different groups. The statistical constraint does not dissolve the ontological uncertainty; it makes it practically urgent.

2. **Attempting to resolve the ontological problem by including all groups**: One might try to resolve the ontological horn by being maximally inclusive—include all intersectional groups definable from available attributes. This defers to a kind of ontological pluralism: if there's disagreement about which groups exist, include them all. But this approach maximally exacerbates the statistical horn. With k binary attributes, including all 2^k intersectional groups creates exponential data sparsity. As documented in Section 2, fairness methods become "practically infeasible" with more than a few intersections (Dwork et al. 2024). Ontological inclusivity purchases statistical infeasibility.

3. **Attempting to resolve both through compromise**: One might try intermediate positions—include more groups than simple demographic marginals but fewer than all possible intersections. But any intermediate position still requires answers to both problems: *which* intermediate set of groups (ontological question)? *How many* groups can be reliably handled (statistical question)? And crucially, the compromise must navigate the interaction: ontological considerations about which groups are most important may conflict with statistical considerations about which groups have adequate data. Intersectional groups identified as most important by critical theory (e.g., Black transgender women) may be precisely those with sparsest data. The compromise cannot independently satisfy both constraints—it involves trading off ontological desiderata against statistical desiderata.

4. **The mutual exacerbation**: Each problem makes the other worse. The statistical problem is worse because there are ontologically many groups that matter (not just one or two intersections but potentially hundreds). The ontological problem is worse because statistical constraints create pressure to exclude groups—but principled exclusion requires resolving ontological debates. If there were consensus on ontology (e.g., only ten specific intersectional groups truly matter), the statistical problem would be manageable. If statistical methods could handle unlimited groups, the ontological uncertainty would not create practical bottlenecks. But absent consensus and absent unlimited statistical power, the problems interact to create a dilemma.

## Existing Work That Comes Closest (But Stops Short)

Several papers approach the dilemma without fully recognizing it:

**Jacobs and Wallach (2021)** make the crucial observation that "fairness itself is an essentially contested construct with different theoretical understandings in different contexts" and that "debates that appear to be about different operationalizations are actually debates about different theoretical understandings." This analysis could extend to groups: debates about which intersectional groups to include are not just technical (which attribute combinations?) but theoretical (what makes something a group warranting fairness consideration?). However, they do not make this extension, and critically, they do not connect theoretical contestedness to statistical consequences. Their framework addresses measurement validity independently of statistical feasibility.

**Celis et al. (2022)** note that "assessing group fairness with respect to multiple sensitive attributes may be unfeasible in most practical cases" and that "the presence of many sensitive features is more of a norm than an exception, [which] represents a huge problem that the literature on fairness in ML has barely begun to address." They recognize the statistical infeasibility of many groups but frame this as a technical challenge requiring better methods. They do not connect it to ontological debates about which groups exist or explore how ontological considerations might justify constraining or expanding the group set.

**Gohar and Cheng (2023)** provide a comprehensive survey noting that "current research overlooks the under-representation of intersectional groups by solely focusing on achieving parity, and more research on non-distributive intersectional fairness is needed." They identify that standard approaches inadequately address intersectionality but frame this as a gap requiring more research—not as a dilemma where ontological and statistical constraints interact.

**Jorba and López de Sa (2024)** develop a sophisticated philosophical account of "intersectionality as emergence," arguing that "intersectional experiences emerge from the conjunction of social categories when social structures make them relevant." Their emergence view has implications for group specification (which conjunctions are relevant? in which contexts?), but they do not discuss implications for statistical estimation. The philosophical analysis proceeds without acknowledging that treating intersectionality as emergent and context-dependent (ontologically) makes determining which groups to analyze statistically highly complex.

**Bright, Malinsky, and Thompson (2016)** show that intersectionality can be interpreted through causal models, making "claims about causal effects of occupying intersecting identity categories" empirically testable. Their approach promises to bridge philosophy and data science. However, they do not address the practical challenge that testing causal effects for all possible intersecting categories requires data that may not exist, or that the set of categories to model must itself be specified—returning us to the ontological question.

**Hüllermeier and Waegeman (2021)** distinguish epistemic from aleatoric uncertainty, noting that "epistemic uncertainty reflects uncertainty due to limited sample size." They recognize that small groups have high epistemic uncertainty. However, they do not connect this to the ontological question of *which* groups exist and thus which groups we should work to reduce epistemic uncertainty for. Their epistemological framework is developed independently of social ontology.

None of these papers—individually excellent—frame the situation as a dilemma arising from interaction between statistical and ontological problems. Each analyzes one dimension without fully engaging the other or recognizing their mutual exacerbation.

## What Would Recognition of the Dilemma Look Like?

Recognition would involve:

1. **Acknowledging both horns**: Not treating either as solved or solvable independently
2. **Analyzing the interaction**: Showing how ontological choices have statistical consequences and statistical constraints require ontological decisions
3. **Framing as dilemma**: Recognizing that standard solutions (technical improvements, conceptual clarification) do not dissolve the fundamental tension
4. **Interdisciplinary approach**: Bringing together computer science, philosophy, measurement theory, normative theory, and epistemology to address the interaction rather than delegating separate problems to separate fields

## The Gap and the Contribution

The gap this review identifies is not that individual literatures are deficient within their domains. It is that the organization of scholarship into separate domains—each with its own methods, venues, and conceptual frameworks—has created a systematic blind spot regarding the interaction. Machine learning researchers assume groups can be specified and focus on statistical challenges. Philosophers analyze intersectionality's nature without considering statistical constraints. Measurement theorists examine construct validity independently of sample size requirements. Normative theorists propose distributive principles without addressing feasibility given ontological uncertainty and statistical limitations. Epistemologists analyze uncertainty without fully integrating ontological and statistical sources.

The systematic nature of this blind spot suggests it is not accidental. Interdisciplinary problems at the boundary of multiple fields can fall between disciplinary cracks, with each field assuming the other has solved its piece. Here, technical researchers assume philosophy provides the groups, philosophers assume technical methods can handle whatever groups exist, measurement theorists assume both problems can be solved separately, normative theorists assume both are solved before normative principles apply, and epistemologists analyze the result without interrogating the interactive genesis of epistemic uncertainty.

The contribution of framing intersectional fairness as a genuine dilemma—arising from the interaction of statistical uncertainty and ontological uncertainty—is to name this blind spot, demonstrate its systematic nature across multiple literatures, and suggest that incremental progress within existing frameworks may not resolve the fundamental challenge. If the dilemma is genuine, what is needed is not better statistical methods (though those help) or clearer ontology (though that helps) or more valid measurement (though that helps) but fundamentally new approaches that grapple directly with the interaction.

**Flag for the Dilemma**: This section establishes the core finding: existing scholarship thoroughly documents the statistical problem (more groups → less reliability) and the ontological problem (unclear which groups exist/matter), but no existing work recognizes these as interacting problems forming a genuine dilemma. The interaction is the gap. Machine learning proceeds as if groups are given; philosophy proceeds as if statistical constraints don't matter; measurement, normative theory, and epistemology proceed as if both problems can be addressed separately. The dilemma framing is novel: it names an interaction that has been systematically overlooked across multiple literatures, despite each literature providing pieces of the puzzle. Recognizing the interaction as a dilemma reframes the challenge from "solve the statistical problem with better methods and solve the ontological problem with clearer concepts" to "grapple with the interaction where solving one exacerbates the other."


# Section 8: Conclusion

This literature review has synthesized 95 papers across six domains—algorithmic fairness and intersectionality (machine learning), philosophy of intersectionality, social ontology, measurement theory and construct validity, normative frameworks for fairness, and applied epistemology—to establish a systematic gap in scholarship on intersectional fairness. Each domain provides sophisticated analysis of challenges that intersectional fairness poses. The machine learning community has developed increasingly innovative technical methods for handling sparse intersectional data, from multicalibration frameworks to hierarchical approaches. Philosophers have revealed deep complexities in understanding what intersectionality is and how intersectional groups should be conceived. Measurement theorists have shown that operationalization is never neutral but always embeds ontological commitments. Normative theorists have offered frameworks from prioritarianism to sufficientarianism for evaluating fairness. Epistemologists have analyzed uncertainty, epistemic justice, and the responsibilities of making inferences about groups.

Yet examining these literatures together reveals what individual literatures miss: the interaction between statistical uncertainty and ontological uncertainty creates a genuine dilemma for intersectional fairness that cannot be resolved through incremental advances within existing frameworks. The dilemma has two horns that mutually exacerbate each other. More intersectional groups create statistical challenges (exponential growth, data sparsity, infeasible sample complexity)—this is well-documented in machine learning. Which intersectional groups exist and warrant consideration is ontologically uncertain (definitional dilemmas, competing metaphysics, contested categories)—this is well-documented in philosophy. But these are not independent problems. Taking ontological considerations seriously (emergence, non-additivity, context-dependence) suggests including many intersectional groups, which exacerbates statistical challenges. Statistical constraints create pressure to limit groups, which requires answering ontological questions that philosophy shows are deeply contested. Each horn makes the other worse.

The gap is not that either horn is unrecognized. Both are thoroughly analyzed within their respective literatures. The gap is the absence of recognition that they *interact* to create a dilemma. Machine learning researchers assume groups can be specified (treating the ontological problem as solved) and focus on statistical challenges. Philosophers analyze intersectionality without considering statistical constraints (treating the statistical problem as outside their domain). Measurement theorists, normative theorists, and epistemologists largely proceed as if these problems can be addressed separately. This disciplinary organization has created a systematic blind spot: the interaction falls between fields, with each assuming the other has solved its piece.

Our review identifies specific moments where existing work approaches the dilemma without fully recognizing it. Jacobs and Wallach (2021) argue that fairness is an essentially contested construct with different theoretical understandings—an insight that could extend to group specification but does not in their analysis, and which does not connect theoretical contestedness to statistical consequences. Celis et al. (2022) note that assessing fairness with multiple attributes "may be unfeasible in most practical cases"—recognizing statistical infeasibility but not connecting it to ontological debates about which groups to include. Jorba and López de Sa (2024) develop intersectionality as emergence—a philosophical advance that has implications for which groups exist but without discussion of statistical challenges this creates. Hüllermeier and Waegeman (2021) distinguish epistemic from aleatoric uncertainty—an epistemological framework applicable to intersectional groups but developed without engaging ontological debates about which groups that framework should apply to.

These moments of near-recognition underscore that the dilemma is not obvious or trivial. Excellent scholarship in multiple domains has worked on related problems without quite seeing the interaction. This makes the gap systematic rather than accidental—a product of how scholarship is organized rather than individual oversight.

The contribution of framing intersectional fairness as a genuine dilemma arising from statistical-ontological interaction is thus both diagnostic and generative. Diagnostically, it names a blind spot that has persisted across multiple literatures despite growing attention to intersectional fairness. It explains why both technical advances (better methods for sparse data) and philosophical clarifications (clearer ontologies of groups) feel insufficient—because addressing one horn without the other leaves the dilemma unresolved. Generatively, recognizing the dilemma suggests that what is needed is not more progress within existing frameworks but new approaches that grapple directly with the interaction.

Such approaches might take various forms. One possibility is developing normative frameworks specifically for the dilemma—principles for navigating trade-offs between ontological inclusivity and statistical reliability rather than treating these as independent desiderata. Another possibility is participatory methods that involve affected communities in determining which intersectional groups matter, making the ontological question a matter of democratic deliberation rather than expert specification, though this still must address how to handle statistical constraints when communities identify numerous groups. A third possibility is developing statistical methods specifically designed for the ontological uncertainty case—methods that remain valid when the set of groups itself is uncertain or contested, rather than presupposing a known group specification. A fourth possibility is measurement approaches that embrace rather than resolve ontological uncertainty—frameworks that can operate with multiple, overlapping group specifications rather than requiring a single definitive categorization.

These are merely possibilities suggested by recognizing the dilemma. The point is not to solve the dilemma within a literature review but to establish its existence and novelty. Our analysis demonstrates that across 95 papers spanning machine learning, philosophy, measurement theory, normative theory, and epistemology, none frame intersectional fairness as this kind of dilemma. Technical papers assume ontological problems are solved; philosophical papers proceed without statistical constraints; other literatures assume both can be addressed separately. The interaction—the mutual exacerbation whereby ontological considerations push toward more groups while statistical constraints push toward fewer—goes unrecognized.

This gap has practical consequences. Current approaches to intersectional fairness make implicit choices about which horn to privilege: statistical approaches prioritize feasibility (fewer groups), critical approaches prioritize inclusivity (more groups), pragmatic approaches seek compromises without acknowledging the underlying dilemma. Making these choices explicit—recognizing them as navigating a genuine dilemma rather than solving a technical problem—would change both the framing and the stakes of intersectional fairness work. It would clarify that disagreements about intersectional fairness are not merely about which methods to use but about how to navigate irreducible tensions between ontological commitments and statistical constraints.

For researchers, the dilemma framing suggests new questions. Rather than "how can we achieve fairness for intersectional groups?" (which presupposes the dilemma is solvable), we might ask: "how should we navigate trade-offs between ontological inclusivity and statistical reliability?" "What makes a compromise between these desiderata legitimate or just?" "Who should have authority to make these trade-offs?" These questions require integrating perspectives from computer science, philosophy, political theory, and affected communities—genuinely interdisciplinary work rather than parallel disciplinary analyses.

For practitioners, the dilemma framing provides language for challenges they likely already face. Practitioners building fairness-aware systems must decide which groups to assess fairness for—a decision that involves both statistical realities (some groups have sparse data) and normative commitments (some groups may be marginalized despite sparse data). Recognizing this as navigating a dilemma rather than solving a technical problem can make the difficulty feel less like a failure of expertise and more like an inherent complexity requiring judgment.

For policymakers and auditors, the dilemma framing suggests that algorithmic fairness requirements should account for this complexity. Regulations requiring "fairness for all groups" may be practically infeasible given statistical constraints; regulations requiring only a minimal set of groups may exclude intersectional communities. Policy might better focus on the *process* of navigating the dilemma—requiring transparency about which groups are included/excluded, justification for these choices, and participation by affected communities—rather than mandating specific outcomes that may be impossible given the statistical-ontological interaction.

In conclusion, this review establishes a novel framing of intersectional algorithmic fairness as a genuine dilemma arising from the interaction between statistical uncertainty and ontological uncertainty. While existing scholarship has made tremendous progress understanding each problem separately—with machine learning developing sophisticated methods for sparse data, philosophy revealing deep complexities in intersectionality's nature, and related fields contributing measurement, normative, and epistemic perspectives—no existing work recognizes these as interacting problems that mutually exacerbate each other. The gap is systematic: disciplinary organization has created a blind spot where the interaction falls between fields. Recognizing the dilemma reframes the challenge from "solve both problems independently" to "navigate the interaction where solving one worsens the other." This reframing is both diagnostic (explaining why progress has felt insufficient) and generative (suggesting new research directions, practical approaches, and policy frameworks). The dilemma may not be fully resolvable, but recognizing it as a dilemma—rather than treating it as two separate problems awaiting technical and philosophical solutions—is a necessary first step toward more sophisticated, honest, and effective approaches to intersectional fairness in algorithmic systems.
