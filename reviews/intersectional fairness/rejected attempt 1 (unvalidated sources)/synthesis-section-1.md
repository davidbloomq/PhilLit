# Section 1: Introduction and the Challenge of Intersectional Fairness

The promise of algorithmic fairness research has been to ensure that machine learning systems make equitable decisions across socially salient demographic groups. Yet as fairness scholarship has matured, a fundamental challenge has emerged: how to address fairness for *intersectional* groups—those defined by combinations of attributes such as race, gender, age, and disability. This challenge is not merely technical. It sits at the intersection of machine learning, philosophy, measurement theory, normative ethics, and epistemology, demanding interdisciplinary analysis.

Recent surveys document the scope of intersectional fairness research in machine learning. Gohar and Cheng (2023) provide a comprehensive taxonomy of intersectional fairness notions, mitigation strategies, and open challenges, noting that "intersectional bias encompasses multiple sensitive attributes, such as race and gender, together" rather than considering them in isolation. Their survey identifies data sparsity in smaller intersectional subgroups as a major technical obstacle. Similarly, high-profile cases of algorithmic discrimination—such as facial recognition systems exhibiting significantly higher error rates for Black women compared to white men (Buolamwini and Gebru 2018)—have demonstrated that ensuring fairness on individual attributes (race alone, or gender alone) does not guarantee fairness at their intersections.

These empirical findings resonate with theoretical developments in philosophy. The concept of intersectionality, originating in Black feminist scholarship and legal theory (Crenshaw 1989), holds that experiences of individuals with overlapping marginalized identities cannot be understood as the simple sum of separate identity categories. Philosophers have worked to formalize this insight: Bright, Malinsky, and Thompson (2016) show that intersectionality can be interpreted through graphical causal models, allowing claims about the causal effects of occupying intersecting identity categories to be empirically tested. More recently, Jorba and López de Sa (2024) propose understanding intersectionality through the metaphysical framework of emergence, arguing that "intersectional experiences emerge from the conjunction of social categories when social structures make them relevant vis-à-vis discrimination and privilege."

The machine learning community has responded to the intersectional fairness challenge with increasingly sophisticated technical approaches. Methods range from multicalibration frameworks ensuring calibrated predictions across exponentially many subgroups (Hébert-Johnson et al. 2018), to hierarchical approaches that leverage the structure of intersectional groups to address data sparsity (Maheshwari et al. 2024), to post-processing algorithms for achieving fairness without retraining models (Kim et al. 2019). Parallel developments address conditional demographic parity (Yurochkin and Sun 2024), fairness in reinforcement learning (Hashimoto et al. 2025), and intersectional fairness in ranking systems (Yan et al. 2024).

Yet despite this rich and growing literature, a critical gap remains. The technical literature primarily addresses a *statistical* problem: how to estimate model performance reliably across intersectional groups when sample sizes become small as the number of groups grows exponentially. As Celis et al. (2022) observe, "assessing group fairness with respect to multiple sensitive attributes may be unfeasible in most practical cases" due to the exponential growth of subgroups and resulting data sparsity. Meanwhile, philosophical work addresses an *ontological* problem: which groups warrant consideration in the first place, given deep disagreements about the nature of social groups, the definition of intersectionality itself, and whether demographic categories should be understood as fixed attribute combinations or as emergent, practice-based phenomena (Haslanger 2012; Sveinsdóttir 2013; Epstein 2019).

What existing scholarship has not recognized is that these are not merely two separate challenges requiring independent solutions. They are *interacting* problems that together constitute a genuine dilemma: expanding the set of groups to be inclusive and responsive to ontological considerations exacerbates statistical unreliability; constraining the set to achieve statistical tractability requires resolving deeply contested ontological questions about which groups truly matter. Each horn of the dilemma makes the other worse.

This literature review synthesizes scholarship across six domains—algorithmic fairness and intersectionality (machine learning), philosophy of intersectionality, social ontology, measurement theory and construct validity, normative frameworks for fairness, and applied epistemology—to establish this gap. We show that while individual literatures provide sophisticated analyses of one or the other horn of the dilemma, no existing work frames intersectional fairness as an interaction between statistical uncertainty and ontological uncertainty that creates a genuine dilemma for algorithmic fairness. This framing—as we will demonstrate—is novel and consequential, suggesting that incremental refinements of existing technical or philosophical approaches cannot resolve the fundamental challenge.

The review proceeds as follows. Section 2 examines technical approaches in machine learning, documenting both the statistical challenges of sparse intersectional data and proposed solutions while noting that these solutions typically presume the set of groups is given. Section 3 explores philosophical foundations, analyzing competing interpretations of intersectionality and social ontology debates about group constitution—debates that create ontological uncertainty about which groups should be addressed. Section 4 examines measurement theory and construct validity, showing how operationalization choices embed ontological commitments. Section 5 reviews normative frameworks for navigating fairness trade-offs, noting that normative principles often presuppose knowledge of which groups matter. Section 6 explores epistemic dimensions of fairness, including the distinction between epistemic and aleatoric uncertainty and questions of epistemic authority over group categories. Section 7 synthesizes these findings to establish the dilemma gap: the absence of any existing work that frames these problems as interacting to create a dilemma. Section 8 concludes by positioning the novel contribution of framing intersectional fairness as a genuine dilemma requiring fundamentally new interdisciplinary approaches.

**Flag for the Dilemma**: Existing literature—both in machine learning and philosophy—treats statistical and ontological problems separately. Technical work assumes groups are specified and focuses on statistical challenges. Philosophical work engages ontological debates without acknowledging statistical constraints those debates create. No existing work recognizes these as mutually exacerbating problems forming a dilemma where "solving" one horn worsens the other.
