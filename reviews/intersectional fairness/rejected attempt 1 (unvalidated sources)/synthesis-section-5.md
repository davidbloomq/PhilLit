# Section 5: Normative Frameworks and Fairness Trade-offs

Even if we could resolve ontological and measurement questions about which groups exist and how to operationalize them, normative questions would remain: what does fairness require across groups? Normative frameworks from political philosophy and ethics offer potential guidance—but these frameworks often presuppose knowing which groups matter, and different frameworks suggest different approaches to the statistical-ontological dilemma.

Satz et al. (2022) provide an important conceptual distinction between "fairness as a property of the algorithm used for prediction tasks" and "justice as a property of the allocation principle used for decision tasks." They argue that "fairness and justice are frequently conflated, with the consequence that distributive justice concerns are not addressed explicitly." Their analysis classifies "prioritarianism as an optimization-type theory of distributive justice alongside utilitarianism," where "prioritarianism's goal is to maximize aggregate utility for all groups, giving greater weight to utility the worse off the group."

This framing is helpful for intersectional fairness: it distinguishes predicting outcomes fairly from allocating resources justly. However, both fairness and justice as they define them require knowing which groups exist. To give "greater weight to utility the worse off the group," we must know which groups to compare. To ensure prediction algorithms are fair "across groups," we must specify the groups across which fairness is assessed. Their framework does not tell us how to determine this specification.

Parfit (1997) provides the canonical philosophical statement of prioritarianism in "Equality and Priority." Prioritarianism "gives greater moral weight to benefits to the worse-off" without requiring equality per se. Unlike egalitarianism (which cares about relative positions), prioritarianism cares about absolute levels with weighting—improvements for the worst-off matter more morally than equal improvements for the better-off. For algorithmic fairness, this suggests we should weight improvements in accuracy or fairness metrics more heavily for intersectional groups that are worst-off.

But which groups are worst-off? Determining this requires reliable measurements of group-level outcomes—precisely what Section 2 showed becomes statistically uncertain with many intersectional groups. Prioritarianism as a normative principle does not solve the statistical problem; rather, applying prioritarianism presupposes it is already solved (we must know group-level performance to identify worst-off groups). Moreover, determining which groups to include in the comparison reintroduces the ontological question: should we compare all possible intersectional groups? Only groups above some size threshold? Groups identified through community engagement?

Frankfurt (1987) develops sufficientarianism in "Equality as a Moral Ideal," arguing that "what matters morally is not equality but whether people have enough." Rather than requiring equal outcomes across groups, sufficientarianism requires that all groups achieve at least a threshold of adequate outcomes. Applied to algorithmic fairness, this suggests ensuring all groups have acceptable accuracy, calibration, or other fairness metrics—even if some groups have better performance than others.

Binns (2024) applies this framework explicitly to algorithms, proposing that "fairness criterion checking if approximately equal shares from all groups are above threshold accounts for structural injustices under sufficientarianism." This shifts focus from relative parity to absolute adequacy. Such a shift might seem to ameliorate the statistical problem: rather than comparing all pairs of groups (which grows quadratically with the number of groups), we only check if each group exceeds a threshold (which grows linearly). However, determining if small intersectional groups meet thresholds still requires adequate sample sizes for reliable estimation. Statistical uncertainty does not disappear; it manifests as uncertainty about whether groups truly meet thresholds rather than uncertainty about relative comparisons.

Furthermore, as Fourie and Rid (2018) argue, "appropriate principle reflecting obligations to desperately badly off cannot allow trade-offs between benefits to desperately bad off and merely bad off." If some intersectional groups fall far below thresholds while others barely miss them, sufficientarianism does not permit sacrificing the former to help the latter. This creates pressure to identify which groups are "desperately badly off"—returning us to the statistical problem of reliably estimating intersectional group performance.

Holm (2023) addresses the "leveling down" objection to egalitarian fairness in "Egalitarianism and Algorithmic Fairness." Classification parity criteria require "equality across groups with respect to performance measures such as error rates." Critics argue this leads to leveling down: achieving equality by worsening outcomes for advantaged groups without improving outcomes for disadvantaged groups. Holm "interprets the criticism as a form of leveling down objection" and "interprets the egalitarianism of classification parity as deontic egalitarianism," then presents an "egalitarian response to the leveling down objection."

His analysis reveals normative complexity: even within egalitarian frameworks, there are disputes about whether equality per se is valuable or whether it matters only instrumentally. These disputes affect what fairness requires. But notice what Holm's analysis presupposes: it compares "groups" without addressing which groups to include in the comparison. The leveling down debate concerns what to do given group-level performance differences—it does not address how to identify which groups those should be.

Mittelstadt et al. (2023) press the leveling down objection in "The Unfairness of Fair Machine Learning." They argue that "levelling down should be rejected in fair ML because it unnecessarily harms advantaged groups when performance is intrinsically valuable, demonstrates lack of equal concern for affected groups, and fails to meet aims of many viable distributive justice theories." Their critique challenges whether parity should be the goal. Yet their alternative—focusing on "aims of viable distributive justice theories"—still requires group specification. Different theories (prioritarian, sufficientarian, maximin) require knowing which groups exist and their relative positions.

Heidari et al. (2022) examine the "impossibility of fairness"—the mathematical incompatibility between different fairness definitions. They argue for moving "from formal to substantive algorithmic fairness" that accounts for social context rather than treating fairness as purely formal mathematical property. Their analysis suggests fairness requirements should vary by context, domain, and stakeholder values. This contextual approach might seem to dissolve the dilemma: in each context, stakeholders determine relevant groups. However, this pushes the problem to stakeholder engagement: which stakeholders? How to aggregate disagreements? And the statistical problem persists—stakeholder preferences for including many intersectional groups do not eliminate the statistical challenges of doing so.

Narayanan et al. (2022) propose a "justice-based framework for analyzing algorithmic fairness-utility trade-offs," noting that "trade-off between fairness and efficiency, and between public safety and prevailing algorithmic fairness notions, can be large in practice." Their framework aims to make trade-offs explicit and subject to normative evaluation rather than treating them as purely technical optimization problems. For intersectional fairness, relevant trade-offs include: (1) statistical reliability versus group inclusion (fewer groups → more reliable estimates; more groups → less reliable estimates), (2) measurement validity versus statistical feasibility (valid measurement of intersectionality suggests many emergent groups; statistical feasibility suggests fewer groups), (3) different groups' interests (resources spent improving fairness for one intersectional group cannot be spent on another).

Fazelpour and Danks (2021) argue that "reductionist representations of fairness often bear little resemblance to real-life fairness considerations, which are highly contextual," and "fairness metrics tend to be implemented within narrow toolkits that are difficult to integrate into broader ethical assessments." Their critique applies to group specification: reducing the question "which groups matter?" to a technical problem (which attribute combinations to consider?) fails to capture the full normative and contextual complexity. Yet their alternative—grounding fairness in "ethical philosophy and welfare economics"—requires answering which groups' welfare to consider, returning us to the specification problem.

Binns (2018) makes this explicit in "Fairness in Machine Learning: Lessons from Political Philosophy," arguing that "fairness metrics implicitly encode political-philosophical commitments" and that "different contexts require different normative frameworks, thus different fairness approaches." His analysis usefully makes implicit commitments explicit. However, the commitment to particular groups (which groups to assess fairness for) often remains implicit even when other commitments (which fairness metric to use) are made explicit. Political philosophy provides frameworks for evaluating fairness given groups; it provides less guidance on group specification itself.

Corbett-Davies and Goel (2018) observe in "The Measure and Mismeasure of Fairness" that "different metrics embody different normative commitments" and "no metric is universally appropriate—depends on normative goals and context." Their analysis shows that normative judgment cannot be eliminated from fairness assessment; it can only be made explicit and subjected to scrutiny. For intersectional fairness, this implies that choosing which groups to include embodies normative commitments that should be explicit and justifiable. Yet the literature on fairness metrics focuses on comparing metrics for given groups rather than on the prior question of group specification.

**Flag for the Dilemma**: Normative frameworks from political philosophy—prioritarianism, sufficientarianism, egalitarianism—offer valuable perspectives on what fairness requires across groups. However, applying these frameworks presupposes knowing which groups exist and having reliable information about their relative positions or absolute levels of well-being (however defined). Prioritarianism requires identifying worst-off groups; sufficientarianism requires checking if all groups meet thresholds; egalitarianism requires comparing group-level outcomes. Each presupposes the statistical problem is solved (reliable group-level estimates) and the ontological problem is solved (specification of which groups to include). Moreover, different normative frameworks might suggest different approaches to the statistical-ontological dilemma—but the normative literature does not recognize this dilemma as such. Philosophers debate which distributive principle is correct without acknowledging that some principles (e.g., prioritarianism focusing on worst-off) might be more or less statistically feasible depending on which groups are included. The normative questions are analyzed independently of the statistical and ontological constraints. Yet in practice, these questions interact: normative commitments affect which groups we think matter (ontological question), which affects how many groups we must reliably measure (statistical question), which may be infeasible, forcing compromise on normative ideals. This interaction goes unanalyzed in the normative literature.
