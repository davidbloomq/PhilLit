# Section 6: Epistemic Dimensions of Fairness

Intersectional fairness raises not only technical, ontological, measurement, and normative questions but also epistemic ones: What counts as adequate knowledge about group-level properties? When is it epistemically responsible to make claims about groups with sparse data? Who has authority to determine which groups matter? These questions connect to broader debates in epistemology about uncertainty, epistemic justice, and the ethics of inference.

Fricker (2007) provides the foundational framework in "Epistemic Injustice: Power and the Ethics of Knowing," distinguishing "testimonial injustice" (credibility deficit due to prejudice) and "hermeneutical injustice" (gap in collective interpretive resources). Barabas et al. (2025) extend this framework to AI systems, developing a "taxonomy mapping testimonial, hermeneutical, and distributive injustices onto four AI development stages: data collection, model training, inference, and dissemination." They argue that "algorithmic systems can be vectors of epistemic injustice—excluding marginalized groups from processes that define, judge, and make decisions about them."

For intersectional fairness, this analysis suggests that determining which groups matter is itself an epistemic question with political dimensions. If intersectional groups (e.g., disabled Black women, elderly transgender Latinx individuals) are excluded from the set of groups for which fairness is assessed, this constitutes epistemic injustice: their experiences are not given credence, and they lack representation in the interpretive frameworks used to understand fairness. Yet including all possible intersectional groups creates the statistical problem documented in Section 2. The epistemic injustice framework reveals the ethical stakes of the statistical-ontological dilemma but does not resolve it.

Anderson (2012) argues in "Epistemic Justice as a Virtue of Social Institutions" that "institutions can systematically advantage or disadvantage different knowers." Algorithmic systems are institutions in this sense—they embody particular epistemic practices, privilege certain forms of knowledge, and systematically advantage or disadvantage different groups as knowers and as subjects of knowledge. When systems determine which groups to assess fairness for based on available data or conventional demographic categories, they make epistemic choices that advantage groups for whom data are plentiful and disadvantage groups for whom data are sparse—often the very intersectional groups most in need of fairness protections.

D'Ignazio and Klein (2020) develop "data feminism," arguing that "data practices systematically privilege certain perspectives while marginalizing others" and that "without diverse perspectives across AI ecosystem, ML advances will fuel epistemic injustice." Their analysis emphasizes that data collection choices are not neutral but embody power relations. Decisions about which demographic categories to record, how to operationalize them, and which intersections to track all reflect and reinforce existing power structures. For intersectional fairness, this suggests that data infrastructure shapes which groups can be analyzed—a point connecting to the measurement literature (Section 4) and revealing epistemic dimensions of measurement choices.

Kalluri (2020) makes this power dimension explicit, arguing that we should "not ask if artificial intelligence is good or fair, ask how it shifts power." She contends that "epistemic authority over group categories is a form of power" and that "fairness debates are partly epistemic disagreements about group ontology." Who has power to define which groups exist and matter? In current practice, this power often lies with system designers, data collectors, and researchers—not necessarily with the communities being categorized. Yet those communities may have crucial epistemic resources (lived experience, community knowledge) that external categorizations miss.

Harding (2004) develops standpoint epistemology, arguing that "certain social positions—such as gender, race, or class—render particular epistemological perspectives" and that "science conducted by Black women would not contain the same knowledge as science created by white men." Applied to algorithmic fairness, this suggests that determining which intersectional groups matter is not a neutral technical question but one where different social positions provide different epistemic access. Black women may have epistemic resources for identifying which intersectional groups face distinctive discrimination that others lack. Yet incorporating such standpoint epistemology into algorithmic systems raises difficult questions about how to institutionalize respect for multiple, sometimes conflicting, standpoints.

Hüllermeier and Waegeman (2021) provide a technical epistemological framework distinguishing "aleatoric (irreducible probabilistic variability)" from "epistemic (reducible through more knowledge)" uncertainty. Epistemic uncertainty is "also known as systematic uncertainty—things one could in principle know but does not in practice." For intersectional groups, both types of uncertainty are present. Aleatoric uncertainty arises from genuinely stochastic processes (individual outcomes vary even within groups). Epistemic uncertainty arises from "limited sample size" (if we had more data about a group, we would have more certainty about its properties).

Critically, Hüllermeier and Waegeman note that "epistemic uncertainty reflects uncertainty about estimates due to limited sample size" and "allows disentangling task-inherent (aleatoric) from sampling (epistemic) effects." For small intersectional groups, epistemic uncertainty dominates: our uncertainty about group properties is primarily due to sparse data rather than inherent variability. This is the epistemic manifestation of the statistical problem documented in Section 2. What Hüllermeier and Waegeman do not address is the ontological source of this epistemic uncertainty: we have sparse data for intersectional groups partly because we have many such groups (exponential growth with attributes), and we have many groups partly because ontological considerations (Section 3) suggest intersectionality involves numerous emergent, context-dependent groups.

Bhatt et al. (2021) argue that "uncertainty quantification is crucial for responsible AI" and that "making uncertainty visible enables more epistemically responsible inferences." They contend this is "particularly important for claims about small groups with high epistemic uncertainty" and that "failing to communicate uncertainty can mislead about reliability of group-level inferences." Their framework suggests that fairness assessments should report uncertainty: not just "Group X has error rate Y" but "Group X has error rate Y with confidence interval [a, b]." For small intersectional groups, these confidence intervals may be wide—communicating the epistemic limitations of what we know.

However, uncertainty communication does not resolve the dilemma. Reporting wide confidence intervals for intersectional groups makes explicit the statistical problem but does not solve it. Stakeholders still face the question: should we make fairness decisions based on uncertain information about many intersectional groups, or constrain our analysis to groups about which we can be more certain (and thereby exclude some intersectional groups from consideration)?

Hullman (2021) distinguishes "epistemic uncertainty (lack of knowledge)" from "agonistic uncertainty (disagreement about values/priorities)." Both are present in intersectional fairness. Epistemic uncertainty concerns what group-level properties are (a matter for statistical estimation). Agonistic uncertainty concerns which groups matter (a matter of normative and political disagreement). Hullman argues these require different responses: epistemic uncertainty can potentially be reduced through more data and better methods; agonistic uncertainty requires negotiation among stakeholders with competing values. For intersectional fairness, the dilemma involves both: there is epistemic uncertainty about intersectional group properties (statistical problem) and agonistic uncertainty about which groups warrant consideration (ontological/normative problem). Treating these as independent—reducing epistemic uncertainty without addressing agonistic uncertainty, or resolving value disagreements without accounting for statistical constraints—fails to recognize their interaction.

Fazelpour and Danks (2020) examine "how data science practices can perpetuate epistemic injustice," showing that "choices about data collection, categories, modeling approaches can systematically privilege some knowers/perspectives while disadvantaging others." For intersectional fairness, epistemic injustice can occur at multiple levels: (1) excluding intersectional groups from fairness analysis perpetuates injustice by rendering their experiences epistemically invisible, (2) including them with inadequate data risks making unreliable inferences that could mischaracterize their experiences or justify harmful decisions, (3) requiring certainty before acting on behalf of intersectional groups privileges epistemic caution over epistemic justice. The dilemma creates an epistemic double-bind: both including and excluding intersectional groups with sparse data risk epistemic injustice, albeit of different kinds.

Critical scholarship documents cases where systems make overconfident inferences about marginalized groups. Eubanks (2018) shows how "automated systems make inferences about marginalized groups with insufficient epistemic basis," claiming "certainty about group properties based on sparse, biased data." O'Neill (2016) documents "cases where algorithmic systems make high-stakes inferences about groups with insufficient data and without appropriate uncertainty quantification," leading to "overconfident, often harmful, inferences about marginalized groups." Benjamin (2019) and Noble (2018) show how "technologies encode assumptions about racial and other social groups," with "inferences about groups involving epistemic choices about which group properties are real/meaningful" often made "without adequate epistemic basis or input from affected communities."

These cases reveal a pattern: systems frequently treat the statistical problem as solved when it is not (making confident inferences despite sparse data) and treat the ontological problem as solved when it is not (using available demographic categories without interrogating whether they capture meaningful groups). The epistemic irresponsibility is compounded: unwarranted confidence about poorly-grounded group categories.

Gebru et al. (2021) propose "datasheets for datasets" as a transparency mechanism, arguing that "part of epistemic responsibility in using data is documenting how categories are defined, measured, and validated." This is "particularly important for demographic categories used in fairness assessment" because "without clear epistemic grounding, unclear what inferences about groups are warranted." Their proposal addresses the measurement dimension (Section 4) from an epistemic perspective: we should be epistemically explicit about our category definitions and their limitations.

Raji et al. (2020) extend this to algorithmic auditing, proposing a framework that includes "epistemic requirements" such that "audits must account for uncertainty, especially for small/intersectional groups" and "making claims about fairness for groups requires epistemic warrant—adequate data, valid measurements, appropriate statistical methods accounting for uncertainty." Their framework recognizes that fairness audits make epistemic claims (this group experiences bias, that group does not) that require epistemic justification. For intersectional groups, providing such warrant becomes increasingly difficult as groups proliferate and data per group thins.

**Flag for the Dilemma**: The epistemology literature reveals multiple epistemic dimensions of intersectional fairness: the distinction between epistemic and aleatoric uncertainty (Hüllermeier and Waegeman 2021), the role of epistemic injustice when groups are excluded from analysis (Fricker 2007; Anderson 2012), the importance of standpoint epistemology suggesting different groups have different epistemic access (Harding 2004), the power dynamics in determining group categories (Kalluri 2020; D'Ignazio and Klein 2020), and the epistemic responsibilities in making inferences under uncertainty (Bhatt et al. 2021; Raji et al. 2020). However, this literature does not connect epistemic uncertainty to the statistical and ontological sources documented in earlier sections. Epistemologists analyze uncertainty about group properties without fully engaging with the statistical problem (many groups → sparse data → high epistemic uncertainty) or the ontological problem (contested group definitions → uncertainty about which groups exist → uncertainty about which groups to reduce epistemic uncertainty for). The epistemic dimensions are analyzed largely independently of the statistical constraints and ontological debates, missing the interaction that creates the dilemma. Moreover, frameworks for epistemic justice (which suggests including marginalized intersectional groups) do not grapple with statistical limitations (which make reliable inferences about numerous groups infeasible), while frameworks for epistemic responsibility (which require adequate warrant for inferences) do not fully address the ontological question (adequate warrant for which groups?). The dilemma persists: epistemic justice considerations push toward including many intersectional groups, epistemic responsibility requires statistical reliability, and these pull in opposite directions when groups are numerous and data are sparse.
