# Section 4: Measurement and Operationalization

The transition from ontological debates to algorithmic implementation requires measurement—operationalizing theoretical constructs through observable variables. Measurement theory reveals that this transition is not neutral: every operationalization embeds ontological commitments, and failures of construct validity can undermine both fairness and accuracy.

Jacobs and Wallach (2021) provide the foundational analysis in their influential paper "Measurement and Fairness." They propose "measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems." Their central insight is that "computational systems often involve unobservable theoretical constructs such as socioeconomic status, teacher effectiveness, and risk of recidivism, which cannot be measured directly and must instead be inferred from measurements of observable properties through operationalization via a measurement model." This operationalization "necessarily involves making assumptions and introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization."

Critically for intersectional fairness, Jacobs and Wallach argue that "many of the harms discussed in fairness literature are direct results of such mismatches." When systems claim to measure a construct but actually measure something else, downstream fairness interventions may address the wrong problem. Their "fairness-oriented conceptualizations of construct reliability and construct validity" provide tools for "making explicit and testing assumptions about constructs and their operationalizations."

Most relevantly for the dilemma at hand, Jacobs and Wallach argue that "fairness itself is an essentially contested construct that has different theoretical understandings in different contexts." They contend that "debates that appear to be about different operationalizations are actually debates about different theoretical understandings of fairness." This analysis comes remarkably close to recognizing the dilemma—they identify that what appears to be a technical question (which fairness metric?) is actually a theoretical question (what is fairness?). However, they do not extend this insight to the question of groups: debates about which groups to include in fairness analysis are not merely technical (which attributes to combine?) but theoretical (what makes something a group warranting fairness consideration?). And crucially, they do not connect the theoretical contestedness to the statistical problem: the fact that different theoretical understandings suggest including different groups, which has direct statistical consequences for feasibility.

Scheuerman et al. (2021) examine "how demographic categories are operationalized in facial analysis datasets," finding "wide variation in how race and gender are defined, measured, and annotated." Their systematic analysis shows that "measurement choices reflect different (often implicit) ontological commitments about what these categories are." When one dataset operationalizes gender through self-identification, another through binary appearance-based coding, and a third through medical records of sex assigned at birth, each embeds different assumptions about what gender is. These choices directly affect which intersectional groups can be analyzed: a dataset with binary gender and four racial categories enables analysis of 8 intersectional groups; one with non-binary gender and six racial categories enables analysis of different groups entirely; one that records gender identity, gender presentation, and sex assigned at birth separately enables exponentially more intersectional combinations.

The problem extends beyond demographic categories. Obermeyer et al. (2019) document a landmark case of construct validity failure in a healthcare algorithm used to manage the health of populations. The algorithm used "healthcare costs as a proxy for health needs," but costs "systematically differ by race even for the same health needs" due to differential access to care. This created racial bias not because the algorithm explicitly discriminated but because the operationalization of the theoretical construct (health needs) through the measurement (healthcare costs) introduced systematic error. Intersectional groups may face multiple sources of such measurement bias, compounding the problem.

Selbst et al. (2019) identify "abstraction traps" where "technical fairness interventions fail due to mismatches between abstract formal models and concrete social contexts." Their framework is relevant to measurement: fairness metrics are abstract mathematical objects; demographic categories are socially complex, context-dependent phenomena. The abstraction required for measurement necessarily simplifies this complexity. As they show, "fairness metrics may be valid in abstract but invalid when applied to real contexts with all their social complexity." For intersectional groups, multiple layers of abstraction (operationalizing race, operationalizing gender, operationalizing their intersection) create multiple opportunities for validity failure.

Blodgett et al. (2020) provide a critical analysis of how "bias" is operationalized in NLP research, finding that "many operationalizations have questionable construct validity—unclear what they actually measure." Their analysis extends to fairness research more broadly: researchers often adopt existing operationalizations without examining whether they validly measure the theoretical constructs of interest. For intersectionality, this means that operationalizing intersectional groups through attribute combinations may fail to capture what intersectionality theoretically signifies (non-additive, emergent, context-dependent effects).

Hu et al. (2023) show that "the validity of fairness auditing hinges on the reliability of the demographic attribute inference process," and "improved reliability leads to less biased and lower-variance estimates of fairness." Their focus on "reliable demographic inference for fairness in face analysis" highlights that measurement error in demographic categories—before any fairness analysis begins—can undermine fairness evaluation. For intersectional groups, measurement error propagates: if gender is misclassified at rate ε₁ and race at rate ε₂, intersectional categories (Black women) will have compounded error. This measurement uncertainty interacts with the statistical uncertainty from small samples discussed in Section 2.

Mayson (2019) analyzes risk assessment instruments used in criminal justice, arguing that "many instruments have poor construct validity—they measure correlates of outcomes rather than theoretically meaningful constructs." Moreover, she shows that "label bias compromises validity of both accuracy and fairness metrics." When the outcome being predicted (e.g., arrest) is itself a biased measure of the theoretical construct of interest (e.g., criminal behavior), then optimizing for accuracy or fairness with respect to that outcome may perpetuate rather than correct injustice. For intersectional groups experiencing multiple forms of bias in outcome measurement, this problem is magnified.

Barocas et al. (2020) examine "hidden assumptions in fairness methods," including assumptions about measurement. They show that "counterfactual fairness approaches assume the ability to intervene on demographic attributes," but "such interventions may be conceptually incoherent if attributes are constitutively socially constructed." This connects measurement to the social ontology debates in Section 3: if demographic categories are practice-based rather than attribute-based, then intervening on attributes (as counterfactual fairness requires) may not capture interventions on socially constructed group membership. The measurement framework presupposes an ontology—but which ontology?

Geiger et al. (2020) document a fundamental transparency problem: machine learning papers often fail to report "where human-labeled training data comes from" and how categories were defined and operationalized. Their finding that "without clear construct definitions and validity assessment, unclear what categories actually capture" is particularly troubling for intersectional fairness. If training data category definitions are opaque, then claims about fairness for intersectional groups rest on unstable foundations.

Hellman (2020) offers a legal scholar's perspective on "measuring algorithmic fairness," arguing that "many metrics fail to capture normatively significant aspects of fairness." Her analysis suggests a measurement validity problem: "metrics may reliably measure something, but not fairness as normatively understood." For intersectional groups, this creates a dual challenge. First, measuring which groups exist requires valid operationalization of group membership. Second, measuring fairness for those groups requires metrics that capture normatively significant inequalities. Both requirements involve theoretical and normative judgment that cannot be fully captured in measurement protocols.

Green (2020) argues that "risk assessment instruments have fundamental epistemic limitations" and "cannot reliably measure constructs they purport to measure." He contends that "error metrics are poor proxies for individual equity or social well-being." This echoes a theme across the measurement literature: what we can measure is often a poor proxy for what we care about. For intersectional fairness, what we can measure (performance differences across groups defined by observable attributes) may poorly proxy for what we care about (substantive justice for intersectional communities as they understand themselves).

**Flag for the Dilemma**: Measurement theory reveals that operationalizing groups for fairness analysis embeds ontological commitments (Scheuerman et al. 2021; Jacobs and Wallach 2021). Different operationalizations of demographic categories enable analysis of different intersectional groups. Construct validity requires that operationalizations align with theoretical understandings of constructs—but as Section 3 showed, there are competing theoretical understandings of intersectional groups. Jacobs and Wallach (2021) recognize that fairness is an essentially contested construct with different theoretical understandings in different contexts, and that apparent debates about operationalizations are often deeper debates about theoretical understandings. However, they do not apply this insight to the group specification problem: which groups to include in fairness analysis is also a contested question with different theoretical answers. More critically, the measurement literature does not acknowledge the statistical consequences of measurement choices: including more granular group categories (to capture more intersectional diversity) exacerbates the statistical problem of sparse data per group. Measurement theorists analyze construct validity independently of statistical feasibility, while statisticians analyze inference feasibility independently of measurement validity. The interaction between these requirements—that valid measurement of intersectionality suggests many groups, but statistical reliability requires fewer—goes unrecognized.
