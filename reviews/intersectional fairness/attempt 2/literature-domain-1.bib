@comment{Domain 1: Algorithmic Fairness and Intersectionality (ML/CS Literature)}
@comment{Focus: Intersectional fairness metrics, handling sparse data in fairness auditing,}
@comment{fairness gerrymandering, multi-group fairness constraints, practical implementations}
@comment{Target: 2020-2025 ML/CS literature from FAccT, NeurIPS, ICML, AAAI}

@inproceedings{GoharCheng2023,
  title = {A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges},
  author = {Gohar, Usman and Cheng, Lu},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI 2023)},
  year = {2023},
  pages = {6619--6627},
  doi = {10.24963/ijcai.2023/742},
  note = {Comprehensive survey covering intersectional bias (multiple sensitive attributes together), taxonomy of fairness notions, mitigation strategies, and key research challenges. Identifies data sparsity in smaller intersectional subgroups as a major challenge.}
}

@article{Sheng2025,
  title = {Toward Unifying Group Fairness Evaluation from a Sparsity Perspective},
  author = {Sheng, Zhecheng and Zhang, Jiawei and Diao, Enmao},
  journal = {arXiv preprint arXiv:2511.00359},
  year = {2025},
  doi = {10.48550/arXiv.2511.00359},
  note = {Proposes unified sparsity-based framework for evaluating algorithmic fairness. Connects various sparsity measures to fairness promotion. Demonstrates applicability across multi-group, multi-class, and regression problems. Addresses exponential growth of subgroups with finite samples.}
}

@article{Maheshwari2024,
  title = {Synthetic Data Generation for Intersectional Fairness by Leveraging Hierarchical Group Structure},
  author = {Maheshwari, Gaurav and Bellet, Aurélien and Denis, Pascal and Keller, Mikaela},
  journal = {arXiv preprint arXiv:2405.14521},
  year = {2024},
  doi = {10.48550/arXiv.2405.14521},
  note = {Data augmentation technique for intersectional fairness. Leverages hierarchical structure by viewing groups as intersections of parent categories. Learns transformation functions to combine parent group data for smaller intersectional groups. Demonstrates superior intersectional fairness and robustness to leveling down across text and image datasets.}
}

@inproceedings{Halevy2025,
  title = {Who's the (Multi-)Fairest of Them All: Rethinking Interpolation-Based Data Augmentation Through the Lens of Multicalibration},
  author = {Halevy, Karina and Hou, Karly and Badrinath, Charumathi},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2025},
  doi = {10.48550/arXiv.2412.10575},
  note = {Challenges Fair Mixup data augmentation. Tests four Fair Mixup versions on problems with up to 81 marginalized groups. Finds Fair Mixup typically worsens performance and fairness metrics. Standard Mixup combined with multicalibration post-processing most effective, especially for small minority groups.}
}

@article{Wastvedt2024,
  title = {An Intersectional Framework for Counterfactual Fairness in Risk Prediction},
  author = {Wastvedt, Solvejg and Huling, Jared D. and Wolfson, Julian},
  journal = {Biostatistics},
  volume = {25},
  number = {3},
  pages = {702--717},
  year = {2024},
  doi = {10.1093/biostatistics/kxad021},
  note = {Presents novel unfairness metrics valid when risk scores guide treatment. Accounts for intersecting forms of discrimination. Complete framework for estimation and inference addressing intersectional fairness in clinical risk prediction. Uses causal inference techniques to simulate hypothetical outcomes under no treatment for fair baseline comparison.}
}

@article{Dutta2024,
  title = {Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions},
  author = {Ramachandranpillai, Resmi and Dutta, Subrata and others},
  journal = {arXiv preprint arXiv:2412.00606},
  year = {2024},
  doi = {10.48550/arXiv.2412.00606},
  note = {Addresses intersectional biases in clinical decision tasks. Demonstrates that fairness challenge becomes more pronounced in multimodal settings as fairness values fluctuate with different modality information. Focuses on identifying and mitigating intersectional bias in multimodal contexts.}
}

@inproceedings{Foulds2020,
  title = {An Intersectional Definition of Fairness},
  author = {Foulds, James R. and Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei},
  booktitle = {2020 IEEE 36th International Conference on Data Engineering (ICDE)},
  year = {2020},
  doi = {10.1109/ICDE48307.2020.00203},
  note = {Proposes differential fairness, a multi-attribute definition of fairness informed by intersectionality. Shows that fairness on individual attributes does not guarantee intersectional fairness. Provides learning algorithm with economic, privacy, and generalization guarantees.}
}

@article{Lett2025,
  title = {Intersectional and Marginal Debiasing in Prediction Models for Emergency Admissions},
  author = {Lett, Elle and Shahbandegan, Shakiba and Barak-Corren, Yuval and Fine, Andrew M. and La Cava, William G.},
  journal = {JAMA Network Open},
  volume = {8},
  number = {5},
  year = {2025},
  doi = {10.1001/jamanetworkopen.2025.15308},
  note = {Compares intersectional vs. marginal debiasing approaches. Finds intersectional debiasing produces greater reductions in subgroup calibration error (MIMIC-IV: 21.2 percent; BCH: 27.2 percent) compared to marginal debiasing (MIMIC-IV: 10.6 percent; BCH: 22.7 percent). Shows 5.7-11.1 percent additional reduction in calibration errors and 4.5 percent reduction in false-negative rates.}
}

@article{Castelnovo2022,
  title = {A Clarification of the Nuances in the Fairness Metrics Landscape},
  author = {Castelnovo, Alessandro and Crupi, Riccardo and Greco, Greta and Regoli, Daniele and Penco, Ilaria Giuseppina and Cosentini, Andrea Claudio},
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {4209},
  year = {2022},
  doi = {10.1038/s41598-022-07939-1},
  note = {Discusses how conditional demographic parity theoretically solves intersectional bias but faces computational and practical issues due to exponential increase of subgroups when adding sensitive features. Assessing group fairness with multiple sensitive attributes may be unfeasible in most practical cases. Presence of many sensitive features represents huge problem that fairness ML literature has barely begun to address.}
}

@article{Ghassemi2024,
  title = {Auditing and Enforcing Conditional Fairness via Optimal Transport},
  author = {Ghassemi, Mohsen and Mishler, Alan and Dalmasso, Niccolò and Zhang, Luhao and Potluru, Vamsi K. and Balch, Tucker and Veloso, Manuela},
  journal = {arXiv preprint arXiv:2410.14029},
  year = {2024},
  doi = {10.48550/arXiv.2410.14029},
  note = {Addresses challenges with conditional demographic parity being harder to achieve than demographic parity, particularly when conditioning variable has many levels or model outputs are continuous. Auditing and enforcing CDP remains understudied in the literature. Proposes FairBiT and FairLeap methods using optimal transport distances.}
}

@article{Hashimoto2025,
  title = {Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces},
  author = {Hashimoto, Kazuki and others},
  journal = {arXiv preprint arXiv:2502.11828},
  year = {2025},
  doi = {10.48550/arXiv.2502.11828},
  note = {Develops methods to solve multi-objective RL problems with possibly exponentially large class of constraints over intersecting groups. Addresses both tabular and large state space MDPs in oracle-efficient manner. Demonstrates feasibility of intersectional fairness in complex RL settings.}
}

@article{Yan2024,
  title = {Intersectional Fair Ranking via Subgroup Divergence},
  author = {Pastor, Eliana and Bonchi, Francesco},
  journal = {Data Mining and Knowledge Discovery},
  year = {2024},
  doi = {10.1007/s10618-024-01029-8},
  note = {Leverages divergence to automatically identify which subgroups (defined as combinations of known protected attributes) show statistically significant deviation in ranking utility compared to overall population. Main challenges include exponential increase in subgroups with added sensitive features and empty/sparse subgroups with finite samples.}
}

@inproceedings{Yang2020,
  title = {Fairness with Overlapping Groups},
  author = {Yang, Forest and Cisse, Moustapha and Koyejo, Oluwasanmi O.},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2020},
  note = {Addresses fairness when groups overlap. Relevant to intersectional contexts where individuals belong to multiple groups simultaneously. Provides theoretical and algorithmic contributions for handling overlapping group memberships in fairness constraints.}
}

@inproceedings{HebertJohnson2018,
  title = {Multicalibration: Calibration for the (Computationally-Identifiable) Masses},
  author = {Hébert-Johnson, Ursula and Kim, Michael P. and Reingold, Omer and Rothblum, Guy N.},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {1939--1948},
  year = {2018},
  publisher = {PMLR},
  note = {Introduces multicalibration requiring predictions be well-calibrated simultaneously over rich collection of subpopulations. Sample complexity grows exponentially with number of class labels. Finer-grained groups require more data. Algorithm runtime inversely proportional to size of smallest group. Foundational work for multi-group fairness.}
}

@inproceedings{LaCava2023,
  title = {Fair Admission Risk Prediction with Proportional Multicalibration},
  author = {La Cava, William G. and Lett, Elle and Wan, Guangya},
  booktitle = {Proceedings of the Conference on Health, Inference, and Learning},
  series = {Proceedings of Machine Learning Research},
  volume = {209},
  pages = {350--378},
  year = {2023},
  publisher = {PMLR},
  note = {Introduces proportional multicalibration (PMC) variant requiring proportion of calibration error within each bin and group to be small. Constrains percent calibration error among groups and within prediction bins. Promising criteria for controlling simultaneous calibration fairness over intersectional groups with virtually no classification performance cost. Best Paper Award.}
}

@inproceedings{Kim2019,
  title = {Multiaccuracy: Black-Box Post-Processing for Fairness in Classification},
  author = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
  year = {2019},
  pages = {247--254},
  doi = {10.1145/3306618.3314287},
  note = {Framework for multiaccuracy auditing and post-processing ensuring accurate predictions across identifiable subgroups. Works with black-box access to predictor and relatively small labeled dataset for auditing. Relevant to intersectional fairness through subgroup-level accuracy guarantees.}
}

@article{Hansen2024,
  title = {When is Multicalibration Post-Processing Necessary?},
  author = {Hansen, Dutch and Devic, Siddartha and Nakkiran, Preetum and Sharan, Vatsal},
  journal = {arXiv preprint arXiv:2406.06487},
  year = {2024},
  doi = {10.48550/arXiv.2406.06487},
  note = {Examines conditions under which multicalibration post-processing is necessary. Addresses practical limitations: attaining multicalibration practically infeasible with more than a few classes. Discusses data requirements and trade-offs in multi-group calibration approaches.}
}
