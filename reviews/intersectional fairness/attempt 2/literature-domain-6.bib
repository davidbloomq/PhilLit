@comment{Domain 6: Applied Epistemology}
@comment{Focus: Reasoning under uncertainty about group-level properties, epistemic justice in data collection,}
@comment{epistemic responsibility when making claims about small groups, statistical inference ethics}

@book{Fricker2007,
  title = {Epistemic Injustice: Power and the Ethics of Knowing},
  author = {Fricker, Miranda},
  year = {2007},
  publisher = {Oxford University Press},
  isbn = {9780198237907},
  note = {Foundational work on epistemic injustice. Distinguishes testimonial injustice (credibility deficit due to prejudice) and hermeneutical injustice (gap in collective interpretive resources). Algorithmic systems can be vectors of epistemic injustice - excluding marginalized groups from processes that define, judge, and decide about them. Relevant to who gets to determine which groups matter in fairness analysis.}
}

@article{Anderson2012,
  title = {Epistemic Justice as a Virtue of Social Institutions},
  author = {Anderson, Elizabeth},
  journal = {Social Epistemology},
  volume = {26},
  number = {2},
  pages = {163--173},
  year = {2012},
  doi = {10.1080/02691728.2011.652211},
  note = {Extends epistemic justice to social institutions. Institutions can systematically advantage or disadvantage different knowers. Relevant to algorithmic fairness: systems are institutions that can promote or undermine epistemic justice through how they categorize groups, collect data, make inferences.}
}

@book{DOgnazio2020,
  title = {Data Feminism},
  author = {D'Ignazio, Catherine and Klein, Lauren F.},
  year = {2020},
  publisher = {MIT Press},
  isbn = {9780262044004},
  note = {Argues data practices systematically privilege certain perspectives while marginalizing others. Data collection and use raise moral and epistemic issues. Without diverse perspectives across AI ecosystem, ML advances will fuel epistemic injustice. Directly relevant to epistemic dimensions of data collection for fairness.}
}

@inproceedings{Kay2024,
  title = {Epistemic Injustice in Generative AI: A Pipeline Taxonomy, Empirical Hypotheses, and Stage-Matched Governance},
  author = {Kay, Jackie and Kasirzadeh, Atoosa and Mohamed, Shakir},
  booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
  pages = {684--697},
  year = {2024},
  doi = {10.48550/arXiv.2408.11441},
  note = {Taxonomy mapping testimonial, hermeneutical, and distributive injustices onto four AI development stages: data collection, model training, inference, dissemination. Systems are not only products of biased data but of institutional and epistemic frameworks favoring surveillance, control, efficiency over equity and care. Published at AIES 2024.}
}

@article{Taylor2016,
  title = {The Ethics of Big Data as a Public Good: Which Public? Whose Good?},
  author = {Taylor, Linnet},
  journal = {Philosophical Transactions of the Royal Society A},
  volume = {374},
  number = {2083},
  year = {2016},
  doi = {10.1098/rsta.2016.0126},
  note = {Questions whose knowledge and whose good are served by data practices. Data collection involves epistemic choices about what to measure, how to categorize - choices that advantage some groups' perspectives while disadvantaging others. Relevant to ontological and epistemic dimensions of specifying groups.}
}

@article{Hüllermeier2021,
  title = {Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
  author = {Hüllermeier, Eyke and Waegeman, Willem},
  journal = {Machine Learning},
  volume = {110},
  pages = {457--506},
  year = {2021},
  doi = {10.1007/s10994-021-05946-3},
  note = {Distinction between aleatoric (irreducible probabilistic variability) and epistemic (reducible through more knowledge) uncertainty is major importance in ML. Epistemic uncertainty known as systematic uncertainty - things one could in principle know but does not in practice. Epistemic uncertainty reflects uncertainty about estimates due to limited sample size. Allows disentangling task-inherent (aleatoric) from sampling (epistemic) effects. What counts as each type depends on level of model simplification and context.}
}

@inproceedings{Bhatt2021,
  title = {Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty},
  author = {Bhatt, Umang and Antorán, Javier and Zhang, Yunfeng and Liao, Q. Vera and Sattigeri, Prasanna and Fogliato, Riccardo and Melançon, Gabrielle and Krishnan, Ranganath and Stanley, Jason and Tickoo, Omesh and others},
  booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages = {401--413},
  year = {2021},
  note = {Argues uncertainty quantification crucial for responsible AI. Making uncertainty visible enables more epistemically responsible inferences. Particularly important for claims about small groups with high epistemic uncertainty. Failing to communicate uncertainty can mislead about reliability of group-level inferences.}
}

@incollection{SEPAlgorithmicFairness2025,
  title = {Algorithmic Fairness},
  author = {Hellman, Deborah},
  booktitle = {The Stanford Encyclopedia of Philosophy},
  editor = {Zalta, Edward N.},
  year = {2025},
  publisher = {Metaphysics Research Lab, Stanford University},
  howpublished = {\url{https://plato.stanford.edu/entries/algorithmic-fairness/}},
  note = {Section 4 focuses on moral and epistemic issues relating to data collection and use in algorithmic fairness. Fairness metrics offer formalized, quantifiable approaches to evaluating bias, but often lack normative grounding necessary for restorative or justice-oriented correction. Overview of epistemic challenges in fairness including construct validity, measurement, and categorization.}
}

@article{Fazelpour2023,
  title = {Algorithmic Fairness and the Situated Dynamics of Justice},
  author = {Fazelpour, Sina and Lipton, Zachary C.},
  journal = {Canadian Journal of Philosophy},
  volume = {52},
  number = {1},
  pages = {44--60},
  year = {2023},
  doi = {10.1017/can.2022.38},
  note = {Argues fairness must be understood in situated context rather than abstract. Different contexts involve different epistemic positions, power dynamics, and justice considerations. What counts as epistemically responsible inference about groups depends on social and political context.}
}

@book{Eubanks2018,
  title = {Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor},
  author = {Eubanks, Virginia},
  year = {2018},
  publisher = {St. Martin's Press},
  isbn = {9781250074317},
  note = {Documents how automated systems make inferences about marginalized groups with insufficient epistemic basis. Systems claim certainty about group properties based on sparse, biased data. Epistemic irresponsibility compounds material harms. Relevant to epistemic requirements for making claims about intersectional groups.}
}

@book{Benjamin2019,
  title = {Race After Technology: Abolitionist Tools for the New Jim Code},
  author = {Benjamin, Ruha},
  year = {2019},
  publisher = {Polity Press},
  isbn = {9781509526437},
  note = {Examines how technologies encode assumptions about racial and other social groups. Making inferences about groups involves epistemic choices about which group properties are real/meaningful. These choices often made without adequate epistemic basis or input from affected communities. Epistemic injustice in group categorization.}
}

@book{Noble2018,
  title = {Algorithms of Oppression: How Search Engines Reinforce Racism},
  author = {Noble, Safiya Umoja},
  year = {2018},
  publisher = {NYU Press},
  isbn = {9781479837243},
  note = {Shows how algorithmic systems make and perpetuate epistemic claims about groups (especially intersectional groups like Black women) without adequate basis. Systems both reflect and produce knowledge about groups - epistemic and political dimensions intertwined.}
}

@book{ONeill2016,
  title = {Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy},
  author = {O'Neil, Cathy},
  year = {2016},
  publisher = {Crown},
  isbn = {9780553418811},
  note = {Documents cases where algorithmic systems make high-stakes inferences about groups with insufficient data and without appropriate uncertainty quantification. Failure to acknowledge epistemic limitations leads to overconfident, often harmful, inferences about marginalized groups.}
}

@article{Selbst2018,
  title = {Disparate Impact in Big Data Policing},
  author = {Selbst, Andrew D.},
  journal = {Georgia Law Review},
  volume = {52},
  pages = {109--195},
  year = {2018},
  note = {Analyzes epistemic assumptions in algorithmic systems used in policing. Systems make inferences about group-level properties (crime risk by demographic group) but often lack adequate epistemic warrant. Statistical uncertainty not properly accounted for, especially for intersectional groups with sparse data.}
}

@article{Gebru2021,
  title = {Datasheets for Datasets},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé III, Hal and Crawford, Kate},
  journal = {Communications of the ACM},
  volume = {64},
  number = {12},
  pages = {86--92},
  year = {2021},
  doi = {10.1145/3458723},
  note = {Proposes transparency framework for datasets. Part of epistemic responsibility in using data is documenting how categories defined, measured, and validated. Particularly important for demographic categories used in fairness assessment. Without clear epistemic grounding, unclear what inferences about groups are warranted.}
}

@inproceedings{Raji2020,
  title = {Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing},
  author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N. and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages = {33--44},
  year = {2020},
  note = {Proposes framework for algorithmic auditing including epistemic requirements. Audits must account for uncertainty, especially for small/intersectional groups. Making claims about fairness for groups requires epistemic warrant - adequate data, valid measurements, appropriate statistical methods accounting for uncertainty.}
}

@article{Kalluri2020,
  title = {Don't Ask If Artificial Intelligence Is Good or Fair, Ask How It Shifts Power},
  author = {Kalluri, Pratyusha},
  journal = {Nature},
  volume = {583},
  pages = {169},
  year = {2020},
  doi = {10.1038/d41586-020-02003-2},
  note = {Argues focus on fairness metrics misses deeper epistemic and political questions: Who has power to define groups, determine what counts as fairness, validate claims about groups? Epistemic authority over group categories is form of power. Fairness debates are partly epistemic disagreements about group ontology.}
}
