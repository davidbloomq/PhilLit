# Measurement and Operationalization

The gap between technical approaches that assume groups are given and philosophical debates revealing deep ontological uncertainty might seem bridgeable through careful operationalization: even if philosophers disagree about the metaphysics of groups, perhaps we can develop measurement practices that work regardless. This section examines research on measurement theory and construct validity in algorithmic fairness, showing that operationalization does not bypass ontological questions but rather embeds particular answers to them. Measurement choices encode implicit ontological commitments, and the validity of fairness assessments depends on the validity of those underlying ontological assumptions.

## Construct Validity and the Measurement of Fairness

Jacobs and Wallach (2021) make a foundational contribution by applying measurement theory from quantitative social sciences to algorithmic fairness. They observe that computational systems routinely involve unobservable theoretical constructs—such as teacher effectiveness, socioeconomic status, or risk of recidivism—that must be inferred through operationalization via measurement models. Operationalization involves making assumptions and introduces potential for mismatches between the theoretical construct (what we aim to measure) and its measurement (what we actually measure). Importantly, Jacobs and Wallach argue that many fairness harms are direct results of such construct-measurement mismatches.

Their analysis distinguishes between fairness as a property of algorithmic systems and fairness as an essentially contested construct with different theoretical understandings in different contexts. Debates about fairness definitions—should we require demographic parity, equalized odds, calibration, or some other metric?—are not merely technical disagreements about operationalization but reflect "different theoretical understandings" of what fairness means. Different contexts and purposes may warrant different theoretical understandings of fairness, and thus different operationalizations.

This framework has direct implications for intersectional groups. If fairness itself is an essentially contested construct, then so too is the question of which groups warrant fairness consideration. Different theoretical understandings of fairness might entail different answers about group specification. An understanding of fairness focused on legal compliance might specify groups based on legally protected categories (race, sex, age). An understanding focused on substantive non-discrimination might require identifying groups that experience distinctive patterns of oppression, which may not align with legal categories. An understanding focused on distributive justice might prioritize groups below particular welfare thresholds regardless of demographic categories.

Selbst et al. (2019) identify "abstraction traps" where technical fairness interventions fail due to mismatches between abstract formal models and concrete social contexts. One key abstraction trap involves treating fairness as context-independent: metrics valid in one context may be invalid in another because the theoretical construct of fairness differs. Similarly, group categories that successfully operationalize meaningful social groups in one context may fail in another. The abstraction from messy social reality to clean dataset categories is not neutral; it embeds decisions about which aspects of social reality matter and how they should be represented.

Blodgett et al. (2020) provide a critical survey of how "bias" is operationalized in natural language processing research, finding that many operationalizations have questionable construct validity—it is unclear what they actually measure or whether it corresponds to any coherent theoretical construct. Some operationalizations labeled "bias detection" measure statistical correlations without establishing that these correlations reflect actual bias in any normatively relevant sense. Others assume particular definitions of bias without justifying why those definitions are appropriate for the context. The lesson extends beyond NLP: operationalizing contested concepts like bias or fairness requires explicit theoretical grounding and validation that the measurement captures what we claim to be measuring.

Hellman (2020) argues that many algorithmic fairness metrics fail to capture normatively significant aspects of fairness. Metrics may reliably measure *something*—statistical patterns in model outputs—but that something may not be fairness as normatively understood. This is a construct validity failure: the measurement does not correspond to the construct. For intersectional groups specifically, we face questions about whether our category specifications have construct validity. When we operationalize "Black women" as individuals with race=Black AND gender=Female in our dataset, does this successfully capture the group that experiences distinctive intersectional oppression? Or does it merely capture an arbitrary database combination?

## Validity of Demographic Categories

Construct validity questions become particularly acute when examining how demographic categories are operationalized in datasets used for fairness evaluation. Scheuerman et al. (2020) analyze how race and gender are constructed in image databases for facial analysis, finding wide variation in how these categories are defined, measured, and annotated. Different datasets use incompatible categorization schemes, reflecting different implicit assumptions about what race and gender are. Some datasets treat race as biological, others as social; some allow only binary gender categories, others recognize non-binary identities. These measurement choices reflect deep ontological commitments about the nature of demographic categories—commitments that are often unstated and unexamined.

Crucially, Scheuerman et al. show that these operationalization differences are not merely technical details but reflect substantive disagreements about social ontology. If race is understood as biological (as some facial recognition datasets implicitly assume), then racial categories might be determinable from physical features. If race is understood as socially constructed (as social scientists generally argue), then racial categories depend on social context and self-identification in ways that facial features cannot capture. Different ontological assumptions lead to different valid operationalizations, and there is no neutral "correct" way to categorize individuals by race or gender that is independent of theoretical commitments.

Fournier-Montgieux et al. (2025) demonstrate that the validity of fairness auditing hinges on the reliability of demographic attribute inference. Measurement error in demographic categories directly undermines fairness assessment, leading to biased and high-variance estimates of fairness metrics. They propose improved demographic attribute inference pipelines, but their work also highlights a fundamental challenge: even with perfect inference algorithms, we face questions about what we are inferring. Are we inferring individuals' self-identified categories? Categories assigned by others? Categories determined by physical features? Each choice embeds different ontological assumptions about what demographic categories are.

The landmark study by Obermeyer et al. (2019) on racial bias in healthcare algorithms illustrates construct validity failure with serious consequences. The algorithm used healthcare costs as a proxy for healthcare needs, reasoning that higher costs indicate greater medical need. However, costs systematically differ by race even for the same health needs due to differential access to care. Black patients must be substantially sicker than white patients to generate the same healthcare costs. The algorithm thus fails to measure what it purports to measure: health needs. This is a textbook construct validity failure, and it demonstrates how measurement choices can introduce bias even when the algorithm itself implements fairness constraints correctly on the measured construct.

Barocas et al. (2020) examine hidden assumptions in counterfactual fairness approaches, which aim to ensure that predictions would be the same in counterfactual scenarios where individuals have different demographic attributes. They argue that counterfactual fairness assumes the ability to intervene on demographic attributes—to consider what would happen if this Black person were instead white, or if this woman were instead a man. However, such interventions may be conceptually incoherent if demographic attributes are constitutively socially constructed. If being Black or being a woman is not a matter of having certain intrinsic properties but rather of being embedded in particular social structures and practices, then there may be no coherent sense in which we can hold everything else fixed while changing only race or gender. The measurement framework presupposes an attribute-based ontology that may be inappropriate.

Mayson (2019) analyzes bias in criminal risk assessment instruments, arguing that many instruments have poor construct validity because they measure correlates of outcomes rather than theoretically meaningful constructs of risk. More fundamentally, she identifies "label bias": the outcomes these systems predict (arrest, conviction, incarceration) themselves reflect biased processes rather than neutral measures of criminality. When we train models to predict arrest rates across demographic groups and then audit them for fairness, we may achieve perfect fairness metrics while perpetuating injustice if arrests themselves reflect racial bias. The construct "arrest risk" is not the same as the construct "criminality" or "public safety risk," and treating them as equivalent embeds particular assumptions about the validity of criminal justice system outcomes.

## Operationalization Embeds Ontology

The measurement literature establishes that operationalizing fairness metrics and demographic categories is not a neutral technical process but one that embeds substantive ontological commitments. When we specify intersectional groups for fairness evaluation through database queries combining protected attributes, we are implicitly adopting an attribute-based ontology where groups are defined by attribute possession. This ontology may be appropriate in some contexts, but it is not the only philosophical option and may conflict with practice-based or structural accounts of groups.

Passi and Barocas (2019) study how fairness problems are formulated in practice, finding that problem formulation choices—including how to define groups and outcomes—raise significant fairness concerns. Different formulations can raise profoundly different ethical issues, and whether we consider a system fair often depends as much on problem formulation as on model properties. Formulating an intersectional fairness problem requires deciding which intersections constitute groups: should we evaluate fairness for all combinations of race and gender? What about three-way intersections with age? Four-way with disability status? Each choice embeds commitments about which groups exist and matter.

Green (2020) argues that risk assessment instruments have fundamental epistemic limitations that undermine both accuracy and fairness claims. These instruments purport to measure constructs like "risk of recidivism" but cannot reliably measure these constructs given available data and methods. Error metrics are poor proxies for individual equity or social well-being. When measurement validity is compromised, fairness metrics computed on those measurements inherit the invalidity. For intersectional groups, this problem compounds: if we have questionable construct validity in our demographic categories and in our outcome measures, then fairness metrics computed across intersectional demographic groups have doubly questionable validity.

Geiger et al. (2020) document widespread lack of transparency about how training data categories are defined and measured in machine learning applications for social computing. Many papers do not report where human-labeled training data comes from, how categories were defined, or what validity checks were performed. Without clear construct definitions and validity assessment, it is unclear what categories actually capture or whether they successfully operationalize any coherent theoretical construct. This opacity extends to demographic categories: datasets rarely document the ontological assumptions behind their race and gender categorization schemes or provide evidence that these schemes have construct validity for the intended uses.

## Measurement as Ontological Commitment

The measurement literature reveals that we cannot bypass ontological questions about groups through operational definitions. Operationalization does not replace theoretical understanding; it presupposes it. Every measurement choice embeds answers to ontological questions: What are we measuring? What makes this measurement valid? What is the relationship between the construct and its operationalization?

For intersectional fairness, specifying groups through dataset attributes (race=Black AND gender=Female) operationalizes an attribute-based ontology of groups. This may be appropriate if groups are in fact constituted through attribute possession, as some philosophical accounts suggest. But if groups are constituted through social practices, structures, or conferral—as other philosophical accounts argue—then attribute-based operationalization may have poor construct validity. The measurement successfully captures attribute combinations but fails to capture groups as socially meaningful entities.

Critically, we cannot determine construct validity purely empirically. Validation requires theoretical justification that the operationalization captures the intended construct. In social science contexts, this requires substantive knowledge of social structures, practices, and how groups are actually constituted in particular contexts. Algorithmic fairness researchers cannot defer these questions to dataset creators or treat demographic categories as given. The choice of which categories to include, how to define them, and how to combine them into intersectional groups involves irreducibly theoretical and normative judgments about the nature of groups and which groups matter for fairness.

The measurement literature thus illuminates but does not resolve the dilemma. It shows that operationalizing intersectional fairness requires making ontological commitments about groups—commitments that Section 3 established are deeply contested philosophically. Different ontological frameworks suggest different valid operationalizations, and there is no neutral measurement scheme that works regardless of ontology. Measurement choices must be justified through theoretical arguments about what groups are and how they are constituted. Without resolving the philosophical debates, we cannot determine which operationalizations have construct validity for intersectional fairness assessment.
