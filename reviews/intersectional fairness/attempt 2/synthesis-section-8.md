# Conclusion

This literature review has synthesized research across machine learning, philosophy, social ontology, measurement theory, normative ethics, and epistemology to reveal a fundamental challenge at the heart of intersectional algorithmic fairness. Auditing algorithmic systems for fairness across intersectional groups requires specifying which groups to evaluate—determining the set $G$ of groups warranting fairness consideration. This specification involves two forms of uncertainty that interact to create a genuine dilemma.

*Statistical uncertainty* arises from the data sparsity inherent in fine-grained group specification. As the number of intersectional groups increases, average group size decreases, yielding performance estimates with high variance and wide confidence intervals. The epistemic uncertainty about whether groups are treated fairly increases as groups become smaller. Technical solutions—multicalibration, synthetic data generation, hierarchical models—provide valuable tools but face fundamental information-theoretic limits. Sample complexity grows exponentially with the number of groups, and attaining precise fairness guarantees becomes statistically infeasible as $G$ expands. The machine learning literature has developed sophisticated approaches to these statistical challenges while consistently treating $G$ as an input determined by available data attributes.

*Ontological uncertainty* arises from deep philosophical disagreements about which groups intersectionality picks out and how they should be individuated. Philosophical analyses reveal contested terrain: anticategorical approaches reject fixed group categories altogether, intracategorical approaches study specific intersections without presuming exhaustive enumeration, and only intercategorical approaches support the kind of group specification algorithmic fairness requires—yet even this approach is provisional rather than metaphysically committed. Competing ontologies—attribute-based accounts suggesting groups can be specified through combinations of protected attributes versus practice-based or emergentist accounts suggesting groups depend on context-specific social structures—yield incompatible guidance about whether algorithmic systems can enumerate meaningful groups through database queries. The philosophy literature has illuminated these ontological complexities without connecting them to the operational constraints of algorithmic systems.

The dilemma arises from the *interaction* between these two forms of uncertainty. Expanding $G$ to respect ontological complexity (acknowledging that meaningful groups may not align with simple attribute combinations, that emergent intersectional groups resist enumeration, that normative frameworks like egalitarianism require comprehensive specification) exacerbates statistical problems by reducing average group size and increasing estimation uncertainty. Conversely, constraining $G$ to handle statistical limitations (including only large groups, limiting to two-way intersections, excluding groups below sample size thresholds) requires principled criteria for which groups to exclude—criteria that depend on resolving contested ontological questions about which groups exist, contested normative questions about which groups matter morally, and contested epistemic questions about who has authority to determine group specifications.

Progress on one horn of the dilemma tends to worsen the situation on the other. Technical advances enabling fairness evaluation across more groups do not resolve ontological questions about which groups to include. Philosophical progress on the ontology of groups would not eliminate statistical constraints on reliable estimation with finite data. The dilemma is not merely two difficult problems occurring together; it is their problematic interaction where attempting to solve one exacerbates the other.

Our review has documented that while each horn is well-recognized within its respective domain, **no existing work frames their interaction as a dilemma**. Machine learning papers treat group specification as a technical input problem, acknowledging statistical challenges but not ontological questions. Philosophy papers explore contested ontologies of groups without addressing statistical feasibility. Measurement theory shows that operationalization embeds ontological commitments but does not synthesize this insight with statistical constraints. Normative ethics reveals pluralism about distributive justice frameworks but does not connect this to questions of which groups to evaluate when comprehensive evaluation is statistically infeasible. Epistemology illuminates questions of authority and responsibility but does not integrate these with statistical and ontological challenges.

This gap—the absence of work synthesizing technical, philosophical, normative, and epistemic dimensions to reveal the dilemma structure—represents the core contribution our research makes. By articulating intersectional fairness as involving a genuine dilemma where statistical and ontological uncertainties reinforce each other, we provide a new framework for understanding why intersectional fairness has proven so intractable and what kinds of responses are appropriate.

Several implications follow from recognizing the dilemma. First, neither technical nor philosophical progress alone suffices. Solving intersectional fairness requires navigating trade-offs between competing values and purposes rather than discovering the single correct technical or ontological solution. Second, context matters profoundly. Different applications involve different purposes (legal compliance, substantive non-discrimination, distributive justice), different normative frameworks (egalitarian, prioritarian, sufficientarian), different epistemic positions (who has authority to determine groups?), and different statistical constraints (available data, sample sizes, precision requirements). No universal specification of $G$ works across all contexts. Third, affected communities must be involved in determining which groups matter for particular applications, making group specification a participatory process rather than a purely technical or academic decision.

Our research will explore potential responses to the dilemma. One strategy involves clarifying the *purposes* of fairness audits. Legal compliance purposes might specify $G$ based on legally protected categories. Substantive non-discrimination purposes might focus on groups experiencing documented discrimination patterns. Distributive justice purposes might prioritize groups falling below adequacy thresholds. Different purposes justify different group specifications and different trade-offs between comprehensiveness and statistical precision.

Another strategy leverages normative frameworks from distributive justice. Prioritarian approaches could justify focusing on worst-off groups, though identifying which groups are worst-off requires some initial fairness measurement. Sufficientarian approaches could justify threshold-based specification, evaluating comprehensive intersectional groups only for whether they meet basic adequacy standards rather than requiring precise performance estimation. These frameworks provide normative guidance for principled trade-offs between statistical precision and ontological inclusiveness.

A third strategy emphasizes epistemic humility and transparency. When fairness claims are made about small intersectional groups, uncertainty should be communicated explicitly through confidence intervals, statistical power analyses, and clear statements about evidential limitations. Algorithmic auditing standards should require reporting not just fairness metrics but also their reliability for different groups. This epistemic transparency enables more responsible inference and decision-making about intersectional fairness.

The significance of our contribution spans multiple dimensions. Theoretically, framing intersectional fairness as a dilemma shows the limits of purely technical or purely philosophical approaches and demonstrates the need for genuine interdisciplinary integration. Practically, it helps practitioners understand why intersectional fairness is fundamentally challenging rather than merely technically difficult, enabling more realistic expectations and more responsible practices. Methodologically, it models productive engagement between machine learning and philosophy, showing how philosophical analysis can illuminate practical challenges in ways that advance both fields.

For ACM FAccT audiences, this research offers a novel framing of a persistent challenge in algorithmic fairness, moving beyond technical optimization to engage the ontological and normative questions that technical work presupposes. For philosophy audiences, it demonstrates how abstract debates about social ontology and distributive justice bear on live practical problems in algorithmic systems, providing concrete contexts for philosophical frameworks. For interdisciplinary audiences, it exemplifies how integration across technical, philosophical, normative, and epistemic dimensions yields insights unavailable within any single discipline.

Intersectional fairness has proven challenging for good reason: it involves a genuine dilemma where statistical and ontological uncertainties interact to create a problem resistant to purely technical or purely philosophical solutions. By articulating this dilemma structure and synthesizing insights across disciplines to document the gap in existing work, this review establishes the foundation for our contribution: the first systematic analysis of the intersectionality dilemma and exploration of strategies for navigating it responsibly.
