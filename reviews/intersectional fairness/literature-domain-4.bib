@comment{Domain 4: Measurement Theory and Construct Validity in ML}
@comment{Focus: Operationalization and construct validity for fairness, what fairness metrics actually measure,}
@comment{validity of group categories in datasets, measurement error and fairness}

@inproceedings{JacobsWallach2021,
  title = {Measurement and Fairness},
  author = {Jacobs, Abigail Z. and Wallach, Hanna},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21)},
  pages = {375--385},
  year = {2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445901},
  note = {Foundational paper proposing measurement modeling from quantitative social sciences as framework for understanding fairness. Computational systems involve unobservable theoretical constructs (socioeconomic status, teacher effectiveness, risk of recidivism) that must be inferred through operationalization via measurement model. Operationalization involves assumptions and introduces potential for mismatches between theoretical construct and its measurement. Many fairness harms are direct results of such mismatches. Contributes fairness-oriented conceptualizations of construct reliability and validity. Argues fairness itself is essentially contested construct with different theoretical understandings in different contexts - debates about fairness definitions are actually debates about different theoretical understandings, not just operationalizations.}
}

@article{Jacobs2019,
  title = {Measurement and Fairness},
  author = {Jacobs, Abigail Z. and Wallach, Hanna},
  journal = {arXiv preprint arXiv:1912.05511},
  year = {2019},
  doi = {10.48550/arXiv.1912.05511},
  note = {Extended arXiv version of FAccT 2021 paper. Provides detailed treatment of measurement theory applied to algorithmic fairness. Essential reading for understanding construct validity challenges in fairness operationalization.}
}

@article{Selbst2019,
  title = {Fairness and Abstraction in Sociotechnical Systems},
  author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
  journal = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency},
  year = {2019},
  pages = {59--68},
  doi = {10.1145/3287560.3287598},
  note = {Identifies abstraction traps where technical fairness interventions fail due to mismatches between abstract formal models and concrete social contexts. Relevant to measurement validity: fairness metrics may be valid in abstract but invalid when applied to real contexts with all their social complexity.}
}

@article{Green2022,
  title = {The Flaws of Policies Requiring Human Oversight of Government Algorithms},
  author = {Green, Ben},
  journal = {Computer Law \& Security Review},
  volume = {45},
  year = {2022},
  doi = {10.1016/j.clsr.2022.105681},
  note = {Examines how operationalization choices in algorithmic systems encode particular understandings of problems and solutions. Relevant to how measurement choices in fairness contexts embed specific theoretical commitments about what fairness is and what groups matter.}
}

@inproceedings{Blodgett2020,
  title = {Language (Technology) is Power: A Critical Survey of "Bias" in NLP},
  author = {Blodgett, Su Lin and Barocas, Solon and Daumé III, Hal and Wallach, Hanna},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year = {2020},
  pages = {5454--5476},
  doi = {10.18653/v1/2020.acl-main.485},
  note = {Critical analysis of how "bias" is operationalized in NLP research. Many operationalizations have questionable construct validity - unclear what they actually measure. Relevant to understanding limitations of operationalizing fairness constructs, including intersectional fairness.}
}

@article{Hellman2020,
  title = {Measuring Algorithmic Fairness},
  author = {Hellman, Deborah},
  journal = {Virginia Law Review},
  volume = {106},
  number = {4},
  pages = {811--866},
  year = {2020},
  note = {Legal scholar's perspective on what algorithmic fairness metrics actually measure. Argues many metrics fail to capture normatively significant aspects of fairness. Measurement validity problem: metrics may reliably measure something, but not fairness as normatively understood.}
}

@article{Fazelpour2021,
  title = {Algorithmic Bias: Senses, Sources, Solutions},
  author = {Fazelpour, Sina and Danks, David},
  journal = {Philosophy Compass},
  volume = {16},
  number = {8},
  year = {2021},
  doi = {10.1111/phc3.12760},
  note = {Philosophical analysis of bias in ML systems. Examines different senses of "bias" and what various bias metrics actually capture. Relevant to construct validity: are bias metrics measuring bias or something else?}
}

@article{Mayson2019,
  title = {Bias In, Bias Out},
  author = {Mayson, Sandra G.},
  journal = {Yale Law Journal},
  volume = {128},
  pages = {2218--2300},
  year = {2019},
  note = {Examines how risk assessment instruments operationalize risk and fairness. Argues many instruments have poor construct validity - they measure correlates of outcomes rather than theoretically meaningful constructs. Label bias compromises validity of both accuracy and fairness metrics.}
}

@article{Barabas2019,
  title = {Technical Flaws of Pretrial Risk Assessments Raise Grave Concerns},
  author = {Barabas, Chelsea and Dinakar, Karthik and Virza, Madars},
  journal = {MIT Media Lab},
  year = {2019},
  url = {https://www.media.mit.edu/posts/algorithmic-risk-assessment/},
  note = {Documents construct validity problems in pretrial risk assessment. Target variables often poor measurements of phenomena being modeled. If measurement does not satisfy construct validity requirements, assumptions like those in equalized odds are compromised.}
}

@inproceedings{Passi2019,
  title = {Problem Formulation and Fairness},
  author = {Passi, Samir and Barocas, Solon},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19)},
  year = {2019},
  pages = {39--48},
  doi = {10.1145/3287560.3287567},
  note = {Empirical study of how fairness problems are formulated in practice. Finds problem formulation choices raise significant fairness concerns - different formulations can raise profoundly different ethical issues. Relates to construct validity: whether we consider a project fair often depends on problem formulation as much as model properties.}
}

@article{FournierMontgieux2025,
  title = {Reliable and Reproducible Demographic Inference for Fairness in Face Analysis},
  author = {Fournier-Montgieux, Alexandre and Le Borgne, Hervé and Popescu, Adrian and Luvison, Bertrand},
  journal = {arXiv preprint arXiv:2510.20482},
  year = {2025},
  doi = {10.48550/arXiv.2510.20482},
  note = {Validity of fairness auditing hinges on reliability of demographic attribute inference process. Proposes fully reproducible DAI pipeline with modular transfer learning approach. Improved reliability leads to less biased and lower-variance estimates of fairness. Measurement error in demographic categories undermines fairness assessment - direct relevance to ontological uncertainty about groups.}
}

@article{Obermeyer2019,
  title = {Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations},
  author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  journal = {Science},
  volume = {366},
  number = {6464},
  pages = {447--453},
  year = {2019},
  doi = {10.1126/science.aax2342},
  note = {Landmark study showing construct validity failure in healthcare algorithm. Used healthcare costs as proxy for health needs, but costs systematically differ by race even for same health needs. Demonstrates how measurement choices can introduce bias even when algorithm itself is "fair" by formal metrics.}
}

@inproceedings{Green2020,
  title = {The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness},
  author = {Green, Ben},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages = {594--606},
  year = {2020},
  doi = {10.1145/3351095.3372869},
  note = {Argues risk assessment instruments have fundamental epistemic limitations. Cannot reliably measure constructs they purport to measure. Error metrics are poor proxies for individual equity or social well-being. Measurement validity problems undermine both accuracy and fairness claims.}
}

@article{Barocas2020,
  title = {The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons},
  author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
  journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages = {80--89},
  year = {2020},
  doi = {10.1145/3351095.3372830},
  note = {Examines hidden assumptions in fairness methods including assumptions about measurement. Counterfactual fairness approaches assume ability to intervene on demographic attributes, but such interventions may be conceptually incoherent if attributes are constitutively socially constructed.}
}

@article{Scheuerman2020,
  title = {How We've Taught Algorithms to See Identity: Constructing Race and Gender in Image Databases for Facial Analysis},
  author = {Scheuerman, Morgan Klaus and Wade, Kandrea and Lustig, Caitlin and Brubaker, Jed R.},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {4},
  number = {CSCW1},
  pages = {1--35},
  year = {2020},
  doi = {10.1145/3392866},
  note = {Examines how demographic categories are operationalized in facial analysis datasets. Shows wide variation in how race and gender are defined, measured, and annotated. Measurement choices reflect different (often implicit) ontological commitments about what these categories are. Directly relevant to ontological uncertainty about groups.}
}

@article{Geiger2020,
  title = {Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?},
  author = {Geiger, R. Stuart and Yu, Kevin and Yang, Yanlai and Dai, Mindy and Qiu, Jie and Tang, Rebekah and Huang, Jenny},
  journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages = {325--336},
  year = {2020},
  doi = {10.1145/3351095.3372862},
  note = {Documents lack of transparency about how training data categories are defined and measured. Without clear construct definitions and validity assessment, unclear what categories actually capture. Measurement validity prerequisite for fairness validity.}
}
