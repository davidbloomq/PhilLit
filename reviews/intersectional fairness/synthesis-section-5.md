# Normative Frameworks for Fairness Trade-offs

Even if we could resolve ontological uncertainties about which groups exist, we would face normative questions about which groups warrant fairness consideration and how to handle inevitable trade-offs between fairness for different groups. This section examines how different theories of distributive justice imply different answers to group specification questions. Rather than providing a univocal normative framework for intersectional fairness, the ethics literature reveals pluralism: multiple defensible normative frameworks yield incompatible practical guidance about which groups should be prioritized and how to navigate fairness-utility and inter-group fairness trade-offs.

## Distributive Justice Principles and Group Specification

Kuppler et al. (2022) make a crucial distinction between fairness as a property of prediction algorithms and justice as a property of allocation principles in decision-making systems. Much algorithmic fairness research conflates these, with the consequence that distributive justice concerns are not addressed explicitly. When we ask which groups should receive fairness consideration, we are asking a question of distributive justice: how should we distribute the benefits and burdens of algorithmic systems across social groups?

Classical distributive justice theories provide different answers. *Egalitarianism* requires equal outcomes (or opportunities, or resources) across groups, suggesting that all groups should receive equal consideration in fairness metrics. If we take egalitarianism seriously, we should evaluate fairness across all intersectional groups and aim for equality across all of them. This implies comprehensive intersectional specification: every combination of protected attributes defines a group warranting equal treatment. The statistical infeasibility of achieving this (due to data sparsity for small groups) becomes a practical constraint on realizing justice, not a reason to limit our moral concern.

*Prioritarianism*, as articulated by Parfit (1997), gives greater moral weight to benefits to those who are worse off. Unlike egalitarianism, prioritarianism does not require equality per se but rather weighted concern for the worst-off. Applying prioritarianism to algorithmic fairness suggests we should identify which groups are worst-served by current systems and prioritize improvements for those groups, even if this means accepting inequality (Kuppler et al. 2022; Binns 2024). This framework could justify limiting group specification to groups experiencing the worst outcomes rather than attempting comprehensive intersectional enumeration. However, it requires empirical knowledge of which groups are worst-off—knowledge that itself requires fairness measurement across groups—and it requires normative judgments about how much weight to give to the worst-off relative to other groups.

*Sufficientarianism*, developed by Frankfurt (1987) and others, holds that what matters morally is not equality or weighted concern for the worst-off but rather ensuring everyone has "enough"—that all groups meet some threshold of adequate treatment. Shields (2012) discusses "priority shift" views where different distributive principles apply below and above the threshold: strict priority for bringing groups above the threshold, more permissive principles once all groups meet basic adequacy. Herlitz (2019) argues that sufficientarian principles are indispensable when addressing desperately badly-off groups, with whom trade-offs should not be permitted: we cannot sacrifice improvements for groups below the threshold for the sake of overall fairness improvements.

Applied to intersectional fairness, sufficientarianism suggests we should focus on identifying groups that fall below adequacy thresholds (perhaps measured through false negative rates in consequential contexts like healthcare or criminal justice) and prioritize ensuring all groups exceed thresholds. This could substantially constrain the group specification problem: rather than considering all possible intersectional groups, we focus on those demonstrably below adequacy thresholds. However, sufficientarianism faces challenges in specifying what counts as "enough" and determining appropriate thresholds, which may themselves depend on context and contested value judgments (Shields 2012).

Binns (2024) proposes using distributive justice principles to formalize approximate justice in algorithmic fairness. Different principles—egalitarianism, desert-based justice, prioritarianism, sufficientarianism, consequentialism—can be formalized as mathematical formulas allowing patterns like maximin (egalitarian), prioritarian weighting, or threshold-based (sufficientarian) evaluation. He argues that a fairness criterion checking if approximately equal shares from all groups are above a threshold accounts for structural injustices under sufficientarianism, leading to egalitarian distributive patterns in practice. Yet this synthesis presupposes agreement on which principle to adopt and what counts as adequate thresholds—agreements that remain contested.

## Leveling Down and the Limits of Equality

A central debate in distributive justice concerns the "leveling down" objection to egalitarianism: if we can achieve equality only by making some groups worse off without benefiting anyone, should we do it? This debate has direct implications for intersectional fairness when satisfying fairness constraints for all groups simultaneously would require reducing performance for some groups.

Holm (2023) examines egalitarianism in algorithmic fairness, interpreting classification parity criteria (requiring equality across groups in error rates or other performance measures) as embodying deontic egalitarianism. He acknowledges the leveling down objection but provides an egalitarian response: equality itself has intrinsic value that may justify leveling down in some cases, particularly when inequality reflects wrongful discrimination. The argument depends on distinguishing between instrumental values (equality as means to other goods) and intrinsic values (equality as good in itself).

Mittelstadt et al. (2023) challenge this egalitarian defense, arguing that leveling down should be rejected in fair machine learning because it unnecessarily harms advantaged groups when performance is intrinsically valuable, demonstrates lack of equal concern for affected groups, and fails to meet the aims of many viable distributive justice theories. They contend that strict egalitarianism should not be the default framework for algorithmic fairness, and that different contexts may warrant different distributive principles.

This debate matters for intersectional fairness because achieving fairness across many small intersectional groups often requires trade-offs. Satisfying stringent fairness constraints for groups with sparse data may require sacrificing overall accuracy or accepting worse performance for groups with more data. If we reject leveling down, we may accept inequality across intersectional groups when improving outcomes for small groups would require harming larger groups. If we embrace egalitarianism, we may accept performance degradation to achieve parity. Different normative frameworks justify different choices, and there is no consensus on which framework is appropriate.

Hertweck et al. (2022) propose a justice-based framework for analyzing fairness-utility trade-offs, arguing that these should be understood as normative questions requiring ethical analysis rather than as purely technical optimization problems. The fairness-utility trade-off—where improving fairness metrics reduces overall accuracy—can be large in practice. So too can trade-offs between public safety and fairness notions. Deciding how to navigate these trade-offs requires normative judgment about which values take priority and under what conditions. Different justice frameworks yield different answers.

Lee et al. (2021) argue that reductionist representations of fairness in narrow toolkits bear little resemblance to real-life fairness considerations, which are highly contextual. Fairness metrics implemented in algorithmic fairness packages are difficult to integrate into broader ethical assessments precisely because they encode particular normative commitments that may not align with the values at stake in specific contexts. Drawing on welfare economics and ethical philosophy, they advocate for richer frameworks that acknowledge context-dependence and pluralism about values.

## Context-Dependence and Normative Pluralism

Green (2022) argues for moving from formal to substantive algorithmic fairness, distinguishing mathematical fairness definitions (formal fairness) from fairness that accounts for social and political context (substantive fairness). The "impossibility of fairness"—the mathematical incompatibility between different fairness definitions like demographic parity and equalized odds—arises partly because different definitions embed different normative commitments appropriate for different contexts. There is no universal fairness metric precisely because fairness is context-dependent and value-laden.

This context-dependence extends to group specification. Which groups warrant fairness consideration depends on the purpose of the algorithmic system, the social context, the stakes for affected individuals, and the relevant legal and ethical norms. In hiring contexts governed by anti-discrimination law, legally protected categories may determine relevant groups. In healthcare allocation contexts, groups experiencing health disparities may be salient regardless of legal categories. In criminal justice contexts, groups with histories of discriminatory policing warrant special attention. Different contexts involve different groups, and there is no context-independent answer to which groups matter.

Corbett-Davies and Goel (2018) provide influential analysis of the normative foundations of different fairness metrics, showing that different metrics embody different normative commitments about what fairness requires. No metric is universally appropriate; choosing a metric requires normative judgment about goals and context. The same pluralism applies to group specification: no single specification of groups is universally appropriate. Choosing which groups to include requires context-specific normative judgment.

Binns (2018) explicitly connects algorithmic fairness metrics to theories from political philosophy, making implicit normative commitments explicit. Different contexts and purposes require different normative frameworks, and thus different fairness approaches. He demonstrates that technical choices about fairness metrics are inseparable from political-philosophical commitments. By extension, technical choices about which groups to monitor for fairness are inseparable from normative judgments about which groups matter morally and practically in particular contexts.

## Normative Uncertainty Compounding Ontological Uncertainty

The normative literature establishes that even with perfect knowledge of which groups exist (resolving ontological uncertainty), we would face genuine disagreement about which groups warrant fairness consideration and how to handle trade-offs. Egalitarian frameworks suggest comprehensive specification of all intersectional groups with equal weight to all. Prioritarian frameworks suggest focus on worst-off groups with weighted concern. Sufficientarian frameworks suggest identifying groups below adequacy thresholds and prioritizing bringing them above thresholds. Different frameworks justify different group specifications.

Moreover, these normative frameworks interact with statistical constraints in complex ways. Egalitarianism requires equal consideration for all groups, including small intersectional groups with sparse data, potentially demanding statistical precision we cannot achieve. Prioritarianism requires identifying worst-off groups, which itself requires fairness measurement—but small groups have high measurement uncertainty, making it unclear which groups are actually worst-off. Sufficientarianism requires determining adequacy thresholds and identifying groups below them, but again faces measurement uncertainty for small groups.

Critically, there is no philosophical consensus on which normative framework is appropriate for algorithmic fairness contexts. Crisp (2003) analyzes the relationship between egalitarianism and prioritarianism, arguing for prioritarianism grounded in compassion rather than equality. But this is one philosophical position among several, not a consensus view. The discipline of normative ethics exhibits pluralism: multiple incompatible frameworks have strong arguments in their favor, and different frameworks may be appropriate in different contexts.

For intersectional algorithmic fairness, this normative pluralism compounds the ontological uncertainty identified in Section 3. We face uncertainty not only about which groups exist but also about which existing groups warrant moral concern, how much concern they warrant relative to each other, and how to handle trade-offs between groups. Technical solutions that assume these questions have determinate answers—that we know which groups to specify and how to weight them—misunderstand the normative landscape. The questions are genuinely contested, with reasonable people holding incompatible positions based on defensible ethical frameworks.
