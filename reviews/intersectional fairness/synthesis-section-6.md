# Epistemic Dimensions

Beyond ontological questions about which groups exist and normative questions about which groups matter morally, intersectional fairness raises epistemic questions: Who has authority to determine which groups warrant consideration? What epistemic responsibilities do practitioners have when making claims about groups with limited data? When is evidence sufficient to support fairness claims about small intersectional groups? This section examines how epistemic justice frameworks and analyses of uncertainty in machine learning illuminate often-overlooked dimensions of the intersectionality dilemma.

## Epistemic Justice and Group Categories

Fricker's (2007) foundational work on epistemic injustice distinguishes two forms: *testimonial injustice*, where speakers receive credibility deficits due to prejudice, and *hermeneutical injustice*, where gaps in collective interpretive resources prevent marginalized groups from making sense of their experiences. Algorithmic fairness systems can perpetuate both forms of epistemic injustice. Testimonial injustice occurs when marginalized groups' testimony about their experiences of algorithmic discrimination is discounted or dismissed. Hermeneutical injustice occurs when the categories and frameworks used in fairness evaluation lack concepts to capture forms of discrimination experienced by marginalized groups, particularly intersectional groups.

Anderson (2012) extends epistemic justice to social institutions, arguing that institutions can systematically advantage or disadvantage different knowers. Algorithmic systems are institutions in this sense: they embody particular ways of categorizing people, collecting data, making inferences, and distributing outcomes. These systems can promote epistemic justice by incorporating diverse perspectives and allowing affected communities to participate in defining categories and standards. Or they can undermine epistemic justice by excluding marginalized groups from processes that define, judge, and decide about them.

The question of which intersectional groups to specify for fairness evaluation is partly an epistemic justice question: Who gets to decide which groups matter? If machine learning researchers and data scientists—who are demographically unrepresentative of affected populations—unilaterally determine which group categories to track, this concentrates epistemic authority in ways that may perpetuate injustice. D'Ignazio and Klein (2020) argue that data practices systematically privilege certain perspectives while marginalizing others, and that without diverse perspectives across the AI ecosystem, machine learning advances will fuel epistemic injustice. Determining which groups warrant fairness consideration involves epistemic choices about what to measure and how to categorize—choices that advantage some groups' perspectives while disadvantaging others.

Kalluri (2020) contends that debates about algorithmic fairness often miss deeper questions about power: "Who has power to define groups, determine what counts as fairness, validate claims about groups?" Epistemic authority over group categories is a form of power. When researchers specify intersectional groups through database queries without consulting affected communities, they exercise power to determine which groups exist for analytical purposes and which experiences of discrimination become visible in fairness metrics. This power may be exercised responsibly or irresponsibly, but it is power nonetheless.

Taylor (2016) asks "which public? whose good?" in the context of big data as public good, emphasizing that data collection involves epistemic choices about what to measure and how to categorize. These choices advantage some groups' perspectives while disadvantaging others. For intersectional fairness, choices about which demographic categories to collect, how to allow individuals to self-identify, which combinations to consider intersectional groups, and what counts as adequate representation all involve epistemic choices with justice implications.

Kay et al. (2024) develop a taxonomy mapping testimonial, hermeneutical, and distributive injustices onto four AI development stages: data collection, model training, inference, and dissemination. At each stage, systems can perpetuate epistemic injustice through whose knowledge is valued, whose perspectives are incorporated, and whose interpretive resources shape how outputs are understood. Critically, they argue that AI systems "are not only products of biased data but of institutional and epistemic frameworks favoring surveillance, control, efficiency over equity and care." The frameworks that determine which groups matter for fairness evaluation reflect particular epistemic and institutional priorities that may not align with justice.

Harding's (2004) standpoint epistemology argues that certain social positions render particular epistemological perspectives, such that knowledge produced by Black women would differ from knowledge produced by white men due to different experiences and perspectives. Applied to intersectional fairness, standpoint epistemology suggests that members of intersectional groups have potential epistemic advantages regarding their own experiences of discrimination. Specifying which groups warrant fairness consideration without incorporating insights from those groups risks missing precisely the experiences fairness metrics should capture. Yet incorporating diverse standpoints requires inclusive processes that algorithmic fairness research often lacks.

## Uncertainty and Epistemic Responsibility

Beyond questions of epistemic justice, intersectional fairness raises questions about epistemic responsibility in the face of uncertainty. When is it epistemically responsible to make fairness claims about small intersectional groups given limited and uncertain data?

Hüllermeier and Waegeman (2021) distinguish *aleatoric uncertainty*—irreducible probabilistic variability inherent in phenomena—from *epistemic uncertainty*—reducible uncertainty reflecting lack of knowledge. Epistemic uncertainty is also called systematic uncertainty: things one could in principle know but does not in practice. Crucially, epistemic uncertainty reflects uncertainty about estimates due to limited sample size. For intersectional groups, statistical uncertainty is partly epistemic: we could reduce it with more data. But what counts as "enough" data depends on stakes, purposes, and what we are trying to learn.

Bhatt et al. (2021) argue that uncertainty quantification is crucial for responsible AI. Making uncertainty visible enables more epistemically responsible inferences, particularly for claims about small groups with high epistemic uncertainty. Failing to communicate uncertainty can mislead about the reliability of group-level inferences, presenting estimates with wide confidence intervals as if they were precise measurements. For intersectional fairness, this means communicating not just fairness metrics but also uncertainty about those metrics for different groups. A group might appear to experience discrimination, or might appear to be treated fairly, but if the uncertainty is high enough, we cannot responsibly draw either conclusion.

But when is uncertainty low enough to support responsible claims? There is no universal answer. What counts as adequate evidence depends on what is at stake. Eubanks (2018) documents cases where automated systems make high-stakes inferences about marginalized groups—whether children should be removed from families, whether people are eligible for benefits, whether someone poses a security risk—based on insufficient epistemic basis. Systems claim certainty about group properties based on sparse, biased data. This epistemic irresponsibility compounds material harms.

O'Neil (2016) identifies similar patterns in "weapons of math destruction": algorithmic systems making high-stakes inferences about groups without appropriate uncertainty quantification. The failure to acknowledge epistemic limitations leads to overconfident, often harmful, inferences about marginalized groups. For intersectional groups specifically, small sample sizes create high epistemic uncertainty, yet fairness reports often present metrics without confidence intervals or uncertainty measures, creating the false impression of precise knowledge.

Noble (2018) and Benjamin (2019) show how algorithmic systems make and perpetuate epistemic claims about groups—especially intersectional groups like Black women—without adequate basis. Systems both reflect and produce knowledge about groups, with epistemic and political dimensions intertwined. When an algorithm encodes particular categorizations of groups or makes predictions about group properties, it is not merely reflecting pre-existing knowledge but actively shaping what is known and knowable about those groups.

Selbst (2018) analyzes epistemic assumptions in predictive policing systems, which make inferences about crime risk by demographic group but often lack adequate epistemic warrant. Statistical uncertainty is not properly accounted for, especially for intersectional groups with sparse data. Systems make confident predictions about small groups despite wide confidence intervals that would counsel epistemic humility. The failure to acknowledge uncertainty serves ideological functions: it makes algorithmic outputs appear objective and authoritative when they are actually highly uncertain.

## Documentation and Epistemic Transparency

Gebru et al. (2021) propose datasheets for datasets as part of epistemic responsibility in using data. Documenting how categories are defined, measured, and validated is crucial for assessing whether inferences drawn from data are warranted. This is particularly important for demographic categories used in fairness assessment. Without clear epistemic grounding in how categories were constructed and validated, it is unclear what inferences about groups are warranted or what the categories actually capture.

Raji et al. (2020) propose a framework for algorithmic auditing that includes epistemic requirements. Audits must account for uncertainty, especially for small or intersectional groups. Making claims about fairness for groups requires epistemic warrant: adequate data, valid measurements, appropriate statistical methods accounting for uncertainty. An audit that reports fairness metrics for intersectional groups without reporting sample sizes, confidence intervals, and statistical power has not met minimal epistemic standards for responsible claims.

Geiger et al. (2020) document that many machine learning papers do not report where training data categories come from or how they were defined. This opacity about epistemic provenance makes it impossible to assess construct validity or determine what inferences are warranted. For demographic categories specifically, failure to document how race and gender were defined, whether categories allowed self-identification, what options were available, and how intersectional combinations were handled undermines any fairness claims based on those categories.

Fazelpour and Lipton (2023) argue that algorithmic fairness must be understood in situated contexts rather than abstractly. Different contexts involve different epistemic positions, power dynamics, and justice considerations. What counts as epistemically responsible inference about groups depends on social and political context: who is making claims, about whom, for what purposes, with what potential consequences. Standards for adequate evidence should be higher for high-stakes decisions affecting vulnerable populations and lower for low-stakes exploratory analyses. Epistemic responsibility is context-dependent.

## Epistemic Dimensions of the Dilemma

The epistemic literature establishes several conclusions relevant to the intersectionality dilemma. First, determining which groups warrant fairness consideration is not merely a technical or ontological question but an epistemic justice question about who has authority to define groups and whose perspectives are incorporated. Excluding affected communities from group specification decisions concentrates epistemic power in problematic ways.

Second, making fairness claims about small intersectional groups raises epistemic responsibility issues. High epistemic uncertainty due to limited data means that responsible claims require appropriate uncertainty quantification and epistemic humility. Yet current practices often present fairness metrics without uncertainty measures, creating false impressions of knowledge.

Third, standards for adequate evidence are context-dependent and contested. There is no universal threshold for when data is sufficient to support fairness claims about a group. What counts as enough depends on stakes, purposes, and consequences of potential errors.

These epistemic dimensions interact with the statistical and ontological horns of the dilemma in complex ways. Statistical uncertainty about small groups is partly epistemic uncertainty—more data could reduce it. But epistemic justice considerations may require including groups even when data is limited, if those groups experience distinctive discrimination. Epistemic responsibility might counsel humility about fairness claims for small groups, but excluding groups from fairness consideration due to data limitations raises epistemic justice concerns about whose experiences count.

Moreover, the question of who decides which groups to include is itself contested. Should machine learning researchers decide based on available data and statistical feasibility? Should affected communities decide based on lived experiences of discrimination? Should legal frameworks determine groups based on protected categories? Different answers reflect different epistemic frameworks about expertise, authority, and whose knowledge matters. The epistemology literature does not provide a single answer but rather illuminates that these are genuinely contested questions with implications for justice.

Critically, no work in this literature connects epistemic dimensions to the ontological and statistical challenges documented in earlier sections. Epistemic justice scholars analyze power and authority in knowledge production but do not typically address statistical constraints on what can be reliably known about small groups. Statisticians and machine learning researchers discuss epistemic uncertainty and its quantification but do not typically frame this as an epistemic justice issue about whose knowledge and experiences are validated. The epistemic dimensions of intersectional fairness remain disconnected from technical and philosophical analyses, leaving another gap in understanding the full dilemma.
