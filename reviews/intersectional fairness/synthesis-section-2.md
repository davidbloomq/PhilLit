# Technical Approaches to Intersectional Fairness

The machine learning and computer science literature on algorithmic fairness has made substantial progress in developing methods for auditing and improving fairness across intersectional groups. This section reviews state-of-the-art technical approaches, focusing on how they handle data sparsity and group specification. A consistent pattern emerges: while statistical challenges of small intersectional groups are well-recognized and addressed through sophisticated techniques, these approaches uniformly treat the set of groups as an input to be provided rather than a problem to be solved.

## Fairness Metrics and the Intersectional Challenge

The foundational insight motivating intersectional fairness research is that fairness on individual attributes does not guarantee fairness at their intersections. Foulds et al. (2020) demonstrate this formally through their concept of *differential fairness*, which requires fairness guarantees to hold over combinations of sensitive attributes. An algorithm might achieve statistical parity for race (equal positive prediction rates across racial groups) and for gender (equal positive prediction rates across genders) while simultaneously exhibiting large disparities for Black women compared to other race-gender combinations. This possibility arises because intersectional identities are not merely additive—the discrimination experienced by Black women cannot be decomposed into race-based discrimination plus gender-based discrimination (Bowleg 2008; Hancock 2007).

Building on this recognition, researchers have proposed fairness metrics that explicitly account for intersectionality. *Conditional demographic parity* extends traditional demographic parity (equal positive rates across groups) to require equality conditional on all combinations of protected attributes (Castelnovo et al. 2022). In principle, conditional demographic parity solves the intersectionality problem: if we condition on race, gender, age, and disability status simultaneously, we ensure fairness across all 64 possible combinations of these four binary attributes. However, Castelnovo et al. (2022) identify a fundamental challenge: the exponential growth of subgroups makes this approach computationally and practically infeasible. Each additional sensitive attribute doubles the number of groups to consider (assuming binary attributes; categorical attributes with *k* levels multiply by *k*). With even modest numbers of attributes, most intersectional groups become too small for reliable performance estimation.

Ghassemi et al. (2024) elaborate on these difficulties, noting that conditional demographic parity becomes progressively harder to achieve as conditioning variables increase in dimensionality or as model outputs become continuous rather than binary. Auditing conditional fairness requires sufficient data in each cell of the contingency table defined by attribute combinations, but many cells will be sparse or empty in finite datasets. This is not merely a computational problem—it reflects fundamental statistical limits. As the number of groups grows, the effective sample size per group shrinks, increasing variance in performance estimates and reducing statistical power to detect disparities.

Recent work by Yan et al. (2024) on intersectional fair ranking illustrates these challenges vividly. They propose using divergence measures to identify subgroups with statistically significant deviations in ranking utility compared to the overall population. However, they identify "exponential increase in subgroups with added sensitive features and empty/sparse subgroups with finite samples" as the main obstacles to this approach. Similarly, Sheng et al. (2025) propose a unified sparsity-based framework for evaluating algorithmic fairness that explicitly addresses "exponential growth of subgroups with finite samples." Their framework connects various sparsity measures to fairness promotion but does not resolve the underlying question: which subgroups should we evaluate?

Critically, while this literature recognizes combinatorial explosion as a significant practical challenge, it frames the problem as one of *statistical feasibility* rather than *ontological specification*. The assumption is that in principle, we know which groups matter—all combinations of protected attributes—but we face technical difficulties measuring performance across all of them. No work in this vein asks whether we *should* consider all combinations, or whether some combinations constitute meaningful social groups while others do not. The set of groups $G$ is treated as determined by the available attributes rather than as a normative or ontological question requiring justification.

## Technical Solutions to Data Sparsity

Given the recognized challenge of sparse data for intersectional groups, machine learning researchers have developed increasingly sophisticated technical solutions. These solutions generally fall into three categories: post-processing methods that enforce fairness constraints over multiple groups simultaneously, data augmentation techniques that generate synthetic data for underrepresented groups, and hierarchical approaches that leverage relationships between intersectional and marginal groups.

The *multicalibration* framework, introduced by Hébert-Johnson et al. (2018), has become a foundational approach for multi-group fairness. Multicalibration requires that predictions be well-calibrated simultaneously over a rich collection of subpopulations. Rather than requiring calibration only for the overall population or for single protected attributes, multicalibration ensures that predicted probabilities match empirical frequencies within each subgroup and prediction bin. This framework explicitly accommodates intersectional groups: if Black women constitute one subgroup in the collection, multicalibration ensures predictions are calibrated specifically for Black women, not merely for Black individuals or for women separately.

However, Hébert-Johnson et al. (2018) identify fundamental statistical tradeoffs. Sample complexity—the amount of data required for the algorithm to learn—grows exponentially with the number of class labels. More fine-grained groups require more data. The algorithm's runtime is inversely proportional to the size of the smallest group being protected: as groups become smaller, computational requirements increase dramatically. Hansen et al. (2024) explore when multicalibration post-processing is necessary, concluding that "attaining multicalibration is practically infeasible with more than a few classes." These are not merely engineering challenges to be overcome with better algorithms; they reflect information-theoretic limits on inference from finite data.

Variants of multicalibration attempt to address these limitations. La Cava et al. (2023) introduce *proportional multicalibration* (PMC), which constrains the proportion of calibration error within each bin and group rather than absolute error. This approach shows promise for "controlling simultaneous calibration fairness over intersectional groups with virtually no classification performance cost." Their work demonstrates that intersectional debiasing (treating race-gender combinations as distinct groups) produces greater reductions in subgroup calibration error than marginal debiasing (treating race and gender separately). Yet even this sophisticated approach requires *pre-specifying* which intersectional groups to protect. The technical question is how to achieve calibration for specified groups; the ontological question of which groups to specify remains unaddressed.

Data augmentation provides an alternative approach to the sparsity problem. Maheshwari et al. (2024) propose generating synthetic data for intersectional fairness by leveraging hierarchical group structure. Their method views intersectional groups as intersections of "parent" categories (e.g., Black women as the intersection of Black individuals and women) and learns transformation functions to combine parent group data for smaller intersectional groups. This approach demonstrates improved intersectional fairness across text and image datasets. However, the hierarchical assumption—that intersectional groups are products of their parent categories—embeds a particular ontological commitment about the nature of these groups, one that intersectionality theorists frequently challenge (Hancock 2007; Bowleg 2008).

Interestingly, Halevy et al. (2025) provide evidence that popular data augmentation techniques may not help and can even harm intersectional fairness. They test four versions of Fair Mixup—a technique that generates synthetic training examples by interpolating between instances—on problems with up to 81 marginalized groups. Contrary to expectations, Fair Mixup typically worsens both performance and fairness metrics. Standard Mixup combined with multicalibration post-processing proves most effective, especially for small minority groups. This finding suggests that data augmentation is not a panacea for the data sparsity problem and that careful empirical validation is required for each context.

What unites all these technical approaches is their treatment of group specification as exogenous. Multicalibration requires researchers to specify which "rich collection of subpopulations" to protect. Hierarchical models require specification of parent categories and their combinations. Synthetic data generation requires identifying which groups need augmentation. In each case, the technical problem is: *given* a set of groups $G$, how can we achieve fairness across $G$ despite limited data? The question of *how to determine $G$*—which combinations of attributes constitute meaningful groups—is not addressed as a technical, normative, or ontological question.

## Domain-Specific Applications

Applications of intersectional fairness to specific domains illustrate both the practical importance and the persistent challenges of group specification. In clinical decision-making contexts, several studies demonstrate that intersectional approaches to debiasing outperform methods that treat demographic attributes separately. Lett et al. (2025) compare intersectional versus marginal debiasing in prediction models for emergency admissions, finding that intersectional debiasing produces substantially greater reductions in subgroup calibration error: 21.2% versus 10.6% in one dataset, 27.2% versus 22.7% in another. The intersectional approach yields additional reductions in false-negative rates as well. These results provide empirical validation that intersectional groups experience distinct bias patterns not captured by marginal analyses.

Wastvedt et al. (2024) develop an intersectional framework for counterfactual fairness in risk prediction that accounts for treatment effects. Their approach recognizes that in clinical contexts, risk scores guide treatment decisions, requiring fairness metrics that account for treatment. They use causal inference techniques to simulate hypothetical outcomes under no treatment, providing a fair baseline for comparison across intersecting identity categories. While methodologically sophisticated, their approach requires researchers to specify which intersecting categories to consider—typically race-sex combinations in their examples.

Dutta et al. (2024) extend intersectional fairness to multimodal clinical predictions, showing that fairness challenges become more pronounced when multiple data modalities (imaging, text, structured data) are combined. Fairness values fluctuate with different modality information, making intersectional bias more complex to identify and mitigate. Again, the technical contribution is substantial, but the question of which intersectional groups warrant analysis is assumed to be determined by available demographic variables in datasets.

Beyond clinical applications, Hashimoto et al. (2025) develop methods for intersectional fairness in reinforcement learning with large state spaces and potentially exponentially large classes of constraints over intersecting groups. Their work demonstrates that intersectional fairness is technically feasible even in complex sequential decision-making contexts. However, the feasibility is conditional on having specified which constraints (corresponding to which groups) to enforce. The technical achievement is developing algorithms that scale to many constraints; the unresolved question is which constraints should be included.

A common pattern emerges across these applications: practitioners inherit demographic categories from existing datasets and treat intersections of these categories as the groups requiring fairness analysis. Dataset construction decisions—which demographic variables to collect, how to categorize them, how to allow individuals to self-identify—thus implicitly determine which intersectional groups receive algorithmic fairness consideration. Yet these dataset construction decisions themselves involve contested ontological and normative choices about group membership, categorization schemes, and which identities are socially salient (Scheuerman et al. 2020; Geiger et al. 2020).

## Technical Sophistication Without Ontological Engagement

The technical literature on intersectional fairness demonstrates impressive sophistication in addressing statistical challenges. Multicalibration and its variants provide theoretically grounded frameworks for simultaneous fairness across many groups. Hierarchical models and synthetic data generation offer practical approaches to data sparsity. Domain-specific applications validate that intersectional approaches outperform marginal ones in identifying and mitigating bias. This body of work has substantially advanced the technical feasibility of intersectional fairness.

Yet throughout this literature, a critical gap persists: the set of groups $G$ requiring fairness consideration is treated as an input determined by available data rather than as a substantive question requiring justification. When papers discuss challenges in specifying $G$, they frame these as *computational* or *statistical* challenges (exponential growth, sparse data, sample complexity) rather than *ontological* or *normative* questions (which groups exist? which groups matter morally?). The assumption appears to be that in principle, we know which groups to consider—ideally, all combinations of protected attributes—but we face practical limitations in achieving fairness across all of them.

This framing misses a crucial dimension of the problem. Even if we had unlimited data eliminating all statistical concerns, we would still face the question: which combinations of attributes correspond to meaningful social groups that warrant fairness consideration? Does the category "Black women over 65 with disabilities" pick out a group with shared experiences of discrimination, or is it merely an arbitrary intersection of attributes? If we have 10 binary attributes, do we evaluate fairness across all 1,024 combinations, or is there a principled basis for selecting a subset? These questions cannot be resolved through technical progress alone; they require engagement with contested philosophical questions about the nature of social groups, the ontology of intersectionality, and the normative purposes of fairness auditing. It is to these philosophical dimensions that we now turn.
