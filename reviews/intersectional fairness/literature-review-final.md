# Introduction: The Challenge of Intersectional Fairness

Algorithmic decision systems—from credit scoring to criminal risk assessment, healthcare triage to hiring—increasingly shape access to critical social goods. As concerns about algorithmic fairness have intensified, researchers have recognized that fairness evaluations focused solely on single demographic attributes (such as race or gender alone) miss crucial dimensions of discrimination. Intersectional groups—individuals characterized by multiple overlapping identity categories—may experience distinct patterns of bias that disappear when groups are analyzed separately (Foulds et al. 2020; Buolamwini and Gebru 2018). This recognition has spurred growing attention to intersectional fairness: ensuring algorithmic systems operate fairly across groups defined by combinations of protected attributes such as race, gender, age, and disability status.

Yet intersectional fairness remains extraordinarily challenging to implement in practice. Recent surveys document persistent difficulties in achieving fairness across intersectional groups, despite sophisticated technical interventions (Gohar and Cheng 2023). This paper argues that these challenges stem from a genuine dilemma involving two distinct forms of uncertainty that interact in problematic ways. First, *statistical uncertainty*: intersectional groups are often small in available datasets, making reliable performance estimation difficult and raising epistemic questions about when inferences are warranted. Second, *ontological uncertainty*: philosophical debates reveal deep disagreement about which groups intersectionality picks out—whether intersectional groups can be exhaustively specified through attribute combinations, or whether they emerge from social practices in ways that resist algorithmic enumeration.

The dilemma arises from the interaction between these two forms of uncertainty. Expanding the set of groups to respect the ontological complexity of intersectionality exacerbates statistical problems: more groups mean less data per group, increasing estimation error and epistemic uncertainty. Conversely, constraining the set of groups to ensure statistical reliability requires principled criteria for deciding which groups to include—criteria that depend on resolving contested philosophical questions about the nature of social groups, distributive justice, and epistemic authority. Improving performance on one horn of this dilemma tends to worsen the situation on the other.

This literature review synthesizes research across machine learning, philosophy, social ontology, measurement theory, normative ethics, and epistemology to show that while each horn of the dilemma is well-recognized in its respective domain, *no existing work frames their interaction as a dilemma*. Machine learning researchers treat group specification as an input to technical fairness problems, not a problem to be solved. Philosophers explore the contested ontology of intersectional groups without addressing operational constraints imposed by data limitations. Measurement theorists recognize that operationalization embeds ontological commitments but do not connect this to the statistical challenges of small groups (Jacobs and Wallach 2021). This review establishes the gap and positions our contribution: the first systematic analysis of intersectional fairness as involving a genuine dilemma where statistical and ontological uncertainties reinforce each other.

The review proceeds as follows. Section 2 surveys technical approaches in machine learning and computer science, showing sophisticated solutions to data sparsity that nevertheless assume groups are given. Section 3 examines philosophical work on intersectionality and social ontology, revealing deep disagreement about what intersectional groups are and how they should be individuated. Section 4 bridges these domains through measurement theory, showing that operationalizing fairness metrics embeds answers to contested ontological questions. Section 5 explores normative frameworks for distributive justice, demonstrating that different theories yield different answers about which groups warrant consideration. Section 6 examines epistemic dimensions, including questions of epistemic justice (who decides which groups matter?) and epistemic responsibility (when is evidence sufficient for claims about small groups?). Section 7 synthesizes these findings to articulate the full dilemma structure and document the gap in existing work. Section 8 concludes by positioning our research contribution.


# Technical Approaches to Intersectional Fairness

The machine learning and computer science literature on algorithmic fairness has made substantial progress in developing methods for auditing and improving fairness across intersectional groups. This section reviews state-of-the-art technical approaches, focusing on how they handle data sparsity and group specification. A consistent pattern emerges: while statistical challenges of small intersectional groups are well-recognized and addressed through sophisticated techniques, these approaches uniformly treat the set of groups as an input to be provided rather than a problem to be solved.

## Fairness Metrics and the Intersectional Challenge

The foundational insight motivating intersectional fairness research is that fairness on individual attributes does not guarantee fairness at their intersections. Foulds et al. (2020) demonstrate this formally through their concept of *differential fairness*, which requires fairness guarantees to hold over combinations of sensitive attributes. An algorithm might achieve statistical parity for race (equal positive prediction rates across racial groups) and for gender (equal positive prediction rates across genders) while simultaneously exhibiting large disparities for Black women compared to other race-gender combinations. This possibility arises because intersectional identities are not merely additive—the discrimination experienced by Black women cannot be decomposed into race-based discrimination plus gender-based discrimination (Bowleg 2008; Hancock 2007).

Building on this recognition, researchers have proposed fairness metrics that explicitly account for intersectionality. *Conditional demographic parity* extends traditional demographic parity (equal positive rates across groups) to require equality conditional on all combinations of protected attributes (Castelnovo et al. 2022). In principle, conditional demographic parity solves the intersectionality problem: if we condition on race, gender, age, and disability status simultaneously, we ensure fairness across all 64 possible combinations of these four binary attributes. However, Castelnovo et al. (2022) identify a fundamental challenge: the exponential growth of subgroups makes this approach computationally and practically infeasible. Each additional sensitive attribute doubles the number of groups to consider (assuming binary attributes; categorical attributes with *k* levels multiply by *k*). With even modest numbers of attributes, most intersectional groups become too small for reliable performance estimation.

Ghassemi et al. (2024) elaborate on these difficulties, noting that conditional demographic parity becomes progressively harder to achieve as conditioning variables increase in dimensionality or as model outputs become continuous rather than binary. Auditing conditional fairness requires sufficient data in each cell of the contingency table defined by attribute combinations, but many cells will be sparse or empty in finite datasets. This is not merely a computational problem—it reflects fundamental statistical limits. As the number of groups grows, the effective sample size per group shrinks, increasing variance in performance estimates and reducing statistical power to detect disparities.

Recent work by Yan et al. (2024) on intersectional fair ranking illustrates these challenges vividly. They propose using divergence measures to identify subgroups with statistically significant deviations in ranking utility compared to the overall population. However, they identify "exponential increase in subgroups with added sensitive features and empty/sparse subgroups with finite samples" as the main obstacles to this approach. Similarly, Sheng et al. (2025) propose a unified sparsity-based framework for evaluating algorithmic fairness that explicitly addresses "exponential growth of subgroups with finite samples." Their framework connects various sparsity measures to fairness promotion but does not resolve the underlying question: which subgroups should we evaluate?

Critically, while this literature recognizes combinatorial explosion as a significant practical challenge, it frames the problem as one of *statistical feasibility* rather than *ontological specification*. The assumption is that in principle, we know which groups matter—all combinations of protected attributes—but we face technical difficulties measuring performance across all of them. No work in this vein asks whether we *should* consider all combinations, or whether some combinations constitute meaningful social groups while others do not. The set of groups $G$ is treated as determined by the available attributes rather than as a normative or ontological question requiring justification.

## Technical Solutions to Data Sparsity

Given the recognized challenge of sparse data for intersectional groups, machine learning researchers have developed increasingly sophisticated technical solutions. These solutions generally fall into three categories: post-processing methods that enforce fairness constraints over multiple groups simultaneously, data augmentation techniques that generate synthetic data for underrepresented groups, and hierarchical approaches that leverage relationships between intersectional and marginal groups.

The *multicalibration* framework, introduced by Hébert-Johnson et al. (2018), has become a foundational approach for multi-group fairness. Multicalibration requires that predictions be well-calibrated simultaneously over a rich collection of subpopulations. Rather than requiring calibration only for the overall population or for single protected attributes, multicalibration ensures that predicted probabilities match empirical frequencies within each subgroup and prediction bin. This framework explicitly accommodates intersectional groups: if Black women constitute one subgroup in the collection, multicalibration ensures predictions are calibrated specifically for Black women, not merely for Black individuals or for women separately.

However, Hébert-Johnson et al. (2018) identify fundamental statistical tradeoffs. Sample complexity—the amount of data required for the algorithm to learn—grows exponentially with the number of class labels. More fine-grained groups require more data. The algorithm's runtime is inversely proportional to the size of the smallest group being protected: as groups become smaller, computational requirements increase dramatically. Hansen et al. (2024) explore when multicalibration post-processing is necessary, concluding that "attaining multicalibration is practically infeasible with more than a few classes." These are not merely engineering challenges to be overcome with better algorithms; they reflect information-theoretic limits on inference from finite data.

Variants of multicalibration attempt to address these limitations. La Cava et al. (2023) introduce *proportional multicalibration* (PMC), which constrains the proportion of calibration error within each bin and group rather than absolute error. This approach shows promise for "controlling simultaneous calibration fairness over intersectional groups with virtually no classification performance cost." Their work demonstrates that intersectional debiasing (treating race-gender combinations as distinct groups) produces greater reductions in subgroup calibration error than marginal debiasing (treating race and gender separately). Yet even this sophisticated approach requires *pre-specifying* which intersectional groups to protect. The technical question is how to achieve calibration for specified groups; the ontological question of which groups to specify remains unaddressed.

Data augmentation provides an alternative approach to the sparsity problem. Maheshwari et al. (2024) propose generating synthetic data for intersectional fairness by leveraging hierarchical group structure. Their method views intersectional groups as intersections of "parent" categories (e.g., Black women as the intersection of Black individuals and women) and learns transformation functions to combine parent group data for smaller intersectional groups. This approach demonstrates improved intersectional fairness across text and image datasets. However, the hierarchical assumption—that intersectional groups are products of their parent categories—embeds a particular ontological commitment about the nature of these groups, one that intersectionality theorists frequently challenge (Hancock 2007; Bowleg 2008).

Interestingly, Halevy et al. (2025) provide evidence that popular data augmentation techniques may not help and can even harm intersectional fairness. They test four versions of Fair Mixup—a technique that generates synthetic training examples by interpolating between instances—on problems with up to 81 marginalized groups. Contrary to expectations, Fair Mixup typically worsens both performance and fairness metrics. Standard Mixup combined with multicalibration post-processing proves most effective, especially for small minority groups. This finding suggests that data augmentation is not a panacea for the data sparsity problem and that careful empirical validation is required for each context.

What unites all these technical approaches is their treatment of group specification as exogenous. Multicalibration requires researchers to specify which "rich collection of subpopulations" to protect. Hierarchical models require specification of parent categories and their combinations. Synthetic data generation requires identifying which groups need augmentation. In each case, the technical problem is: *given* a set of groups $G$, how can we achieve fairness across $G$ despite limited data? The question of *how to determine $G$*—which combinations of attributes constitute meaningful groups—is not addressed as a technical, normative, or ontological question.

## Domain-Specific Applications

Applications of intersectional fairness to specific domains illustrate both the practical importance and the persistent challenges of group specification. In clinical decision-making contexts, several studies demonstrate that intersectional approaches to debiasing outperform methods that treat demographic attributes separately. Lett et al. (2025) compare intersectional versus marginal debiasing in prediction models for emergency admissions, finding that intersectional debiasing produces substantially greater reductions in subgroup calibration error: 21.2% versus 10.6% in one dataset, 27.2% versus 22.7% in another. The intersectional approach yields additional reductions in false-negative rates as well. These results provide empirical validation that intersectional groups experience distinct bias patterns not captured by marginal analyses.

Wastvedt et al. (2024) develop an intersectional framework for counterfactual fairness in risk prediction that accounts for treatment effects. Their approach recognizes that in clinical contexts, risk scores guide treatment decisions, requiring fairness metrics that account for treatment. They use causal inference techniques to simulate hypothetical outcomes under no treatment, providing a fair baseline for comparison across intersecting identity categories. While methodologically sophisticated, their approach requires researchers to specify which intersecting categories to consider—typically race-sex combinations in their examples.

Dutta et al. (2024) extend intersectional fairness to multimodal clinical predictions, showing that fairness challenges become more pronounced when multiple data modalities (imaging, text, structured data) are combined. Fairness values fluctuate with different modality information, making intersectional bias more complex to identify and mitigate. Again, the technical contribution is substantial, but the question of which intersectional groups warrant analysis is assumed to be determined by available demographic variables in datasets.

Beyond clinical applications, Hashimoto et al. (2025) develop methods for intersectional fairness in reinforcement learning with large state spaces and potentially exponentially large classes of constraints over intersecting groups. Their work demonstrates that intersectional fairness is technically feasible even in complex sequential decision-making contexts. However, the feasibility is conditional on having specified which constraints (corresponding to which groups) to enforce. The technical achievement is developing algorithms that scale to many constraints; the unresolved question is which constraints should be included.

A common pattern emerges across these applications: practitioners inherit demographic categories from existing datasets and treat intersections of these categories as the groups requiring fairness analysis. Dataset construction decisions—which demographic variables to collect, how to categorize them, how to allow individuals to self-identify—thus implicitly determine which intersectional groups receive algorithmic fairness consideration. Yet these dataset construction decisions themselves involve contested ontological and normative choices about group membership, categorization schemes, and which identities are socially salient (Scheuerman et al. 2020; Geiger et al. 2020).

## Technical Sophistication Without Ontological Engagement

The technical literature on intersectional fairness demonstrates impressive sophistication in addressing statistical challenges. Multicalibration and its variants provide theoretically grounded frameworks for simultaneous fairness across many groups. Hierarchical models and synthetic data generation offer practical approaches to data sparsity. Domain-specific applications validate that intersectional approaches outperform marginal ones in identifying and mitigating bias. This body of work has substantially advanced the technical feasibility of intersectional fairness.

Yet throughout this literature, a critical gap persists: the set of groups $G$ requiring fairness consideration is treated as an input determined by available data rather than as a substantive question requiring justification. When papers discuss challenges in specifying $G$, they frame these as *computational* or *statistical* challenges (exponential growth, sparse data, sample complexity) rather than *ontological* or *normative* questions (which groups exist? which groups matter morally?). The assumption appears to be that in principle, we know which groups to consider—ideally, all combinations of protected attributes—but we face practical limitations in achieving fairness across all of them.

This framing misses a crucial dimension of the problem. Even if we had unlimited data eliminating all statistical concerns, we would still face the question: which combinations of attributes correspond to meaningful social groups that warrant fairness consideration? Does the category "Black women over 65 with disabilities" pick out a group with shared experiences of discrimination, or is it merely an arbitrary intersection of attributes? If we have 10 binary attributes, do we evaluate fairness across all 1,024 combinations, or is there a principled basis for selecting a subset? These questions cannot be resolved through technical progress alone; they require engagement with contested philosophical questions about the nature of social groups, the ontology of intersectionality, and the normative purposes of fairness auditing. It is to these philosophical dimensions that we now turn.


# Philosophical Foundations of Intersectionality

While machine learning research treats intersectional group specification as a technical input problem, philosophical scholarship reveals deep and unresolved disagreements about what intersectionality is, which groups it picks out, and how those groups should be individuated. This section examines philosophical debates about the nature of intersectionality as a theoretical framework and about the ontology of social groups more broadly. These debates establish that ontological uncertainty about intersectional groups is not a gap in current knowledge to be filled through further research, but rather reflects genuinely contested conceptual terrain where multiple incompatible frameworks have philosophical pedigree.

## Intersectionality as a Contested Concept

The term "intersectionality" originated in Black feminist legal scholarship to identify how individuals with multiple marginalized identities experience forms of oppression that cannot be understood through single-axis frameworks (Crenshaw 1989). Yet what intersectionality requires and what it means to adopt an intersectional approach remain matters of substantial disagreement. As Collins (2015) documents in her analysis of "intersectionality's definitional dilemmas," the concept faces persistent questions about its scope, meaning, and methodological implications. Crenshaw herself characterizes intersectionality not as a theory with determinate implications but as "an analytic sensibility, a way of thinking about identity and its relationship to power" (Crenshaw 2015). This characterization suggests intersectionality functions more as a critical orientation than as a theoretical framework with clear operational consequences.

Philosophical analyses of intersectionality reveal multiple incompatible interpretations. Garry (2011) distinguishes three characteristic uses: as metaphor (intersecting roads), as heuristic (tool for discovery), and as paradigm (comprehensive framework). Each interpretation suggests different relationships between identity categories and lived experience. Ruíz (2017) emphasizes that intersectionality arose within Black feminist thought to identify "interlocking systems of oppression" and notes persistent debates about whether the concept has coherent boundaries or whether it risks becoming diluted through broad application. Cho et al. (2013) distinguish structural, political, and representational dimensions of intersectionality, arguing that different applications may emphasize different dimensions without necessarily contradicting each other. Yet this pluralism about intersectionality's meaning creates uncertainty for operationalization: if intersectionality means different things in different contexts, how should algorithmic fairness researchers operationalize it?

Perhaps the most consequential philosophical disagreement concerns what McCall (2005) identifies as three distinct approaches to categories in intersectional analysis. The *anticategorical* approach rejects fixed categories altogether, viewing them as reifying social constructions that obscure the fluidity and complexity of lived experience. From this perspective, attempting to enumerate intersectional groups for algorithmic fairness analysis fundamentally misunderstands intersectionality, which critiques rather than reproduces categorical thinking. The *intracategorical* approach focuses on particular intersections as case studies—studying the specific experiences of, say, Black lesbian women—without presuming that all possible intersections are equally meaningful or that they can be exhaustively enumerated. The *intercategorical* approach provisionally adopts existing categories to analyze inequality patterns across groups, acknowledging that categories are constructed but treating them as analytically useful.

Of these three approaches, only the intercategorical appears compatible with algorithmic fairness applications that require specifying discrete groups for performance evaluation. Yet even the intercategorical approach is "provisional"—it uses categories as analytical tools without claiming they carve nature at its joints or exhaust meaningful group identities. This provisionality creates tension for algorithmic systems that must make definite commitments about which groups to monitor. The anticategorical approach would reject the entire enterprise of group-based fairness metrics as misconceived. The intracategorical approach might accept studying specific intersections but would resist the idea that we can or should enumerate all intersections warranting consideration. Only by adopting the intercategorical approach—which intersectionality theorists recognize as one option among several—can algorithmic fairness research proceed, and even then without strong ontological commitments about which group categories are real or fundamental.

## Causal versus Emergent Interpretations

A more specific philosophical debate concerns whether intersectional groups should be understood through causal-combinatorial frameworks or through emergence frameworks. This debate has direct implications for whether intersectional groups can be specified algorithmically through attribute combinations.

Bright et al. (2016) propose interpreting intersectionality through graphical causal models, arguing that characteristic claims in intersectionality literature can be understood as claims about causal effects of occupying intersecting identity categories. For instance, the claim that Black women face distinct forms of discrimination can be interpreted as asserting that race and gender causally interact in producing outcomes: being both Black and a woman has effects not reducible to the sum of being Black plus being a woman. This interpretation is explicitly designed to make intersectionality theory amenable to empirical testing through standard causal inference techniques. Importantly, it suggests that intersectional groups can be specified through combinations of attributes: if we know an individual's race and gender, we can place them in the appropriate intersectional category for analyzing causal effects.

However, this causal-combinatorial interpretation faces challenges from emergence-based accounts. Jorba and López de Sa (2024) argue for "intersectionality as emergence," proposing that intersectional experiences emerge from the conjunction of social categories when social structures make them relevant vis-à-vis discrimination and privilege. On their account, intersectionality is not merely about causal interactions between attributes but about emergent properties that arise from social structures. An intersectional group like "Black women" is not simply the set of individuals who are both Black and women; it is a group constituted through social practices and structures that treat Black women as a distinct category. This emergent interpretation is explicitly metaphysically neutral—it does not presuppose specific views about social construction—but it suggests that intersectional groups cannot be exhaustively enumerated through attribute combinations. Which groups emerge depends on context-specific social structures, not on combinatorial logic.

The tension between these interpretations becomes clear in Hancock's (2007) analysis of "when multiplication doesn't equal quick addition." Hancock argues that intersectionality fundamentally rejects additive models where, for example, being a Black woman equals being Black plus being a woman. Instead, intersectional identities involve non-additive interactions that create distinct experiences. Bowleg (2008) makes this point vivid with her formulation: "Black + Lesbian + Woman ≠ Black Lesbian Woman." The left side represents an additive model; the right side represents an intersectional identity that cannot be decomposed into its constituent parts.

For algorithmic fairness, this philosophical disagreement matters enormously. If the causal-combinatorial interpretation is correct, then in principle we can specify intersectional groups through database queries combining protected attributes: WHERE race='Black' AND gender='Female' picks out the group. If the emergence interpretation is correct, then intersectional groups depend on social contexts and practices in ways that resist algorithmic enumeration. We cannot determine which groups exist merely by examining our dataset attributes; we need substantive knowledge of social structures and how they constitute groups. The philosophical debate between these interpretations is not settled, leaving deep uncertainty about the ontology of intersectional groups.

## Social Ontology of Groups

Debates about intersectionality connect to broader philosophical questions in social ontology about the nature and individuation of social groups. How are social groups constituted? Can membership in social groups be determined through individuals' attributes, or do groups depend on social relations, practices, or structures that transcend individual properties?

Epstein (2019) distinguishes between two types of social groups. Type 1 groups—such as committees, corporations, or sports teams—have clear membership criteria, organizational structure, and often require volitional participation. Type 2 groups—such as racial or ethnic groups—lack these features. For type 2 groups, Epstein argues that "common possession of the racial or ethnic feature constitutes at least a minimal structure," though this structure is "attributive rather than relational." This analysis suggests that demographic groups like "Black" or "women" can be individuated through attributes (racial or gender features), supporting the idea that intersectional combinations of these attributes also define groups.

However, other accounts of social group ontology resist attribute-based individuation. Ritchie (2020) develops a structuralist ontology where social groups are constituted by social structures—networks of relations constitutively dependent on social factors. On this view, what makes someone a member of a group is not possession of intrinsic attributes but rather their position in social structures. Thomasson (2019) surveys competing accounts of what social groups are metaphysically, noting fundamental disagreements about whether groups are sets of individuals, structured entities, mereological sums, or something else entirely. Different accounts yield different answers to questions about group identity and persistence over time.

Haslanger (2012) distinguishes between causal social construction (where social factors cause kinds to exist) and constitutive social construction (where social factors are ontologically constitutive of kinds). She further distinguishes construction of representations (concepts, categories) from construction of objects (people, groups, properties). Applying this framework to race and gender, Haslanger argues these categories are both causally and constitutively socially constructed. For algorithmic fairness, this raises questions about whether database categories for race and gender successfully refer to socially constructed groups or merely represent contested conceptual schemes.

Sveinsdóttir's (2013) conferralist account of social construction offers yet another framework. On her view, something is socially constructed if it has social significance in a context such that items taken to have the relevant feature are conferred extra social constraints and enablements. This account makes group membership depend on context-specific social practices of conferral rather than on intrinsic attributes or even stable social structures. Whether someone is treated as a member of a group depends on local practices, suggesting that group membership may not be algorithmically determinable from demographic data alone.

Debates in feminist metaphysics further complicate the picture. As Haslanger and Ásta (2017) document, philosophers disagree profoundly about how social practices, attitudes, biological regularities, or other factors "set up or anchor" gender and race categories. Mikkola (2016) reviews feminist debates about whether gender is best understood through attribute-based accounts (possession of certain biological, psychological, or social features) or through practice-based accounts (participation in gendered social practices). These debates have direct analogues for intersectional groups: should "Black women" be understood as individuals possessing both racial and gender attributes, or as individuals participating in social practices specific to Black women?

## Implications for Algorithmic Fairness

The philosophical literature on intersectionality and social ontology establishes several conclusions relevant to algorithmic fairness. First, there is no philosophical consensus on what intersectionality is or what it requires. Multiple interpretive frameworks—anticategorical, intracategorical, intercategorical—coexist, and only one (intercategorical) is clearly compatible with algorithmic fairness metrics requiring enumerated groups. Second, even among those who accept the intercategorical approach, there is deep disagreement about whether intersectional groups are constituted through attribute combinations (supporting algorithmic enumeration) or through emergent social structures and practices (resisting it). Third, broader social ontology debates reveal fundamental disagreements about how social groups are individuated, with some accounts supporting attribute-based specification and others requiring attention to context-specific social relations and practices.

These are not gaps in current knowledge that further empirical or philosophical research might fill. They are genuinely contested areas where multiple incompatible positions have strong philosophical arguments supporting them. Machine learning researchers cannot simply wait for philosophers to resolve these debates and then implement the consensus answer. The debates may be irresolvable in principle, reflecting different legitimate purposes for theorizing about groups and different normative commitments about what matters morally.

For algorithmic fairness research, this creates profound ontological uncertainty. When we ask "which intersectional groups should we evaluate for fairness?", we are asking a question that presupposes answers to contested philosophical questions about group ontology. Different philosophical frameworks give different answers. An attribute-based ontology suggests we can enumerate groups through combinations: all individuals with race=Black AND gender=Female form a group. A practice-based or structural ontology suggests groups cannot be enumerated this way; which groups exist depends on context-specific social structures that the algorithm cannot access through demographic attributes alone. A conferralist account suggests group membership depends on local practices of social conferral that vary across contexts.

The machine learning literature treats group specification as a technical input problem, assuming that we know which groups to consider (ideally all attribute combinations, practically constrained by data availability). The philosophical literature reveals this assumption is untenable. We face deep ontological uncertainty about which groups exist and how they should be individuated. This uncertainty is not merely about what is true but about which framework for understanding groups is appropriate for algorithmic fairness contexts. As we will see in subsequent sections, this ontological uncertainty interacts with measurement challenges (Section 4), normative questions about distributive justice (Section 5), and epistemic questions about authority and responsibility (Section 6) to create a genuine dilemma for intersectional fairness.


# Measurement and Operationalization

The gap between technical approaches that assume groups are given and philosophical debates revealing deep ontological uncertainty might seem bridgeable through careful operationalization: even if philosophers disagree about the metaphysics of groups, perhaps we can develop measurement practices that work regardless. This section examines research on measurement theory and construct validity in algorithmic fairness, showing that operationalization does not bypass ontological questions but rather embeds particular answers to them. Measurement choices encode implicit ontological commitments, and the validity of fairness assessments depends on the validity of those underlying ontological assumptions.

## Construct Validity and the Measurement of Fairness

Jacobs and Wallach (2021) make a foundational contribution by applying measurement theory from quantitative social sciences to algorithmic fairness. They observe that computational systems routinely involve unobservable theoretical constructs—such as teacher effectiveness, socioeconomic status, or risk of recidivism—that must be inferred through operationalization via measurement models. Operationalization involves making assumptions and introduces potential for mismatches between the theoretical construct (what we aim to measure) and its measurement (what we actually measure). Importantly, Jacobs and Wallach argue that many fairness harms are direct results of such construct-measurement mismatches.

Their analysis distinguishes between fairness as a property of algorithmic systems and fairness as an essentially contested construct with different theoretical understandings in different contexts. Debates about fairness definitions—should we require demographic parity, equalized odds, calibration, or some other metric?—are not merely technical disagreements about operationalization but reflect "different theoretical understandings" of what fairness means. Different contexts and purposes may warrant different theoretical understandings of fairness, and thus different operationalizations.

This framework has direct implications for intersectional groups. If fairness itself is an essentially contested construct, then so too is the question of which groups warrant fairness consideration. Different theoretical understandings of fairness might entail different answers about group specification. An understanding of fairness focused on legal compliance might specify groups based on legally protected categories (race, sex, age). An understanding focused on substantive non-discrimination might require identifying groups that experience distinctive patterns of oppression, which may not align with legal categories. An understanding focused on distributive justice might prioritize groups below particular welfare thresholds regardless of demographic categories.

Selbst et al. (2019) identify "abstraction traps" where technical fairness interventions fail due to mismatches between abstract formal models and concrete social contexts. One key abstraction trap involves treating fairness as context-independent: metrics valid in one context may be invalid in another because the theoretical construct of fairness differs. Similarly, group categories that successfully operationalize meaningful social groups in one context may fail in another. The abstraction from messy social reality to clean dataset categories is not neutral; it embeds decisions about which aspects of social reality matter and how they should be represented.

Blodgett et al. (2020) provide a critical survey of how "bias" is operationalized in natural language processing research, finding that many operationalizations have questionable construct validity—it is unclear what they actually measure or whether it corresponds to any coherent theoretical construct. Some operationalizations labeled "bias detection" measure statistical correlations without establishing that these correlations reflect actual bias in any normatively relevant sense. Others assume particular definitions of bias without justifying why those definitions are appropriate for the context. The lesson extends beyond NLP: operationalizing contested concepts like bias or fairness requires explicit theoretical grounding and validation that the measurement captures what we claim to be measuring.

Hellman (2020) argues that many algorithmic fairness metrics fail to capture normatively significant aspects of fairness. Metrics may reliably measure *something*—statistical patterns in model outputs—but that something may not be fairness as normatively understood. This is a construct validity failure: the measurement does not correspond to the construct. For intersectional groups specifically, we face questions about whether our category specifications have construct validity. When we operationalize "Black women" as individuals with race=Black AND gender=Female in our dataset, does this successfully capture the group that experiences distinctive intersectional oppression? Or does it merely capture an arbitrary database combination?

## Validity of Demographic Categories

Construct validity questions become particularly acute when examining how demographic categories are operationalized in datasets used for fairness evaluation. Scheuerman et al. (2020) analyze how race and gender are constructed in image databases for facial analysis, finding wide variation in how these categories are defined, measured, and annotated. Different datasets use incompatible categorization schemes, reflecting different implicit assumptions about what race and gender are. Some datasets treat race as biological, others as social; some allow only binary gender categories, others recognize non-binary identities. These measurement choices reflect deep ontological commitments about the nature of demographic categories—commitments that are often unstated and unexamined.

Crucially, Scheuerman et al. show that these operationalization differences are not merely technical details but reflect substantive disagreements about social ontology. If race is understood as biological (as some facial recognition datasets implicitly assume), then racial categories might be determinable from physical features. If race is understood as socially constructed (as social scientists generally argue), then racial categories depend on social context and self-identification in ways that facial features cannot capture. Different ontological assumptions lead to different valid operationalizations, and there is no neutral "correct" way to categorize individuals by race or gender that is independent of theoretical commitments.

Fournier-Montgieux et al. (2025) demonstrate that the validity of fairness auditing hinges on the reliability of demographic attribute inference. Measurement error in demographic categories directly undermines fairness assessment, leading to biased and high-variance estimates of fairness metrics. They propose improved demographic attribute inference pipelines, but their work also highlights a fundamental challenge: even with perfect inference algorithms, we face questions about what we are inferring. Are we inferring individuals' self-identified categories? Categories assigned by others? Categories determined by physical features? Each choice embeds different ontological assumptions about what demographic categories are.

The landmark study by Obermeyer et al. (2019) on racial bias in healthcare algorithms illustrates construct validity failure with serious consequences. The algorithm used healthcare costs as a proxy for healthcare needs, reasoning that higher costs indicate greater medical need. However, costs systematically differ by race even for the same health needs due to differential access to care. Black patients must be substantially sicker than white patients to generate the same healthcare costs. The algorithm thus fails to measure what it purports to measure: health needs. This is a textbook construct validity failure, and it demonstrates how measurement choices can introduce bias even when the algorithm itself implements fairness constraints correctly on the measured construct.

Barocas et al. (2020) examine hidden assumptions in counterfactual fairness approaches, which aim to ensure that predictions would be the same in counterfactual scenarios where individuals have different demographic attributes. They argue that counterfactual fairness assumes the ability to intervene on demographic attributes—to consider what would happen if this Black person were instead white, or if this woman were instead a man. However, such interventions may be conceptually incoherent if demographic attributes are constitutively socially constructed. If being Black or being a woman is not a matter of having certain intrinsic properties but rather of being embedded in particular social structures and practices, then there may be no coherent sense in which we can hold everything else fixed while changing only race or gender. The measurement framework presupposes an attribute-based ontology that may be inappropriate.

Mayson (2019) analyzes bias in criminal risk assessment instruments, arguing that many instruments have poor construct validity because they measure correlates of outcomes rather than theoretically meaningful constructs of risk. More fundamentally, she identifies "label bias": the outcomes these systems predict (arrest, conviction, incarceration) themselves reflect biased processes rather than neutral measures of criminality. When we train models to predict arrest rates across demographic groups and then audit them for fairness, we may achieve perfect fairness metrics while perpetuating injustice if arrests themselves reflect racial bias. The construct "arrest risk" is not the same as the construct "criminality" or "public safety risk," and treating them as equivalent embeds particular assumptions about the validity of criminal justice system outcomes.

## Operationalization Embeds Ontology

The measurement literature establishes that operationalizing fairness metrics and demographic categories is not a neutral technical process but one that embeds substantive ontological commitments. When we specify intersectional groups for fairness evaluation through database queries combining protected attributes, we are implicitly adopting an attribute-based ontology where groups are defined by attribute possession. This ontology may be appropriate in some contexts, but it is not the only philosophical option and may conflict with practice-based or structural accounts of groups.

Passi and Barocas (2019) study how fairness problems are formulated in practice, finding that problem formulation choices—including how to define groups and outcomes—raise significant fairness concerns. Different formulations can raise profoundly different ethical issues, and whether we consider a system fair often depends as much on problem formulation as on model properties. Formulating an intersectional fairness problem requires deciding which intersections constitute groups: should we evaluate fairness for all combinations of race and gender? What about three-way intersections with age? Four-way with disability status? Each choice embeds commitments about which groups exist and matter.

Green (2020) argues that risk assessment instruments have fundamental epistemic limitations that undermine both accuracy and fairness claims. These instruments purport to measure constructs like "risk of recidivism" but cannot reliably measure these constructs given available data and methods. Error metrics are poor proxies for individual equity or social well-being. When measurement validity is compromised, fairness metrics computed on those measurements inherit the invalidity. For intersectional groups, this problem compounds: if we have questionable construct validity in our demographic categories and in our outcome measures, then fairness metrics computed across intersectional demographic groups have doubly questionable validity.

Geiger et al. (2020) document widespread lack of transparency about how training data categories are defined and measured in machine learning applications for social computing. Many papers do not report where human-labeled training data comes from, how categories were defined, or what validity checks were performed. Without clear construct definitions and validity assessment, it is unclear what categories actually capture or whether they successfully operationalize any coherent theoretical construct. This opacity extends to demographic categories: datasets rarely document the ontological assumptions behind their race and gender categorization schemes or provide evidence that these schemes have construct validity for the intended uses.

## Measurement as Ontological Commitment

The measurement literature reveals that we cannot bypass ontological questions about groups through operational definitions. Operationalization does not replace theoretical understanding; it presupposes it. Every measurement choice embeds answers to ontological questions: What are we measuring? What makes this measurement valid? What is the relationship between the construct and its operationalization?

For intersectional fairness, specifying groups through dataset attributes (race=Black AND gender=Female) operationalizes an attribute-based ontology of groups. This may be appropriate if groups are in fact constituted through attribute possession, as some philosophical accounts suggest. But if groups are constituted through social practices, structures, or conferral—as other philosophical accounts argue—then attribute-based operationalization may have poor construct validity. The measurement successfully captures attribute combinations but fails to capture groups as socially meaningful entities.

Critically, we cannot determine construct validity purely empirically. Validation requires theoretical justification that the operationalization captures the intended construct. In social science contexts, this requires substantive knowledge of social structures, practices, and how groups are actually constituted in particular contexts. Algorithmic fairness researchers cannot defer these questions to dataset creators or treat demographic categories as given. The choice of which categories to include, how to define them, and how to combine them into intersectional groups involves irreducibly theoretical and normative judgments about the nature of groups and which groups matter for fairness.

The measurement literature thus illuminates but does not resolve the dilemma. It shows that operationalizing intersectional fairness requires making ontological commitments about groups—commitments that Section 3 established are deeply contested philosophically. Different ontological frameworks suggest different valid operationalizations, and there is no neutral measurement scheme that works regardless of ontology. Measurement choices must be justified through theoretical arguments about what groups are and how they are constituted. Without resolving the philosophical debates, we cannot determine which operationalizations have construct validity for intersectional fairness assessment.


# Normative Frameworks for Fairness Trade-offs

Even if we could resolve ontological uncertainties about which groups exist, we would face normative questions about which groups warrant fairness consideration and how to handle inevitable trade-offs between fairness for different groups. This section examines how different theories of distributive justice imply different answers to group specification questions. Rather than providing a univocal normative framework for intersectional fairness, the ethics literature reveals pluralism: multiple defensible normative frameworks yield incompatible practical guidance about which groups should be prioritized and how to navigate fairness-utility and inter-group fairness trade-offs.

## Distributive Justice Principles and Group Specification

Kuppler et al. (2022) make a crucial distinction between fairness as a property of prediction algorithms and justice as a property of allocation principles in decision-making systems. Much algorithmic fairness research conflates these, with the consequence that distributive justice concerns are not addressed explicitly. When we ask which groups should receive fairness consideration, we are asking a question of distributive justice: how should we distribute the benefits and burdens of algorithmic systems across social groups?

Classical distributive justice theories provide different answers. *Egalitarianism* requires equal outcomes (or opportunities, or resources) across groups, suggesting that all groups should receive equal consideration in fairness metrics. If we take egalitarianism seriously, we should evaluate fairness across all intersectional groups and aim for equality across all of them. This implies comprehensive intersectional specification: every combination of protected attributes defines a group warranting equal treatment. The statistical infeasibility of achieving this (due to data sparsity for small groups) becomes a practical constraint on realizing justice, not a reason to limit our moral concern.

*Prioritarianism*, as articulated by Parfit (1997), gives greater moral weight to benefits to those who are worse off. Unlike egalitarianism, prioritarianism does not require equality per se but rather weighted concern for the worst-off. Applying prioritarianism to algorithmic fairness suggests we should identify which groups are worst-served by current systems and prioritize improvements for those groups, even if this means accepting inequality (Kuppler et al. 2022; Binns 2024). This framework could justify limiting group specification to groups experiencing the worst outcomes rather than attempting comprehensive intersectional enumeration. However, it requires empirical knowledge of which groups are worst-off—knowledge that itself requires fairness measurement across groups—and it requires normative judgments about how much weight to give to the worst-off relative to other groups.

*Sufficientarianism*, developed by Frankfurt (1987) and others, holds that what matters morally is not equality or weighted concern for the worst-off but rather ensuring everyone has "enough"—that all groups meet some threshold of adequate treatment. Shields (2012) discusses "priority shift" views where different distributive principles apply below and above the threshold: strict priority for bringing groups above the threshold, more permissive principles once all groups meet basic adequacy. Herlitz (2019) argues that sufficientarian principles are indispensable when addressing desperately badly-off groups, with whom trade-offs should not be permitted: we cannot sacrifice improvements for groups below the threshold for the sake of overall fairness improvements.

Applied to intersectional fairness, sufficientarianism suggests we should focus on identifying groups that fall below adequacy thresholds (perhaps measured through false negative rates in consequential contexts like healthcare or criminal justice) and prioritize ensuring all groups exceed thresholds. This could substantially constrain the group specification problem: rather than considering all possible intersectional groups, we focus on those demonstrably below adequacy thresholds. However, sufficientarianism faces challenges in specifying what counts as "enough" and determining appropriate thresholds, which may themselves depend on context and contested value judgments (Shields 2012).

Binns (2024) proposes using distributive justice principles to formalize approximate justice in algorithmic fairness. Different principles—egalitarianism, desert-based justice, prioritarianism, sufficientarianism, consequentialism—can be formalized as mathematical formulas allowing patterns like maximin (egalitarian), prioritarian weighting, or threshold-based (sufficientarian) evaluation. He argues that a fairness criterion checking if approximately equal shares from all groups are above a threshold accounts for structural injustices under sufficientarianism, leading to egalitarian distributive patterns in practice. Yet this synthesis presupposes agreement on which principle to adopt and what counts as adequate thresholds—agreements that remain contested.

## Leveling Down and the Limits of Equality

A central debate in distributive justice concerns the "leveling down" objection to egalitarianism: if we can achieve equality only by making some groups worse off without benefiting anyone, should we do it? This debate has direct implications for intersectional fairness when satisfying fairness constraints for all groups simultaneously would require reducing performance for some groups.

Holm (2023) examines egalitarianism in algorithmic fairness, interpreting classification parity criteria (requiring equality across groups in error rates or other performance measures) as embodying deontic egalitarianism. He acknowledges the leveling down objection but provides an egalitarian response: equality itself has intrinsic value that may justify leveling down in some cases, particularly when inequality reflects wrongful discrimination. The argument depends on distinguishing between instrumental values (equality as means to other goods) and intrinsic values (equality as good in itself).

Mittelstadt et al. (2023) challenge this egalitarian defense, arguing that leveling down should be rejected in fair machine learning because it unnecessarily harms advantaged groups when performance is intrinsically valuable, demonstrates lack of equal concern for affected groups, and fails to meet the aims of many viable distributive justice theories. They contend that strict egalitarianism should not be the default framework for algorithmic fairness, and that different contexts may warrant different distributive principles.

This debate matters for intersectional fairness because achieving fairness across many small intersectional groups often requires trade-offs. Satisfying stringent fairness constraints for groups with sparse data may require sacrificing overall accuracy or accepting worse performance for groups with more data. If we reject leveling down, we may accept inequality across intersectional groups when improving outcomes for small groups would require harming larger groups. If we embrace egalitarianism, we may accept performance degradation to achieve parity. Different normative frameworks justify different choices, and there is no consensus on which framework is appropriate.

Hertweck et al. (2022) propose a justice-based framework for analyzing fairness-utility trade-offs, arguing that these should be understood as normative questions requiring ethical analysis rather than as purely technical optimization problems. The fairness-utility trade-off—where improving fairness metrics reduces overall accuracy—can be large in practice. So too can trade-offs between public safety and fairness notions. Deciding how to navigate these trade-offs requires normative judgment about which values take priority and under what conditions. Different justice frameworks yield different answers.

Lee et al. (2021) argue that reductionist representations of fairness in narrow toolkits bear little resemblance to real-life fairness considerations, which are highly contextual. Fairness metrics implemented in algorithmic fairness packages are difficult to integrate into broader ethical assessments precisely because they encode particular normative commitments that may not align with the values at stake in specific contexts. Drawing on welfare economics and ethical philosophy, they advocate for richer frameworks that acknowledge context-dependence and pluralism about values.

## Context-Dependence and Normative Pluralism

Green (2022) argues for moving from formal to substantive algorithmic fairness, distinguishing mathematical fairness definitions (formal fairness) from fairness that accounts for social and political context (substantive fairness). The "impossibility of fairness"—the mathematical incompatibility between different fairness definitions like demographic parity and equalized odds—arises partly because different definitions embed different normative commitments appropriate for different contexts. There is no universal fairness metric precisely because fairness is context-dependent and value-laden.

This context-dependence extends to group specification. Which groups warrant fairness consideration depends on the purpose of the algorithmic system, the social context, the stakes for affected individuals, and the relevant legal and ethical norms. In hiring contexts governed by anti-discrimination law, legally protected categories may determine relevant groups. In healthcare allocation contexts, groups experiencing health disparities may be salient regardless of legal categories. In criminal justice contexts, groups with histories of discriminatory policing warrant special attention. Different contexts involve different groups, and there is no context-independent answer to which groups matter.

Corbett-Davies and Goel (2018) provide influential analysis of the normative foundations of different fairness metrics, showing that different metrics embody different normative commitments about what fairness requires. No metric is universally appropriate; choosing a metric requires normative judgment about goals and context. The same pluralism applies to group specification: no single specification of groups is universally appropriate. Choosing which groups to include requires context-specific normative judgment.

Binns (2018) explicitly connects algorithmic fairness metrics to theories from political philosophy, making implicit normative commitments explicit. Different contexts and purposes require different normative frameworks, and thus different fairness approaches. He demonstrates that technical choices about fairness metrics are inseparable from political-philosophical commitments. By extension, technical choices about which groups to monitor for fairness are inseparable from normative judgments about which groups matter morally and practically in particular contexts.

## Normative Uncertainty Compounding Ontological Uncertainty

The normative literature establishes that even with perfect knowledge of which groups exist (resolving ontological uncertainty), we would face genuine disagreement about which groups warrant fairness consideration and how to handle trade-offs. Egalitarian frameworks suggest comprehensive specification of all intersectional groups with equal weight to all. Prioritarian frameworks suggest focus on worst-off groups with weighted concern. Sufficientarian frameworks suggest identifying groups below adequacy thresholds and prioritizing bringing them above thresholds. Different frameworks justify different group specifications.

Moreover, these normative frameworks interact with statistical constraints in complex ways. Egalitarianism requires equal consideration for all groups, including small intersectional groups with sparse data, potentially demanding statistical precision we cannot achieve. Prioritarianism requires identifying worst-off groups, which itself requires fairness measurement—but small groups have high measurement uncertainty, making it unclear which groups are actually worst-off. Sufficientarianism requires determining adequacy thresholds and identifying groups below them, but again faces measurement uncertainty for small groups.

Critically, there is no philosophical consensus on which normative framework is appropriate for algorithmic fairness contexts. Crisp (2003) analyzes the relationship between egalitarianism and prioritarianism, arguing for prioritarianism grounded in compassion rather than equality. But this is one philosophical position among several, not a consensus view. The discipline of normative ethics exhibits pluralism: multiple incompatible frameworks have strong arguments in their favor, and different frameworks may be appropriate in different contexts.

For intersectional algorithmic fairness, this normative pluralism compounds the ontological uncertainty identified in Section 3. We face uncertainty not only about which groups exist but also about which existing groups warrant moral concern, how much concern they warrant relative to each other, and how to handle trade-offs between groups. Technical solutions that assume these questions have determinate answers—that we know which groups to specify and how to weight them—misunderstand the normative landscape. The questions are genuinely contested, with reasonable people holding incompatible positions based on defensible ethical frameworks.


# Epistemic Dimensions

Beyond ontological questions about which groups exist and normative questions about which groups matter morally, intersectional fairness raises epistemic questions: Who has authority to determine which groups warrant consideration? What epistemic responsibilities do practitioners have when making claims about groups with limited data? When is evidence sufficient to support fairness claims about small intersectional groups? This section examines how epistemic justice frameworks and analyses of uncertainty in machine learning illuminate often-overlooked dimensions of the intersectionality dilemma.

## Epistemic Justice and Group Categories

Fricker's (2007) foundational work on epistemic injustice distinguishes two forms: *testimonial injustice*, where speakers receive credibility deficits due to prejudice, and *hermeneutical injustice*, where gaps in collective interpretive resources prevent marginalized groups from making sense of their experiences. Algorithmic fairness systems can perpetuate both forms of epistemic injustice. Testimonial injustice occurs when marginalized groups' testimony about their experiences of algorithmic discrimination is discounted or dismissed. Hermeneutical injustice occurs when the categories and frameworks used in fairness evaluation lack concepts to capture forms of discrimination experienced by marginalized groups, particularly intersectional groups.

Anderson (2012) extends epistemic justice to social institutions, arguing that institutions can systematically advantage or disadvantage different knowers. Algorithmic systems are institutions in this sense: they embody particular ways of categorizing people, collecting data, making inferences, and distributing outcomes. These systems can promote epistemic justice by incorporating diverse perspectives and allowing affected communities to participate in defining categories and standards. Or they can undermine epistemic justice by excluding marginalized groups from processes that define, judge, and decide about them.

The question of which intersectional groups to specify for fairness evaluation is partly an epistemic justice question: Who gets to decide which groups matter? If machine learning researchers and data scientists—who are demographically unrepresentative of affected populations—unilaterally determine which group categories to track, this concentrates epistemic authority in ways that may perpetuate injustice. D'Ignazio and Klein (2020) argue that data practices systematically privilege certain perspectives while marginalizing others, and that without diverse perspectives across the AI ecosystem, machine learning advances will fuel epistemic injustice. Determining which groups warrant fairness consideration involves epistemic choices about what to measure and how to categorize—choices that advantage some groups' perspectives while disadvantaging others.

Kalluri (2020) contends that debates about algorithmic fairness often miss deeper questions about power: "Who has power to define groups, determine what counts as fairness, validate claims about groups?" Epistemic authority over group categories is a form of power. When researchers specify intersectional groups through database queries without consulting affected communities, they exercise power to determine which groups exist for analytical purposes and which experiences of discrimination become visible in fairness metrics. This power may be exercised responsibly or irresponsibly, but it is power nonetheless.

Taylor (2016) asks "which public? whose good?" in the context of big data as public good, emphasizing that data collection involves epistemic choices about what to measure and how to categorize. These choices advantage some groups' perspectives while disadvantaging others. For intersectional fairness, choices about which demographic categories to collect, how to allow individuals to self-identify, which combinations to consider intersectional groups, and what counts as adequate representation all involve epistemic choices with justice implications.

Kay et al. (2024) develop a taxonomy mapping testimonial, hermeneutical, and distributive injustices onto four AI development stages: data collection, model training, inference, and dissemination. At each stage, systems can perpetuate epistemic injustice through whose knowledge is valued, whose perspectives are incorporated, and whose interpretive resources shape how outputs are understood. Critically, they argue that AI systems "are not only products of biased data but of institutional and epistemic frameworks favoring surveillance, control, efficiency over equity and care." The frameworks that determine which groups matter for fairness evaluation reflect particular epistemic and institutional priorities that may not align with justice.

Harding's (2004) standpoint epistemology argues that certain social positions render particular epistemological perspectives, such that knowledge produced by Black women would differ from knowledge produced by white men due to different experiences and perspectives. Applied to intersectional fairness, standpoint epistemology suggests that members of intersectional groups have potential epistemic advantages regarding their own experiences of discrimination. Specifying which groups warrant fairness consideration without incorporating insights from those groups risks missing precisely the experiences fairness metrics should capture. Yet incorporating diverse standpoints requires inclusive processes that algorithmic fairness research often lacks.

## Uncertainty and Epistemic Responsibility

Beyond questions of epistemic justice, intersectional fairness raises questions about epistemic responsibility in the face of uncertainty. When is it epistemically responsible to make fairness claims about small intersectional groups given limited and uncertain data?

Hüllermeier and Waegeman (2021) distinguish *aleatoric uncertainty*—irreducible probabilistic variability inherent in phenomena—from *epistemic uncertainty*—reducible uncertainty reflecting lack of knowledge. Epistemic uncertainty is also called systematic uncertainty: things one could in principle know but does not in practice. Crucially, epistemic uncertainty reflects uncertainty about estimates due to limited sample size. For intersectional groups, statistical uncertainty is partly epistemic: we could reduce it with more data. But what counts as "enough" data depends on stakes, purposes, and what we are trying to learn.

Bhatt et al. (2021) argue that uncertainty quantification is crucial for responsible AI. Making uncertainty visible enables more epistemically responsible inferences, particularly for claims about small groups with high epistemic uncertainty. Failing to communicate uncertainty can mislead about the reliability of group-level inferences, presenting estimates with wide confidence intervals as if they were precise measurements. For intersectional fairness, this means communicating not just fairness metrics but also uncertainty about those metrics for different groups. A group might appear to experience discrimination, or might appear to be treated fairly, but if the uncertainty is high enough, we cannot responsibly draw either conclusion.

But when is uncertainty low enough to support responsible claims? There is no universal answer. What counts as adequate evidence depends on what is at stake. Eubanks (2018) documents cases where automated systems make high-stakes inferences about marginalized groups—whether children should be removed from families, whether people are eligible for benefits, whether someone poses a security risk—based on insufficient epistemic basis. Systems claim certainty about group properties based on sparse, biased data. This epistemic irresponsibility compounds material harms.

O'Neil (2016) identifies similar patterns in "weapons of math destruction": algorithmic systems making high-stakes inferences about groups without appropriate uncertainty quantification. The failure to acknowledge epistemic limitations leads to overconfident, often harmful, inferences about marginalized groups. For intersectional groups specifically, small sample sizes create high epistemic uncertainty, yet fairness reports often present metrics without confidence intervals or uncertainty measures, creating the false impression of precise knowledge.

Noble (2018) and Benjamin (2019) show how algorithmic systems make and perpetuate epistemic claims about groups—especially intersectional groups like Black women—without adequate basis. Systems both reflect and produce knowledge about groups, with epistemic and political dimensions intertwined. When an algorithm encodes particular categorizations of groups or makes predictions about group properties, it is not merely reflecting pre-existing knowledge but actively shaping what is known and knowable about those groups.

Selbst (2018) analyzes epistemic assumptions in predictive policing systems, which make inferences about crime risk by demographic group but often lack adequate epistemic warrant. Statistical uncertainty is not properly accounted for, especially for intersectional groups with sparse data. Systems make confident predictions about small groups despite wide confidence intervals that would counsel epistemic humility. The failure to acknowledge uncertainty serves ideological functions: it makes algorithmic outputs appear objective and authoritative when they are actually highly uncertain.

## Documentation and Epistemic Transparency

Gebru et al. (2021) propose datasheets for datasets as part of epistemic responsibility in using data. Documenting how categories are defined, measured, and validated is crucial for assessing whether inferences drawn from data are warranted. This is particularly important for demographic categories used in fairness assessment. Without clear epistemic grounding in how categories were constructed and validated, it is unclear what inferences about groups are warranted or what the categories actually capture.

Raji et al. (2020) propose a framework for algorithmic auditing that includes epistemic requirements. Audits must account for uncertainty, especially for small or intersectional groups. Making claims about fairness for groups requires epistemic warrant: adequate data, valid measurements, appropriate statistical methods accounting for uncertainty. An audit that reports fairness metrics for intersectional groups without reporting sample sizes, confidence intervals, and statistical power has not met minimal epistemic standards for responsible claims.

Geiger et al. (2020) document that many machine learning papers do not report where training data categories come from or how they were defined. This opacity about epistemic provenance makes it impossible to assess construct validity or determine what inferences are warranted. For demographic categories specifically, failure to document how race and gender were defined, whether categories allowed self-identification, what options were available, and how intersectional combinations were handled undermines any fairness claims based on those categories.

Fazelpour and Lipton (2023) argue that algorithmic fairness must be understood in situated contexts rather than abstractly. Different contexts involve different epistemic positions, power dynamics, and justice considerations. What counts as epistemically responsible inference about groups depends on social and political context: who is making claims, about whom, for what purposes, with what potential consequences. Standards for adequate evidence should be higher for high-stakes decisions affecting vulnerable populations and lower for low-stakes exploratory analyses. Epistemic responsibility is context-dependent.

## Epistemic Dimensions of the Dilemma

The epistemic literature establishes several conclusions relevant to the intersectionality dilemma. First, determining which groups warrant fairness consideration is not merely a technical or ontological question but an epistemic justice question about who has authority to define groups and whose perspectives are incorporated. Excluding affected communities from group specification decisions concentrates epistemic power in problematic ways.

Second, making fairness claims about small intersectional groups raises epistemic responsibility issues. High epistemic uncertainty due to limited data means that responsible claims require appropriate uncertainty quantification and epistemic humility. Yet current practices often present fairness metrics without uncertainty measures, creating false impressions of knowledge.

Third, standards for adequate evidence are context-dependent and contested. There is no universal threshold for when data is sufficient to support fairness claims about a group. What counts as enough depends on stakes, purposes, and consequences of potential errors.

These epistemic dimensions interact with the statistical and ontological horns of the dilemma in complex ways. Statistical uncertainty about small groups is partly epistemic uncertainty—more data could reduce it. But epistemic justice considerations may require including groups even when data is limited, if those groups experience distinctive discrimination. Epistemic responsibility might counsel humility about fairness claims for small groups, but excluding groups from fairness consideration due to data limitations raises epistemic justice concerns about whose experiences count.

Moreover, the question of who decides which groups to include is itself contested. Should machine learning researchers decide based on available data and statistical feasibility? Should affected communities decide based on lived experiences of discrimination? Should legal frameworks determine groups based on protected categories? Different answers reflect different epistemic frameworks about expertise, authority, and whose knowledge matters. The epistemology literature does not provide a single answer but rather illuminates that these are genuinely contested questions with implications for justice.

Critically, no work in this literature connects epistemic dimensions to the ontological and statistical challenges documented in earlier sections. Epistemic justice scholars analyze power and authority in knowledge production but do not typically address statistical constraints on what can be reliably known about small groups. Statisticians and machine learning researchers discuss epistemic uncertainty and its quantification but do not typically frame this as an epistemic justice issue about whose knowledge and experiences are validated. The epistemic dimensions of intersectional fairness remain disconnected from technical and philosophical analyses, leaving another gap in understanding the full dilemma.


# Synthesis: The Intersectionality Dilemma

The preceding sections have documented sophisticated research on intersectional fairness across multiple disciplines. Machine learning researchers have developed impressive technical solutions to data sparsity. Philosophers have illuminated deep ontological questions about the nature of groups. Measurement theorists have shown how operationalization embeds ontological commitments. Normative ethicists have analyzed competing frameworks for distributive justice. Epistemologists have examined questions of authority and responsibility. Yet critically, *no existing work synthesizes these insights to reveal that intersectional fairness involves a genuine dilemma* where statistical and ontological uncertainties interact to create a problem that cannot be solved through progress in any single domain.

## The Dilemma Structure

The dilemma has two horns, each involving a distinct form of uncertainty, and these horns interact such that progress on one exacerbates problems on the other.

**Horn 1: Statistical Uncertainty.** Evaluating algorithmic fairness across intersectional groups requires estimating performance metrics (error rates, calibration, predictive parity) for each group. As groups become more fine-grained through intersectional specification, they become smaller in available datasets. Small groups yield unreliable performance estimates with high variance and wide confidence intervals. The epistemic uncertainty about whether a group is being treated fairly increases as group size decreases. At the limit, groups with only a handful of members provide essentially no information about systematic algorithmic behavior toward that group. The statistical challenge is well-documented: multicalibration sample complexity grows exponentially with the number of groups (Hébert-Johnson et al. 2018), conditional demographic parity becomes infeasible with many conditioning variables (Castelnovo et al. 2022), and sparse subgroups undermine fairness evaluation (Yan et al. 2024; Sheng et al. 2025).

The standard technical response is to constrain the set of groups $G$ to those with sufficient data for reliable estimation. Limit intersectional analysis to groups above some minimum size threshold. Use only two-way intersections (race × gender) rather than three-way or four-way. Focus on groups with at least 100 members, or 500, or whatever threshold provides adequate statistical power. This response appears reasonable—we should not make claims about groups when evidence is insufficient. It leads to Horn 2.

**Horn 2: Ontological Uncertainty.** Which groups should be included in $G$? Which intersections of protected attributes constitute meaningful groups warranting fairness consideration? Sections 3–6 established that this question has no determinate answer; it depends on contested philosophical commitments about social ontology, normative frameworks for justice, and epistemic authority.

Different ontological frameworks give different answers. Attribute-based accounts suggest that any combination of protected attributes defines a group (all individuals with race=Black AND gender=Female AND age>65 form a group). Practice-based or structural accounts suggest that groups emerge from social practices and cannot be exhaustively enumerated through attribute combinations—we must identify which specific intersections correspond to meaningful social groups in particular contexts. Emergentist accounts emphasize that intersectional experiences are non-additive, arising from social structures that make certain combinations salient in ways that resist algorithmic specification.

Different normative frameworks give different answers. Egalitarianism suggests we should consider all groups equally, implying comprehensive intersectional specification. Prioritarianism suggests we should focus on worst-off groups, which requires first identifying them (raising the statistical problem). Sufficientarianism suggests we should ensure all groups meet adequacy thresholds, which requires first determining which groups exist.

Different epistemic frameworks give different answers. Epistemic justice considerations suggest affected communities should help determine which groups warrant consideration, but affected communities may disagree among themselves and with researchers. Epistemic responsibility suggests we should only track groups for which we have adequate data, but this defers to statistical considerations.

Constraining $G$ to address statistical problems (Horn 1) requires principled criteria for which groups to exclude. But providing such criteria requires resolving these contested ontological, normative, and epistemic questions. There is no neutral, uncontroversial basis for determining which intersectional groups to omit. Any choice embeds substantive philosophical commitments that reasonable people dispute.

**The Interaction.** The horns interact to create a genuine dilemma. Expanding $G$ to respect ontological complexity (include more intersectional groups, recognize that groups cannot be exhaustively specified through simple combinations, acknowledge normative requirements to consider vulnerable groups) exacerbates statistical problems. Each additional group reduces average group size, increasing estimation uncertainty and making it harder to detect genuine discrimination or confirm fairness. Conversely, constraining $G$ to handle statistical limitations (include only large groups, use only two-way intersections, exclude groups below sample size thresholds) requires ontological and normative judgments about which groups not to consider—judgments that are deeply contested.

This interaction means that progress on one horn tends to worsen the situation on the other. Technical advances that allow fair treatment of more groups (better multicalibration algorithms, synthetic data generation, hierarchical models) do not resolve the question of which groups to include. Philosophical progress clarifying the ontology of groups would not eliminate statistical constraints on how many groups we can reliably evaluate with finite data. The dilemma is not merely the conjunction of two difficult problems; it is their problematic interaction.

## What Existing Work Does and Doesn't Address

Having articulated the dilemma structure, we can now systematically assess what existing work contributes and where the gap lies.

**Machine Learning and Computer Science (Section 2).** This literature demonstrates sophisticated approaches to statistical challenges but uniformly treats $G$ as an input determined by available data attributes. When papers discuss challenges in specifying groups, they frame these as *computational* (exponential growth of combinations) or *statistical* (sparse data, sample complexity) rather than *ontological* (which combinations correspond to meaningful groups?) or *normative* (which groups should we prioritize?). Representative papers:

- Castelnovo et al. (2022) note that "assessing group fairness with multiple sensitive attributes may be unfeasible in most practical cases" due to exponential increase of subgroups, but frame this as a computational/practical issue, not an ontological question about which subgroups are meaningful.
- Hansen et al. (2024) conclude that "attaining multicalibration is practically infeasible with more than a few classes" but do not ask whether we should be considering all classes or how to select among them.
- Sheng et al. (2025) address "exponential growth of subgroups with finite samples" through a sparsity framework but do not question whether all subgroups warrant consideration.

No paper in this literature asks: "Given that we cannot evaluate all possible intersections due to data constraints, how should we decide which intersections to include?" The question is treated as answered by data availability rather than requiring ontological, normative, or epistemic justification.

**Philosophy of Intersectionality and Social Ontology (Section 3).** This literature establishes deep disagreement about what intersectional groups are and how they should be individuated but does not connect these debates to operational constraints. Representative contributions:

- Collins (2015) identifies "definitional dilemmas" in intersectionality scholarship but does not relate these to challenges in algorithmic fairness implementation.
- McCall (2005) distinguishes anticategorical, intracategorical, and intercategorical approaches, showing fundamental disagreement about how to understand categories in intersectional analysis. This directly bears on whether algorithmic systems can enumerate groups, but McCall does not address algorithmic contexts.
- Bright et al. (2016) versus Jorba and López de Sa (2024) represent competing ontologies (causal-combinatorial vs. emergentist) with different implications for group specification, but neither connects to statistical feasibility questions.
- Epstein (2019), Ritchie (2020), and Thomasson (2019) analyze the ontology of social groups without addressing how ontological frameworks should inform algorithmic fairness practice.

No philosophical work asks: "Given operational constraints on how many groups algorithmic systems can reliably evaluate, how should contested ontologies of groups inform practice?" The debates proceed at a level of abstraction that does not engage technical constraints.

**Measurement Theory (Section 4).** This literature shows that operationalization embeds ontological commitments and that construct validity depends on theoretical understanding. Jacobs and Wallach (2021) argue that fairness is an essentially contested construct with different valid operationalizations in different contexts, and that operationalization involves assumptions introducing potential mismatches. Scheuerman et al. (2020) show that demographic category operationalizations reflect often-implicit ontological commitments.

However, this literature does not frame the connection between ontology and operationalization as creating a dilemma when combined with statistical constraints. It identifies measurement challenges and validity questions but does not synthesize these with the statistical problems documented in ML literature or the ontological debates documented in philosophy. The insight that operationalization embeds ontology is profound but incomplete without connecting to the full dilemma structure.

**Normative Ethics (Section 5).** This literature establishes that different distributive justice frameworks imply different answers about which groups warrant consideration and how to handle trade-offs. Egalitarian, prioritarian, and sufficientarian frameworks yield incompatible guidance. However, no work connects this normative pluralism to the question of how normative frameworks should interact with statistical constraints.

Papers analyzing leveling down (Holm 2023; Mittelstadt et al. 2023) debate whether equality is worth achieving when it requires reducing performance for some groups, but they do not connect this to the question of which groups to include in fairness evaluation when data constraints prevent comprehensive evaluation. Normative frameworks are analyzed in isolation from operational and statistical realities.

**Epistemology (Section 6).** This literature illuminates epistemic justice questions (who decides which groups matter?) and epistemic responsibility questions (when is evidence sufficient?). But it does not connect these to the statistical challenge that evidence is inherently limited for small intersectional groups, nor to the ontological debates about how to individuate groups. Epistemic justice scholars emphasize including marginalized voices in determining categories but do not address what happens when including more groups exacerbates statistical problems.

## The Gap: No Dilemma Framing

The critical gap is that **no existing work frames the interaction between statistical and ontological uncertainty as a dilemma**. Papers addressing statistical challenges assume ontological questions have answers (groups are determined by data attributes). Papers addressing ontological questions do not consider statistical constraints. Papers on measurement, normativity, and epistemology provide crucial insights but do not synthesize them with statistical and ontological dimensions to reveal the dilemma structure.

Who comes closest to recognizing the dilemma? Several papers acknowledge partial aspects:

- Castelnovo et al. (2022) recognize that comprehensive intersectional specification is statistically infeasible but treat this as a practical limitation rather than revealing deeper ontological questions about which groups to prioritize.
- Jacobs and Wallach (2021) identify fairness as essentially contested, implying different contexts require different operationalizations, but do not connect this to group specification or statistical constraints.
- Collins (2015) identifies "definitional dilemmas" in intersectionality theory but does not apply this to algorithmic fairness.
- Bowleg (2008) discusses "methodological challenges of intersectionality research" in both qualitative and quantitative contexts, arguing that additive approaches fail to capture intersectional experiences, but does not frame this as a dilemma involving statistical-ontological interaction.

No work synthesizes the technical literature on data sparsity, the philosophical literature on contested ontologies, the measurement literature on construct validity, the normative literature on distributive justice, and the epistemic literature on authority and responsibility to show that these create a dilemma where improving along one dimension worsens others.

## Why This Matters

Framing intersectional fairness through the dilemma lens matters for several reasons. First, it explains why technical progress alone cannot solve the problem. Better multicalibration algorithms, improved synthetic data generation, and more sophisticated hierarchical models are valuable contributions, but they do not answer which groups to include in $G$. That question requires engaging contested ontological, normative, and epistemic terrain.

Second, it explains why philosophical progress alone cannot solve the problem. Even if philosophers reached consensus on the correct ontology of groups (which seems unlikely), statistical constraints would remain. We cannot achieve fairness across unlimited groups with finite data. Some prioritization or selection is necessary, which reintroduces normative and epistemic questions.

Third, recognizing the dilemma clarifies what kinds of responses are available. We cannot dissolve the dilemma through technical or philosophical breakthroughs. Instead, we must navigate it through context-specific judgment that acknowledges trade-offs, embraces pluralism about purposes and frameworks, and involves affected communities in determining which groups matter for particular applications. The dilemma does not have a solution; it has strategies for responsible navigation that acknowledge both horns.

Fourth, the dilemma framing positions intersectional fairness as a fundamentally interdisciplinary challenge requiring integration of technical, philosophical, normative, and epistemic insights. Neither computer scientists nor philosophers alone can adequately address it. Progress requires sustained collaboration across disciplines and with affected communities.

The gap this review has identified—the absence of work framing intersectional fairness as involving a statistical-ontological dilemma—represents the core contribution our research will make. By synthesizing insights across disciplines to articulate the dilemma structure, we provide a new framework for understanding why intersectional fairness is so challenging and what kinds of responses are appropriate. The next section concludes by positioning this contribution and outlining its implications.


# Conclusion

This literature review has synthesized research across machine learning, philosophy, social ontology, measurement theory, normative ethics, and epistemology to reveal a fundamental challenge at the heart of intersectional algorithmic fairness. Auditing algorithmic systems for fairness across intersectional groups requires specifying which groups to evaluate—determining the set $G$ of groups warranting fairness consideration. This specification involves two forms of uncertainty that interact to create a genuine dilemma.

*Statistical uncertainty* arises from the data sparsity inherent in fine-grained group specification. As the number of intersectional groups increases, average group size decreases, yielding performance estimates with high variance and wide confidence intervals. The epistemic uncertainty about whether groups are treated fairly increases as groups become smaller. Technical solutions—multicalibration, synthetic data generation, hierarchical models—provide valuable tools but face fundamental information-theoretic limits. Sample complexity grows exponentially with the number of groups, and attaining precise fairness guarantees becomes statistically infeasible as $G$ expands. The machine learning literature has developed sophisticated approaches to these statistical challenges while consistently treating $G$ as an input determined by available data attributes.

*Ontological uncertainty* arises from deep philosophical disagreements about which groups intersectionality picks out and how they should be individuated. Philosophical analyses reveal contested terrain: anticategorical approaches reject fixed group categories altogether, intracategorical approaches study specific intersections without presuming exhaustive enumeration, and only intercategorical approaches support the kind of group specification algorithmic fairness requires—yet even this approach is provisional rather than metaphysically committed. Competing ontologies—attribute-based accounts suggesting groups can be specified through combinations of protected attributes versus practice-based or emergentist accounts suggesting groups depend on context-specific social structures—yield incompatible guidance about whether algorithmic systems can enumerate meaningful groups through database queries. The philosophy literature has illuminated these ontological complexities without connecting them to the operational constraints of algorithmic systems.

The dilemma arises from the *interaction* between these two forms of uncertainty. Expanding $G$ to respect ontological complexity (acknowledging that meaningful groups may not align with simple attribute combinations, that emergent intersectional groups resist enumeration, that normative frameworks like egalitarianism require comprehensive specification) exacerbates statistical problems by reducing average group size and increasing estimation uncertainty. Conversely, constraining $G$ to handle statistical limitations (including only large groups, limiting to two-way intersections, excluding groups below sample size thresholds) requires principled criteria for which groups to exclude—criteria that depend on resolving contested ontological questions about which groups exist, contested normative questions about which groups matter morally, and contested epistemic questions about who has authority to determine group specifications.

Progress on one horn of the dilemma tends to worsen the situation on the other. Technical advances enabling fairness evaluation across more groups do not resolve ontological questions about which groups to include. Philosophical progress on the ontology of groups would not eliminate statistical constraints on reliable estimation with finite data. The dilemma is not merely two difficult problems occurring together; it is their problematic interaction where attempting to solve one exacerbates the other.

Our review has documented that while each horn is well-recognized within its respective domain, **no existing work frames their interaction as a dilemma**. Machine learning papers treat group specification as a technical input problem, acknowledging statistical challenges but not ontological questions. Philosophy papers explore contested ontologies of groups without addressing statistical feasibility. Measurement theory shows that operationalization embeds ontological commitments but does not synthesize this insight with statistical constraints. Normative ethics reveals pluralism about distributive justice frameworks but does not connect this to questions of which groups to evaluate when comprehensive evaluation is statistically infeasible. Epistemology illuminates questions of authority and responsibility but does not integrate these with statistical and ontological challenges.

This gap—the absence of work synthesizing technical, philosophical, normative, and epistemic dimensions to reveal the dilemma structure—represents the core contribution our research makes. By articulating intersectional fairness as involving a genuine dilemma where statistical and ontological uncertainties reinforce each other, we provide a new framework for understanding why intersectional fairness has proven so intractable and what kinds of responses are appropriate.

Several implications follow from recognizing the dilemma. First, neither technical nor philosophical progress alone suffices. Solving intersectional fairness requires navigating trade-offs between competing values and purposes rather than discovering the single correct technical or ontological solution. Second, context matters profoundly. Different applications involve different purposes (legal compliance, substantive non-discrimination, distributive justice), different normative frameworks (egalitarian, prioritarian, sufficientarian), different epistemic positions (who has authority to determine groups?), and different statistical constraints (available data, sample sizes, precision requirements). No universal specification of $G$ works across all contexts. Third, affected communities must be involved in determining which groups matter for particular applications, making group specification a participatory process rather than a purely technical or academic decision.

Our research will explore potential responses to the dilemma. One strategy involves clarifying the *purposes* of fairness audits. Legal compliance purposes might specify $G$ based on legally protected categories. Substantive non-discrimination purposes might focus on groups experiencing documented discrimination patterns. Distributive justice purposes might prioritize groups falling below adequacy thresholds. Different purposes justify different group specifications and different trade-offs between comprehensiveness and statistical precision.

Another strategy leverages normative frameworks from distributive justice. Prioritarian approaches could justify focusing on worst-off groups, though identifying which groups are worst-off requires some initial fairness measurement. Sufficientarian approaches could justify threshold-based specification, evaluating comprehensive intersectional groups only for whether they meet basic adequacy standards rather than requiring precise performance estimation. These frameworks provide normative guidance for principled trade-offs between statistical precision and ontological inclusiveness.

A third strategy emphasizes epistemic humility and transparency. When fairness claims are made about small intersectional groups, uncertainty should be communicated explicitly through confidence intervals, statistical power analyses, and clear statements about evidential limitations. Algorithmic auditing standards should require reporting not just fairness metrics but also their reliability for different groups. This epistemic transparency enables more responsible inference and decision-making about intersectional fairness.

The significance of our contribution spans multiple dimensions. Theoretically, framing intersectional fairness as a dilemma shows the limits of purely technical or purely philosophical approaches and demonstrates the need for genuine interdisciplinary integration. Practically, it helps practitioners understand why intersectional fairness is fundamentally challenging rather than merely technically difficult, enabling more realistic expectations and more responsible practices. Methodologically, it models productive engagement between machine learning and philosophy, showing how philosophical analysis can illuminate practical challenges in ways that advance both fields.

For ACM FAccT audiences, this research offers a novel framing of a persistent challenge in algorithmic fairness, moving beyond technical optimization to engage the ontological and normative questions that technical work presupposes. For philosophy audiences, it demonstrates how abstract debates about social ontology and distributive justice bear on live practical problems in algorithmic systems, providing concrete contexts for philosophical frameworks. For interdisciplinary audiences, it exemplifies how integration across technical, philosophical, normative, and epistemic dimensions yields insights unavailable within any single discipline.

Intersectional fairness has proven challenging for good reason: it involves a genuine dilemma where statistical and ontological uncertainties interact to create a problem resistant to purely technical or purely philosophical solutions. By articulating this dilemma structure and synthesizing insights across disciplines to document the gap in existing work, this review establishes the foundation for our contribution: the first systematic analysis of the intersectionality dilemma and exploration of strategies for navigating it responsibly.


