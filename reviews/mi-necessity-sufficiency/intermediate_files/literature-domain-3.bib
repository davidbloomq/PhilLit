@comment{
====================================================================
DOMAIN: Mechanistic Interpretability in Machine Learning (Technical Literature)
SEARCH_DATE: 2026-01-01
PAPERS_FOUND: 18 total (High: 10, Medium: 6, Low: 2)
SEARCH_SOURCES: Semantic Scholar, OpenAlex, Web (Transformer Circuits Thread)
====================================================================

DOMAIN_OVERVIEW:
Mechanistic interpretability (MI) is a research paradigm that aims to reverse-engineer the internal computations of neural networks into human-understandable algorithms and circuits. The field emerged prominently around 2021-2022 with foundational work from Anthropic (Elhage, Olah, Olsson) and has since developed into a distinct subfield with specific methods and objectives. MI researchers focus on identifying "circuits"—computational subgraphs that implement specific behaviors—and "features"—directions in activation space corresponding to interpretable concepts.

Central to MI is the challenge of "superposition": neural networks represent more features than they have neurons by storing them as overcomplete sets of directions in activation space. This phenomenon leads to "polysemanticity" where individual neurons respond to multiple unrelated concepts. To address superposition, researchers have developed sparse autoencoders (SAEs) that decompose activations into more interpretable, monosemantic features. Key methods include activation patching (intervening on activations to measure causal effects), circuit discovery (identifying minimal computational subgraphs), and feature visualization (generating inputs that maximally activate specific components).

Recent work has scaled these techniques to production models like GPT-4 and Claude 3 Sonnet, discovering interpretable features including safety-relevant concepts. The field distinguishes itself from broader XAI by emphasizing mechanistic precision—seeking to understand not just what models do but how they do it at the level of individual components and their interactions.

RELEVANCE_TO_PROJECT:
This domain provides the technical foundation for evaluating Hendrycks & Hiscott's (2025) claims about MI. Understanding what MI practitioners actually mean by "mechanistic interpretability," what methods they employ, and what they consider successful interpretation is essential for assessing whether MI is necessary or sufficient for AI safety. The domain establishes the narrow technical conception of MI that philosophical arguments often target or presuppose.

NOTABLE_GAPS:
Limited work on vision models compared to language models; debates about scalability remain unresolved; theoretical foundations (especially around what counts as an adequate "mechanistic explanation") are still developing. The relationship between discovered circuits and actual model behavior under distribution shift is under-explored.

SYNTHESIS_GUIDANCE:
Organize around: (1) Core methods (activation patching, SAEs, circuit discovery), (2) Key findings (superposition, induction heads, monosemantic features), (3) Scaling challenges, (4) Theoretical frameworks (causal abstraction). Highlight tensions between empirical success and theoretical justification. Note how practitioners define their object of study—this matters for philosophical arguments about necessity/sufficiency.

KEY_POSITIONS:
- Circuit-centric approach: 8 papers - Focuses on identifying minimal computational subgraphs
- Feature-centric approach: 6 papers - Emphasizes discovering interpretable features via dictionary learning
- Methodological foundations: 4 papers - Develops theoretical frameworks and best practices
====================================================================
}

@article{nanda2023progress,
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  title = {Progress Measures for Grokking via Mechanistic Interpretability},
  journal = {arXiv},
  year = {2023},
  volume = {abs/2301.05217},
  doi = {10.48550/arXiv.2301.05217},
  arxivId = {2301.05217},
  note = {
  CORE ARGUMENT: The paper fully reverse-engineers the algorithm learned by small transformers trained on modular addition, showing that grokking arises from gradual amplification of structured mechanisms (using discrete Fourier transforms and trigonometric identities) rather than sudden emergence. The authors define continuous progress measures that split training into memorization, circuit formation, and cleanup phases, demonstrating that mechanistic interpretability can provide fine-grained understanding of learning dynamics.

  RELEVANCE: Exemplifies successful mechanistic interpretability on a synthetic task where the ground truth algorithm can be verified. Demonstrates that MI can trace the development of circuits during training, not just analyze final models. Establishes "circuit formation" as a measurable phenomenon, which is relevant for assessing whether MI provides sufficient insight into how capabilities develop. Shows that what appears as emergent behavior (grokking) has continuous mechanistic precursors.

  POSITION: Represents the circuit-centric approach to MI, emphasizing complete algorithmic understanding. Illustrative of MI's ambitions but also its current limitation to relatively simple tasks.
  },
  keywords = {mechanistic-interpretability, circuits, grokking, High}
}

@inproceedings{conmy2023acdc,
  author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  booktitle = {Neural Information Processing Systems},
  year = {2023},
  journal = {arXiv},
  volume = {abs/2304.14997},
  doi = {10.48550/arXiv.2304.14997},
  arxivId = {2304.14997},
  note = {
  CORE ARGUMENT: The paper introduces ACDC (Automatic Circuit DisCovery), an algorithm that automates the identification of computational circuits in transformer models by systematically applying activation patching to prune edges in the computational graph. On GPT-2 Small, ACDC rediscovered 5/5 component types and 68/68 edges in a manually-identified Greater-Than circuit, demonstrating that circuit discovery can be systematized rather than requiring manual intuition for each task.

  RELEVANCE: Critical for assessing scalability claims about MI—if circuit discovery can be automated, MI becomes more practically viable for analyzing large models. The paper operationalizes what counts as a "circuit" (edges in a computational graph necessary for a behavior) and provides metrics for circuit quality (faithfulness to original behavior). ACDC's success on GPT-2 Small but unclear scalability to larger models highlights current limitations of MI methods.

  POSITION: Advances circuit-centric MI by providing systematic methodology. Addresses the reproducibility and scalability challenges that critics might raise about MI's necessity for safety.
  },
  keywords = {mechanistic-interpretability, circuit-discovery, activation-patching, High}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety -- A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  doi = {10.48550/arXiv.2404.14082},
  arxivId = {2404.14082},
  note = {
  CORE ARGUMENT: This comprehensive review establishes MI as reverse-engineering neural network computations into human-understandable algorithms and concepts to provide granular, causal understanding. The paper surveys MI methodologies (feature visualization, activation patching, circuit discovery), assesses relevance to AI safety (benefits for understanding, control, alignment versus risks like capability gains), and identifies key challenges around scalability, automation, and comprehensive interpretation.

  RELEVANCE: Provides authoritative definition of what MI practitioners mean by "mechanistic interpretability," distinguishing it from broader XAI approaches. Essential reference for understanding the scope and limitations of current MI methods, which matters for evaluating whether MI is necessary or sufficient for safety. The paper's assessment of MI's safety relevance (both benefits and dual-use risks) directly informs philosophical debates about MI's role in AI governance.

  POSITION: Comprehensive overview representing mainstream MI perspective. Emphasizes both promise (granular causal understanding) and limitations (scalability, automation challenges). Positions MI as complementary to other safety approaches rather than universally necessary.
  },
  keywords = {mechanistic-interpretability, ai-safety, survey, High}
}

@article{rai2024practical,
  author = {Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  title = {A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2407.02646},
  doi = {10.48550/arXiv.2407.02646},
  arxivId = {2407.02646},
  note = {
  CORE ARGUMENT: Organizes MI research around a task-centric taxonomy (understanding individual components, compositional understanding, task-specific understanding, etc.) rather than method-centric approaches. The review provides practical guidance for MI practitioners by mapping specific research questions to appropriate techniques and evaluation methods, while identifying gaps like limited scaling to large models and lack of standardized evaluation metrics.

  RELEVANCE: Clarifies the scope of what MI can currently achieve versus aspirational goals. The task-centric taxonomy helps assess which types of understanding MI provides (e.g., local component behavior vs. global emergent properties) and which remain challenging. Important for evaluating claims about MI's sufficiency—the paper reveals that MI excels at certain tasks (e.g., identifying attention head functions) but struggles with others (e.g., explaining compositional capabilities across layers).

  POSITION: Pragmatic, practitioner-focused perspective that acknowledges both achievements and limitations. Emphasizes MI as a toolkit for specific understanding tasks rather than a universal explanation method.
  },
  keywords = {mechanistic-interpretability, transformers, survey, Medium}
}

@misc{elhage2021framework,
  author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  title = {A Mathematical Framework for Transformer Circuits},
  year = {2021},
  howpublished = {\url{https://transformer-circuits.pub/2021/framework/index.html}},
  note = {
  CORE ARGUMENT: Establishes foundational mathematical framework for analyzing transformer computations by treating attention heads and MLP layers as components that read from and write to a shared residual stream. The paper introduces key concepts including the residual stream as a communication channel, attention as a bilinear operation enabling token-to-token information movement, and the notion that circuits can be identified by tracing these information flows. This framework enables decomposition of model behavior into interpretable computational paths.

  RELEVANCE: Provides the theoretical foundation that subsequent MI work builds upon—without this framework, techniques like activation patching and circuit discovery lack a coherent basis. The residual stream perspective fundamentally shapes how MI practitioners conceptualize transformer computation. Critical for understanding what MI researchers mean by "circuits" and why they believe transformer components can be meaningfully decomposed.

  POSITION: Foundational work establishing core MI concepts and methodology. Represents the architectural assumptions underlying circuit-centric interpretability.
  },
  keywords = {mechanistic-interpretability, transformers, circuits, framework, High}
}

@misc{elhage2022superposition,
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Chris},
  title = {Toy Models of Superposition},
  year = {2022},
  howpublished = {\url{https://transformer-circuits.pub/2022/toy_model/index.html}},
  arxivId = {2209.10652},
  doi = {10.48550/arXiv.2209.10652},
  note = {
  CORE ARGUMENT: Introduces and rigorously analyzes "superposition"—the phenomenon where neural networks represent more features than they have neurons by storing them as directions in activation space that interfere with each other. Using toy models where ground truth is known, the paper demonstrates that superposition arises when features are sparse and the model is incentivized to compress information. Discovers phase transitions in how features organize geometrically (privileged basis vs. uniform polytope configurations) and connects superposition to polysemanticity and adversarial examples.

  RELEVANCE: Central to understanding why MI is challenging and why standard neuron-level analysis fails. Superposition explains the core obstacle that motivates dictionary learning approaches (sparse autoencoders). The paper establishes that individual neurons are not the right unit of analysis for understanding networks, fundamentally shifting MI methodology. Essential for assessing claims about MI's feasibility—if superposition is pervasive, neuron-level interpretability is insufficient, and alternative decompositions become necessary.

  POSITION: Foundational theoretical work explaining a core MI challenge. Motivates the shift from neuron-centric to feature-centric interpretability.
  },
  keywords = {superposition, features, polysemanticity, High}
}

@misc{olsson2022induction,
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  title = {In-Context Learning and Induction Heads},
  year = {2022},
  howpublished = {\url{https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}},
  note = {
  CORE ARGUMENT: Identifies "induction heads"—a circuit motif consisting of two attention heads working in composition—as the primary mechanism underlying in-context learning in transformers. The first head (previous token head) identifies token positions that follow a given token, while the second head (induction head) uses this to predict the next token based on the established pattern. The paper demonstrates strong correlations between induction head formation and in-context learning capability development during training, including a phase change where both emerge simultaneously.

  RELEVANCE: Demonstrates MI's ability to identify specific algorithmic mechanisms (circuits) responsible for emergent capabilities. Induction heads are now the canonical example of a discovered circuit, often cited in debates about MI's explanatory power. The paper shows that MI can link micro-level mechanisms (attention head operations) to macro-level capabilities (in-context learning), which is crucial for assessing MI's sufficiency for understanding model behavior. However, the specificity of this finding (one circuit for one capability) also highlights limitations in generalizing to more complex behaviors.

  POSITION: Landmark empirical discovery establishing circuit-centric MI's viability. Demonstrates that interpretable circuits can explain important capabilities, but also reveals the labor-intensive nature of circuit discovery.
  },
  keywords = {mechanistic-interpretability, circuits, induction-heads, in-context-learning, High}
}

@misc{bricken2023monosemantic,
  author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E. and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Chris},
  title = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  year = {2023},
  howpublished = {\url{https://transformer-circuits.pub/2023/monosemantic-features/index.html}},
  note = {
  CORE ARGUMENT: Applies sparse autoencoders (SAEs) to decompose neural network activations into interpretable, "monosemantic" features—directions in activation space that respond to single, coherent concepts rather than multiple unrelated patterns. Training SAEs on a one-layer transformer reveals features corresponding to specific tokens, concepts, and even some abstract properties. The paper demonstrates that these features are more interpretable than individual neurons and can be causally validated through interventions.

  RELEVANCE: Provides a concrete method for addressing superposition, the central obstacle to interpretability identified in prior work. Monosemantic features represent a potential solution to polysemanticity, making MI more tractable. The paper's success in finding interpretable features in a small model raises questions about scalability to larger models (addressed in subsequent work). Critical for evaluating claims about MI's feasibility—if dictionary learning can reliably extract interpretable features, MI becomes more viable as a safety tool.

  POSITION: Feature-centric approach to MI. Demonstrates that learned representations can be decomposed into interpretable components, though questions about completeness and scalability remain.
  },
  keywords = {sparse-autoencoders, dictionary-learning, monosemanticity, features, High}
}

@misc{templeton2024scaling,
  author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L. and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
  title = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
  year = {2024},
  howpublished = {\url{https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}},
  note = {
  CORE ARGUMENT: Scales sparse autoencoder approach to a production language model (Claude 3 Sonnet), training SAEs with up to 34 million features on middle-layer activations. Discovers interpretable features ranging from simple concepts (Golden Gate Bridge) to abstract patterns (code vulnerabilities, deceptive behavior) and safety-relevant features (bias, dangerous knowledge). Demonstrates that features can be manipulated to steer model behavior, and that some features exhibit multilinguality and compositionality. Provides evidence that dictionary learning scales to frontier models.

  RELEVANCE: Directly addresses the scalability objection to MI—shows that sparse autoencoders can extract interpretable features from large, production-scale models. The discovery of safety-relevant features (bias, deception) makes MI's practical safety applications concrete rather than speculative. However, the sheer number of discovered features (millions) and questions about coverage (how many features exist? are we finding them all?) highlight remaining challenges. Critical for assessing whether MI is sufficient for safety—finding features is necessary but not sufficient for full understanding.

  POSITION: Demonstrates MI's viability at scale while revealing new challenges (feature counting, completeness, computational cost). Shows that MI can discover safety-relevant concepts but doesn't yet provide comprehensive model understanding.
  },
  keywords = {sparse-autoencoders, scaling, claude-3-sonnet, safety, High}
}

@inproceedings{cunningham2023sparse,
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  title = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  arxivId = {2309.08600},
  doi = {10.48550/arXiv.2309.08600},
  note = {
  CORE ARGUMENT: Demonstrates that sparse autoencoders learn features that are more interpretable and monosemantic than alternative decomposition methods (PCA, ICA, neuron activations). Using automated interpretability metrics and causal interventions on the indirect object identification task, the paper shows that SAE features enable finer-grained understanding of model behavior—interventions on learned features can pinpoint causal responsibility for counterfactual behavior more precisely than neuron-level or attention-head-level interventions.

  RELEVANCE: Provides empirical validation that dictionary learning addresses superposition effectively. The paper's comparison of interpretability methods establishes SAEs as state-of-the-art for feature extraction. The causal intervention experiments demonstrate that interpretable features aren't just descriptively useful but causally meaningful, which matters for assessing MI's sufficiency for safety (we need features that actually control behavior, not just correlate with it). The scalable, unsupervised nature of SAE training is crucial for MI's practical viability.

  POSITION: Feature-centric MI approach. Establishes SAEs as the leading method for decomposing neural activations, though questions about optimal hyperparameters and feature completeness remain.
  },
  keywords = {sparse-autoencoders, features, interpretability, causal-intervention, High}
}

@inproceedings{gao2024scaling,
  author = {Gao, Leo and la Tour, Tom Dupré and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  title = {Scaling and Evaluating Sparse Autoencoders},
  booktitle = {International Conference on Learning Representations},
  year = {2024},
  arxivId = {2406.04093},
  doi = {10.48550/arXiv.2406.04093},
  note = {
  CORE ARGUMENT: Introduces k-sparse autoencoders that directly control sparsity (unlike L1-regularized SAEs) and discovers clean scaling laws relating autoencoder size, sparsity, and reconstruction quality. Proposes modifications that eliminate "dead latents" (features that never activate) even at large scales. Develops new evaluation metrics for feature quality: recovery of hypothesized features, explainability of activation patterns, and sparsity of downstream effects. Trains a 16-million-latent autoencoder on GPT-4 activations, demonstrating unprecedented scale.

  RELEVANCE: Addresses key technical challenges in scaling dictionary learning: how to set hyperparameters, how to evaluate feature quality beyond manual inspection, and how to avoid dead latents. The scaling laws provide principled guidance for training larger SAEs, making MI more systematic. The novel evaluation metrics (feature recovery, explainability, downstream sparsity) operationalize what makes features "good," which matters for assessing MI's quality beyond subjective judgments. Shows that SAE quality improves with scale, supporting optimism about MI's future viability.

  POSITION: Methodological advance in feature-centric MI. Focuses on engineering improvements and rigorous evaluation rather than new conceptual insights.
  },
  keywords = {sparse-autoencoders, scaling, evaluation-metrics, Medium}
}

@article{heimersheim2024activation,
  author = {Heimersheim, Stefan and Nanda, Neel},
  title = {How to Use and Interpret Activation Patching},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2404.15255},
  arxivId = {2404.15255},
  doi = {10.48550/arXiv.2404.15255},
  note = {
  CORE ARGUMENT: Provides comprehensive best practices for activation patching, the core causal intervention technique in MI. The paper clarifies different patching variants (zero ablation, mean ablation, resampling ablation, patching from corrupted runs), discusses how to choose appropriate metrics and baselines, and analyzes what causal claims can be legitimately inferred from patching experiments. Emphasizes that patching results depend heavily on methodological choices and that different corruption methods test different counterfactual scenarios.

  RELEVANCE: Activation patching is the primary method for testing causal hypotheses in MI—without rigorous patching methodology, circuit discovery claims lack empirical support. This paper addresses concerns about MI's methodological rigor by systematizing the technique and identifying potential pitfalls. The analysis of what patching does and doesn't tell us about circuits matters for evaluating MI's sufficiency: patching identifies necessary components but doesn't guarantee complete understanding. Essential reference for interpreting MI empirical claims.

  POSITION: Methodological foundations of circuit-centric MI. Focuses on ensuring causal validity of interpretability claims rather than advancing new techniques.
  },
  keywords = {activation-patching, methodology, causal-inference, Medium}
}

@inproceedings{zhang2023activation,
  author = {Zhang, Fred and Nanda, Neel},
  title = {Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  arxivId = {2309.16042},
  doi = {10.48550/arXiv.2309.16042},
  note = {
  CORE ARGUMENT: Systematically examines how methodological choices in activation patching affect localization results, finding that varying corruption methods (zero, mean, resample ablation) and evaluation metrics can lead to disparate circuit discoveries for the same model and task. Recommends specific combinations of methods and metrics based on empirical performance across multiple tasks. Shows that no single patching method is universally best, and practitioners must carefully consider their specific use case.

  RELEVANCE: Demonstrates that MI results can be method-dependent, raising concerns about the robustness of circuit discovery claims. If different patching methods identify different circuits for the same behavior, which circuit is "correct"? This methodological sensitivity matters for assessing MI's reliability as a safety tool. The paper's recommendations help standardize MI practice but also reveal that interpretability claims require careful methodological justification.

  POSITION: Methodological critique and improvement of activation patching. Highlights that MI's empirical findings depend on technical choices, requiring epistemic humility about circuit claims.
  },
  keywords = {activation-patching, methodology, evaluation, Medium}
}

@article{geiger2024causal,
  author = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah D. and Potts, Christopher and Icard, Thomas F.},
  title = {Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2301.04709},
  arxivId = {2301.04709},
  note = {
  CORE ARGUMENT: Develops a formal framework for mechanistic interpretability based on causal abstraction theory, which specifies when a high-level causal model (e.g., an algorithmic explanation) faithfully represents the low-level causal structure of a neural network. The paper generalizes beyond simple intervention to arbitrary mechanism transformations, provides formal definitions for concepts like polysemanticity and the linear representation hypothesis, and unifies diverse MI methods (activation patching, causal mediation, sparse autoencoders, circuit analysis) under a common theoretical language.

  RELEVANCE: Addresses the theoretical foundations that MI has often lacked—provides a rigorous framework for answering "what counts as a correct mechanistic explanation?" The causal abstraction framework makes precise what it means for a discovered circuit or feature to be faithful to the original model's computation. This matters for assessing MI's sufficiency: even if we identify circuits, we need criteria for when those circuits constitute adequate explanations. The framework also reveals limitations—faithful causal abstractions can exist at many levels, raising questions about which level provides the "right" interpretation.

  POSITION: Theoretical foundation for MI. Provides formal rigor but also reveals conceptual challenges around uniqueness and level of abstraction in mechanistic explanations.
  },
  keywords = {causal-abstraction, theory, foundations, Medium}
}

@article{he2024dictionary,
  author = {He, Zhengfu and Ge, Xuyang and Tang, Qiong and Sun, Tianxiang and Cheng, Qinyuan and Qiu, Xipeng},
  title = {Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2402.12201},
  arxivId = {2402.12201},
  doi = {10.48550/arXiv.2402.12201},
  note = {
  CORE ARGUMENT: Proposes a circuit discovery framework that combines sparse dictionary learning with attribution-based methods as an alternative to activation patching. The approach decomposes all modules writing to the residual stream (embedding, attention, MLP) into dictionary features, then traces contributions from lower-level features to higher-level behaviors (logits, attention scores) using attribution rather than counterfactual interventions. Applied to Othello-GPT, the method discovers interpretable fine-grained circuits more efficiently than patching-based approaches.

  RELEVANCE: Addresses two limitations of standard circuit discovery: (1) activation patching can suffer from out-of-distribution effects when interventions create unnatural activation patterns, and (2) patching is computationally expensive at scale. The attribution-based approach offers potentially more scalable circuit discovery, which matters for assessing MI's practical viability. The integration of dictionary learning with circuit discovery shows how feature-centric and circuit-centric MI can be unified.

  POSITION: Hybrid feature/circuit approach to MI. Attempts to combine benefits of dictionary learning (addressing superposition) with circuit discovery (identifying computational graphs).
  },
  keywords = {dictionary-learning, circuit-discovery, attribution, Medium}
}

@article{hsu2024contextual,
  author = {Hsu, Aliyah R. and Zhou, Georgia and Cherapanamjeri, Yeshwanth and Huang, Yaxuan and Odisho, Anobel and Carroll, Peter R. and Yu, Bin},
  title = {Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2407.00886},
  note = {
  CORE ARGUMENT: Introduces Contextual Decomposition for Transformers (CD-T), a mathematical framework that decomposes transformer computations to isolate contributions of specific components without requiring interventions. CD-T enables circuit discovery by recursively computing node contributions in the computational graph, achieving circuit identification in seconds rather than hours compared to activation patching methods. On standard circuit evaluation datasets, CD-T matches or exceeds ACDC and EAP in recovering manually-identified circuits while being dramatically faster.

  RELEVANCE: Addresses the computational bottleneck in circuit discovery that limits MI's scalability. If circuit discovery can be made orders of magnitude faster, MI becomes more practical for analyzing large models and diverse behaviors. CD-T's mathematical approach (decomposition rather than intervention) also avoids distributional shift concerns that affect patching. The speed-accuracy tradeoff and faithfulness metrics matter for assessing whether automated circuit discovery can replace manual circuit analysis.

  POSITION: Methodological advance in circuit discovery. Focuses on computational efficiency while maintaining faithfulness, enabling broader application of circuit-centric MI.
  },
  keywords = {circuit-discovery, contextual-decomposition, efficiency, Low}
}

@article{kastner2024explaining,
  author = {Kästner, Lena and Crook, Barnaby},
  title = {Explaining AI through Mechanistic Interpretability},
  journal = {European Journal for Philosophy of Science},
  year = {2024},
  volume = {14},
  doi = {10.1007/s13194-024-00614-4},
  note = {
  CORE ARGUMENT: Argues that standard XAI methods using divide-and-conquer strategies (analyzing individual components in isolation) fail to illuminate how AI systems work as integrated wholes. Drawing on philosophy of science, the paper advocates for mechanistic interpretability as applying coordinated discovery strategies from life sciences to uncover functional organization. MI should seek functional understanding—how components work together to produce system-level behavior—rather than just identifying individual component properties.

  RELEVANCE: Provides philosophical perspective on what makes MI distinctive from other interpretability approaches. The emphasis on "functional organization" and "how systems work as a whole" helps clarify what MI aims to achieve and why simple component analysis is insufficient. The connection to philosophy of science positions MI within broader debates about mechanistic explanation. Important for understanding the normative and epistemological commitments underlying MI practice, which matters for evaluating philosophical arguments about MI's role in safety.

  POSITION: Philosophical defense of MI as the appropriate framework for understanding AI systems. Emphasizes the importance of system-level functional understanding over component-level description.
  },
  keywords = {mechanistic-interpretability, philosophy, explanation, Medium}
}

@article{michaud2024program,
  author = {Michaud, Eric J. and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukelić, Mateja and Tegmark, Max},
  title = {Opening the AI Black Box: Program Synthesis via Mechanistic Interpretability},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2402.05110},
  arxivId = {2402.05110},
  doi = {10.48550/arXiv.2402.05110},
  note = {
  CORE ARGUMENT: Presents MIPS (Mechanistic Interpretability for Program Synthesis), which uses MI techniques to automatically extract executable Python programs from trained RNNs. The method converts RNNs to finite state machines via integer autoencoders, then applies symbolic regression to capture learned algorithms. On 62 algorithmic tasks, MIPS succeeds on 32 tasks including 13 that GPT-4 fails, demonstrating that MI can recover human-readable algorithms without relying on human-written training code.

  RELEVANCE: Demonstrates an ambitious application of MI—automatic extraction of complete algorithms from neural networks. If successful at scale, this would provide a path to understanding learned algorithms without manual circuit discovery. However, MIPS's limitation to simple RNN tasks highlights the gap between current MI capabilities and the aspiration to understand large language models. The comparison with GPT-4 shows MI's complementarity to LLM-based explanation methods.

  POSITION: Program synthesis approach to MI. Demonstrates proof-of-concept for automatic algorithm extraction but reveals significant scalability challenges.
  },
  keywords = {program-synthesis, finite-state-machines, symbolic-regression, Low}
}

@article{pearce2024bilinear,
  author = {Pearce, Michael T. and Dooms, Thomas and Rigg, Alice and Oramas, José and Sharkey, Lee},
  title = {Bilinear MLPs Enable Weight-Based Mechanistic Interpretability},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2410.08417},
  arxivId = {2410.08417},
  doi = {10.48550/arXiv.2410.08417},
  note = {
  CORE ARGUMENT: Analyzes bilinear MLPs (a Gated Linear Unit variant without elementwise nonlinearities) as more interpretable alternatives to standard MLPs while maintaining competitive performance. Bilinear layers can be fully expressed as linear operations using third-order tensors, enabling weight-based interpretability—analyzing model behavior directly from parameters rather than activations. Eigendecomposition of bilinear weights reveals interpretable low-rank structure corresponding to learned features, and enables circuit identification in small language models directly from weights.

  RELEVANCE: Proposes an architectural modification that could make MI more tractable by reducing the complexity introduced by nonlinearities. Weight-based interpretability would enable static analysis of models without running them on data. However, the approach's reliance on architectural changes limits its applicability to existing models. Demonstrates a tradeoff: MI can be easier with purpose-built architectures, but understanding deployed models with standard architectures remains the practical challenge.

  POSITION: Architectural approach to improving interpretability. Shows that design choices can make MI easier but doesn't address interpretation of existing deployed models.
  },
  keywords = {architecture, bilinear-layers, weight-analysis, Low}
}

