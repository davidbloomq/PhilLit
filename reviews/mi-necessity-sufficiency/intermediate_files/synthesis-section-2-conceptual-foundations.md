## What Counts as Mechanistic Interpretability?

The debate over mechanistic interpretability's role in AI safety rests on a prior question: what makes an interpretation "mechanistic" in the first place? This section establishes the conceptual foundations by mapping definitional disputes in machine learning onto the philosophy of mechanistic explanation, revealing that disagreements between critics and proponents reflect deeper philosophical tensions about levels, decomposition, and the relationship between mechanism and function.

### The New Mechanism Framework

The "new mechanism" philosophy of science, emerging in the early 2000s, fundamentally reoriented explanation away from law-based models toward understanding how things work. Machamer, Darden, and Craver (2000) established the foundational characterization: mechanisms are "entities and activities organized such that they are productive of regular changes from start or set-up to finish or termination conditions." This minimal characterization identifies three requirements for mechanistic explanation: specifying component entities, describing their activities, and showing how organization produces the phenomenon of interest.

Craver (2007) developed this framework further by distinguishing constitutive from etiological mechanistic explanation. Constitutive explanations reveal how a phenomenon is "built up" from organized components at lower levels, while etiological explanations trace causal histories. Crucially, Craver introduced the mutual manipulability criterion for constitutive relevance: components are mechanistically relevant to a phenomenon if interventions on components change the phenomenon's behavior and vice versa. This criterion provides an empirical test for whether proposed mechanisms identify genuine structure rather than mere correlation.

Bechtel and Richardson (2010) caution that mechanistic decomposition can fail when systems are not modular. Their analysis of "looking around" (understanding horizontal organization) rather than merely "looking down" (vertical decomposition) highlights that complex systems with distributed dynamics may resist clean hierarchical analysis. Glennan (2017) synthesizes these developments, arguing for pluralism about mechanistic levels: what matters is whether each proposed level identifies genuine part-whole structure, not whether decomposition reaches some privileged fundamental level.

### MI in Machine Learning Practice

Mechanistic interpretability in machine learning aims to reverse-engineer neural network computations into human-understandable algorithms. The transformer circuits framework introduced by Elhage et al. (2021) provides the technical foundation: the residual stream serves as a communication channel through which attention heads and MLP layers read and write information. This framework enables tracing computational pathways and identifying "circuits"—minimal subgraphs that implement specific behaviors.

Two obstacles complicate this project. First, Elhage et al. (2022) demonstrated that neural networks exhibit "superposition"—representing more features than they have neurons by storing them as overlapping directions in activation space. This creates "polysemanticity" where individual neurons respond to multiple unrelated concepts, undermining neuron-level analysis. Second, the labor-intensive nature of circuit discovery limits scalability, though automated methods like ACDC (Conmy et al. 2023) have made progress.

The response to superposition has been sparse autoencoders (SAEs), which decompose activations into more interpretable, monosemantic features (Bricken et al. 2023). Templeton et al. (2024) scaled this approach to Claude 3 Sonnet, discovering millions of interpretable features including safety-relevant concepts like deception and dangerous knowledge. Meanwhile, circuit discovery has yielded paradigmatic successes: Olsson et al. (2022) identified "induction heads"—two attention heads working in composition—as the mechanism underlying in-context learning, demonstrating that MI can link micro-level operations to macro-level capabilities.

The operational definition implicit in this practice is: MI = reverse-engineering computational mechanisms into human-understandable algorithms. Yet this definition leaves ambiguous the grain of analysis required—whether understanding must reach individual activations or whether higher-level functional descriptions suffice.

### Resolving the Definitional Dispute

The philosophy of mechanistic explanation provides resources for resolving this ambiguity. Piccinini and Craver (2011) argue that functional analyses—decomposing capacities into subfunctions without specifying implementation—are not alternatives to mechanistic explanation but rather "mechanism sketches" that abstract from implementation details while retaining mechanistic structure. Functional analysis identifies what a mechanism does; implementation reveals how. Both constitute legitimate mechanistic levels integrated through part-whole relations.

Povich and Craver (2017) extend this insight: multiple mechanistic levels can coexist non-competitively because each level identifies genuine constitutive structure. Mechanistic levels are defined by part-whole relations, not by spatial scale or disciplinary boundaries. Thus, attention heads (a higher-level functional description) and individual activations (a lower-level implementation description) can both be mechanistic if they satisfy appropriate criteria—specifically, if interventions on components at that level change system behavior and vice versa.

This framework illuminates the Hendrycks/Kastner disagreement. Kastner and Crook (2024) emphasize that MI should seek "functional organization"—how components work together to produce system-level behavior—rather than merely cataloging individual component properties. Their critique of "divide-and-conquer" XAI strategies echoes Bechtel's concern that horizontal organization matters, not just vertical decomposition. By contrast, critics who demand neuron-level or activation-level analysis implicitly assume that only the finest-grained level counts as genuinely mechanistic.

The philosophical resolution is that both can be right: narrow (circuit-level) and broad (functional) construals can be genuinely mechanistic if they satisfy mutual manipulability criteria—if interventions at the proposed level of description causally affect model behavior. Ayonrinde and Jaburi (2025) explicitly bridge philosophy of mechanism and MI, arguing that neural networks contain "implicit explanations" extractable through principled mechanistic investigation. Their "Principle of Explanatory Optimism" conjectures that networks are sufficiently structured to support such explanation.

Yet this resolution also exposes a gap: philosophical criteria for mechanistic explanation have not been systematically applied to evaluate MI methods. Whether circuit discovery and sparse autoencoders identify genuine constitutive structure—satisfying mutual manipulability—remains largely unexamined. The field operates with implicit operational definitions rather than explicit criteria drawn from the philosophy of mechanism. Establishing which MI methods provide genuine mechanistic understanding, and at which levels of analysis, requires exactly this systematic application of philosophical standards.

