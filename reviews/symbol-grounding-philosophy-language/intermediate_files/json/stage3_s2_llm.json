{
  "status": "success",
  "source": "semantic_scholar",
  "query": "large language models meaning understanding philosophy",
  "results": [
    {
      "paperId": "23b78d6e063982f1d110b6365601ebfc00304059",
      "title": "Understanding Conversational AI: Philosophy, Ethics, and Social Impact of Large Language Models",
      "authors": [
        {
          "name": "Thierry Poibeau",
          "authorId": "2392718362"
        }
      ],
      "year": 2025,
      "abstract": "\n \n What do large language models really know\u2014and what does it mean to live alongside them?\n \n \n This book offers a critical and interdisciplinary exploration of large language models (LLMs), examining how they reshape our understanding of language, cognition, and society. Drawing on philosophy of language, linguistics, cognitive science, and AI ethics, it investigates how these models generate meaning, simulate reasoning, and perform tasks that once seemed uniquely human\u2014from translation to moral judgment and literary creation.\n Rather than offering a purely technical account, the book interrogates the epistemic, ethical, and political dimensions of LLMs. It explores their limitations, their embedded biases, and their role in processes of automation, misinformation, and platform enclosure. At the same time, it reflects on how LLMs prompt us to revisit fundamental questions: What is understanding? What is creativity? How do we ascribe agency or trust in a world of synthetic language?\n Written for scholars, students, and curious readers across the humanities, social sciences, and computer science, this is both a philosophical inquiry and a practical guide to navigating the era of generative AI. It invites readers to think critically about the promises and perils of language technologies\u2014and about the kind of future we are shaping with them.",
      "citationCount": 0,
      "doi": "10.5334/bde",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/23b78d6e063982f1d110b6365601ebfc00304059",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "2e3c5e48083ae5a0cdd268da97e909e1fd754d3f",
      "title": "\"Understanding AI\": Semantic Grounding in Large Language Models",
      "authors": [
        {
          "name": "Holger Lyre",
          "authorId": "2284679884"
        }
      ],
      "year": 2024,
      "abstract": "Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2402.10992",
      "arxivId": "2402.10992",
      "url": "https://www.semanticscholar.org/paper/2e3c5e48083ae5a0cdd268da97e909e1fd754d3f",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.10992"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "24aeaac737e7c689cfa737cb4a4133de495c7e14",
      "title": "Bridging Philosophical Foundations andComputational Realities: Semantic Under-Specification from Frege to Large Language Models",
      "authors": [
        {
          "name": "Tahir Qayyum",
          "authorId": "2376454970"
        },
        {
          "name": "Anam Tahir",
          "authorId": "2376455021"
        },
        {
          "name": "Iftikhar Ahmed Shaheen",
          "authorId": "2277333003"
        },
        {
          "name": "Syed Atif Amir Gardazi",
          "authorId": "2376455110"
        }
      ],
      "year": 2025,
      "abstract": "Semantic underspecificationoccurs when linguistic expressions carry partial meaning, requiring context for full understanding. It poses key challenges across philosophy, cognitive science, and NLP. This review identifies five developmental stages: (1)Classical theories by Frege, Russell, and Davidson created truth-conditional frameworks but encountered difficulties with indexicaland belief contexts due to assumptions of full specification; (2)Formal models like QLF, MRS, and Hole Semantics introduced computational underspecification to handle structural ambiguities such as quantifier scope; (3)Cognitive studies show humans use underspecification strategically for efficiency, relying on pragmatic inference and semantic memory; (4)Hybrid neuro-symbolic models like UMR and Glue Semantics combined structural ambiguity resolution with neural inference but lacked uncertainty modeling; (5)Modern NLP research highlights gaps: LLMs can detect underspecification but often overcommit to deterministic interpretations, and multimodal systems do not effectively utilize context. Cross-linguistic entropy models suggest grammatical underspecification as a strategy for cognitive efficiency. To bridge human semantic flexibilityenabled by incremental processing and pragmatic co-construction\u2014with computational systems, we propose integrated neuro-symbolic architectures incorporating explicit uncertainty modeling, multimodal grounding, and entropy-aware design. This approach paves the way for AI to achieve human-like language understanding.",
      "citationCount": 0,
      "doi": "10.55966/assaj.2025.4.1.0117",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/24aeaac737e7c689cfa737cb4a4133de495c7e14",
      "venue": "Advance Social Science Archive Journal",
      "journal": {
        "name": "Advance Social Science Archive Journal"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "a55c1d40c874a205aca1d7b5a4f2a4424168c0fa",
      "title": "Do Large Language Models Advocate for Inferentialism?",
      "authors": [
        {
          "name": "Yuzuki Arai",
          "authorId": "2336080095"
        },
        {
          "name": "Sho Tsugawa",
          "authorId": "46403544"
        }
      ],
      "year": 2024,
      "abstract": "The emergence of large language models (LLMs) such as ChatGPT and Claude presents new challenges for philosophy of language, particularly regarding the nature of linguistic meaning and representation. While LLMs have traditionally been understood through distributional semantics, this paper explores Robert Brandom's inferential semantics as an alternative foundational framework for understanding these systems. We examine how key features of inferential semantics -- including its anti-representationalist stance, logical expressivism, and quasi-compositional approach -- align with the architectural and functional characteristics of Transformer-based LLMs. Through analysis of the ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs exhibit fundamentally anti-representationalist properties in their processing of language. We further develop a consensus theory of truth appropriate for LLMs, grounded in their interactive and normative dimensions through mechanisms like RLHF. While acknowledging significant tensions between inferentialism's philosophical commitments and LLMs'sub-symbolic processing, this paper argues that inferential semantics provides valuable insights into how LLMs generate meaning without reference to external world representations. Our analysis suggests that LLMs may challenge traditional assumptions in philosophy of language, including strict compositionality and semantic externalism, though further empirical investigation is needed to fully substantiate these theoretical claims.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2412.14501",
      "url": "https://www.semanticscholar.org/paper/a55c1d40c874a205aca1d7b5a4f2a4424168c0fa",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "7de32673db5bdd4b6fe1de3a35eba9c998ceb668",
      "title": "The Surprising Victory of NLP: From Philosophy to Agentic Language Models",
      "authors": [
        {
          "name": "C. Manning",
          "authorId": "2285299483"
        }
      ],
      "year": 2025,
      "abstract": "Language Models have been around for decades but have suddenly taken the world by storm. In a surprising third act for anyone doing Natural Language Processing (NLP) in the 70s, 80s, 90s, or 2000s, in much of the popular media, artificial intelligence is now synonymous with language models. In this talk, I want to take a look backward at where language models came from and why they were so slow to emerge, a look inward to give a few thoughts on meaning, intelligence, and what language models understand and know, and a look forward at some topics of recent research: the possibilities for progress with new versions of Universal Transformers and using Large Language Models (LLMs) to build intelligent language-using agents for the digital world. I will argue that material beyond language is not necessary to having meaning and understanding, but it is very useful in most cases, and that composability, adaptability, and learning are vital to intelligence. Rather than simply continuing to build ever-larger LLMs from huge passive collections of text data, we need to explore developing better neural architectures and agents that can learn through interactions. I will discuss how web agents can learn through interactions and how such an interaction-first learning approach can work very effectively in web environments with relatively small language models.",
      "citationCount": 0,
      "doi": "10.1145/3711896.3736801",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7de32673db5bdd4b6fe1de3a35eba9c998ceb668",
      "venue": "Knowledge Discovery and Data Mining",
      "journal": {
        "name": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "2e7165723cfa9543dcee6db2c1546b80d391b377",
      "title": "Wittgenstein\u2019s Philosophy of Language The Philosophical Origins of Modern NLP Thinking",
      "authors": [
        {
          "name": "Robert Bain",
          "authorId": "153718379"
        },
        {
          "name": "Andrew Festa",
          "authorId": "2150559839"
        },
        {
          "name": "Ga Young Lee",
          "authorId": "2150561091"
        },
        {
          "name": "A. Zhang",
          "authorId": "2153659129"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2e7165723cfa9543dcee6db2c1546b80d391b377",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "64516525d62ac9ac3473c4514832f592281383f1",
      "title": "Not Minds, but Signs: Reframing LLMs through Semiotics",
      "authors": [
        {
          "name": "Davide Picca",
          "authorId": "2321177729"
        }
      ],
      "year": 2025,
      "abstract": "This paper challenges the prevailing tendency to frame Large Language Models (LLMs) as cognitive systems, arguing instead for a semiotic perspective that situates these models within the broader dynamics of sign manipulation and meaning-making. Rather than assuming that LLMs understand language or simulate human thought, we propose that their primary function is to recombine, recontextualize, and circulate linguistic forms based on probabilistic associations. By shifting from a cognitivist to a semiotic framework, we avoid anthropomorphism and gain a more precise understanding of how LLMs participate in cultural processes, not by thinking, but by generating texts that invite interpretation. Through theoretical analysis and practical examples, the paper demonstrates how LLMs function as semiotic agents whose outputs can be treated as interpretive acts, open to contextual negotiation and critical reflection. We explore applications in literature, philosophy, education, and cultural production, emphasizing how LLMs can serve as tools for creativity, dialogue, and critical inquiry. The semiotic paradigm foregrounds the situated, contingent, and socially embedded nature of meaning, offering a more rigorous and ethically aware framework for studying and using LLMs. Ultimately, this approach reframes LLMs as technological participants in an ongoing ecology of signs. They do not possess minds, but they alter how we read, write, and make meaning, compelling us to reconsider the foundations of language, interpretation, and the role of artificial systems in the production of knowledge.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.17080",
      "arxivId": "2505.17080",
      "url": "https://www.semanticscholar.org/paper/64516525d62ac9ac3473c4514832f592281383f1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.17080"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "399bb1d8aa0fcb06cbc21b93a8d3e930a9cc0e78",
      "title": "On the Understanding of Large Language Models: Based on Wittgenstein\u2019s Later Philosophy",
      "authors": [
        {
          "name": "\u5f66\u98de \u674e",
          "authorId": "2388412282"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.12677/acpp.2025.1410525",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/399bb1d8aa0fcb06cbc21b93a8d3e930a9cc0e78",
      "venue": "Advances in Philosophy",
      "journal": {
        "name": "Advances in Philosophy"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2e11ace144dd72d5f30303b96eb3a1ac61c4bd5d",
      "title": "Meaning and understanding in large language models",
      "authors": [
        {
          "name": "Vladim'ir Havl'ik",
          "authorId": "2261737538"
        }
      ],
      "year": 2023,
      "abstract": "Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. This article critically evaluates the prevailing tendency to regard machine language performance as mere syntactic manipulation and the imitations of understanding, which is only partial and very shallow, without sufficient grounding in the world. The article analyses the views on possible ways of grounding as a condition for successful understanding in LLMs and offers an alternative way in view of the prevailing belief that the success of understanding depends mainly on the referential grounding. An alternative conception seeks to show that semantic fragmentism offers a viable account of natural language understanding and explains how LLMs ground the meanings of linguistic expressions. Uncovering how meanings are grounded allows us to also explain why LLMs\u2019 ability to understand is possible and so remarkably successful.",
      "citationCount": 10,
      "doi": "10.1007/s11229-024-04878-4",
      "arxivId": "2310.17407",
      "url": "https://www.semanticscholar.org/paper/2e11ace144dd72d5f30303b96eb3a1ac61c4bd5d",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "205"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9194e910f7c63333a80eb3c52fa4a04942f4ab65",
      "title": "Large language models and M. Heidegger\u2019s philosophy of language",
      "authors": [
        {
          "name": "Maxim Alexandrovich Repin",
          "authorId": "2392542901"
        }
      ],
      "year": 2025,
      "abstract": "The article provides a philosophical analysis of the ability of large language models (LLMs) to generate authentic meaning. The aim of the study is to identify the fundamental ontological conditions for meaningful discourse based on Martin Heidegger\u2019s philosophy of language and to apply them for a critical assessment of LLM-generated texts. The article compares key concepts of M. Heidegger\u2019s philosophy (\u201cDasein\u201d, \u201cIn-der-Welt-sein\u201d, \u201cSorge\u201d, \u201cSein-zum-Tode\u201d) with the architecture and principles of operation of LLMs. The scientific novelty of the research lies in a new approach to the philosophical analysis of large language models through the prism of M. Heidegger\u2019s existential-ontological philosophy. Unlike the epistemological and cognitivist interpretations dominant in contemporary literature, which focus on questions of information processing and functional equivalence to human intelligence, this study transfers the problematic to a fundamental ontological level, revealing the structural conditions for the possibility of authentic meaning-generation. The innovative aspect is establishing a direct connection between the absence of existential structure in LLMs and their inability to generate authentic meaning, which shifts the discussion of machine understanding from the plane of increasing computational power and data volumes to the realm of fundamental questions about the nature of human being and language. The research makes an original contribution to understanding LLMs as a technological embodiment of Gestell, demonstrating how contemporary language models institutionalize an ultimately technical relation to language, transforming it from the \u201chouse of Being\u201d into a calculable resource, which has far-reaching philosophical and cultural consequences for understanding the nature of communication and cognition in the digital age. The results amount to the following: LLMs operate outside the existential structure of Dasein (human presence), reducing language to a calculable statistical resource (Bestand) and stripping it of its event-of-truth dimension. The technological paradigm (Gestell) underlying LLMs engenders epistemic, ontological, and anthropological risks, including the spread of pseudo-knowledge and the devaluation of human thought.",
      "citationCount": 0,
      "doi": "10.30853/mns20250207",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/9194e910f7c63333a80eb3c52fa4a04942f4ab65",
      "venue": "Manuscript",
      "journal": {
        "name": "Manuscript"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4af6213d767259388865115431571bae0452047d",
      "title": "Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives",
      "authors": [
        {
          "name": "Arno Simons",
          "authorId": "2331613160"
        },
        {
          "name": "Michael Zichert",
          "authorId": "2327048546"
        },
        {
          "name": "Adrian W\u00fcthrich",
          "authorId": "2264327337"
        }
      ],
      "year": 2025,
      "abstract": "This paper explores the use of large language models (LLMs) as research tools in the history, philosophy, and sociology of science (HPSS). LLMs are remarkably effective at processing unstructured text and inferring meaning from context, offering new affordances that challenge long-standing divides between computational and interpretive methods. This raises both opportunities and challenges for HPSS, which emphasizes interpretive methodologies and understands meaning as context-dependent, ambiguous, and historically situated. We argue that HPSS is uniquely positioned not only to benefit from LLMs' capabilities but also to interrogate their epistemic assumptions and infrastructural implications. To this end, we first offer a concise primer on LLM architectures and training paradigms tailored to non-technical readers. We frame LLMs not as neutral tools but as epistemic infrastructures that encode assumptions about meaning, context, and similarity, conditioned by their training data, architecture, and patterns of use. We then examine how computational techniques enhanced by LLMs, such as structuring data, detecting patterns, and modeling dynamic processes, can be applied to support interpretive research in HPSS. Our analysis compares full-context and generative models, outlines strategies for domain and task adaptation (e.g., continued pretraining, fine-tuning, and retrieval-augmented generation), and evaluates their respective strengths and limitations for interpretive inquiry in HPSS. We conclude with four lessons for integrating LLMs into HPSS: (1) model selection involves interpretive trade-offs; (2) LLM literacy is foundational; (3) HPSS must define its own benchmarks and corpora; and (4) LLMs should enhance, not replace, interpretive methods.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2506.12242",
      "arxivId": "2506.12242",
      "url": "https://www.semanticscholar.org/paper/4af6213d767259388865115431571bae0452047d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.12242"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "76a1640fa11c8f08fc6038e7e85f9c8bf97cf114",
      "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier",
      "authors": [
        {
          "name": "D. Nissani",
          "authorId": "1981198"
        }
      ],
      "year": 2025,
      "abstract": "A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2505.00654",
      "arxivId": "2505.00654",
      "url": "https://www.semanticscholar.org/paper/76a1640fa11c8f08fc6038e7e85f9c8bf97cf114",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.00654"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9952daa8fdd1153ee2b1e9809899a5d237c011c9",
      "title": "From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to Enhance Large Language Models",
      "authors": [
        {
          "name": "Qiantong Wang",
          "authorId": "2350647983"
        }
      ],
      "year": 2025,
      "abstract": "This paper explores hallucination phenomena in large language models (LLMs) through the lens of language philosophy and psychoanalysis. By incorporating Lacan's concepts of the\"chain of signifiers\"and\"suture points,\"we propose the Anchor-RAG framework as a novel approach to mitigate hallucinations. In contrast to the predominant reliance on trial-and-error experiments, constant adjustments of mathematical formulas, or resource-intensive methods that emphasize quantity over quality, our approach returns to the fundamental principles of linguistics to analyze the root causes of hallucinations in LLMs. Drawing from robust theoretical foundations, we derive algorithms and models that are not only effective in reducing hallucinations but also enhance LLM performance and improve output quality. This paper seeks to establish a comprehensive theoretical framework for understanding hallucinations in LLMs and aims to challenge the prevalent\"guess-and-test\"approach and rat race mentality in the field. We aspire to pave the way for a new era of interpretable LLMs, offering deeper insights into the inner workings of language-based AI systems.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2503.14392",
      "arxivId": "2503.14392",
      "url": "https://www.semanticscholar.org/paper/9952daa8fdd1153ee2b1e9809899a5d237c011c9",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.14392"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0f8206b87a455f2fecb1b4de9dee84de0d510613",
      "title": "The bewitching AI: The Illusion of Communication with Large Language Models",
      "authors": [
        {
          "name": "Emanuele Bottazzi Grifoni",
          "authorId": "2359377295"
        },
        {
          "name": "Roberta Ferrario",
          "authorId": "2265144644"
        }
      ],
      "year": 2025,
      "abstract": "We investigate the \u2018bewitchment\u2019 of understanding interactions between humans and systems based on large language models (LLMs) inspired by Wittgenstein\u2019s later view on language. This framework is particularly apt for analyzing human-LLM interaction as it treats understanding as a public phenomenon manifested in observable communicative practices, rather than as a mental or computational state\u2014an approach especially valuable given LLMs\u2019 inherent opacity. Drawing on this perspective, we show that successful communication requires not merely regularity in language use, but constancy in maintaining reference points through agreement in both definitions and judgments. Crucially, LLMs lack the constancy needed to track negations and contradictions throughout a dialogue, thereby disrupting the reference points necessary for genuine communication. The apparent understanding in human-LLM interactions arises from what we characterize as a \u2018bewitchment\u2019: the interaction between LLMs\u2019 statistical adherence to linguistic patterns and humans\u2019 tendency to blindly follow familiar language games. Moreover, when interaction with LLMs is based on stereotyped contexts in which the system seems capable of identifying reference points, we humans automatically apply the practical principle that there is understanding until proven otherwise. The bewitchment becomes thus more profound as LLMs improve in modeling stereotypical aspects of human interaction. This improvement, far from addressing the highlighted limitations, can only deepen the illusion of understanding, raising significant concerns for meaningful control over such systems.",
      "citationCount": 4,
      "doi": "10.1007/s13347-025-00893-6",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0f8206b87a455f2fecb1b4de9dee84de0d510613",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "38"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "da4d8bdd6fad69eb021c031337a489b0aca3f300",
      "title": "Jurists of the Gaps: Large Language Models and the Quiet Erosion of Legal Authority",
      "authors": [
        {
          "name": "Philippe Prince Tritto",
          "authorId": "2384598879"
        },
        {
          "name": "Ilsse Carolina Torres Ortega",
          "authorId": "2183427580"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) are not merely tools to assist legal professionals\u2014they represent a deeper epistemic and normative challenge to the foundations of legal authority. While LLMs allow humans to produce outputs that convincingly simulate legal reasoning, they lack the embodied judgment, ethical intentionality, and contextual awareness that define legitimate legal decision-making. This paper argues that the social legitimacy of the legal profession relies on capacities that are not reproducible through computational systems. We first examine the epistemological limitations of LLMs, drawing on Kantian philosophy and complexity theory to show that their outputs are simulations, not acts of understanding. We then analyze how this technological shift risks reducing legal professionals to jurists of the gaps \u2013 filling in only where machines fall short \u2013 thereby hollowing out the humanistic mission of law. Against this backdrop, we call for a renewed professional ethic centered on interpretation, creativity, and normative judgment, rather than technical supplementation. The automation of law is not the end of the profession, but it could be the end of its authority \u2013 unless its practitioners reclaim what cannot be outsourced.",
      "citationCount": 0,
      "doi": "10.5817/mujlt2025-2-4",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/da4d8bdd6fad69eb021c031337a489b0aca3f300",
      "venue": "Masaryk University Journal of Law and Technology",
      "journal": {
        "name": "Masaryk University Journal of Law and Technology"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "454ce3673efaecfdc3f472a5a59f4e61fb2f5f59",
      "title": "Teaching with Large Language Models in the History of Philosophy",
      "authors": [
        {
          "name": "Michael Otteson",
          "authorId": "2340613790"
        }
      ],
      "year": 2025,
      "abstract": "This paper examines and catalogues my experiences assigning students to use large language models (LLMs) to engage with material from the history of philosophy.\u00a0 Following suggestions to make recursive assignments from Julia Staffel, the LLM centered assignment asked students to elicit two responses from generative AI with the same prompt and then evaluate which response was superior.\u00a0 Student performance on the assignments generally mirrored what they had done on traditional essay assignments in my classes, suggesting that the LLM assignment was appropriate for college level classes.\u00a0\u00a0",
      "citationCount": 0,
      "doi": "10.33043/66z64ff68",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/454ce3673efaecfdc3f472a5a59f4e61fb2f5f59",
      "venue": "Teaching History: A Journal of Methods",
      "journal": {
        "name": "Teaching History: A Journal of Methods"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "914faf9494fbfde1fa96584e9578356a344818ed",
      "title": "Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?",
      "authors": [
        {
          "name": "Louis Vervoort",
          "authorId": "2367271256"
        },
        {
          "name": "Vitaly Nikolaev",
          "authorId": "48942032"
        }
      ],
      "year": 2025,
      "abstract": "We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2506.14239",
      "arxivId": "2506.14239",
      "url": "https://www.semanticscholar.org/paper/914faf9494fbfde1fa96584e9578356a344818ed",
      "venue": "Journal for General Philosophy of Science, Zeitschrift f\u00fcr allgemeine Wissenschaftstheorie",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.14239"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bcbb02a484ca501dcedd8bc6be0c56cf69e42b2a",
      "title": "Artificial Intelligence and the Illusion of Understanding: A Systematic Review of Theory of Mind and Large Language Models",
      "authors": [
        {
          "name": "Antonella Marchetti",
          "authorId": "2312398690"
        },
        {
          "name": "F. Manzi",
          "authorId": "31636934"
        },
        {
          "name": "Giuseppe Riva",
          "authorId": "2279720491"
        },
        {
          "name": "A. Gaggioli",
          "authorId": "1700503"
        },
        {
          "name": "D. Massaro",
          "authorId": "4623295"
        }
      ],
      "year": 2025,
      "abstract": "The development of Large Language Models (LLMs) has sparked significant debate regarding their capacity for Theory of Mind (ToM)\u2014the ability to attribute mental states to oneself and others. This systematic review examines the extent to which LLMs exhibit Artificial ToM (AToM) by evaluating their performance on ToM tasks and comparing it with human responses. While LLMs, particularly GPT-4, perform well on first-order false belief tasks, they struggle with more complex reasoning, such as second-order beliefs and recursive inferences, where humans consistently outperform them. Moreover, the review underscores the variability in ToM assessments, as many studies adapt classical tasks for LLMs, raising concerns about comparability with human ToM. Most evaluations remain constrained to text-based tasks, overlooking embodied and multimodal dimensions crucial to human social cognition. This review discusses the \u201cillusion of understanding\u201d in LLMs for two primary reasons: First, their lack of the developmental and cognitive mechanisms necessary for genuine ToM, and second, methodological biases in test designs that favor LLMs\u2019 strengths, limiting direct comparisons with human performance. The findings highlight the need for more ecologically valid assessments and interdisciplinary research to better delineate the limitations and potential of AToM. This set of issues is highly relevant to psychology, as language is generally considered just one component in the broader development of human ToM, a perspective that contrasts with the dominant approach in AToM studies. This discrepancy raises critical questions about the extent to which human ToM and AToM are comparable.",
      "citationCount": 3,
      "doi": "10.1089/cyber.2024.0536",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/bcbb02a484ca501dcedd8bc6be0c56cf69e42b2a",
      "venue": "Cyberpsychology, Behavior, and Social Networking",
      "journal": {
        "name": "Cyberpsychology, Behavior, and Social Networking",
        "pages": "505 - 514",
        "volume": "28"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f8467b8a6c4a8aa65f7ef1c478505236b238a3cd",
      "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models",
      "authors": [
        {
          "name": "Giuseppe Russo",
          "authorId": "2057794866"
        },
        {
          "name": "Debora Nozza",
          "authorId": "2101317501"
        },
        {
          "name": "Paul R\u00f6ttger",
          "authorId": "2043232919"
        },
        {
          "name": "Dirk Hovy",
          "authorId": "2267334203"
        }
      ],
      "year": 2025,
      "abstract": "People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans'decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, we introduce the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. We treat this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. We find that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, we show that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2507.17216",
      "arxivId": "2507.17216",
      "url": "https://www.semanticscholar.org/paper/f8467b8a6c4a8aa65f7ef1c478505236b238a3cd",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.17216"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "35c0738762574b1bcbc2c9a46e78f07f70772eb2",
      "title": "Mechanistic Indicators of Understanding in Large Language Models",
      "authors": [
        {
          "name": "Pierre Beckmann",
          "authorId": "2371998450"
        },
        {
          "name": "Matthieu Queloz",
          "authorId": "2372000852"
        }
      ],
      "year": 2025,
      "abstract": "Recent findings in mechanistic interpretability (MI), the field probing the inner workings of Large Language Models (LLMs), challenge the view that these models rely solely on superficial statistics. We offer an accessible synthesis of these findings that doubles as an introduction to MI while integrating these findings within a novel theoretical framework for thinking about machine understanding. We argue that LLMs develop internal structures that are functionally analogous to the kind of understanding that consists in seeing connections. To sharpen this idea, we propose a three-tiered conception of understanding. First, conceptual understanding emerges when a model forms\"features\"as directions in latent space, learning the connections between diverse manifestations of something. Second, state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world. Third, principled understanding emerges when a model ceases to rely on a collection of memorized facts and discovers a\"circuit\"connecting these facts. However, these forms of understanding remain radically different from human understanding, as the phenomenon of\"parallel mechanisms\"shows. We conclude that the debate should move beyond the yes-or-no question of whether LLMs understand to investigate how their strange minds work and forge conceptions that fit them.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2507.08017",
      "arxivId": "2507.08017",
      "url": "https://www.semanticscholar.org/paper/35c0738762574b1bcbc2c9a46e78f07f70772eb2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.08017"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8b8fe1fc0d88dc28d75ec0a00375a3d0c3dc658a",
      "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language Models",
      "authors": [
        {
          "name": "Rikard Rosenbacke",
          "authorId": "2287320882"
        },
        {
          "name": "Carl Rosenbacke",
          "authorId": "2386015108"
        },
        {
          "name": "Victor Rosenbacke",
          "authorId": "2386016228"
        },
        {
          "name": "Martin McKee",
          "authorId": "2386015037"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) are becoming deeply embedded in human communication and decision-making, yet they inherit the ambiguity, bias, and lack of direct access to truth inherent in language itself. While their outputs are fluent, emotionally resonant, and coherent, they are generated through statistical prediction rather than grounded reasoning. This creates the risk of hallucination, responses that sound convincing but lack factual validity. Building on Geoffrey Hinton's observation that AI mirrors human intuition rather than reasoning, this paper argues that LLMs operationalize System 1 cognition at scale: fast, associative, and persuasive, but without reflection or falsification. To address this, we introduce the Rose-Frame, a three-dimensional framework for diagnosing cognitive and epistemic drift in human-AI interaction. The three axes are: (i) Map vs. Territory, which distinguishes representations of reality (epistemology) from reality itself (ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to separate fast, emotional judgments from slow, reflective thinking; and (iii) Conflict vs. Confirmation, which examines whether ideas are critically tested through disagreement or simply reinforced through mutual validation. Each dimension captures a distinct failure mode, and their combination amplifies misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules. Instead, it offers a reflective tool that makes both the model's limitations and the user's assumptions visible, enabling more transparent and critically aware AI deployment. It reframes alignment as cognitive governance: intuition, whether human or artificial, must remain governed by human reason. Only by embedding reflective, falsifiable oversight can we align machine fluency with human understanding.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.14665",
      "arxivId": "2510.14665",
      "url": "https://www.semanticscholar.org/paper/8b8fe1fc0d88dc28d75ec0a00375a3d0c3dc658a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.14665"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a54e0011f478d476f4582ce1234d56d1974729fe",
      "title": "Understanding Large Language Models' Ability on Interdisciplinary Research",
      "authors": [
        {
          "name": "Yuanhao Shen",
          "authorId": "2329746837"
        },
        {
          "name": "Daniel Xavier de Sousa",
          "authorId": "2373288346"
        },
        {
          "name": "Ricardo Mar\u00e7al",
          "authorId": "2376131172"
        },
        {
          "name": "A. Asad",
          "authorId": "2367042486"
        },
        {
          "name": "Hongyu Guo",
          "authorId": "2278393469"
        },
        {
          "name": "Xiaodan Zhu",
          "authorId": "2329740584"
        }
      ],
      "year": 2025,
      "abstract": "Recent advancements in Large Language Models (LLMs) have revealed their impressive ability to perform multi-step, logic-driven reasoning across complex domains, positioning them as powerful tools and collaborators in scientific discovery while challenging the long-held view that inspiration-driven ideation is uniquely human. However, the lack of a dedicated benchmark that evaluates LLMs'ability to develop ideas in Interdisciplinary Research (IDR) settings poses a critical barrier to fully understanding their strengths and limitations. To address this gap, we introduce IDRBench -- a pioneering benchmark featuring an expert annotated dataset and a suite of tasks tailored to evaluate LLMs'capabilities in proposing valuable research ideas from different scientific domains for interdisciplinary research. This benchmark aims to provide a systematic framework for assessing LLM performance in complex, cross-domain scientific research. Our dataset consists of scientific publications sourced from the ArXiv platform covering six distinct disciplines, and is annotated by domain experts with diverse academic backgrounds. To ensure high-quality annotations, we emphasize clearly defined dimensions that characterize authentic interdisciplinary research. The design of evaluation tasks in IDRBench follows a progressive, real-world perspective, reflecting the natural stages of interdisciplinary research development, including 1) IDR Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation. Using IDRBench, we construct baselines across 10 LLMs and observe that despite fostering some level of IDR awareness, LLMs still struggle to produce quality IDR ideas. These findings could not only spark new research directions, but also help to develop next-generation LLMs that excel in interdisciplinary research.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.15736",
      "arxivId": "2507.15736",
      "url": "https://www.semanticscholar.org/paper/a54e0011f478d476f4582ce1234d56d1974729fe",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.15736"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c0993b9ee7288e2976600cc1520952fb2d3dacad",
      "title": "Understanding Large Language Models through the Lens of Artificial Agency",
      "authors": [
        {
          "name": "Maud van Lier",
          "authorId": "2239130884"
        }
      ],
      "year": 2023,
      "abstract": "This paper is motivated by Floridi\u2019s recent claim that Large Language Models like ChatGPT can be seen as \u2018intelligence-free\u2019 agents. Where I do not agree with Floridi that such systems are intelligence-free, my paper does question whether they can be called agents, and if so, what kind. I argue for the adoption of a more restricted understanding of agent in AI-research, one that comes closer in its meaning to how the term is used in the philosophies of mind, action, and agency. I propose such a more narrowing understanding of agent, suggesting that an agent can be seen as entity or system that things can be \u2018up to\u2019, that can act autonomously in a way that is best understood on the basis of Husserl\u2019s notion of indeterminate determinability.",
      "citationCount": 4,
      "doi": "10.3384/ecp199008",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c0993b9ee7288e2976600cc1520952fb2d3dacad",
      "venue": "Annual Workshop of the Swedish Artificial Intelligence Society",
      "journal": {
        "pages": "79-84"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6115f4011fbf21bfe9e2227d7904abf9a8962358",
      "title": "Symbol ungrounding: what the successes (and failures) of large language models reveal about human cognition",
      "authors": [
        {
          "name": "Guy Dove",
          "authorId": "16673247"
        }
      ],
      "year": 2024,
      "abstract": "Large language models can handle sophisticated natural language processing tasks. This raises the question of how their understanding of semantic meaning compares to that of human beings. Supporters of embodied cognition often point out that because these models are trained solely on text, their representations of semantic content are not grounded in sensorimotor experience. This paper contends that human cognition exhibits capabilities that fit with both the embodied and artificial intelligence approaches. Evidence suggests that semantic memory is partially grounded in sensorimotor systems and dependent on language-specific learning. From this perspective, large language models demonstrate the richness of language as a source of semantic information. They show how our experience with language might scaffold and extend our capacity to make sense of the world. In the context of an embodied mind, language provides access to a valuable form of ungrounded cognition. This article is part of the theme issue \u2018Minds in movement: embodied cognition in the age of artificial intelligence\u2019.",
      "citationCount": 6,
      "doi": "10.1098/rstb.2023.0149",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6115f4011fbf21bfe9e2227d7904abf9a8962358",
      "venue": "Philosophical Transactions B",
      "journal": {
        "name": "Philosophical Transactions B",
        "volume": "379"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "f577654d9dd29d88c6db9ee39a4fd831573b8770",
      "title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",
      "authors": [
        {
          "name": "Alex Tamkin",
          "authorId": "88726969"
        },
        {
          "name": "Miles Brundage",
          "authorId": "35167962"
        },
        {
          "name": "Jack Clark",
          "authorId": "2115193883"
        },
        {
          "name": "Deep Ganguli",
          "authorId": "2081806483"
        }
      ],
      "year": 2021,
      "abstract": "On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.",
      "citationCount": 307,
      "doi": null,
      "arxivId": "2102.02503",
      "url": "https://www.semanticscholar.org/paper/f577654d9dd29d88c6db9ee39a4fd831573b8770",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2102.02503"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2c0cd87470d441229d5e8ea018574729955f4a64",
      "title": "A shift towards oration: teaching philosophy in the age of large language models",
      "authors": [
        {
          "name": "Ryan Lemasters",
          "authorId": "1381977017"
        },
        {
          "name": "Clint Hurshman",
          "authorId": "2295809455"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/s43681-024-00455-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2c0cd87470d441229d5e8ea018574729955f4a64",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "pages": "1203 - 1215",
        "volume": "5"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d314fb62f1419faeb1c651f1c2f1b0b9fd8ea4e9",
      "title": "Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy",
      "authors": [
        {
          "name": "Jianqiiu Zhang",
          "authorId": "2290133868"
        }
      ],
      "year": 2024,
      "abstract": "In the rapidly evolving field of Large Language Models (LLMs), there is a critical need to thoroughly analyze their capabilities and risks. Central to our investigation are two novel elements. Firstly, it is the innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of\"ready-to-hand\"and\"present-at-hand,\"which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world. This comparison lays the groundwork for positioning LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning. Secondly, a structural analysis of human reasoning, viewed through Heidegger's notion of truth as\"unconcealment\"is conducted This foundational principle enables us to map out the inputs and outputs of the reasoning system and divide reasoning into four distinct categories. Respective cognitive faculties are delineated, allowing us to place LLMs within the broader schema of human reasoning, thus clarifying their strengths and inherent limitations. Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement. The potential and risks of LLMs when they are augmented with other AI technologies are also evaluated. The results indicate that although LLMs have achieved proficiency in some reasoning abilities, the aspiration to match or exceed human intellectual capabilities is yet unattained. This research not only enriches our comprehension of LLMs but also propels forward the discourse on AI's potential and its bounds, paving the way for future explorations into AI's evolving landscape.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2403.03288",
      "arxivId": "2403.03288",
      "url": "https://www.semanticscholar.org/paper/d314fb62f1419faeb1c651f1c2f1b0b9fd8ea4e9",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2403.03288"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8e4b8bc4a9c67da607756364bdc5a77852a617e0",
      "title": "Towards Responsible AI: Understanding and Mitigating Ethical Concerns of Large Language Models",
      "authors": [
        {
          "name": "Akshata Upadhye",
          "authorId": "2267973563"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools in the field of natural language processing and have transformed the way we interact with text data and generate textual content.",
      "citationCount": 0,
      "doi": "10.47363/jaicc/2024(3)289",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8e4b8bc4a9c67da607756364bdc5a77852a617e0",
      "venue": "Journal of Artificial Intelligence &amp; Cloud Computing",
      "journal": {
        "name": "Journal of Artificial Intelligence &amp; Cloud Computing"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5932a504a1b53ea4eb33325a8e34a57b00921183",
      "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
      "authors": [
        {
          "name": "Mihir Parmar",
          "authorId": "1423660254"
        },
        {
          "name": "Nisarg Patel",
          "authorId": "2218094729"
        },
        {
          "name": "Neeraj Varshney",
          "authorId": "2067056655"
        },
        {
          "name": "Mutsumi Nakamura",
          "authorId": "2287764"
        },
        {
          "name": "Man Luo",
          "authorId": "145779426"
        },
        {
          "name": "Santosh Mashetty",
          "authorId": "2219861482"
        },
        {
          "name": "Arindam Mitra",
          "authorId": "2146720788"
        },
        {
          "name": "Chitta Baral",
          "authorId": "2064619864"
        }
      ],
      "year": 2024,
      "abstract": "Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really\"reason\"over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench.",
      "citationCount": 60,
      "doi": "10.18653/v1/2024.acl-long.739",
      "arxivId": "2404.15522",
      "url": "https://www.semanticscholar.org/paper/5932a504a1b53ea4eb33325a8e34a57b00921183",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "13679-13707"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "3868e87a24f671f8789b9ef2f788506126d4fd8c",
      "title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models",
      "authors": [
        {
          "name": "Chirag Agarwal",
          "authorId": "40228633"
        },
        {
          "name": "Sree Harsha Tanneru",
          "authorId": "2219860381"
        },
        {
          "name": "Himabindu Lakkaraju",
          "authorId": "1892673"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",
      "citationCount": 84,
      "doi": "10.48550/arXiv.2402.04614",
      "arxivId": "2402.04614",
      "url": "https://www.semanticscholar.org/paper/3868e87a24f671f8789b9ef2f788506126d4fd8c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.04614"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6f526899fcf5169626f68bbe94241cf36635dd7e",
      "title": "MedEthicEval: Evaluating Large Language Models Based on Chinese Medical Ethics",
      "authors": [
        {
          "name": "Haoan Jin",
          "authorId": "2266814680"
        },
        {
          "name": "Jiacheng Shi",
          "authorId": "2348564487"
        },
        {
          "name": "Hanhui Xu",
          "authorId": "2348411863"
        },
        {
          "name": "Ke Zhu",
          "authorId": "2151999086"
        },
        {
          "name": "Mengyue Wu",
          "authorId": "2266915353"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) demonstrate significant potential in advancing medical applications, yet their capabilities in addressing medical ethics challenges remain underexplored. This paper introduces MedEthicEval, a novel benchmark designed to systematically evaluate LLMs in the domain of medical ethics. Our framework encompasses two key components: knowledge, assessing the models' grasp of medical ethics principles, and application, focusing on their ability to apply these principles across diverse scenarios. To support this benchmark, we consulted with medical ethics researchers and developed three datasets addressing distinct ethical challenges: blatant violations of medical ethics, priority dilemmas with clear inclinations, and equilibrium dilemmas without obvious resolutions. MedEthicEval serves as a critical tool for understanding LLMs' ethical reasoning in healthcare, paving the way for their responsible and effective use in medical contexts.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2503.02374",
      "arxivId": "2503.02374",
      "url": "https://www.semanticscholar.org/paper/6f526899fcf5169626f68bbe94241cf36635dd7e",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.02374"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "4123282c3cb0491de0714a93ca3ef3e46dcee5c6",
      "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
      "authors": [
        {
          "name": "Kaiyu He",
          "authorId": "2316529381"
        },
        {
          "name": "Zhiyu Chen",
          "authorId": "2316509145"
        }
      ],
      "year": 2025,
      "abstract": "Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors''into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2505.21935",
      "arxivId": "2505.21935",
      "url": "https://www.semanticscholar.org/paper/4123282c3cb0491de0714a93ca3ef3e46dcee5c6",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.21935"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "70c4c6a5dc844d7919983776d3cbd00f41af3347",
      "title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts",
      "authors": [
        {
          "name": "Youcheng Huang",
          "authorId": "2156083891"
        },
        {
          "name": "Chen Huang",
          "authorId": "2270744273"
        },
        {
          "name": "Duanyu Feng",
          "authorId": "2280334389"
        },
        {
          "name": "Wenqiang Lei",
          "authorId": "2287107801"
        },
        {
          "name": "Jiancheng Lv",
          "authorId": "2274774166"
        }
      ],
      "year": 2025,
      "abstract": "Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2501.02009",
      "arxivId": "2501.02009",
      "url": "https://www.semanticscholar.org/paper/70c4c6a5dc844d7919983776d3cbd00f41af3347",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "3686-3704"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "6f9bb7c92ca99081dd656427eae7c9a2be5b25c8",
      "title": "Beyond the Turing Test: A Bergsonian Exploration of Proto-Consciousness in Large Language Models",
      "authors": [
        {
          "name": "Ani Thomas",
          "authorId": "2368697617"
        }
      ],
      "year": 2025,
      "abstract": "This paper offers a comprehensive exploration of the potential for proto-consciousness in Large Language Models (LLMs), drawing upon the philosophical insights of Henri Bergson. By examining the adaptive behavior, memory mechanisms, and emergent qualities of these Artificial Intelligence (AI) systems, I challenge traditional notions of consciousness and propose a new framework for understanding their potential for awareness. The paper delves into concepts such as proto-subjectiveness, proto-intentionality, and the role of dur\u00e9e in shaping LLM interactions. Through a rigorous analysis of counterarguments and refutations, I provide a nuanced understanding of the challenges and opportunities associated with the development of proto-conscious AI. This work aims to contribute to the ongoing philosophical and scientific discourse on the nature of consciousness and the future of AI.",
      "citationCount": 0,
      "doi": "10.1142/s2705078524500097",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6f9bb7c92ca99081dd656427eae7c9a2be5b25c8",
      "venue": "Journal of Artificial Intelligence and Consciousness",
      "journal": {
        "name": "J. Artif. Intell. Conscious.",
        "pages": "43-82",
        "volume": "12"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "292d1a563a4731e942f6ad68cad1b0aa5983dc0b",
      "title": "Modes of Cognition: Implications for Large Language Models",
      "authors": [
        {
          "name": "N. K. Hayles",
          "authorId": "1969743"
        }
      ],
      "year": 2025,
      "abstract": "In \u201cModes of Cognition: Implications for Large Language Models\u201d, N. Katherine Hayles proposes a broadened framework for understanding cognition, extending it beyond conscious human thought to include implicit, nonconscious, and even non-neural processes. Using the SIRAL criteria\u2014sensing, interpreting, responding, anticipating, and learning\u2014she explores cognition across organisms from bacteria to plants, arguing that many life forms meet the cognitive threshold despite lacking brains. Hayles then applies these insights to large language models (LLMs), contending that their behavior satisfies the modified SIRAL criteria. Although LLMs lack physical embodiment, they navigate and interpret vast conceptual environments, produce flexible and anticipatory responses, and learn from data. Their cognitive capacities, she argues, stem not from consciousness but from their place within a broader evolutionary lineage of technics. By reframing cognition and decentering anthropocentric models, Hayles opens pathways to new ethical frameworks for engaging with artificial and biological intelligences alike.",
      "citationCount": 0,
      "doi": "10.1162/anti.5czf",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/292d1a563a4731e942f6ad68cad1b0aa5983dc0b",
      "venue": "Antikythera Digital Journal",
      "journal": {
        "name": "Antikythera Digital Journal"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "feb4eb98451b07ef6fe72daccfd10acf735498f6",
      "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning",
      "authors": [
        {
          "name": "Jiahao Wang",
          "authorId": "2386382191"
        },
        {
          "name": "Songkai Xue",
          "authorId": "2386233935"
        },
        {
          "name": "Jinghui Li",
          "authorId": "2384317650"
        },
        {
          "name": "Xiaozhen Wang",
          "authorId": "2384159236"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.",
      "citationCount": 0,
      "doi": "10.1609/aies.v8i3.36744",
      "arxivId": "2511.00379",
      "url": "https://www.semanticscholar.org/paper/feb4eb98451b07ef6fe72daccfd10acf735498f6",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.00379"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "7d7956c01bb6ef46beff4ed0523cb083b4e29688",
      "title": "Large language models: Technology, intelligence, and thought",
      "authors": [
        {
          "name": "Zhidong Cao",
          "authorId": "2337250898"
        },
        {
          "name": "Xiangyu Zhang",
          "authorId": "2370072867"
        },
        {
          "name": "D. Zeng",
          "authorId": "2054124499"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/s42524-025-5004-3",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7d7956c01bb6ef46beff4ed0523cb083b4e29688",
      "venue": "Frontiers of Engineering Management",
      "journal": {
        "name": "Frontiers of Engineering Management",
        "pages": "710 - 715",
        "volume": "12"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6588cae1b7f4629d889e0a6405c4d328b5ff3c00",
      "title": "UCEPs and Second-Order Science in the Era of Large Language Models: New Dimensions for Reflexive Scientific Practice",
      "authors": [
        {
          "name": "Michael Lissack",
          "authorId": "2386688070"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s10699-025-10015-2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6588cae1b7f4629d889e0a6405c4d328b5ff3c00",
      "venue": "Foundations of Science",
      "journal": {
        "name": "Foundations of Science",
        "pages": "1145 - 1179",
        "volume": "30"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2d665874d4783d043728751c559e43689d6acdb5",
      "title": "Epistemology in the Age of Large Language Models",
      "authors": [
        {
          "name": "Jennifer Mugleston",
          "authorId": "2343973618"
        },
        {
          "name": "Vuong Hung Truong",
          "authorId": "2269311965"
        },
        {
          "name": "Cindy Kuang",
          "authorId": "2343980867"
        },
        {
          "name": "Lungile Sibiya",
          "authorId": "2343973346"
        },
        {
          "name": "J. Myung",
          "authorId": "37011102"
        }
      ],
      "year": 2025,
      "abstract": "Epistemology and technology have been working in synergy throughout history. This relationship has culminated in large language models (LLMs). LLMs are rapidly becoming integral parts of our daily lives through smartphones and personal computers, and we are coming to accept the functionality of LLMs as a given. As LLMs become more entrenched in societal functioning, questions have begun to emerge: Are LLMs capable of real understanding? What is knowledge in LLMs? Can knowledge exist independently of a conscious observer? While these questions cannot be answered definitively, we can argue that modern LLMs are more than mere symbol-manipulators and that LLMs in deep neural networks should be considered capable of a form of knowledge, though it may not qualify as justified true belief (JTB) in the traditional definition. This deep neural network design may have endowed LLMs with the capacity for internal representations, basic reasoning, and the performance of seemingly cognitive tasks, possible only through a compressive but generative form of representation that can be best termed as knowledge. In addition, the non-symbolic nature of LLMs renders them incompatible with the criticism posed by Searle\u2019s \u201cChinese room\u201d argument. These insights encourage us to revisit fundamental questions of epistemology in the age of LLMs, which we believe can advance the field.",
      "citationCount": 1,
      "doi": "10.3390/knowledge5010003",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2d665874d4783d043728751c559e43689d6acdb5",
      "venue": "Knowledge",
      "journal": {
        "name": "Knowledge"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1fa4be727067d2bc9d2cbedf8b37aa9514aeea02",
      "title": "\"Pull or Not to Pull?\": Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas",
      "authors": [
        {
          "name": "Junchen Ding",
          "authorId": "2316556701"
        },
        {
          "name": "Penghao Jiang",
          "authorId": "2375965239"
        },
        {
          "name": "Zihao Xu",
          "authorId": "2284948586"
        },
        {
          "name": "Ziqi Ding",
          "authorId": "2347332250"
        },
        {
          "name": "Yichen Zhu",
          "authorId": "2375853757"
        },
        {
          "name": "Jiaojiao Jiang",
          "authorId": "2375688747"
        },
        {
          "name": "Yuekang Li",
          "authorId": "2356257609"
        }
      ],
      "year": 2025,
      "abstract": "As large language models (LLMs) increasingly mediate ethically sensitive decisions, understanding their moral reasoning processes becomes imperative. This study presents a comprehensive empirical evaluation of 14 leading LLMs, both reasoning enabled and general purpose, across 27 diverse trolley problem scenarios, framed by ten moral philosophies, including utilitarianism, deontology, and altruism. Using a factorial prompting protocol, we elicited 3,780 binary decisions and natural language justifications, enabling analysis along axes of decisional assertiveness, explanation answer consistency, public moral alignment, and sensitivity to ethically irrelevant cues. Our findings reveal significant variability across ethical frames and model types: reasoning enhanced models demonstrate greater decisiveness and structured justifications, yet do not always align better with human consensus. Notably,\"sweet zones\"emerge in altruistic, fairness, and virtue ethics framings, where models achieve a balance of high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments. However, models diverge under frames emphasizing kinship, legality, or self interest, often producing ethically controversial outcomes. These patterns suggest that moral prompting is not only a behavioral modifier but also a diagnostic tool for uncovering latent alignment philosophies across providers. We advocate for moral reasoning to become a primary axis in LLM alignment, calling for standardized benchmarks that evaluate not just what LLMs decide, but how and why.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2508.07284",
      "arxivId": "2508.07284",
      "url": "https://www.semanticscholar.org/paper/1fa4be727067d2bc9d2cbedf8b37aa9514aeea02",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.07284"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
