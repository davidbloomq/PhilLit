{
  "status": "success",
  "source": "semantic_scholar",
  "query": "Chalmers language models understanding",
  "results": [
    {
      "paperId": "7802eaf427157ebc51dc6d6e2862ed4144892159",
      "title": "On the issue of the integrity of artificial intelligence",
      "authors": [
        {
          "name": "I. P. Polyakova",
          "authorId": "2383378579"
        },
        {
          "name": "Anton S. Sysoev",
          "authorId": "2383391387"
        }
      ],
      "year": 2025,
      "abstract": "The article explores the integrity of artificial intelligence (AI) from philosophical and interdisciplinary perspectives. The authors present AI as an independent branch of philosophical knowledge, outlining its subject, methods, and key issues. Special attention is paid to whether AI can possess integrity akin to human consciousness. The philosophical concepts of Aristotle, Kant, and Hegel are analyzed, alongside contemporary views from Dennett, Searle, and Chalmers, who emphasize the uniqueness of human consciousness. The Turing Test is presented as a practical criterion for evaluating machine \u00abthinking,\u00bb though the question of AI consciousness remains unresolved. Integrity is interpreted as an emergent property of complex systems, where interactions between elements create new qualities. For AI, integrity is framed through technical reliability, ethical consistency, and safety, rather than consciousness. Examples from large language models (LLMs) demonstrate emergent properties, such as context understanding or multilingual translation, arising from complex interactions of parameters and data. The authors conclude that current AI lacks integrity in the philosophical sense, but its development necessitates a systemic approach for design and regulation. The article highlights philosophy's role as a \u00abmeta-discipline\u00bb that unites disparate aspects of AI research.",
      "citationCount": 0,
      "doi": "10.53015/3034-3151_2025_6_3_43-49",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7802eaf427157ebc51dc6d6e2862ed4144892159",
      "venue": "\u0427\u0435\u043b\u043e\u0432\u0435\u043a. \u041e\u0431\u0449\u0435\u0441\u0442\u0432\u043e. \u041d\u0430\u0443\u043a\u0430",
      "journal": {
        "name": "\u0427\u0435\u043b\u043e\u0432\u0435\u043a \u041e\u0431\u0449\u0435\u0441\u0442\u0432\u043e \u041d\u0430\u0443\u043a\u0430"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "76a1640fa11c8f08fc6038e7e85f9c8bf97cf114",
      "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier",
      "authors": [
        {
          "name": "D. Nissani",
          "authorId": "1981198"
        }
      ],
      "year": 2025,
      "abstract": "A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2505.00654",
      "arxivId": "2505.00654",
      "url": "https://www.semanticscholar.org/paper/76a1640fa11c8f08fc6038e7e85f9c8bf97cf114",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.00654"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f8467b8a6c4a8aa65f7ef1c478505236b238a3cd",
      "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models",
      "authors": [
        {
          "name": "Giuseppe Russo",
          "authorId": "2057794866"
        },
        {
          "name": "Debora Nozza",
          "authorId": "2101317501"
        },
        {
          "name": "Paul R\u00f6ttger",
          "authorId": "2043232919"
        },
        {
          "name": "Dirk Hovy",
          "authorId": "2267334203"
        }
      ],
      "year": 2025,
      "abstract": "People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans'decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, we introduce the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. We treat this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. We find that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, we show that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2507.17216",
      "arxivId": "2507.17216",
      "url": "https://www.semanticscholar.org/paper/f8467b8a6c4a8aa65f7ef1c478505236b238a3cd",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.17216"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bcbb02a484ca501dcedd8bc6be0c56cf69e42b2a",
      "title": "Artificial Intelligence and the Illusion of Understanding: A Systematic Review of Theory of Mind and Large Language Models",
      "authors": [
        {
          "name": "Antonella Marchetti",
          "authorId": "2312398690"
        },
        {
          "name": "F. Manzi",
          "authorId": "31636934"
        },
        {
          "name": "Giuseppe Riva",
          "authorId": "2279720491"
        },
        {
          "name": "A. Gaggioli",
          "authorId": "1700503"
        },
        {
          "name": "D. Massaro",
          "authorId": "4623295"
        }
      ],
      "year": 2025,
      "abstract": "The development of Large Language Models (LLMs) has sparked significant debate regarding their capacity for Theory of Mind (ToM)\u2014the ability to attribute mental states to oneself and others. This systematic review examines the extent to which LLMs exhibit Artificial ToM (AToM) by evaluating their performance on ToM tasks and comparing it with human responses. While LLMs, particularly GPT-4, perform well on first-order false belief tasks, they struggle with more complex reasoning, such as second-order beliefs and recursive inferences, where humans consistently outperform them. Moreover, the review underscores the variability in ToM assessments, as many studies adapt classical tasks for LLMs, raising concerns about comparability with human ToM. Most evaluations remain constrained to text-based tasks, overlooking embodied and multimodal dimensions crucial to human social cognition. This review discusses the \u201cillusion of understanding\u201d in LLMs for two primary reasons: First, their lack of the developmental and cognitive mechanisms necessary for genuine ToM, and second, methodological biases in test designs that favor LLMs\u2019 strengths, limiting direct comparisons with human performance. The findings highlight the need for more ecologically valid assessments and interdisciplinary research to better delineate the limitations and potential of AToM. This set of issues is highly relevant to psychology, as language is generally considered just one component in the broader development of human ToM, a perspective that contrasts with the dominant approach in AToM studies. This discrepancy raises critical questions about the extent to which human ToM and AToM are comparable.",
      "citationCount": 3,
      "doi": "10.1089/cyber.2024.0536",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/bcbb02a484ca501dcedd8bc6be0c56cf69e42b2a",
      "venue": "Cyberpsychology, Behavior, and Social Networking",
      "journal": {
        "name": "Cyberpsychology, Behavior, and Social Networking",
        "pages": "505 - 514",
        "volume": "28"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "35c0738762574b1bcbc2c9a46e78f07f70772eb2",
      "title": "Mechanistic Indicators of Understanding in Large Language Models",
      "authors": [
        {
          "name": "Pierre Beckmann",
          "authorId": "2371998450"
        },
        {
          "name": "Matthieu Queloz",
          "authorId": "2372000852"
        }
      ],
      "year": 2025,
      "abstract": "Recent findings in mechanistic interpretability (MI), the field probing the inner workings of Large Language Models (LLMs), challenge the view that these models rely solely on superficial statistics. We offer an accessible synthesis of these findings that doubles as an introduction to MI while integrating these findings within a novel theoretical framework for thinking about machine understanding. We argue that LLMs develop internal structures that are functionally analogous to the kind of understanding that consists in seeing connections. To sharpen this idea, we propose a three-tiered conception of understanding. First, conceptual understanding emerges when a model forms\"features\"as directions in latent space, learning the connections between diverse manifestations of something. Second, state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world. Third, principled understanding emerges when a model ceases to rely on a collection of memorized facts and discovers a\"circuit\"connecting these facts. However, these forms of understanding remain radically different from human understanding, as the phenomenon of\"parallel mechanisms\"shows. We conclude that the debate should move beyond the yes-or-no question of whether LLMs understand to investigate how their strange minds work and forge conceptions that fit them.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2507.08017",
      "arxivId": "2507.08017",
      "url": "https://www.semanticscholar.org/paper/35c0738762574b1bcbc2c9a46e78f07f70772eb2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.08017"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "23b78d6e063982f1d110b6365601ebfc00304059",
      "title": "Understanding Conversational AI: Philosophy, Ethics, and Social Impact of Large Language Models",
      "authors": [
        {
          "name": "Thierry Poibeau",
          "authorId": "2392718362"
        }
      ],
      "year": 2025,
      "abstract": "\n \n What do large language models really know\u2014and what does it mean to live alongside them?\n \n \n This book offers a critical and interdisciplinary exploration of large language models (LLMs), examining how they reshape our understanding of language, cognition, and society. Drawing on philosophy of language, linguistics, cognitive science, and AI ethics, it investigates how these models generate meaning, simulate reasoning, and perform tasks that once seemed uniquely human\u2014from translation to moral judgment and literary creation.\n Rather than offering a purely technical account, the book interrogates the epistemic, ethical, and political dimensions of LLMs. It explores their limitations, their embedded biases, and their role in processes of automation, misinformation, and platform enclosure. At the same time, it reflects on how LLMs prompt us to revisit fundamental questions: What is understanding? What is creativity? How do we ascribe agency or trust in a world of synthetic language?\n Written for scholars, students, and curious readers across the humanities, social sciences, and computer science, this is both a philosophical inquiry and a practical guide to navigating the era of generative AI. It invites readers to think critically about the promises and perils of language technologies\u2014and about the kind of future we are shaping with them.",
      "citationCount": 0,
      "doi": "10.5334/bde",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/23b78d6e063982f1d110b6365601ebfc00304059",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "a54e0011f478d476f4582ce1234d56d1974729fe",
      "title": "Understanding Large Language Models' Ability on Interdisciplinary Research",
      "authors": [
        {
          "name": "Yuanhao Shen",
          "authorId": "2329746837"
        },
        {
          "name": "Daniel Xavier de Sousa",
          "authorId": "2373288346"
        },
        {
          "name": "Ricardo Mar\u00e7al",
          "authorId": "2376131172"
        },
        {
          "name": "A. Asad",
          "authorId": "2367042486"
        },
        {
          "name": "Hongyu Guo",
          "authorId": "2278393469"
        },
        {
          "name": "Xiaodan Zhu",
          "authorId": "2329740584"
        }
      ],
      "year": 2025,
      "abstract": "Recent advancements in Large Language Models (LLMs) have revealed their impressive ability to perform multi-step, logic-driven reasoning across complex domains, positioning them as powerful tools and collaborators in scientific discovery while challenging the long-held view that inspiration-driven ideation is uniquely human. However, the lack of a dedicated benchmark that evaluates LLMs'ability to develop ideas in Interdisciplinary Research (IDR) settings poses a critical barrier to fully understanding their strengths and limitations. To address this gap, we introduce IDRBench -- a pioneering benchmark featuring an expert annotated dataset and a suite of tasks tailored to evaluate LLMs'capabilities in proposing valuable research ideas from different scientific domains for interdisciplinary research. This benchmark aims to provide a systematic framework for assessing LLM performance in complex, cross-domain scientific research. Our dataset consists of scientific publications sourced from the ArXiv platform covering six distinct disciplines, and is annotated by domain experts with diverse academic backgrounds. To ensure high-quality annotations, we emphasize clearly defined dimensions that characterize authentic interdisciplinary research. The design of evaluation tasks in IDRBench follows a progressive, real-world perspective, reflecting the natural stages of interdisciplinary research development, including 1) IDR Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation. Using IDRBench, we construct baselines across 10 LLMs and observe that despite fostering some level of IDR awareness, LLMs still struggle to produce quality IDR ideas. These findings could not only spark new research directions, but also help to develop next-generation LLMs that excel in interdisciplinary research.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.15736",
      "arxivId": "2507.15736",
      "url": "https://www.semanticscholar.org/paper/a54e0011f478d476f4582ce1234d56d1974729fe",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.15736"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8b8fe1fc0d88dc28d75ec0a00375a3d0c3dc658a",
      "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language Models",
      "authors": [
        {
          "name": "Rikard Rosenbacke",
          "authorId": "2287320882"
        },
        {
          "name": "Carl Rosenbacke",
          "authorId": "2386015108"
        },
        {
          "name": "Victor Rosenbacke",
          "authorId": "2386016228"
        },
        {
          "name": "Martin McKee",
          "authorId": "2386015037"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) are becoming deeply embedded in human communication and decision-making, yet they inherit the ambiguity, bias, and lack of direct access to truth inherent in language itself. While their outputs are fluent, emotionally resonant, and coherent, they are generated through statistical prediction rather than grounded reasoning. This creates the risk of hallucination, responses that sound convincing but lack factual validity. Building on Geoffrey Hinton's observation that AI mirrors human intuition rather than reasoning, this paper argues that LLMs operationalize System 1 cognition at scale: fast, associative, and persuasive, but without reflection or falsification. To address this, we introduce the Rose-Frame, a three-dimensional framework for diagnosing cognitive and epistemic drift in human-AI interaction. The three axes are: (i) Map vs. Territory, which distinguishes representations of reality (epistemology) from reality itself (ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to separate fast, emotional judgments from slow, reflective thinking; and (iii) Conflict vs. Confirmation, which examines whether ideas are critically tested through disagreement or simply reinforced through mutual validation. Each dimension captures a distinct failure mode, and their combination amplifies misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules. Instead, it offers a reflective tool that makes both the model's limitations and the user's assumptions visible, enabling more transparent and critically aware AI deployment. It reframes alignment as cognitive governance: intuition, whether human or artificial, must remain governed by human reason. Only by embedding reflective, falsifiable oversight can we align machine fluency with human understanding.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.14665",
      "arxivId": "2510.14665",
      "url": "https://www.semanticscholar.org/paper/8b8fe1fc0d88dc28d75ec0a00375a3d0c3dc658a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.14665"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2166983b9a1ea298f2509551e27c5c28d5a63393",
      "title": "Conscience conflict? Evaluating language models' moral understanding",
      "authors": [
        {
          "name": "Asutosh Hota",
          "authorId": "71753843"
        },
        {
          "name": "Jussi P. P. Jokinen",
          "authorId": "2375136788"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2166983b9a1ea298f2509551e27c5c28d5a63393",
      "venue": "Modern Machine Learning Technologies",
      "journal": {
        "pages": "25-38"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ae0602db5ef14c326c807ae014661f9edd19b891",
      "title": "THE PROBLEM OF UNDERSTANDING HUMAN IDEAS BY ARTIFICIAL INTELLIGENCE AND SPECIFICALLY LARGE LANGUAGE MODELS: A PHILOSOPHICAL ANALYSIS OF POSSIBILITIES AND LIMITATIONS",
      "authors": [
        {
          "name": "Mykyta Skrypchenko",
          "authorId": "2347121081"
        },
        {
          "name": "V. Shtanko",
          "authorId": "2279153965"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.36074/logos-24.01.2025.066",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ae0602db5ef14c326c807ae014661f9edd19b891",
      "venue": "THEORETICAL AND PRACTICAL ASPECTS OF MODERN SCIENTIFIC RESEARCH",
      "journal": {
        "name": "THEORETICAL AND PRACTICAL ASPECTS OF MODERN SCIENTIFIC RESEARCH"
      },
      "publicationTypes": null
    },
    {
      "paperId": "399bb1d8aa0fcb06cbc21b93a8d3e930a9cc0e78",
      "title": "On the Understanding of Large Language Models: Based on Wittgenstein\u2019s Later Philosophy",
      "authors": [
        {
          "name": "\u5f66\u98de \u674e",
          "authorId": "2388412282"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.12677/acpp.2025.1410525",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/399bb1d8aa0fcb06cbc21b93a8d3e930a9cc0e78",
      "venue": "Advances in Philosophy",
      "journal": {
        "name": "Advances in Philosophy"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d1ed32ea2f3e11b739218b9c5dd4ceecfe90aa0f",
      "title": "Does Thought Require Sensory Grounding? From Pure Thinkers to Large Language Models",
      "authors": [
        {
          "name": "David J. Chalmers",
          "authorId": "2244742161"
        }
      ],
      "year": 2024,
      "abstract": "Does the capacity to think require the capacity to sense? A lively debate on this topic runs throughout the history of philosophy and now animates discussions of artificial intelligence. I argue that in principle, there can be pure thinkers: thinkers that lack the capacity to sense altogether. I also argue for significant limitations in just what sort of thought is possible in the absence of the capacity to sense. Regarding AI, I do not argue directly that large language models can think or understand, but I rebut one important argument (the argument from sensory grounding) that they cannot. I also use recent results regarding language models to address the question of whether or how sensory grounding enhances cognitive capacities.",
      "citationCount": 19,
      "doi": "10.48550/arXiv.2408.09605",
      "arxivId": "2408.09605",
      "url": "https://www.semanticscholar.org/paper/d1ed32ea2f3e11b739218b9c5dd4ceecfe90aa0f",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2408.09605"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8e4b8bc4a9c67da607756364bdc5a77852a617e0",
      "title": "Towards Responsible AI: Understanding and Mitigating Ethical Concerns of Large Language Models",
      "authors": [
        {
          "name": "Akshata Upadhye",
          "authorId": "2267973563"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools in the field of natural language processing and have transformed the way we interact with text data and generate textual content.",
      "citationCount": 0,
      "doi": "10.47363/jaicc/2024(3)289",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8e4b8bc4a9c67da607756364bdc5a77852a617e0",
      "venue": "Journal of Artificial Intelligence &amp; Cloud Computing",
      "journal": {
        "name": "Journal of Artificial Intelligence &amp; Cloud Computing"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2e3c5e48083ae5a0cdd268da97e909e1fd754d3f",
      "title": "\"Understanding AI\": Semantic Grounding in Large Language Models",
      "authors": [
        {
          "name": "Holger Lyre",
          "authorId": "2284679884"
        }
      ],
      "year": 2024,
      "abstract": "Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2402.10992",
      "arxivId": "2402.10992",
      "url": "https://www.semanticscholar.org/paper/2e3c5e48083ae5a0cdd268da97e909e1fd754d3f",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.10992"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2e11ace144dd72d5f30303b96eb3a1ac61c4bd5d",
      "title": "Meaning and understanding in large language models",
      "authors": [
        {
          "name": "Vladim'ir Havl'ik",
          "authorId": "2261737538"
        }
      ],
      "year": 2023,
      "abstract": "Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. This article critically evaluates the prevailing tendency to regard machine language performance as mere syntactic manipulation and the imitations of understanding, which is only partial and very shallow, without sufficient grounding in the world. The article analyses the views on possible ways of grounding as a condition for successful understanding in LLMs and offers an alternative way in view of the prevailing belief that the success of understanding depends mainly on the referential grounding. An alternative conception seeks to show that semantic fragmentism offers a viable account of natural language understanding and explains how LLMs ground the meanings of linguistic expressions. Uncovering how meanings are grounded allows us to also explain why LLMs\u2019 ability to understand is possible and so remarkably successful.",
      "citationCount": 10,
      "doi": "10.1007/s11229-024-04878-4",
      "arxivId": "2310.17407",
      "url": "https://www.semanticscholar.org/paper/2e11ace144dd72d5f30303b96eb3a1ac61c4bd5d",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "205"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e9ab1dfe79c16ba8311995ac3596d77525d85d4c",
      "title": "A Loosely Wittgensteinian Conception of the Linguistic Understanding of Large Language Models like BERT, GPT-3, and ChatGPT",
      "authors": [
        {
          "name": "Reto Gubelmann",
          "authorId": "119338633"
        }
      ],
      "year": 2023,
      "abstract": "\nIn this article, I develop a loosely Wittgensteinian conception of what it takes for a being, including an AI system, to understand language, and I suggest that current state of the art systems are closer to fulfilling these requirements than one might think. Developing and defending this claim has both empirical and conceptual aspects. The conceptual aspects concern the criteria that are reasonably applied when judging whether some being understands language; the empirical aspects concern the question whether a given being fulfills these criteria. On the conceptual side, the article builds on Glock\u2019s concept of intelligence, Taylor\u2019s conception of intrinsic rightness as well as Wittgenstein\u2019s rule-following considerations. On the empirical side, it is argued that current transformer-based NNLP models, such as BERT and GPT-3 come close to fulfilling these criteria.",
      "citationCount": 12,
      "doi": "10.1163/18756735-00000182",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e9ab1dfe79c16ba8311995ac3596d77525d85d4c",
      "venue": "Grazer Philosophische Studien",
      "journal": {
        "name": "Grazer Philosophische Studien"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c0993b9ee7288e2976600cc1520952fb2d3dacad",
      "title": "Understanding Large Language Models through the Lens of Artificial Agency",
      "authors": [
        {
          "name": "Maud van Lier",
          "authorId": "2239130884"
        }
      ],
      "year": 2023,
      "abstract": "This paper is motivated by Floridi\u2019s recent claim that Large Language Models like ChatGPT can be seen as \u2018intelligence-free\u2019 agents. Where I do not agree with Floridi that such systems are intelligence-free, my paper does question whether they can be called agents, and if so, what kind. I argue for the adoption of a more restricted understanding of agent in AI-research, one that comes closer in its meaning to how the term is used in the philosophies of mind, action, and agency. I propose such a more narrowing understanding of agent, suggesting that an agent can be seen as entity or system that things can be \u2018up to\u2019, that can act autonomously in a way that is best understood on the basis of Husserl\u2019s notion of indeterminate determinability.",
      "citationCount": 4,
      "doi": "10.3384/ecp199008",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c0993b9ee7288e2976600cc1520952fb2d3dacad",
      "venue": "Annual Workshop of the Swedish Artificial Intelligence Society",
      "journal": {
        "pages": "79-84"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5932a504a1b53ea4eb33325a8e34a57b00921183",
      "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
      "authors": [
        {
          "name": "Mihir Parmar",
          "authorId": "1423660254"
        },
        {
          "name": "Nisarg Patel",
          "authorId": "2218094729"
        },
        {
          "name": "Neeraj Varshney",
          "authorId": "2067056655"
        },
        {
          "name": "Mutsumi Nakamura",
          "authorId": "2287764"
        },
        {
          "name": "Man Luo",
          "authorId": "145779426"
        },
        {
          "name": "Santosh Mashetty",
          "authorId": "2219861482"
        },
        {
          "name": "Arindam Mitra",
          "authorId": "2146720788"
        },
        {
          "name": "Chitta Baral",
          "authorId": "2064619864"
        }
      ],
      "year": 2024,
      "abstract": "Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really\"reason\"over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench.",
      "citationCount": 60,
      "doi": "10.18653/v1/2024.acl-long.739",
      "arxivId": "2404.15522",
      "url": "https://www.semanticscholar.org/paper/5932a504a1b53ea4eb33325a8e34a57b00921183",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "13679-13707"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "6dcd668c9f8d7170a1dd5846877081c53335216c",
      "title": "Consciousness and Intelligence in the Age of Large Language Models",
      "authors": [
        {
          "name": "Adriano Martins-Costa",
          "authorId": "2401339513"
        },
        {
          "name": "Fabiana Prazeres-Martins",
          "authorId": "2401337925"
        }
      ],
      "year": 2025,
      "abstract": "This paper investigates the philosophical implications of Large Language Models (LLMs), such as ChatGPT, in light of classical and contemporary theories of mind and consciousness. The study revisits key philosophical debates to evaluate the plausibility of machine consciousness. Based on recent discussions, including Chalmers' 2023 analysis, the paper argues that LLMs may not yet exhibit consciousness, but their development demands new conceptual frameworks. In conclusion, it suggests that traditional notions of mind may be insufficient to grasp the cognitive nature of emerging technologies and that a new ontology of the mind might be necessary.",
      "citationCount": 0,
      "doi": "10.5354/0719-790x.2025.80188",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6dcd668c9f8d7170a1dd5846877081c53335216c",
      "venue": "Resonancias. Revista de filosof\u00eda",
      "journal": {
        "name": "Resonancias. Revista de Filosof\u00eda"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "3868e87a24f671f8789b9ef2f788506126d4fd8c",
      "title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models",
      "authors": [
        {
          "name": "Chirag Agarwal",
          "authorId": "40228633"
        },
        {
          "name": "Sree Harsha Tanneru",
          "authorId": "2219860381"
        },
        {
          "name": "Himabindu Lakkaraju",
          "authorId": "1892673"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",
      "citationCount": 84,
      "doi": "10.48550/arXiv.2402.04614",
      "arxivId": "2402.04614",
      "url": "https://www.semanticscholar.org/paper/3868e87a24f671f8789b9ef2f788506126d4fd8c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.04614"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 20,
  "errors": []
}
