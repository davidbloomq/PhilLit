@comment{
====================================================================
DOMAIN: Technical Mechanistic Interpretability Methods
SEARCH_DATE: 2025-12-28
PAPERS_FOUND: 18 (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: Semantic Scholar, arXiv, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
Mechanistic interpretability (MI) has emerged as a distinct research program
within explainable AI, focusing on reverse-engineering the specific computational
algorithms learned by neural networks rather than providing post-hoc explanations.
The field coalesced around 2020 with the Circuits thread from Distill/Anthropic,
which introduced the concept of identifying minimal computational subgraphs
(circuits) in vision models. Since then, the field has expanded dramatically to
language models, developing key technical methods including: (1) Sparse Autoencoders
(SAEs) for disentangling polysemantic neurons into monosemantic features
(Cunningham et al. 2023), (2) activation patching for causal intervention analysis
(Zhang et al. 2023, Heimersheim & Nanda 2024), (3) automated circuit discovery
algorithms like ACDC (Conmy et al. 2023), and (4) analysis of specific mechanisms
like induction heads that enable in-context learning (Elhage et al. 2021, Nanda
et al. 2023). Recent work (2024-2025) has focused on scaling these methods to
larger models (Gemma Scope, Llama Scope), developing better evaluation frameworks
(SAEBench), and understanding the limitations of current techniques (circuit
faithfulness, feature splitting). A major 2024 review (Bereska & Gavves)
synthesizes the state of the field and its connections to AI safety.

RELEVANCE_TO_PROJECT:
This domain is central to the research project as it provides the concrete
technical practices that constitute "mechanistic interpretability" in contemporary
ML research. Understanding these methods is essential for evaluating philosophical
claims about mechanisms, explanation, and understanding in AI systems. The
domain reveals how practitioners operationalize concepts like "mechanism" and
"circuit" - which may differ from traditional philosophy of science definitions.

NOTABLE_GAPS:
Limited theoretical foundations for why these methods work; most evaluation
is empirical rather than grounded in formal learning theory. Unclear how methods
scale beyond current transformer architectures (e.g., to diffusion models, SSMs).
Little cross-pollination with traditional philosophy of science literature on
mechanistic explanation (Craver, Bechtel).

SYNTHESIS_GUIDANCE:
Organize by method type (SAEs, patching, circuit discovery) rather than
chronologically. Highlight the evolution from vision models (Circuits 2020)
to language models (2021-present). Emphasize methodological debates about
evaluation, faithfulness, and what counts as genuine mechanistic understanding.
Link technical methods to broader epistemological questions about explanation.

KEY_POSITIONS:
- SAE Methods: 6 papers - Sparse autoencoders as primary tool for feature disentanglement
- Activation Patching: 5 papers - Causal intervention for circuit identification
- Circuit Discovery: 4 papers - Automated methods for finding computational subgraphs
- Review/Survey: 3 papers - Comprehensive overviews of MI landscape and methods
====================================================================
}

@article{olah2020zoom,
  author = {Olah, Christopher and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  volume = {5},
  number = {3},
  doi = {10.23915/distill.00024.001},
  url = {https://distill.pub/2020/circuits/zoom-in/},
  note = {
  CORE ARGUMENT: Introduces the "circuits" framework for mechanistic interpretability in vision models (InceptionV1), arguing that neural networks learn interpretable algorithms composed of specific features (neurons that respond to meaningful concepts) connected by weighted edges (weights between neurons). The paper demonstrates this by reverse-engineering specific circuits like curve detectors and high-low frequency detectors, showing how simple features combine into more complex computations through systematic analysis of weights and activations.

  RELEVANCE: This is the foundational paper that established mechanistic interpretability as a distinct research program. It provides the conceptual framework (circuits as interpretable algorithms) that subsequent MI work builds upon. Critical for understanding how MI practitioners operationalize "mechanism" - not as abstract causal models but as specific computational subgraphs with identifiable features and connections. The vision-model focus (pre-transformer) shows the historical development of MI methods before their application to LLMs.

  POSITION: Foundational circuits framework; establishes MI as reverse-engineering learned algorithms rather than post-hoc explanation. Influential for defining what practitioners mean by "mechanistic."
  },
  keywords = {circuits, mechanistic-interpretability, vision-models, foundational, High}
}

@article{cammarata2020thread,
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Christopher and Petrov, Michael and Schubert, Ludwig},
  title = {Thread: Circuits},
  journal = {Distill},
  year = {2020},
  doi = {10.23915/distill.00024},
  url = {https://distill.pub/2020/circuits/},
  note = {
  CORE ARGUMENT: Presents the broader "Circuits Thread" research program, arguing that neural networks can be understood as collections of small computational circuits that implement interpretable algorithms. Expands beyond individual circuit analysis to propose three speculative claims: (1) features are the fundamental unit of neural networks, (2) circuits are meaningful algorithmic components, and (3) universality - similar architectures learn similar features and circuits. Demonstrates this across multiple vision tasks.

  RELEVANCE: Establishes the theoretical framework and research program that underpins contemporary MI work. The three speculative claims (features, circuits, universality) frame ongoing debates about whether MI methods genuinely capture how networks work or merely provide useful but incomplete abstractions. The universality hypothesis is particularly relevant for questions about whether MI insights generalize across models and architectures - critical for evaluating MI's scientific status.

  POSITION: Programmatic statement of circuits research paradigm; sets agenda for MI as scientific investigation of learned algorithms. Introduces key conceptual commitments about features, circuits, and universality.
  },
  keywords = {circuits, research-program, universality-hypothesis, foundational, High}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety - A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  doi = {10.48550/arXiv.2404.14082},
  arxivId = {2404.14082},
  url = {https://arxiv.org/abs/2404.14082},
  note = {
  CORE ARGUMENT: Provides comprehensive review of mechanistic interpretability as a research program focused on reverse-engineering neural networks into human-understandable algorithms and concepts. Argues that MI offers a causal, granular understanding of model internals that is essential for AI safety - enabling better understanding, control, and alignment. Surveys key concepts (features, circuits, superposition), methodologies (activation patching, circuit discovery, SAEs), and challenges (scalability, automation). Also examines risks including capability gains and dual-use concerns.

  RELEVANCE: This is the most comprehensive recent survey of MI methods and their connection to AI safety. Essential for understanding the state-of-the-art as of 2024 and the field's self-conception. The paper explicitly connects MI technical methods to safety concerns, making it critical for evaluating whether MI delivers on its promise of enabling safer AI systems. Identifies key open questions about scalability and theoretical foundations that are relevant to assessing MI's epistemic status.

  POSITION: Comprehensive 2024 review synthesizing MI landscape; connects technical methods to AI safety goals; identifies scalability and theoretical foundations as major challenges.
  },
  keywords = {mechanistic-interpretability, review, AI-safety, 2024-survey, High}
}

@inproceedings{nanda2023progress,
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  title = {Progress measures for grokking via mechanistic interpretability},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2301.05217},
  arxivId = {2301.05217},
  url = {https://arxiv.org/abs/2301.05217},
  note = {
  CORE ARGUMENT: Demonstrates how mechanistic interpretability can provide continuous progress measures to explain seemingly discontinuous emergent behaviors (grokking - sudden generalization long after training loss plateaus). Fully reverse-engineers the algorithm learned by transformers on modular addition: the network uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. By analyzing weights, activations, and performing ablations in Fourier space, they identify three continuous training phases (memorization, circuit formation, cleanup) underlying the discrete grokking transition.

  RELEVANCE: Exemplar of successful mechanistic interpretability achieving deep algorithmic understanding. Shows MI methods can reveal specific learned algorithms (DFT-based computation) and explain training dynamics. Methodologically important for demonstrating how circuit analysis, activation analysis, and ablation studies combine to validate mechanistic hypotheses. The Fourier-space ablations show creativity in adapting MI methods to the specific computational structure discovered. Raises questions about whether this level of understanding is achievable for more complex, real-world tasks.

  POSITION: Case study demonstrating MI's power to reveal specific algorithms and explain emergence; validates circuit analysis through multiple convergent methods (weights, activations, ablations).
  },
  keywords = {grokking, circuit-discovery, transformers, algorithmic-understanding, High}
}

@inproceedings{cunningham2023sparse,
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs Smith, Logan and Huben, Robert and Sharkey, Lee},
  title = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2309.08600},
  arxivId = {2309.08600},
  url = {https://arxiv.org/abs/2309.08600},
  note = {
  CORE ARGUMENT: Introduces sparse autoencoders (SAEs) as a scalable, unsupervised method for resolving superposition in language models by learning monosemantic features from polysemantic neurons. SAEs decompose neural activations into sparse, interpretable directions that correspond to individual concepts rather than neuron-level representations. The learned features are more interpretable than alternative methods (measured by automated evaluation) and enable finer-grained causal analysis of model behavior on tasks like indirect object identification.

  RELEVANCE: Foundational paper establishing SAEs as the dominant method for feature disentanglement in modern MI work. Addresses the core technical challenge of polysemanticity (neurons activating for multiple unrelated concepts) which prevents neuron-level interpretability. The superposition hypothesis - that networks represent more features than neurons by using overcomplete bases - is a key theoretical motivation. This paper's success catalyzed extensive follow-up work (Gemma Scope, Llama Scope, JumpReLU SAEs) making it essential for understanding contemporary MI methods.

  POSITION: Introduces SAEs as solution to polysemanticity/superposition problem; establishes unsupervised feature learning as viable MI approach; spawns major research direction in 2024-2025.
  },
  keywords = {sparse-autoencoders, polysemanticity, superposition, language-models, foundational, High}
}

@article{rajamanoharan2024jumping,
  author = {Rajamanoharan, Senthooran and Lieberum, Tom and Sonnerat, Nicolas and Conmy, Arthur and Varma, Vikrant and Kram\'{a}r, J\'{a}nos and Nanda, Neel},
  title = {Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2407.14435},
  arxivId = {2407.14435},
  url = {https://arxiv.org/abs/2407.14435},
  note = {
  CORE ARGUMENT: Proposes JumpReLU SAEs as an architectural improvement over standard ReLU SAEs, achieving state-of-the-art reconstruction fidelity at given sparsity levels on Gemma 2 9B activations. The key innovation is replacing ReLU with discontinuous JumpReLU activation and using straight-through estimators (STEs) to enable training despite the discontinuity. This allows direct optimization of L0 sparsity rather than L1 proxies, avoiding shrinkage problems. Validates that improved fidelity doesn't sacrifice interpretability through manual and automated analysis.

  RELEVANCE: Represents the state-of-the-art in SAE methods as of 2024, showing continuing technical refinement of MI tools. The tension between reconstruction fidelity and sparsity is a fundamental tradeoff in SAE design - high fidelity means features accurately represent network computation, while sparsity enables interpretability. JumpReLU's improvement on this tradeoff suggests SAE methods are still advancing rapidly. The interpretability validation (manual and automated) shows awareness of concerns that better metrics might come at cost of genuine understanding.

  POSITION: State-of-the-art SAE architecture (2024); demonstrates continuing improvement in core MI methods; balances reconstruction fidelity with interpretability.
  },
  keywords = {sparse-autoencoders, JumpReLU, reconstruction-fidelity, SOTA-method, Medium}
}

@article{lieberum2024gemma,
  author = {Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kram\'{a}r, J\'{a}nos and Dragan, Anca and Shah, Rohin and Nanda, Neel},
  title = {Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2},
  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  year = {2024},
  doi = {10.48550/arXiv.2408.05147},
  arxivId = {2408.05147},
  url = {https://arxiv.org/abs/2408.05147},
  note = {
  CORE ARGUMENT: Releases comprehensive suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2 2B, 9B, and select layers of 27B models, addressing the high cost barrier to SAE training that limits research applications. Provides standard quality metrics for each SAE and comparison between base and instruction-tuned models. Makes weights, tutorial, and interactive demo publicly available to enable community MI research without requiring expensive SAE training.

  RELEVANCE: Major infrastructure contribution democratizing access to SAE tools. The comprehensiveness (all layers, multiple model sizes) enables systematic study of how feature representations vary across depth and scale. The open release philosophy reflects MI community norms around sharing interpretability tools. Comparison of base vs. instruction-tuned SAEs provides empirical data on how training procedures affect learned features - relevant for understanding what SAEs actually capture. The interactive demo (Neuronpedia) shows emphasis on making MI accessible.

  POSITION: Infrastructure release enabling broader MI research; demonstrates scale-up of SAE methods to billion-parameter models; open-source community resource.
  },
  keywords = {sparse-autoencoders, Gemma, infrastructure, open-source, Medium}
}

@article{he2024llama,
  author = {He, Zhengfu and Shu, Wentao and Ge, Xuyang and Chen, Lingjie and Wang, Junxuan and Zhou, Yunhua and Liu, Frances and Guo, Qipeng and Huang, Xuanjing and Wu, Zuxuan and Jiang, Yu-Gang and Qiu, Xipeng},
  title = {Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2410.20526},
  arxivId = {2410.20526},
  url = {https://arxiv.org/abs/2410.20526},
  note = {
  CORE ARGUMENT: Trains comprehensive suite of 256 SAEs on each layer and sublayer of Llama-3.1-8B-Base using Top-K SAE variant, with 32K and 128K features. Evaluates SAE modifications across multiple dimensions including generalization to longer contexts and fine-tuned models. Analyzes geometry of learned SAE latents, confirming "feature splitting" phenomenon where SAEs discover progressively more fine-grained features. Releases weights, training tools, and visualization interfaces to support community MI research.

  RELEVANCE: Parallel infrastructure effort to Gemma Scope, demonstrating convergence on comprehensive SAE releases as community norm. The analysis of generalization (base to fine-tuned, shorter to longer contexts) addresses key questions about SAE feature stability and transferability. Feature splitting analysis provides evidence for theoretical claims about SAE learning dynamics. The scalable training tools release (language-model-saes library) shows emphasis on reproducibility and reducing redundant compute. Comparison with Gemma Scope allows assessing consistency of SAE findings across model families.

  POSITION: Infrastructure release for Llama models; confirms feature splitting phenomenon; provides tools for SAE training at scale; complements Gemma Scope.
  },
  keywords = {sparse-autoencoders, Llama, feature-splitting, infrastructure, Medium}
}

@article{zhang2023towards,
  author = {Zhang, Fred and Nanda, Neel},
  title = {Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2309.16042},
  arxivId = {2309.16042},
  url = {https://arxiv.org/abs/2309.16042},
  note = {
  CORE ARGUMENT: Systematically examines methodological choices in activation patching (causal intervention technique for circuit localization) including evaluation metrics and corruption methods. Demonstrates that varying hyperparameters leads to substantially different interpretability results in localization and circuit discovery tasks. Provides conceptual arguments for why certain metrics (e.g., logit difference vs. cross-entropy) or corruption methods (random vs. zero ablation) may be preferred in different contexts. Offers evidence-backed recommendations for best practices going forward.

  RELEVANCE: Critical methodological paper revealing that activation patching, despite being a standard MI technique, has significant methodological degrees of freedom that affect results. The sensitivity to hyperparameters raises concerns about robustness and replicability of circuit discovery findings. The conceptual framework for choosing metrics and methods shows increasing sophistication in MI methodology. Essential for evaluating the epistemic status of circuit discovery claims - if results are highly sensitive to arbitrary methodological choices, this undermines confidence in discovered circuits.

  POSITION: Methodological critique showing activation patching sensitivity to hyperparameters; establishes best practices for intervention-based circuit discovery; highlights replicability concerns.
  },
  keywords = {activation-patching, methodology, best-practices, causal-intervention, High}
}

@article{heimersheim2024how,
  author = {Heimersheim, Stefan and Nanda, Neel},
  title = {How to use and interpret activation patching},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2404.15255},
  arxivId = {2404.15255},
  url = {https://arxiv.org/abs/2404.15255},
  note = {
  CORE ARGUMENT: Provides comprehensive practical guide to activation patching based on authors' extensive experience applying the technique. Covers different ways to apply patching (mean ablation, resampling, zero ablation) and discusses interpretation of results - particularly what evidence patching provides about circuits and pitfalls in metric choice. Emphasizes that patching experiments must be carefully designed and interpreted to yield valid mechanistic insights.

  RELEVANCE: Synthesizes practical wisdom about a core MI technique, complementing Zhang & Nanda's systematic evaluation. The emphasis on interpretation pitfalls suggests growing awareness in MI community that technical tools require careful epistemological reflection. The discussion of what patching can and cannot tell us about circuits shows sophistication about the gap between intervention effects and mechanistic understanding. Valuable for understanding how expert MI practitioners think about evidence and inference from patching experiments.

  POSITION: Practical guide to activation patching interpretation; emphasizes careful experimental design and epistemic humility about what patching reveals.
  },
  keywords = {activation-patching, methodology, interpretation, practical-guide, Medium}
}

@inproceedings{makelov2023subspace,
  author = {Makelov, Aleksandar and Lange, Georg and Nanda, Neel},
  title = {Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2311.17030},
  arxivId = {2311.17030},
  url = {https://arxiv.org/abs/2311.17030},
  note = {
  CORE ARGUMENT: Demonstrates fundamental limitation of subspace activation patching: even when an intervention makes model behavior change as if a feature was manipulated, this may occur through activating dormant parallel pathways causally disconnected from normal computation, creating an "interpretability illusion." Shows this phenomenon in both toy examples and real tasks (indirect object identification, factual recall). Argues this doesn't invalidate patching but requires additional evidence for faithfulness - namely, prior manual circuit analysis to verify the patched subspace is actually involved in normal computation.

  RELEVANCE: Critical paper revealing deep limitation of activation patching methodology. The interpretability illusion shows that behavior-based validation is insufficient - a circuit that produces correct behavior when patched might not be the circuit normally used. This undermines naive interpretations of patching results and raises the bar for claiming genuine mechanistic understanding. The link to fact editing failures provides practical consequences. The requirement for prior manual circuit analysis limits scalability. Essential for epistemological assessment of MI methods.

  POSITION: Methodological critique identifying interpretability illusion in activation patching; argues for stricter standards of evidence for circuit faithfulness.
  },
  keywords = {activation-patching, interpretability-illusion, faithfulness, methodological-critique, High}
}

@inproceedings{conmy2023towards,
  author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\`{a}},
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  booktitle = {Neural Information Processing Systems},
  year = {2023},
  doi = {10.48550/arXiv.2304.14997},
  arxivId = {2304.14997},
  url = {https://arxiv.org/abs/2304.14997},
  note = {
  CORE ARGUMENT: Systematizes the mechanistic interpretability process into steps: (1) choose metric and dataset eliciting target behavior, (2) apply activation patching to find relevant units, (3) understand component functionality by varying dataset/metric/units. Proposes ACDC algorithm to automate step 2 by identifying minimal circuits in the computational graph. Validates by rediscovering 5/5 component types in GPT-2 Small's Greater-Than circuit (68 of 32,000 edges), all previously found manually, demonstrating automated circuit discovery can match human analysis.

  RELEVANCE: Major methodological contribution automating labor-intensive circuit discovery. The systematization of MI workflow makes implicit practices explicit and enables scrutiny. ACDC's success in recovering manually-found circuits validates automated methods against human baselines. However, the validation strategy (reproducing prior manual findings) raises questions: does this just automate finding circuits humans already knew about, or can ACDC discover genuinely novel circuits? The emphasis on minimal circuits reflects commitment to sparse explanations.

  POSITION: Introduces ACDC automated circuit discovery; systematizes MI workflow; validates automation by reproducing manual discoveries.
  },
  keywords = {ACDC, circuit-discovery, automation, GPT-2, High}
}

@inproceedings{marks2024sparse,
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  title = {Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models},
  booktitle = {International Conference on Learning Representations},
  year = {2024},
  doi = {10.48550/arXiv.2403.19647},
  arxivId = {2403.19647},
  url = {https://arxiv.org/abs/2403.19647},
  note = {
  CORE ARGUMENT: Introduces sparse feature circuits - circuits composed of interpretable SAE features rather than polysemantic neurons/heads - enabling detailed mechanistic understanding. Proposes SHIFT method where humans judge task-relevance of features and ablate irrelevant ones to improve classifier generalization. Demonstrates scalable, unsupervised discovery of thousands of feature circuits for automatically discovered behaviors. Argues feature-level circuits are superior to neuron/head-level circuits for understanding and manipulation.

  RELEVANCE: Combines two major MI approaches (SAEs + circuit discovery) into unified framework. The shift from neuron-level to feature-level circuits addresses polysemanticity problem while preserving circuit methodology. SHIFT demonstrates human-in-the-loop feature editing for practical tasks (improving generalization), showing MI can enable model manipulation. The scalability claim (thousands of circuits, automatic behavior discovery) addresses concerns about MI being too labor-intensive. However, reliance on human judgment for feature relevance introduces subjective element.

  POSITION: Integrates SAEs with circuit discovery; proposes feature-level rather than neuron-level circuits; demonstrates human-guided feature editing (SHIFT) for model improvement.
  },
  keywords = {sparse-feature-circuits, SAE-integration, circuit-discovery, SHIFT-method, Medium}
}

@inproceedings{syed2023attribution,
  author = {Syed, Aaquib and Rager, Can and Conmy, Arthur},
  title = {Attribution Patching Outperforms Automated Circuit Discovery},
  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  year = {2023},
  doi = {10.48550/arXiv.2310.10348},
  arxivId = {2310.10348},
  url = {https://arxiv.org/abs/2310.10348},
  note = {
  CORE ARGUMENT: Proposes attribution patching - using linear approximation to activation patching to estimate edge importance in computational subgraph - as simpler, faster alternative to methods like ACDC. Requires only two forward passes and one backward pass (versus many forward passes for ACDC). Shows attribution patching has higher AUC for circuit recovery across tasks while being orders of magnitude faster. Demonstrates that gradient-based approximation can match or exceed more complex search-based methods.

  RELEVANCE: Methodological competition between circuit discovery approaches, showing gradient-based methods can outperform search-based ones. The computational efficiency advantage (2 forward + 1 backward vs. many forwards) makes circuit discovery more tractable at scale. However, reliance on linear approximation raises questions about when approximation is valid - highly nonlinear networks might violate assumptions. The head-to-head comparison with ACDC provides empirical evidence for evaluating circuit discovery methods.

  POSITION: Proposes attribution patching as faster, simpler alternative to ACDC; demonstrates gradient-based methods can outperform search for circuit discovery.
  },
  keywords = {attribution-patching, circuit-discovery, gradient-based-methods, methodology, Medium}
}

@article{lieberum2023does,
  author = {Lieberum, Tom and Rahtz, Matthew and Kram\'{a}r, J\'{a}nos and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  title = {Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla},
  journal = {arXiv preprint},
  year = {2023},
  doi = {10.48550/arXiv.2307.09458},
  arxivId = {2307.09458},
  url = {https://arxiv.org/abs/2307.09458},
  note = {
  CORE ARGUMENT: Tests whether circuit analysis techniques developed on small models (GPT-2) scale to state-of-the-art 70B Chinchilla model. Studies multiple-choice question answering, investigating how Chinchilla identifies correct answer labels given correct answer text. Finds traditional techniques (logit attribution, attention visualization, activation patching) naturally scale, allowing identification of small set of output nodes (attention heads/MLPs). However, attempts to fully understand "correct letter" heads' features yield mixed results - the discovered "Nth item in enumeration" feature only partially explains behavior.

  RELEVANCE: Critical evaluation of MI scalability to frontier models. The mixed results - methods scale but full understanding proves elusive - suggest fundamental limitations. The partial explanations for "correct letter" heads (working on normal cases but failing on randomized labels) indicate discovered features may be incomplete or task-specific rather than fully general. This challenges optimistic claims about MI achieving complete mechanistic understanding. The DeepMind/Anthropic collaboration shows industry investment in MI research.

  POSITION: Empirical test of MI scalability to 70B model; finds methods scale but achieving complete understanding remains challenging; suggests limitations of current MI approaches.
  },
  keywords = {scalability, Chinchilla, circuit-analysis, partial-understanding, Medium}
}

@article{kowalska2025unboxing,
  author = {Kowalska, Bianka and Kwa\'{s}nicka, Halina},
  title = {Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks},
  journal = {arXiv preprint},
  year = {2025},
  doi = {10.48550/arXiv.2511.19265},
  arxivId = {2511.19265},
  url = {https://arxiv.org/abs/2511.19265},
  note = {
  CORE ARGUMENT: Proposes unified taxonomy of MI approaches and provides detailed technical analysis of key techniques with concrete examples and pseudo-code. Contextualizes MI within broader XAI landscape by comparing goals, methods, and insights. Traces historical development of MI as research area and accelerating recent progress. Argues MI enables treating ML models not just as tools but as systems worthy of scientific study and understanding.

  RELEVANCE: Recent (2025) synthesis attempting to systematize MI as coherent research program. The unified taxonomy provides framework for organizing diverse methods (SAEs, patching, circuit discovery). The XAI comparison clarifies MI's distinctive focus on reverse-engineering algorithms versus providing post-hoc explanations. The scientific framing - models as objects of study - reflects MI community's self-conception as doing science of deep learning. The pseudo-code examples make technical methods accessible for philosophical analysis.

  POSITION: 2025 synthesis proposing unified MI taxonomy; positions MI as scientific study of learned algorithms; provides accessible technical introduction.
  },
  keywords = {taxonomy, synthesis, 2025, scientific-understanding, Low}
}

@article{michaud2024opening,
  author = {Michaud, Eric J. and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli\'{c}, Mateja and Tegmark, Max},
  title = {Opening the AI black box: program synthesis via mechanistic interpretability},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2402.05110},
  arxivId = {2402.05110},
  url = {https://arxiv.org/abs/2402.05110},
  note = {
  CORE ARGUMENT: Presents MIPS (Mechanistic Interpretability Program Synthesis) - novel approach synthesizing programs from RNNs trained on algorithmic tasks by reverse-engineering learned algorithms into Python code. Uses integer autoencoder to convert RNN to finite state machine, then applies Boolean/integer symbolic regression to extract algorithm. Successfully solves 32/62 tasks (including 13 GPT-4 fails), demonstrating automated program synthesis without training data. Argues this reveals learned algorithms' actual structure.

  RELEVANCE: Demonstrates MI's potential to extract executable algorithms from trained networks, going beyond understanding to automated knowledge extraction. The complementarity with LLMs (solves different tasks than GPT-4) suggests MI accesses different knowledge than prompt-based approaches. The finite state machine abstraction and symbolic regression show creative combination of MI with program synthesis. However, limitation to small RNNs raises scalability questions. The lack of reliance on human training data (no GitHub code) shows MI can discover algorithms networks invented rather than memorized.

  POSITION: Applies MI to automatic program synthesis; demonstrates extracting executable algorithms from RNNs; shows complementarity with LLM approaches.
  },
  keywords = {program-synthesis, MIPS, symbolic-regression, algorithmic-extraction, Low}
}

@inproceedings{geiger2023causal,
  author = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah D. and Potts, Christopher and Icard, Thomas F.},
  title = {Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability},
  year = {2023},
  arxivId = {2301.04709},
  url = {https://arxiv.org/abs/2301.04709},
  note = {
  CORE ARGUMENT: Develops causal abstraction as theoretical foundation for MI, generalizing from mechanism replacement (interventions) to arbitrary mechanism transformation (functionals). Provides formal framework for core MI concepts: polysemantic neurons, linear representation hypothesis, modular features, graded faithfulness. Unifies diverse MI methods (activation/path patching, causal mediation, circuit analysis, SAEs, steering) in common causal abstraction language. Argues this formalization enables precise reasoning about what MI methods can and cannot establish.

  RELEVANCE: Major theoretical contribution providing formal foundations for MI. The generalization to mechanism transformations goes beyond standard interventionist frameworks. The formalization of key concepts (polysemanticity, features, faithfulness) enables precise philosophical analysis - what exactly do MI practitioners mean by these terms? The unification of diverse methods shows they instantiate common abstract principles. However, the high level of mathematical abstraction may limit accessibility. Critical for evaluating MI's theoretical coherence and what mechanistic claims are actually justified.

  POSITION: Develops causal abstraction as formal foundation for MI; unifies diverse methods; provides precise definitions of core concepts; theoretical framework for MI.
  },
  keywords = {causal-abstraction, theoretical-foundations, formalization, unification, High}
}

@inproceedings{edelman2024evolution,
  author = {Edelman, Benjamin L. and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
  title = {The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains},
  booktitle = {Neural Information Processing Systems},
  year = {2024},
  doi = {10.48550/arXiv.2402.11004},
  arxivId = {2402.11004},
  url = {https://arxiv.org/abs/2402.11004},
  note = {
  CORE ARGUMENT: Introduces simple Markov Chain sequence modeling task to study emergence of in-context learning via statistical induction heads - attention mechanisms computing accurate next-token probabilities from bigram statistics. Shows transformers trained on this task pass through multiple phases: initial uniform predictions, then sub-optimal unigram solution, finally rapid phase transition to correct bigram solution. Investigates how learning is affected by prior distribution over Markov chains and generalizes to n-grams.

  RELEVANCE: Simplified setting enabling rigorous study of induction head emergence and in-context learning mechanisms. The multi-phase training dynamics (uniform → unigram → bigram) suggest simpler solutions may delay discovery of more complex correct solutions - relevant for understanding training dynamics and when networks learn genuine algorithms. The statistical induction head framework provides cleaner setting than natural language for understanding ICL. However, abstraction from real language raises questions about generalization of insights.

  POSITION: Studies induction head emergence in simplified Markov chain setting; reveals multi-phase training dynamics; provides theoretical framework for understanding ICL.
  },
  keywords = {induction-heads, in-context-learning, training-dynamics, Markov-chains, Low}
}
