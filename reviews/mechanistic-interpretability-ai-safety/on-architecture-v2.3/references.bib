@comment{
====================================================================
AGGREGATED BIBLIOGRAPHY
Literature Review: Is Mechanistic Interpretability Necessary or
Sufficient for AI Safety?
Date: 2025-12-28
Total Papers: 104 across 7 domains
====================================================================
}

% =================================================================
% DOMAIN 1: Definitions and Taxonomy of Interpretability (18 papers)
% =================================================================

@article{ayonrinde2025mathematical,
  author = {Ayonrinde, Kola and Jaburi, Louis},
  title = {A Mathematical Philosophy of Explanations in Mechanistic Interpretability: The Strange Science Part I.i},
  journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2025},
  doi = {10.48550/arXiv.2505.00808},
  url = {https://www.semanticscholar.org/paper/e7596e53f54d9a90be8b960771b1265013b2e864},
  keywords = {mechanistic-interpretability, definition, philosophy-of-science}
}

@article{saphra2024mechanistic,
  author = {Saphra, Naomi and Wiegreffe, Sarah},
  title = {Mechanistic?},
  journal = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  year = {2024},
  doi = {10.48550/arXiv.2410.09087},
  url = {https://www.semanticscholar.org/paper/6a821e1e9f43d440de3db97415d1947d5e89d406},
  keywords = {mechanistic-interpretability, terminology, community-history}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety: A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  doi = {10.48550/arXiv.2404.14082},
  url = {https://www.semanticscholar.org/paper/8b750488d139f9beba0815ff8f46ebe15ebb3e58},
  keywords = {mechanistic-interpretability, survey, ai-safety}
}

@article{rai2024practical,
  author = {Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  title = {A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2407.02646},
  url = {https://www.semanticscholar.org/paper/2ac231b9cff4f5f9054d86c9b540429d4dd687f4},
  keywords = {mechanistic-interpretability, taxonomy, transformers}
}

@article{sutter2025nonlinear,
  author = {Sutter, Denis and Minder, Julian and Hofmann, Thomas and Pimentel, Tiago},
  title = {The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?},
  journal = {arXiv preprint},
  year = {2025},
  doi = {10.48550/arXiv.2507.08802},
  url = {https://www.semanticscholar.org/paper/b4d81c89c973b6d207dbc0562760200b7d7974ed},
  keywords = {mechanistic-interpretability, causal-abstraction, critique}
}

@article{chalmers2025propositional,
  author = {Chalmers, David J.},
  title = {Propositional Interpretability in Artificial Intelligence},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2501.15740},
  doi = {10.48550/arXiv.2501.15740},
  url = {https://www.semanticscholar.org/paper/ff47ee9b586cbbb9090f397fe52b569fd066ff1f},
  keywords = {propositional-interpretability, philosophy-of-mind, alternative-framework}
}

@misc{erasmus2023what,
  author = {Erasmus, Adrian and Brunet, Tyler D. P. and Fisher, Eyal},
  title = {What is Interpretability?},
  year = {2023},
  howpublished = {PhilPapers entry ERAWII-2},
  url = {https://philpapers.org/rec/ERAWII-2},
  keywords = {interpretability-definition, taxonomy, conceptual-clarification}
}

@book{craver2007explaining,
  author = {Craver, Carl F.},
  title = {Explaining the Brain: Mechanisms and the Mosaic Unity of Neuroscience},
  publisher = {Oxford University Press},
  year = {2007},
  doi = {10.1093/acprof:oso/9780199299317.001.0001},
  url = {https://doi.org/10.1093/acprof:oso/9780199299317.001.0001},
  keywords = {mechanistic-explanation, philosophy-of-science, neuroscience}
}

@article{machamer2000thinking,
  author = {Machamer, Peter and Darden, Lindley and Craver, Carl F.},
  title = {Thinking About Mechanisms},
  journal = {Philosophy of Science},
  year = {2000},
  volume = {67},
  number = {1},
  pages = {1--25},
  keywords = {mechanistic-explanation, philosophy-of-science, activities}
}

@book{bechtel2008mental,
  author = {Bechtel, William},
  title = {Mental Mechanisms: Philosophical Perspectives on Cognitive Neuroscience},
  publisher = {Routledge},
  year = {2008},
  doi = {10.4324/9780203810095},
  url = {https://doi.org/10.4324/9780203810095},
  keywords = {mechanistic-explanation, cognitive-science, decomposition}
}

% =================================================================
% DOMAIN 2: Technical Mechanistic Interpretability Methods (18 papers)
% =================================================================

@article{olah2020zoom,
  author = {Olah, Christopher and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  volume = {5},
  number = {3},
  doi = {10.23915/distill.00024.001},
  url = {https://distill.pub/2020/circuits/zoom-in/},
  keywords = {circuits, mechanistic-interpretability, vision-models}
}

@article{cammarata2020thread,
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Christopher and Petrov, Michael and Schubert, Ludwig},
  title = {Thread: Circuits},
  journal = {Distill},
  year = {2020},
  doi = {10.23915/distill.00024},
  url = {https://distill.pub/2020/circuits/},
  keywords = {circuits, research-program, universality-hypothesis}
}

@inproceedings{nanda2023progress,
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  title = {Progress measures for grokking via mechanistic interpretability},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2301.05217},
  url = {https://arxiv.org/abs/2301.05217},
  keywords = {grokking, circuit-discovery, transformers}
}

@inproceedings{cunningham2023sparse,
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs Smith, Logan and Huben, Robert and Sharkey, Lee},
  title = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2309.08600},
  url = {https://arxiv.org/abs/2309.08600},
  keywords = {sparse-autoencoders, polysemanticity, superposition}
}

@article{zhang2023towards,
  author = {Zhang, Fred and Nanda, Neel},
  title = {Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2309.16042},
  url = {https://arxiv.org/abs/2309.16042},
  keywords = {activation-patching, methodology, best-practices}
}

@article{heimersheim2024how,
  author = {Heimersheim, Stefan and Nanda, Neel},
  title = {How to use and interpret activation patching},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2404.15255},
  url = {https://arxiv.org/abs/2404.15255},
  keywords = {activation-patching, methodology, interpretation}
}

@inproceedings{makelov2023subspace,
  author = {Makelov, Aleksandar and Lange, Georg and Nanda, Neel},
  title = {Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2311.17030},
  url = {https://arxiv.org/abs/2311.17030},
  keywords = {activation-patching, interpretability-illusion, faithfulness}
}

@inproceedings{conmy2023towards,
  author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\`{a}},
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  booktitle = {Neural Information Processing Systems},
  year = {2023},
  doi = {10.48550/arXiv.2304.14997},
  url = {https://arxiv.org/abs/2304.14997},
  keywords = {ACDC, circuit-discovery, automation}
}

@article{lieberum2023does,
  author = {Lieberum, Tom and Rahtz, Matthew and Kram\'{a}r, J\'{a}nos and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  title = {Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla},
  journal = {arXiv preprint},
  year = {2023},
  doi = {10.48550/arXiv.2307.09458},
  url = {https://arxiv.org/abs/2307.09458},
  keywords = {scalability, Chinchilla, circuit-analysis}
}

@inproceedings{geiger2023causal,
  author = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah D. and Potts, Christopher and Icard, Thomas F.},
  title = {Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability},
  year = {2023},
  url = {https://arxiv.org/abs/2301.04709},
  keywords = {causal-abstraction, theoretical-foundations, formalization}
}

% =================================================================
% DOMAIN 3: Philosophical Foundations of Explanation in AI (15 papers)
% =================================================================

@article{kastner2024explaining,
  author = {K{\"a}stner, Lena and Crook, Barnaby},
  title = {Explaining AI through Mechanistic Interpretability},
  journal = {European Journal for Philosophy of Science},
  year = {2024},
  volume = {14},
  number = {4},
  pages = {52},
  doi = {10.1007/s13194-024-00614-4},
  url = {https://doi.org/10.1007/s13194-024-00614-4},
  keywords = {mechanistic-interpretability, explanation, AI-safety}
}

@article{williams2025mechanistic,
  author = {Williams, Iwan and Oldenburg, Ninell and Dhar, Ruchira and Hatherley, Joshua and Fierro, Constanza and Rajcic, Nina and Schiller, Sandrine R. and Stamatiou, Filippos and S{\o}gaard, Anders},
  title = {Mechanistic Interpretability Needs Philosophy},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2506.18852},
  doi = {10.48550/arXiv.2506.18852},
  url = {https://arxiv.org/abs/2506.18852},
  keywords = {mechanistic-interpretability, philosophy-of-science, interdisciplinary}
}

@article{fleisher2022understanding,
  author = {Fleisher, Will},
  title = {Understanding, Idealization, and Explainable AI},
  journal = {Episteme},
  year = {2022},
  volume = {19},
  pages = {534--560},
  doi = {10.1017/epi.2022.39},
  url = {https://doi.org/10.1017/epi.2022.39},
  keywords = {understanding, XAI, idealization}
}

@article{smart2024beyond,
  author = {Smart, Andrew and Kasirzadeh, Atoosa},
  title = {Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning},
  journal = {AI \& Society},
  year = {2024},
  volume = {40},
  pages = {2045--2053},
  doi = {10.1007/s00146-024-02056-1},
  url = {https://doi.org/10.1007/s00146-024-02056-1},
  keywords = {XAI, social-structures, bias}
}

@article{meloux2025everything,
  author = {M\'{e}loux, Maxime and Maniu, Silviu and Portet, Fran\c{c}ois and Peyrard, Maxime},
  title = {Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2502.20914},
  doi = {10.48550/arXiv.2502.20914},
  url = {https://arxiv.org/abs/2502.20914},
  keywords = {mechanistic-interpretability, identifiability, epistemology}
}

@article{vilas2024position,
  author = {Vilas, Mart\'{i}n and Adolfi, Federico and Poeppel, David and Roig, Gemma},
  title = {Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience},
  booktitle = {International Conference on Machine Learning},
  year = {2024},
  doi = {10.48550/arXiv.2406.01352},
  url = {https://arxiv.org/abs/2406.01352},
  keywords = {mechanistic-interpretability, cognitive-neuroscience, methodology}
}

% =================================================================
% DOMAIN 4: AI Safety Theory and Requirements (15 papers)
% =================================================================

@article{salhab2024systematic,
  author = {Salhab, Wissam and Ameyed, Darine and Jaafar, Fehmi and Mcheick, Hamid},
  title = {A Systematic Literature Review on AI Safety: Identifying Trends, Challenges, and Future Directions},
  journal = {IEEE Access},
  year = {2024},
  volume = {12},
  pages = {131762--131784},
  doi = {10.1109/ACCESS.2024.3440647},
  url = {https://www.semanticscholar.org/paper/02930d9a116eaec470a31c6a758386276e090f55},
  keywords = {ai-safety, systematic-review, pluralistic-framework}
}

@article{gyevnar2025ai,
  author = {Gyevnar, Balint and Kasirzadeh, Atoosa},
  title = {AI Safety for Everyone},
  journal = {Nature Machine Intelligence},
  year = {2025},
  volume = {7},
  pages = {531--542},
  doi = {10.1038/s42256-025-01020-y},
  url = {https://www.semanticscholar.org/paper/ce8fd5f05ebe5bc0ca325c8c8a7b5133a6834eb5},
  keywords = {ai-safety, pluralism, practical-safety}
}

@article{zou2023representation,
  author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex Troy and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, Zico and Hendrycks, Dan},
  title = {Representation Engineering: A Top-Down Approach to AI Transparency},
  journal = {arXiv preprint},
  year = {2023},
  doi = {10.48550/arXiv.2310.01405},
  url = {https://arxiv.org/abs/2310.01405},
  keywords = {representation-engineering, transparency, ai-safety}
}

@article{alzubaidi2023towards,
  author = {Alzubaidi, Laith and Al-Sabaawi, Aiman and Bai, Jinshuai and Dukhan, Ammar and Alkenani, Ahmed H. and Al-Asadi, Ahmed and Alwzwazy, Haider A. and Manoufali, Mohamed and Fadhel, Mohammed A. and Albahri, A. S. and Moreira, Catarina and Ouyang, Chun and Zhang, Jinglan and Santamar\'{i}a, Jos\'{e} and Salhi, Asma and Hollman, Freek and Gupta, Ashish and Duan, Ye and Rabczuk, Timon and Abbosh, Amin and Gu, Yuantong},
  title = {Towards Risk-Free Trustworthy Artificial Intelligence: Significance and Requirements},
  journal = {International Journal of Intelligent Systems},
  year = {2023},
  doi = {10.1155/2023/4459198},
  url = {https://openalex.org/W4387956680},
  keywords = {trustworthy-ai, requirements, pluralistic-framework}
}

@article{diaz-rodriguez2023connecting,
  author = {D\'{i}az-Rodr\'{i}guez, Natalia and Del Ser, Javier and Coeckelbergh, Mark and L\'{o}pez de Prado, Marcos and Herrera-Viedma, Enrique and Herrera, Francisco},
  title = {Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation},
  journal = {arXiv preprint},
  year = {2023},
  doi = {10.48550/arXiv.2305.02231},
  url = {https://arxiv.org/abs/2305.02231},
  keywords = {trustworthy-ai, requirements-framework, responsible-ai}
}

% =================================================================
% DOMAIN 5: Arguments FOR MI as Necessary/Sufficient (12 papers)
% =================================================================

@article{sengupta2025interpretability,
  author = {Sengupta, Aadit and Seth, Pratinav and Sankarapu, Vinay Kumar},
  title = {Interpretability as Alignment: Making Internal Understanding a Design Principle},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2509.08592},
  doi = {10.48550/arXiv.2509.08592},
  url = {https://www.semanticscholar.org/paper/ffdadb6c06013a7ad4265087bce7dbd15811ebdd},
  keywords = {mechanistic-interpretability, AI-safety, governance}
}

@article{lee2025mechanistic,
  author = {Lee, Yoon Pyo},
  title = {Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2507.09931},
  doi = {10.48550/arXiv.2507.09931},
  url = {https://www.semanticscholar.org/paper/4e2bafb0a851afa00bc4a6a635acb175ee16fc2d},
  keywords = {mechanistic-interpretability, safety-critical-systems, nuclear-safety}
}

@article{chen2024towards,
  author = {Chen, Jianhui and Wang, Xiaozhi and Yao, Zijun and Bai, Yushi and Hou, Lei and Li, Juanzi},
  title = {Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons},
  journal = {arXiv preprint},
  year = {2024},
  volume = {abs/2406.14144},
  url = {https://www.semanticscholar.org/paper/33bc46dff817e4afe6bc2c11bc808478b5207847},
  keywords = {mechanistic-interpretability, safety-alignment, activation-steering}
}

@inproceedings{gross2024compact,
  author = {Gross, Jason and Agrawal, Rajashree and Kwa, Thomas and Ong, Euan and Yip, Chun Hei and Gibson, Alex and Noubir, Soufiane and Chan, Lawrence},
  title = {Compact Proofs of Model Performance via Mechanistic Interpretability},
  booktitle = {Neural Information Processing Systems},
  year = {2024},
  doi = {10.48550/arXiv.2406.11779},
  url = {https://www.semanticscholar.org/paper/8487f133fed81e18caf17ecb0d2917a84d5fd218},
  keywords = {mechanistic-interpretability, formal-verification, model-performance}
}

@article{pan2025hidden,
  author = {Pan, Wenbo and Liu, Zhichao and Chen, Qiguang and Zhou, Xiangyang and Yu, Haining and Jia, Xiaohua},
  title = {The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2502.09674},
  url = {https://www.semanticscholar.org/paper/ba37af16b72f5250774e014d7ac6a7051585c0ca},
  keywords = {mechanistic-interpretability, safety-alignment, multi-dimensional-analysis}
}

@article{perrier2025control,
  author = {Perrier, Elija},
  title = {Out of Control: Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2506.17846},
  doi = {10.48550/arXiv.2506.17846},
  url = {https://www.semanticscholar.org/paper/0041659f2dec5fe14037578764a8018909561b24},
  keywords = {AI-safety, control-theory, alignment}
}

% =================================================================
% DOMAIN 6: Arguments AGAINST MI as Necessary/Sufficient (14 papers)
% =================================================================

@misc{hendrycks2025misguided,
  author = {Hendrycks, Dan and Hiscott, Laura},
  title = {The Misguided Quest for Mechanistic {AI} Interpretability},
  year = {2025},
  month = {May},
  howpublished = {\url{https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability}},
  keywords = {critique-MI, representation-engineering}
}

@article{madsen2024interpretability,
  author = {Madsen, Andreas and Lakkaraju, Himabindu and Reddy, Siva and Chandar, Sarath},
  title = {Interpretability Needs a New Paradigm},
  journal = {ArXiv},
  year = {2024},
  volume = {abs/2405.05386},
  doi = {10.48550/arXiv.2405.05386},
  url = {https://arxiv.org/abs/2405.05386},
  keywords = {faithfulness, paradigm-critique, XAI-limitations}
}

@article{vijayaraghavan2023minimum,
  author = {Vijayaraghavan, Avish and Badea, C.},
  title = {Minimum Levels of Interpretability for Artificial Moral Agents},
  journal = {AI and Ethics},
  year = {2023},
  volume = {5},
  pages = {2071--2087},
  doi = {10.1007/s43681-024-00536-0},
  url = {https://doi.org/10.1007/s43681-024-00536-0},
  keywords = {minimum-interpretability, moral-agents, context-dependence}
}

@article{panigrahy2025limitations,
  author = {Panigrahy, Rina and Sharan, Vatsal},
  title = {Limitations on Safe, Trusted, Artificial General Intelligence},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2509.21654},
  doi = {10.48550/arXiv.2509.21654},
  url = {https://arxiv.org/abs/2509.21654},
  keywords = {formal-limitations, impossibility-results, AGI-safety}
}

@article{yampolskiy2024monitorability,
  author = {Yampolskiy, Roman V.},
  title = {On Monitorability of {AI}},
  journal = {AI and Ethics},
  year = {2024},
  volume = {5},
  pages = {689--707},
  doi = {10.1007/s43681-024-00420-x},
  url = {https://doi.org/10.1007/s43681-024-00420-x},
  keywords = {monitorability, emergent-capabilities, impossibility-argument}
}

@article{barez2025unlearning,
  author = {Barez, Fazl and Fu, Tingchen and Prabhu, Ameya and Casper, Stephen and Sanyal, Amartya and Bibi, Adel and O'Gara, Aidan and Kirk, Robert and Bucknall, Ben and Fist, Tim and Ong, Luke and Torr, Philip H. S. and Lam, Kwok-Yan and Trager, Robert and Krueger, David and Mindermann, S. and Hern{\'a}ndez-Orallo, J. and Geva, Mor and Gal, Yarin},
  title = {Open Problems in Machine Unlearning for {AI} Safety},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2501.04952},
  doi = {10.48550/arXiv.2501.04952},
  url = {https://arxiv.org/abs/2501.04952},
  keywords = {machine-unlearning, safety-limitations, alternative-approaches}
}

@article{lehalleur2025alignment,
  author = {Lehalleur, Simon Pepin and Hoogland, Jesse and Farrugia-Roberts, Matthew and Wei, Susan and Oldenziel, Alexander Gietelink and Wang, George and Carroll, Liam and Murfet, Daniel},
  title = {You Are What You Eat: {AI} Alignment Requires Understanding How Data Shapes Structure and Generalisation},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2502.05475},
  doi = {10.48550/arXiv.2502.05475},
  url = {https://arxiv.org/abs/2502.05475},
  keywords = {data-structure-generalization, statistical-foundations, alignment-theory}
}

@article{sang2024weak,
  author = {Sang, Jitao and Wang, Yuhang and Zhang, Jing and Zhu, Yanxu and Kong, Chao and Ye, Junhong and Wei, Shuyu and Xiao, Jinlin},
  title = {Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning},
  journal = {ArXiv},
  year = {2024},
  volume = {abs/2402.00667},
  doi = {10.48550/arXiv.2402.00667},
  url = {https://arxiv.org/abs/2402.00667},
  keywords = {scalable-oversight, weak-to-strong, superalignment}
}

@article{kim2024superalignment,
  author = {Kim, Hyunjin and Yi, Xiaoyuan and Yao, Jing and Lian, Jianxun and Huang, Muhua and Duan, Shitong and Bak, J. and Xie, Xing},
  title = {The Road to Artificial {SuperIntelligence}: {A} Comprehensive Survey of Superalignment},
  journal = {ArXiv},
  year = {2024},
  volume = {abs/2412.16468},
  doi = {10.48550/arXiv.2412.16468},
  url = {https://arxiv.org/abs/2412.16468},
  keywords = {superalignment, scalable-oversight-survey, ASI-limitations}
}

@inproceedings{lopez2023nnv,
  author = {Lopez, Diego Manzanas and Choi, Sung Woo and Tran, Hoang-Dung and Johnson, Taylor T.},
  title = {{NNV} 2.0: The Neural Network Verification Tool},
  booktitle = {Lecture Notes in Computer Science},
  year = {2023},
  pages = {397--412},
  publisher = {Springer},
  doi = {10.1007/978-3-031-37703-7_19},
  keywords = {formal-verification, neural-network-verification, alternative-to-MI}
}

@article{guidotti2023leveraging,
  author = {Guidotti, Dario and Pandolfo, Laura and Pulina, Luca},
  title = {Leveraging Satisfiability Modulo Theory Solvers for Verification of Neural Networks in Predictive Maintenance Applications},
  journal = {Information},
  year = {2023},
  volume = {14},
  number = {7},
  pages = {397},
  doi = {10.3390/info14070397},
  keywords = {SMT-verification, predictive-maintenance, practical-alternative}
}

% =================================================================
% DOMAIN 7: Case Studies and Empirical Evidence (12 papers)
% =================================================================

@inproceedings{bhalla2024towards,
  author = {Bhalla, Usha and Srinivas, Suraj and Ghandeharioun, Asma and Lakkaraju, Himabindu},
  title = {Towards Unifying Interpretability and Control: Evaluation via Intervention},
  booktitle = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2411.04430},
  url = {https://www.semanticscholar.org/paper/7c7e1160c6fc55444378e1cc6205c68a29d13571},
  keywords = {mechanistic-interpretability, empirical-evaluation, intervention}
}

@article{kadir2023evaluation,
  author = {Kadir, Md Abdul and Mosavi, Amir and Sonntag, Daniel},
  title = {Evaluation Metrics for XAI: A Review, Taxonomy, and Practical Applications},
  booktitle = {2023 IEEE 27th International Conference on Intelligent Engineering Systems (INES)},
  year = {2023},
  pages = {000111--000124},
  doi = {10.1109/INES59282.2023.10297629},
  url = {https://www.semanticscholar.org/paper/c1aebb992a8a0640eb047486fa05e5c38fef95c3},
  keywords = {XAI, evaluation-metrics, taxonomy}
}

@inproceedings{casper2023red,
  author = {Casper, Stephen and Li, Yuxiao and Li, Jiawei and Bu, Tong and Zhang, Ke and Hariharan, K. and Hadfield-Menell, Dylan},
  title = {Red Teaming Deep Neural Networks with Feature Synthesis Tools},
  booktitle = {Neural Information Processing Systems},
  year = {2023},
  url = {https://www.semanticscholar.org/paper/5fdf01476628b2afe4853d747ddfc6677bab13bc},
  keywords = {red-teaming, trojan-detection, debugging}
}

@inproceedings{yu2024mechanistic,
  author = {Yu, Lei and Cao, Meng and Cheung, J. C. K. and Dong, Yue},
  title = {Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year = {2024},
  pages = {7943--7956},
  doi = {10.18653/v1/2024.findings-emnlp.466},
  url = {https://www.semanticscholar.org/paper/01b4977937966694c00f6c7b55e712eef50603a4},
  keywords = {hallucinations, mechanistic-interpretability, mitigation}
}

@article{huang2024neuron,
  author = {Huang, Li and Sun, Weifeng and Yan, Meng and Liu, Zhongxin and Lei, Yan and Lo, David},
  title = {Neuron Semantic-Guided Test Generation for Deep Neural Networks Fuzzing},
  journal = {ACM Transactions on Software Engineering and Methodology},
  year = {2024},
  volume = {34},
  pages = {1--38},
  doi = {10.1145/3688835},
  url = {https://www.semanticscholar.org/paper/c11e3b8d153a135375f595845b86d527880c168d},
  keywords = {fuzzing, semantic-interpretability, testing}
}
