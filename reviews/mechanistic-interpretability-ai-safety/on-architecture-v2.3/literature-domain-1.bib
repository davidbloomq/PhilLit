@comment{
====================================================================
DOMAIN: Definitions and Taxonomy of Interpretability
SEARCH_DATE: 2025-12-28
PAPERS_FOUND: 18 total (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, arXiv
====================================================================

DOMAIN_OVERVIEW:
The domain of interpretability definitions and taxonomy sits at the intersection
of philosophy of science, AI ethics, and technical machine learning research.
Three major debates structure this domain: (1) the narrow vs. broad definition
of mechanistic interpretability (MI), where MI is either specifically reverse-
engineering causal mechanisms (Bereska & Gavves 2024, Ayonrinde & Jaburi 2025) or
any exploration of model internals (Saphra & Wiegreffe 2024); (2) the distinction
between interpretability, explainability, and understandability, with significant
terminological confusion in the ML literature (Erasmus et al. 2023, Porcedda 2023);
and (3) the relationship between mechanistic explanations in philosophy of science
(Craver 2007, Machamer et al. 2000) and mechanistic interpretability in AI.

The field exhibits a critical divide between two research communities: the traditional
NLP interpretability community and the "mechanistic interpretability" movement
(Saphra & Wiegreffe 2024). Recent work establishes that MI research is grounded
in producing "Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations"
(Ayonrinde & Jaburi 2025), connecting directly to philosophy of science debates
about mechanistic explanation. However, the linear representation hypothesis—the
assumption that features are encoded linearly—may be insufficient for principled
interpretability (Sutter et al. 2025).

RELEVANCE_TO_PROJECT:
Understanding how MI relates to broader interpretability paradigms is foundational
for assessing its role in AI safety. The definitional work establishes what MI can
and cannot deliver, while the taxonomy literature reveals competing interpretability
approaches that may complement or challenge MI's causal-mechanistic approach. The
connection to philosophy of science provides conceptual foundations for evaluating
MI's explanatory adequacy.

NOTABLE_GAPS:
Limited work connects MI definitions to epistemology of scientific understanding.
Few papers bridge the technical MI literature with philosophical debates about
mechanisms, levels, and reduction. The relationship between propositional
interpretability (Chalmers 2025) and mechanistic approaches remains unexplored.

SYNTHESIS_GUIDANCE:
Structure synthesis around three axes: (1) Narrow vs. broad definitions of MI,
(2) MI's relationship to other interpretability paradigms (functional, behavioral,
counterfactual), (3) Philosophical foundations in mechanistic explanation. Use
Saphra & Wiegreffe's historical account to frame the cultural/technical divide.
Prioritize recent 2024-2025 work that explicitly defines MI.

KEY_POSITIONS:
- Narrow mechanistic: 8 papers - MI requires causal claims about internal mechanisms
- Broad mechanistic: 4 papers - MI encompasses any internal model exploration
- Taxonomic/integrative: 6 papers - Multiple interpretability types complement each other
====================================================================
}

@article{ayonrinde2025mathematical,
  author = {Ayonrinde, Kola and Jaburi, Louis},
  title = {A Mathematical Philosophy of Explanations in Mechanistic Interpretability: The Strange Science Part I.i},
  journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2025},
  doi = {10.48550/arXiv.2505.00808},
  url = {https://www.semanticscholar.org/paper/e7596e53f54d9a90be8b960771b1265013b2e864},
  note = {
  CORE ARGUMENT: Proposes a formal definition of Mechanistic Interpretability (MI) as
  producing "Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations" of neural
  networks. Argues for the Explanatory View Hypothesis: that MI is principled because neural
  networks contain implicit explanations that can be extracted. Introduces the Principle of
  Explanatory Optimism as a necessary precondition for MI's success and defines "Explanatory
  Faithfulness" as a well-defined assessment metric.

  RELEVANCE: Provides the most explicit philosophical definition of MI to date, grounding
  it in philosophy of science debates about mechanistic explanation. Directly addresses
  the conceptual foundations needed to distinguish MI from other interpretability paradigms.
  The four-component definition (Model-level, Ontic, Causal-Mechanistic, Falsifiable) offers
  precise criteria for evaluating whether interpretability work qualifies as MI and reveals
  MI's inherent limits.

  POSITION: Narrow mechanistic view. Argues MI is a specific approach requiring causal-mechanistic
  explanations, not merely any form of model analysis. Positions MI within philosophy of science
  traditions of mechanistic explanation (Craver, Machamer et al.).
  },
  keywords = {mechanistic-interpretability, definition, philosophy-of-science, High}
}

@article{saphra2024mechanistic,
  author = {Saphra, Naomi and Wiegreffe, Sarah},
  title = {Mechanistic?},
  journal = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  year = {2024},
  doi = {10.48550/arXiv.2410.09087},
  url = {https://www.semanticscholar.org/paper/6a821e1e9f43d440de3db97415d1947d5e89d406},
  note = {
  CORE ARGUMENT: Identifies four distinct uses of "mechanistic" in interpretability research,
  from narrow technical (requiring causality claims) to broad cultural (encompassing all
  interpretability). Presents a historical account of how the NLP interpretability community
  and mechanistic interpretability community developed separately, creating a "critical divide"
  within interpretability research. Argues the polysemy of "mechanistic" reflects substantive
  methodological and cultural differences, not merely terminological confusion.

  RELEVANCE: Essential for understanding the landscape of interpretability research and why
  "mechanistic interpretability" has become a contested term. Reveals that MI is both a
  technical approach and a cultural movement, which affects how its contributions are evaluated.
  The historical account explains why traditional NLP interpretability work and MI work often
  talk past each other, crucial for synthesizing the literature.

  POSITION: Diagnostic/historical position. Does not advocate for one definition but documents
  semantic drift and community formation. Shows that definitional debates reflect deeper
  methodological commitments about what counts as explanation in AI.
  },
  keywords = {mechanistic-interpretability, terminology, community-history, High}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety: A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  doi = {10.48550/arXiv.2404.14082},
  url = {https://www.semanticscholar.org/paper/8b750488d139f9beba0815ff8f46ebe15ebb3e58},
  note = {
  CORE ARGUMENT: Comprehensive survey defining mechanistic interpretability as "reverse engineering
  the computational mechanisms and representations learned by neural networks into human-understandable
  algorithms and concepts to provide a granular, causal understanding." Establishes foundational concepts
  (features as units encoding knowledge, circuits as computational pathways) and surveys methodologies for
  causal dissection of model behaviors. Assesses MI's relevance to AI safety through benefits (understanding,
  control, alignment) and risks (capability gains, dual-use concerns).

  RELEVANCE: The most comprehensive technical review of MI for AI safety, directly connecting MI
  definitions to safety applications. Establishes the conceptual framework (features, circuits,
  mechanistic hypotheses) that structures current MI research. Critical for understanding what MI
  can contribute to safety and where its limitations lie. The 288 citations indicate this is a
  standard reference in the field.

  POSITION: Narrow mechanistic view emphasizing causal understanding. Advocates for clarifying
  concepts, setting standards, and scaling MI techniques to handle complex models. Frames MI as
  essential for AI safety but acknowledges scalability and automation challenges.
  },
  keywords = {mechanistic-interpretability, survey, ai-safety, High}
}

@article{rai2024practical,
  author = {Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  title = {A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2407.02646},
  url = {https://www.semanticscholar.org/paper/2ac231b9cff4f5f9054d86c9b540429d4dd687f4},
  note = {
  CORE ARGUMENT: Provides a comprehensive task-centric taxonomy of MI research, organizing
  the field around specific research questions rather than methods. Outlines fundamental
  objects of study in MI (features, circuits, mechanisms) and maps techniques, evaluation
  methods, and key findings to tasks. Presents MI as "reverse-engineering internal computations"
  and emphasizes that recent work has yielded "many novel insights yet introducing new challenges."

  RELEVANCE: Offers a practical roadmap for understanding the scope and structure of MI research,
  particularly valuable for assessing what MI has accomplished and where gaps remain. The task-centric
  taxonomy reveals which aspects of model behavior MI can address and which remain intractable.
  Essential for evaluating MI's maturity as a field and its readiness for safety applications.

  POSITION: Narrow mechanistic view. Defines MI specifically as reverse-engineering internal
  computations, distinguishing it from other interpretability approaches. The taxonomy structures
  the field around causal understanding of specific model behaviors.
  },
  keywords = {mechanistic-interpretability, taxonomy, transformers, High}
}

@article{sutter2025nonlinear,
  author = {Sutter, Denis and Minder, Julian and Hofmann, Thomas and Pimentel, Tiago},
  title = {The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?},
  journal = {arXiv preprint},
  year = {2025},
  doi = {10.48550/arXiv.2507.08802},
  url = {https://www.semanticscholar.org/paper/b4d81c89c973b6d207dbc0562760200b7d7974ed},
  note = {
  CORE ARGUMENT: Proves that unrestricted causal abstraction becomes vacuous—any neural network
  can be mapped to any algorithm using sufficiently powerful alignment maps. Demonstrates empirically
  that perfect alignment maps can be found even for randomly initialized models incapable of solving
  tasks. Argues this creates a "non-linear representation dilemma": lifting the linearity constraint
  leaves no principled way to balance map complexity and accuracy. Concludes that causal abstraction
  requires assumptions about information encoding (e.g., linear representation hypothesis) to be meaningful.

  RELEVANCE: Directly challenges a foundational assumption in MI research—that causal abstraction
  provides a principled framework for mechanistic interpretability. Reveals that MI's success depends
  critically on additional assumptions (like linearity) that may not hold. This has profound implications
  for evaluating MI explanations: without constraints on representation, MI may lack the principled
  foundations claimed by advocates. Essential for understanding limits of current MI approaches.

  POSITION: Critical of narrow mechanistic interpretability claims. Argues current MI methods rest
  on underdetermined theoretical foundations and that causal abstraction alone is insufficient for
  principled interpretability without additional structural assumptions.
  },
  keywords = {mechanistic-interpretability, causal-abstraction, critique, High}
}

@article{chalmers2025propositional,
  author = {Chalmers, David J.},
  title = {Propositional Interpretability in Artificial Intelligence},
  journal = {arXiv preprint},
  year = {2025},
  doi = {10.48550/arXiv.2501.15740},
  url = {https://www.semanticscholar.org/paper/ff47ee9b586cbbb9090f397fe52b569fd066ff1f},
  note = {
  CORE ARGUMENT: Argues for the centrality of propositional interpretability—interpreting AI systems
  via propositional attitudes (beliefs, desires, subjective probabilities directed at propositions).
  Claims propositional attitudes are the central framework for understanding human cognition and will
  be central for AI. Introduces "thought logging" as a key challenge: creating systems that log all
  relevant propositional attitudes over time. Evaluates current MI methods (probing, sparse autoencoders,
  chain-of-thought) and philosophical interpretation methods (psychosemantics) as methods of propositional
  interpretability.

  RELEVANCE: Offers a philosophically rigorous alternative framework for interpretability grounded in
  philosophy of mind rather than philosophy of science. Reveals potential limitations of mechanistic
  approaches that focus on causal mechanisms rather than propositional content. The "thought logging"
  challenge provides a novel criterion for evaluating interpretability methods. Important for understanding
  whether MI's focus on mechanisms is sufficient or whether propositional content is essential.

  POSITION: Distinct from mechanistic interpretability. Proposes propositional attitudes as the fundamental
  interpretive framework, though notes MI methods may contribute to propositional interpretability. Represents
  an alternative philosophical foundation grounded in philosophy of mind rather than mechanistic explanation.
  },
  keywords = {propositional-interpretability, philosophy-of-mind, alternative-framework, High}
}

@misc{erasmus2023what,
  author = {Erasmus, Adrian and Brunet, Tyler D. P. and Fisher, Eyal},
  title = {What is Interpretability?},
  year = {2023},
  howpublished = {PhilPapers entry ERAWII-2},
  url = {https://philpapers.org/rec/ERAWII-2},
  note = {
  CORE ARGUMENT: Diagnoses confusion in ML literature as equivocation on "explainability," "understandability,"
  and "interpretability." Offers a theory and typology of interpretation in machine learning, arguing that
  interpretation is "something one does to an explanation with the aim of producing another, more understandable,
  explanation." Distinguishes these concepts to provide clear definitions and theoretical foundations for
  interpretability research.

  RELEVANCE: Addresses fundamental terminological confusion that pervades interpretability literature,
  essential for clarifying what different approaches claim to achieve. The distinction between explainability,
  understandability, and interpretability helps evaluate whether MI provides explanations, interpretations,
  or something else. Critical for establishing common vocabulary for cross-disciplinary synthesis.

  POSITION: Taxonomic/integrative. Seeks to clarify conceptual distinctions rather than advocate for one
  interpretability approach. Provides philosophical foundations for distinguishing different interpretability
  goals and methods.
  },
  keywords = {interpretability-definition, taxonomy, conceptual-clarification, High}
}

@article{porcedda2023interpretability,
  author = {Porcedda, Riccardo},
  title = {Interpretability is not Explainability: New Quantitative XAI Approach with a Focus on Recommender Systems in Education},
  journal = {ArXiv preprint},
  year = {2023},
  doi = {10.5121/csit.2023.131612},
  url = {https://www.semanticscholar.org/paper/830c2f7d3c5e41aac1ba148837cafedf5218d058},
  note = {
  CORE ARGUMENT: Proposes a novel XAI taxonomy distinguishing transparency, interpretability, completeness,
  complexity, and understandability as essential dimensions of explainability. Argues current XAI literature
  lacks a widely accepted taxonomy for quantitative evaluation. Demonstrates the taxonomy through a case study
  using SHAP to quantify and enhance explainability in a recommender system, providing a "shared vocabulary for
  future research."

  RELEVANCE: Contributes to establishing standardized concepts and metrics for interpretability evaluation,
  directly relevant to assessing MI methods' explanatory quality. The five-dimensional taxonomy (transparency,
  interpretability, completeness, complexity, understandability) offers criteria for comparing MI to other XAI
  approaches. The emphasis on quantitative evaluation addresses a gap in philosophical analysis of interpretability.

  POSITION: Taxonomic/integrative. Seeks to establish shared conceptual framework and quantitative metrics across
  XAI approaches. Treats interpretability as one dimension of broader explainability construct.
  },
  keywords = {XAI-taxonomy, quantitative-evaluation, conceptual-framework, Medium}
}

@article{kadir2023evaluation,
  author = {Kadir, Md Abdul and Mosavi, Amir and Sonntag, Daniel},
  title = {Evaluation Metrics for XAI: A Review, Taxonomy, and Practical Applications},
  journal = {IEEE 27th International Conference on Intelligent Engineering Systems},
  year = {2023},
  doi = {10.1109/INES59282.2023.10297629},
  url = {https://www.semanticscholar.org/paper/c1aebb992a8a0640eb047486fa05e5c38fef95c3},
  note = {
  CORE ARGUMENT: Reviews evaluation metrics for explainable AI through systematic PRISMA guidelines.
  Proposes two taxonomies: one based on applications, one based on evaluation metrics themselves.
  Argues that diversity of data and learning methodologies requires multiple evaluation approaches
  and that validity, reliability, and evaluation metrics for explainability lack clear definitions.

  RELEVANCE: Addresses critical question of how to evaluate interpretability methods, essential for
  comparing MI to alternative approaches. The dual taxonomy (application-based and metric-based) reveals
  that evaluation criteria are context-dependent, challenging universal claims about MI's superiority.
  Important for understanding what counts as success in interpretability research.

  POSITION: Taxonomic/methodological. Focuses on evaluation rather than advocating specific interpretability
  approach. Reveals diversity of evaluation criteria and challenges in establishing universal metrics.
  },
  keywords = {XAI-evaluation, metrics, taxonomy, Medium}
}

@article{arsenault2024survey,
  author = {Arsenault, Pierre-Daniel and Wang, Shengrui and Patenaude, Jean-Marc},
  title = {A Survey of Explainable Artificial Intelligence (XAI) in Financial Time Series Forecasting},
  journal = {ACM Computing Surveys},
  year = {2024},
  volume = {57},
  pages = {1--37},
  doi = {10.1145/3729531},
  url = {https://www.semanticscholar.org/paper/62fbc78884d1b2ab02e0ad60a9795f7596369f26},
  note = {
  CORE ARGUMENT: Distinguishes explainability and interpretability as separate concepts requiring
  different treatment in practice, contrary to common assumption they are interchangeable. Categorizes
  XAI approaches in financial forecasting (2018-2024) and provides rigorous taxonomy with complementary
  characterization. Emphasizes that explainability and interpretability "are not applied the same way in
  practice" despite often being conflated.

  RELEVANCE: The distinction between explainability and interpretability clarifies what different approaches
  claim to deliver, essential for evaluating MI's contributions. The domain-specific application (financial
  forecasting) reveals how interpretability requirements vary by context, challenging one-size-fits-all
  approaches. The rigorous taxonomy provides framework for comparing MI to other XAI methods.

  POSITION: Taxonomic/integrative with emphasis on conceptual distinctions. Argues for treating explainability
  and interpretability as distinct concepts with different practical applications and evaluation criteria.
  },
  keywords = {XAI-taxonomy, explainability-vs-interpretability, domain-specific, Medium}
}

@article{craver2007explaining,
  author = {Craver, Carl F.},
  title = {Explaining the Brain: Mechanisms and the Mosaic Unity of Neuroscience},
  journal = {Oxford University Press},
  year = {2007},
  doi = {10.1093/acprof:oso/9780199299317.001.0001},
  url = {https://plato.stanford.edu/entries/science-mechanisms/},
  note = {
  CORE ARGUMENT: Develops comprehensive account of mechanistic explanation in neuroscience, arguing that
  mechanisms consist of organized entities and activities that produce regular changes from start to finish.
  Introduces constitutive mechanistic explanation (explaining capacities of wholes by organization of parts)
  and develops "mosaic unity" view of neuroscience as unified through mechanistic explanations across levels
  without reductive unification. Establishes criteria for constitutive relevance and mechanistic explanation.

  RELEVANCE: Foundational work in philosophy of science on mechanistic explanation that directly informs
  MI's conceptual foundations. Craver's account of mechanisms (entities, activities, organization) and
  constitutive explanation (part-whole relations) provides philosophical grounding for MI's goal of
  explaining neural network behavior through internal mechanisms. The "mosaic unity" framework is relevant
  for understanding how MI relates to other levels of AI analysis.

  POSITION: Canonical position in mechanistic philosophy of science. Defines mechanistic explanation as
  identifying entities, activities, and organization that constitute mechanisms. Emphasizes explanatory
  detail and completeness as virtues of mechanistic explanation.
  },
  keywords = {mechanistic-explanation, philosophy-of-science, neuroscience, High}
}

@article{machamer2000thinking,
  author = {Machamer, Peter and Darden, Lindley and Craver, Carl F.},
  title = {Thinking About Mechanisms},
  journal = {Philosophy of Science},
  year = {2000},
  volume = {67},
  number = {1},
  pages = {1--25},
  url = {https://plato.stanford.edu/entries/science-mechanisms/},
  note = {
  CORE ARGUMENT: Introduces influential account of mechanisms as "entities and activities organized
  such that they are productive of regular changes from start or set-up to finish or termination
  conditions." Emphasizes that mechanistic explanations describe "how things work" by identifying
  entities engaged in activities. Argues mechanisms involve both entities (things with properties) and
  activities (what entities do), rejecting accounts that reduce mechanisms to entities or properties alone.

  RELEVANCE: Foundational paper establishing the "new mechanist" framework in philosophy of science
  that MI explicitly draws upon (Bereska & Gavves, Ayonrinde & Jaburi cite mechanistic explanation
  tradition). The emphasis on activities (not just entities) and organization (not just components)
  provides conceptual framework for evaluating whether MI actually delivers mechanistic explanations.
  The "productive of regular changes" criterion is important for assessing MI's explanatory adequacy.

  POSITION: Foundational new mechanist position. Defines mechanisms through entities-and-activities
  framework, emphasizing organization and productivity. Influential account that subsequent mechanistic
  philosophy builds upon or responds to.
  },
  keywords = {mechanistic-explanation, philosophy-of-science, activities, Medium}
}

@article{woodward2002mechanism,
  author = {Woodward, James},
  title = {What Is a Mechanism? A Counterfactual Account},
  journal = {Philosophy Compass},
  year = {2002},
  url = {https://philpapers.org/rec/WOOWIA-3},
  note = {
  CORE ARGUMENT: Presents counterfactual account of mechanisms, arguing mechanisms consist of parts
  whose behavior conforms to generalizations that are invariant under interventions and which are modular.
  Emphasizes that mechanistic explanation involves identifying parts and generalizations governing their
  behavior that support counterfactual reasoning about what would happen under interventions. Offers
  interventionist alternative to Machamer-Darden-Craver activities-based account.

  RELEVANCE: Provides alternative philosophical foundation for mechanistic explanation based on
  counterfactual interventions rather than activities, relevant for evaluating MI's causal claims.
  The interventionist framework connects directly to MI's use of activation patching and causal
  interventions to identify mechanisms. The emphasis on invariance and modularity provides criteria
  for assessing whether discovered "mechanisms" are genuine or artifacts of methodology.

  POSITION: Interventionist mechanistic account. Defines mechanisms through invariant generalizations
  and modular parts that support counterfactual reasoning. Alternative to activities-based accounts,
  emphasizing causal relationships over ontological categories of activities.
  },
  keywords = {mechanistic-explanation, interventionism, counterfactuals, Medium}
}

@article{bechtel2005explanation,
  author = {Bechtel, William and Abrahamsen, Adele},
  title = {Explanation: A Mechanist Alternative},
  journal = {Studies in History and Philosophy of Biological and Biomedical Sciences},
  year = {2005},
  volume = {36},
  number = {2},
  pages = {421--441},
  doi = {10.1016/j.shpsc.2005.03.010},
  url = {https://philpapers.org/rec/BECEAM-5},
  note = {
  CORE ARGUMENT: Argues mechanistic explanation offers alternative to deductive-nomological (D-N) model,
  emphasizing that scientists developing mechanistic explanations are "not limited to linguistic
  representations and logical inference" but "frequently employ diagrams to characterize mechanisms and
  simulations to reason about them." Highlights role of non-linguistic representations (diagrams, models,
  simulations) in mechanistic explanation and discovery.

  RELEVANCE: The emphasis on diagrams and simulations (not just linguistic descriptions) is directly
  relevant to MI's use of visualizations and computational models to represent mechanisms. Challenges
  assumption that mechanistic explanations must be propositional (cf. Chalmers's propositional interpretability).
  Important for understanding what forms MI explanations can take and whether they count as genuine explanations.

  POSITION: Mechanistic explanation as alternative to D-N model. Emphasizes role of non-linguistic
  representations and dynamic modeling in mechanistic explanation, not just static descriptions of
  entities and activities.
  },
  keywords = {mechanistic-explanation, representation, non-linguistic, Medium}
}

@article{zhou2023mystery,
  author = {Zhou, Yuxiang and Li, Jiazheng and Xiang, Yanzheng and Yan, Hanqi and Gui, Lin and He, Yulan},
  title = {The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis},
  journal = {Conference on Empirical Methods in Natural Language Processing},
  year = {2023},
  pages = {14365--14378},
  doi = {10.18653/v1/2024.emnlp-main.795},
  url = {https://www.semanticscholar.org/paper/ae16932164b3be704671f25af7989f2346a689a5},
  note = {
  CORE ARGUMENT: Surveys interpretation and analysis of in-context learning (ICL) from theoretical
  (mechanistic interpretability, mathematical foundations) and empirical (factors affecting ICL) perspectives.
  Emphasizes that understanding ICL is important not just for capability improvement but for "proactive
  identification and mitigation of potential risks, including concerns regarding truthfulness, bias, and
  toxicity." Reviews mechanistic interpretability approaches to understanding ICL mechanisms.

  RELEVANCE: Demonstrates application of MI to specific AI capability (in-context learning), revealing
  both MI's contributions and limitations for understanding emergent behaviors. The dual theoretical/empirical
  perspective shows how MI complements other analysis approaches. Important case study for evaluating MI's
  practical utility for understanding and improving AI safety.

  POSITION: Integrative survey position. Treats mechanistic interpretability as one valuable approach
  among others for understanding ICL, neither privileging nor dismissing MI methods.
  },
  keywords = {in-context-learning, mechanistic-interpretability-application, survey, Medium}
}

@article{nanda2023progress,
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  title = {Progress Measures for Grokking via Mechanistic Interpretability},
  journal = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2301.05217},
  url = {https://www.semanticscholar.org/paper/f680d47a51a0e470fcb228bf0110c026535ead1b},
  note = {
  CORE ARGUMENT: Fully reverse engineers algorithm learned by transformers on modular addition,
  discovering it uses discrete Fourier transforms and trigonometric identities to convert addition
  to rotation. Defines progress measures splitting "grokking" into three continuous phases (memorization,
  circuit formation, cleanup) rather than sudden shift. Demonstrates that mechanistic understanding
  enables identifying continuous progress measures underlying seemingly discontinuous emergent behavior.

  RELEVANCE: Landmark MI case study demonstrating complete reverse engineering of learned algorithm,
  providing concrete example of MI's ambitions and achievements. Shows MI can provide interpretable
  progress measures and reveal structure in seemingly opaque phenomena. However, limited to small
  models on simple tasks—raises questions about scalability to frontier models. Important exemplar
  for evaluating MI's maturity and promise.

  POSITION: Demonstrates narrow mechanistic interpretability in practice. Exemplifies "reverse engineering
  learned behaviors into individual components" through complete algorithmic understanding, confirming
  MI's potential while revealing current limitations to toy settings.
  },
  keywords = {mechanistic-interpretability, case-study, grokking, Medium}
}

@article{conmy2023automated,
  author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  journal = {Neural Information Processing Systems},
  year = {2023},
  doi = {10.48550/arXiv.2304.14997},
  url = {https://www.semanticscholar.org/paper/eefbd8b384a58f464827b19e30a6920ba976def9},
  note = {
  CORE ARGUMENT: Systematizes the mechanistic interpretability process followed by previous reverse-engineering
  work and automates circuit discovery through activation patching algorithms. Proposes ACDC algorithm that
  automatically identifies computational subgraphs (circuits) implementing specific behaviors. Validates by
  rediscovering 5/5 component types in GPT-2 Small's greater-than circuit, selecting 68 of 32,000 edges,
  all previously found manually.

  RELEVANCE: Addresses critical scalability challenge for MI by automating circuit discovery, essential
  for applying MI beyond toy models. The systematization of MI methodology (choose metric/dataset, apply
  activation patching, identify units, vary dataset/metric/units) clarifies what MI researchers actually do.
  However, automation raises questions about interpretability of automatically discovered circuits—can we
  understand circuits we didn't manually discover?

  POSITION: Advances narrow mechanistic interpretability through automation and systematization. Frames
  MI as a multi-step process (metric selection, activation patching, circuit identification) that can be
  partially automated, addressing scalability concerns while preserving causal-mechanistic approach.
  },
  keywords = {mechanistic-interpretability, automation, circuit-discovery, Low}
}

@article{krishnan2021against,
  author = {Krishnan, Maya},
  title = {Against Interpretability: A Critical Examination of the Interpretability Problem in Machine Learning},
  journal = {PhilPapers entry KRIAIA-3},
  year = {2021},
  url = {https://philpapers.org/rec/KRIAIA-3},
  note = {
  CORE ARGUMENT: Challenges widespread agreement about existence and importance of ML's "black box problem."
  Argues that framing ML as suffering from opacity/interpretability problems assumes without justification
  that ML must be interpretable or explicable in certain ways. Questions whether lack of interpretability
  is genuinely problematic or merely reflects inappropriate expectations about how ML should function.

  RELEVANCE: Provides important critical perspective questioning foundational assumptions of interpretability
  research, including MI. Relevant for evaluating whether MI addresses a genuine problem or creates unnecessary
  requirements. The critique challenges whether mechanistic interpretability's goals (understanding internal
  mechanisms) are necessary or even desirable for AI safety and reliable deployment.

  POSITION: Critical of interpretability as goal. Questions whether the "black box problem" is genuine
  problem requiring solution or artifact of inappropriate expectations. Challenges foundational assumptions
  shared by MI and other interpretability approaches.
  },
  keywords = {interpretability-critique, black-box-problem, skeptical, Low}
}

@article{scorzato2022reliability,
  author = {Scorzato, Luigi},
  title = {Reliability and Interpretability in Science and Deep Learning},
  journal = {PhilPapers entry SCORAI},
  year = {2022},
  url = {https://philpapers.org/rec/SCORAI},
  note = {
  CORE ARGUMENT: Clarifies relationship between lack of understanding (black-box problem) and interpretability
  in a way "independent of individual skills." Argues interpretability is precondition for plausible reliability
  assessment, which "cannot be based on statistical analysis alone." Identifies close relation between model's
  epistemic complexity and its interpretability in responsible AI context.

  RELEVANCE: Connects interpretability to epistemic goals (reliability assessment) rather than merely pragmatic
  goals (trust, compliance). Relevant for understanding why mechanistic interpretability matters for AI safety—not
  just for explaining decisions but for assessing reliability. The emphasis on epistemic complexity provides
  criterion for evaluating whether MI explanations actually reduce uncertainty about model behavior.

  POSITION: Defends importance of interpretability for epistemic reasons (reliability assessment) while
  acknowledging that interpretability challenges are partly independent of researcher skills. Frames
  interpretability as necessary for moving beyond purely statistical reliability evaluation.
  },
  keywords = {interpretability-epistemology, reliability, epistemic-complexity, Low}
}
