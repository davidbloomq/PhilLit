@comment{
====================================================================
DOMAIN: Philosophical Foundations of Explanation in AI
SEARCH_DATE: 2025-12-28
PAPERS_FOUND: 15 total (High: 8, Medium: 5, Low: 2)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, CrossRef
====================================================================

DOMAIN_OVERVIEW:
This domain examines how philosophical theories of explanation -- particularly
mechanistic explanation from philosophy of science -- apply to understanding
AI systems. The central debate concerns whether mechanistic frameworks developed
for biological sciences (Craver 2007, Bechtel 2008) can illuminate how opaque
AI systems work, or whether AI demands new explanatory frameworks altogether.

Key positions: (1) Mechanistic interpretability advocates (Kästner & Crook 2024)
argue that coordinated discovery strategies from life sciences can uncover
functional organization of AI systems, providing genuine mechanistic understanding.
(2) XAI pragmatists (Fleisher 2022) treat explanations as idealized models that
provide understanding without complete accuracy. (3) Critics (Baron 2025, Smart
& Kasirzadeh 2024) argue mechanistic or model-centric explanations miss socio-
structural factors essential to AI behavior.

Recent work (2023-2025) shows increasing philosophical engagement with
mechanistic interpretability as an emerging ML paradigm, with philosophers
analyzing its epistemic foundations (Ayonrinde & Jaburi 2025, Williams et al.
2025) and limits (Méloux et al. 2025). This creates productive tension between
explanation-as-understanding (philosophical tradition) and explanation-for-
control (safety engineering).

RELEVANCE_TO_PROJECT:
This domain directly addresses the project's core question: what kind of
explanation does mechanistic interpretability provide, and is it the right kind
for AI safety? It connects mechanistic philosophy frameworks to contemporary MI
practice, revealing both theoretical foundations and conceptual challenges. The
domain is essential for understanding whether MI's mechanistic commitments are
philosophically well-grounded or require revision.

NOTABLE_GAPS:
Few papers bridge classical mechanistic philosophy (Machamer, Darden, Craver
2000) with specific MI techniques like circuit analysis or sparse autoencoders.
Limited analysis of whether AI "mechanisms" satisfy Craver's mutual manipulability
criteria. Insufficient discussion of how ML's stochasticity affects mechanistic
regularity requirements.

SYNTHESIS_GUIDANCE:
Organize around the central question: "What is the explanatory target in MI?"
Contrast mechanism-discovery (Kästner & Crook) with model-approximation
(Fleisher) approaches. Highlight how different philosophical commitments yield
different standards for explanatory success. Consider whether MI explanations
are ontic (about the model itself) or epistemic (about our understanding).

KEY_POSITIONS:
- Mechanistic Interpretability Framework: 3 papers - MI applies life science
  mechanistic discovery strategies to AI systems
- XAI as Understanding: 4 papers - Explanations provide understanding through
  idealized models, not complete accuracy
- Socio-Structural Critique: 2 papers - Model-centric explanations insufficient
  without social context
- Foundational Mechanisms: 4 papers - Classical mechanistic philosophy frameworks
- Epistemology of Opacity: 2 papers - Black-box AI poses unique epistemic challenges
====================================================================
}

@article{kastner2024explaining,
  author = {Kästner, Lena and Crook, Barnaby},
  title = {Explaining AI through Mechanistic Interpretability},
  journal = {European Journal for Philosophy of Science},
  year = {2024},
  volume = {14},
  number = {4},
  pages = {52},
  doi = {10.1007/s13194-024-00614-4},
  url = {https://doi.org/10.1007/s13194-024-00614-4},
  note = {
  CORE ARGUMENT: Recent XAI approaches use divide-and-conquer strategies that fail to illuminate how trained AI systems work as wholes. To achieve functional understanding needed for safety, AI researchers should adopt mechanistic interpretability—applying coordinated discovery strategies from life sciences to uncover functional organization of complex AI systems. The paper argues that mechanistic interpretability (MI) provides a more systematic framework than existing XAI methods by targeting how-actually questions rather than how-possibly explanations, drawing on Craver and Bechtel's mechanistic philosophy frameworks.

  RELEVANCE: This paper is the required centerpiece of Domain 3, directly connecting mechanistic explanation philosophy to MI practice. It provides the primary theoretical justification for why MI should be understood through mechanistic frameworks rather than other XAI paradigms. The paper's argument that functional understanding (not just behavioral prediction) is needed for safety directly addresses the research project's core concern about whether MI can deliver on its epistemic promises. Its discussion of coordinated discovery strategies provides a bridge between Craver's philosophical framework and actual ML research practices.

  POSITION: Mechanistic interpretability framework—argues for applying philosophy of science mechanistic explanation standards to evaluate and guide AI interpretability research. Represents the strongest current philosophical defense of MI as mechanistic explanation.
  },
  keywords = {mechanistic-interpretability, explanation, AI-safety, High}
}

@book{craver2007explaining,
  author = {Craver, Carl F.},
  title = {Explaining the Brain: Mechanisms and the Mosaic Unity of Neuroscience},
  publisher = {Oxford University Press},
  year = {2007},
  doi = {10.1093/acprof:oso/9780199299317.001.0001},
  url = {https://doi.org/10.1093/acprof:oso/9780199299317.001.0001},
  note = {
  CORE ARGUMENT: Mechanistic explanations in neuroscience explain phenomena by describing mechanisms—entities and activities organized to produce regular changes. Craver develops the "mutual manipulability" criterion for constitutive mechanistic explanation: components are explanatorily relevant when intervening on the component changes the mechanism's behavior and vice versa. The book argues neuroscience achieves unity not through reduction to fundamental laws but through integrated multilevel mechanistic explanations connecting phenomena across organizational levels.

  RELEVANCE: This is the foundational text for mechanistic explanation that Kästner & Crook explicitly draw upon. Craver's mutual manipulability criterion provides the philosophical standard against which MI's mechanistic claims should be evaluated. His framework for multilevel mechanisms is directly relevant to questions about whether MI techniques (circuit analysis, activation patching) actually identify genuine mechanistic components or merely predictive correlations. Understanding Craver's distinctions between etiological vs constitutive explanation helps clarify what type of understanding MI provides.

  POSITION: Foundational mechanistic philosophy—establishes the conceptual framework and criteria for what counts as mechanistic explanation in special sciences. This framework is now being applied to AI systems.
  },
  keywords = {mechanistic-explanation, philosophy-of-neuroscience, levels, High}
}

@book{bechtel2008mental,
  author = {Bechtel, William},
  title = {Mental Mechanisms: Philosophical Perspectives on Cognitive Neuroscience},
  publisher = {Routledge},
  year = {2008},
  doi = {10.4324/9780203810095},
  url = {https://doi.org/10.4324/9780203810095},
  note = {
  CORE ARGUMENT: Mechanistic explanation in cognitive neuroscience involves decomposing cognitive capacities into component operations and localizing them in brain structures, then recomposing to understand how organized components produce the capacity. Bechtel emphasizes that successful mechanistic explanation requires both decomposition (identifying parts) and dynamic recomposition (understanding temporal organization). He argues that computational and representational frameworks are compatible with mechanistic explanation when properly understood as characterizing operations within mechanisms.

  RELEVANCE: Bechtel's emphasis on decomposition-and-recomposition directly parallels MI's strategy of identifying neural network components (features, circuits) and understanding their organization. His discussion of when computational descriptions constitute genuine mechanistic explanations is crucial for evaluating whether MI's computational analyses of transformers provide mechanistic understanding or mere redescription. The book's treatment of how representation fits into mechanistic frameworks addresses questions about whether MI's "features" are genuinely representational or merely predictive patterns.

  POSITION: Foundational mechanistic philosophy—extends Craver's framework to cognitive mechanisms, emphasizing dynamic organization and reconciling computational and mechanistic perspectives.
  },
  keywords = {mechanistic-explanation, cognitive-science, decomposition, High}
}

@article{chalmers2025propositional,
  author = {Chalmers, David J.},
  title = {Propositional Interpretability in Artificial Intelligence},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2501.15740},
  doi = {10.48550/arXiv.2501.15740},
  arxivId = {2501.15740},
  url = {https://arxiv.org/abs/2501.15740},
  note = {
  CORE ARGUMENT: Mechanistic interpretability should target propositional interpretability—interpreting AI systems' mechanisms in terms of propositional attitudes (beliefs, desires, subjective probabilities). This contrasts with purely mechanistic or structural interpretations. The central challenge is "thought logging": creating systems that log all relevant propositional attitudes over time. Chalmers analyzes current MI methods (probing, sparse autoencoders, chain of thought) and philosophical interpretation methods (psychosemantics) to assess their utility for propositional interpretability, arguing that capturing propositional content is both more challenging and more valuable than purely mechanistic description.

  RELEVANCE: Chalmers's framework reveals a potential gap in mechanistic interpretability: MI may identify mechanisms without capturing propositional content that makes those mechanisms intelligible as cognitive processes. This matters for the research project because if AI safety requires understanding what systems "believe" or "intend," pure mechanistic explanation might be insufficient. His analysis of current MI techniques through the lens of propositional attitudes provides critical assessment tools. The paper's discussion of psychosemantic interpretation methods offers alternative or complementary approaches to mechanistic interpretation.

  POSITION: Propositional interpretation framework—argues that mechanistic interpretability requires, but isn't reducible to, identifying structural mechanisms; it must also interpret those mechanisms in terms of propositional content.
  },
  keywords = {propositional-attitudes, mechanistic-interpretability, philosophy-of-mind, High}
}

@article{williams2025mechanistic,
  author = {Williams, Iwan and Oldenburg, Ninell and Dhar, Ruchira and Hatherley, Joshua and Fierro, Constanza and Rajcic, Nina and Schiller, Sandrine R. and Stamatiou, Filippos and Søgaard, Anders},
  title = {Mechanistic Interpretability Needs Philosophy},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2506.18852},
  doi = {10.48550/arXiv.2506.18852},
  arxivId = {2506.18852},
  url = {https://arxiv.org/abs/2506.18852},
  note = {
  CORE ARGUMENT: Mechanistic interpretability research involves implicit assumptions, concepts, and explanatory strategies that require philosophical examination. The paper takes three open problems from MI literature as case studies, demonstrating how philosophical analysis can clarify concepts (what counts as a "mechanism"?), refine methods (what makes an explanation good?), and assess epistemic stakes (what understanding do we gain?). The authors argue philosophy should be an ongoing partner, not an afterthought, in MI research—helping to articulate what MI aims to achieve and how to evaluate whether it succeeds.

  RELEVANCE: This position paper directly addresses meta-level questions about the research project itself: what role should philosophy play in assessing MI? The paper's case studies (examining specific MI problems through philosophical lenses) provide methodological models for philosophical analysis of MI. Its argument that MI's concepts and standards are philosophically underdeveloped supports the project's premise that careful philosophical work is needed. The call for interdisciplinary dialogue between ML and philosophy validates the project's integrative approach.

  POSITION: Meta-philosophical position on MI—argues that MI's conceptual foundations and success criteria require philosophical clarification, making philosophy essential rather than optional for MI progress.
  },
  keywords = {mechanistic-interpretability, philosophy-of-science, interdisciplinary, High}
}

@inproceedings{ayonrinde2025mathematical,
  author = {Ayonrinde, Kola and Jaburi, Louis},
  title = {A Mathematical Philosophy of Explanations in Mechanistic Interpretability---The Strange Science Part I.i},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2025},
  doi = {10.48550/arXiv.2505.00808},
  arxivId = {2505.00808},
  url = {https://arxiv.org/abs/2505.00808},
  note = {
  CORE ARGUMENT: Mechanistic Interpretability should be defined as producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable (MOCF) explanations. The authors propose the "Explanatory View Hypothesis": neural networks contain implicit explanations that can be extracted. This implies Explanatory Faithfulness (how well explanations fit models) is well-defined. They formulate the "Principle of Explanatory Optimism"—a conjecture that successful MI requires assuming neural networks are structured in ways amenable to mechanistic explanation. The paper provides philosophical foundations for distinguishing MI from other interpretability paradigms and delineating MI's inherent limits.

  RELEVANCE: This paper directly tackles the epistemic status of MI explanations through philosophical analysis. The MOCF framework provides criteria for evaluating what MI does and should deliver—essential for the research project's task of assessing MI's philosophical foundations. The Explanatory View Hypothesis makes explicit a key assumption underlying MI practice, allowing critical evaluation. The limits discussion (what MI cannot do) is as important as capabilities discussion for AI safety applications. The paper's formal approach connects philosophy of explanation to MI's technical implementation.

  POSITION: Foundational epistemology of MI—provides formal philosophical framework for defining MI's explanatory goals and evaluating its epistemological commitments.
  },
  keywords = {mechanistic-interpretability, explanation, epistemology, High}
}

@inproceedings{geiger2023causal,
  author = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah D. and Potts, Christopher and Icard, Thomas F.},
  title = {Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability},
  year = {2023},
  arxivId = {2301.04709},
  url = {https://arxiv.org/abs/2301.04709},
  note = {
  CORE ARGUMENT: Causal abstraction provides theoretical foundations for mechanistic interpretability by generalizing from mechanism replacement to arbitrary mechanism transformation (functionals mapping old mechanisms to new). This framework formalizes core MI concepts: polysemantic neurons, linear representation hypothesis, modular features, and graded faithfulness. The paper unifies diverse MI methods (activation/path patching, causal mediation, circuit analysis, sparse autoencoders, etc.) within causal abstraction formalism, showing they share common theoretical structure despite surface differences.

  RELEVANCE: This paper provides the technical-philosophical bridge between causal reasoning frameworks and MI practice. By showing how MI methods implicitly use causal abstraction, it makes explicit the philosophical commitments of MI techniques. The formalization of "faithfulness" addresses a central epistemic question: when do explanations accurately represent what models do? The unification of MI methods reveals underlying conceptual coherence that supports treating MI as a genuine research paradigm rather than ad hoc collection of techniques. For AI safety, understanding causal abstraction clarifies what guarantees (and limitations) MI provides.

  POSITION: Technical-philosophical foundations—provides formal framework connecting causal reasoning philosophy to MI implementation, enabling rigorous evaluation of MI's epistemic status.
  },
  keywords = {causal-abstraction, mechanistic-interpretability, formal-methods, High}
}

@article{fleisher2022understanding,
  author = {Fleisher, Will},
  title = {Understanding, Idealization, and Explainable AI},
  journal = {Episteme},
  year = {2022},
  volume = {19},
  pages = {534--560},
  doi = {10.1017/epi.2022.39},
  url = {https://doi.org/10.1017/epi.2022.39},
  note = {
  CORE ARGUMENT: XAI methods should be evaluated as providing understanding rather than literal explanations. The "rationalization objection" claims XAI methods provide rationalizations not genuine explanations because they use separate "explanation systems" approximating original models. Fleisher defuses this by treating XAI methods as idealized scientific models: like models in science, they misrepresent their targets yet provide significant genuine understanding. Idealizations trade accuracy for tractability and comprehensibility—a legitimate epistemic tradeoff when understanding is the goal.

  RELEVANCE: This paper challenges the assumption that MI must provide literally accurate mechanistic explanations to be valuable. If understanding (not perfect accuracy) is the epistemic goal, then MI's approximations might be features not bugs. This reframes evaluation criteria: instead of asking "does MI correctly describe mechanisms?" ask "does MI provide understanding that supports reasoning and prediction?" For AI safety, the question becomes whether idealized-but-approximate mechanistic models provide sufficient understanding for safety validation. Fleisher's framework accommodates MI's practical reality: circuits and features are simplified models, not complete mechanism descriptions.

  POSITION: Understanding-focused XAI framework—argues explanations should be evaluated by understanding they provide, not literal accuracy, treating MI as idealized modeling rather than complete mechanistic description.
  },
  keywords = {understanding, XAI, idealization, Medium}
}

@article{smart2024beyond,
  author = {Smart, Andrew and Kasirzadeh, Atoosa},
  title = {Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning},
  journal = {AI {\&} Society},
  year = {2024},
  volume = {40},
  pages = {2045--2053},
  doi = {10.1007/s00146-024-02056-1},
  arxivId = {2409.03632},
  url = {https://doi.org/10.1007/s00146-024-02056-1},
  note = {
  CORE ARGUMENT: Interpreting ML outputs in normatively salient domains requires "socio-structural explanations" that show how social structures contribute to and partially explain model outputs. Model-centric explanations (mechanistic or non-mechanistic) are insufficient because ML models are embedded within and shaped by social structures. Using a racially biased healthcare algorithm as case study, the authors demonstrate that understanding model outputs requires analyzing social factors (institutional racism, healthcare access disparities) beyond the model itself. This demands transparency beyond model interpretability.

  RELEVANCE: This paper challenges the sufficiency of mechanistic interpretability for AI safety by arguing that even complete mechanistic understanding of model internals wouldn't explain outputs in social contexts. For the research project, it raises the question: is MI's mechanistic framework too narrow for AI safety, which inevitably involves social deployment? The socio-structural critique doesn't reject mechanistic explanation but contextualizes it as one explanatory layer among several needed. This suggests MI's explanatory scope limitations rather than failures—important for calibrating expectations about what MI can deliver for safety.

  POSITION: Socio-structural critique—argues model-centric explanations (including mechanistic) must be supplemented with socio-structural analysis for comprehensive understanding in deployment contexts.
  },
  keywords = {XAI, social-structures, bias, Medium}
}

@article{meloux2025everything,
  author = {Méloux, Maxime and Maniu, Silviu and Portet, François and Peyrard, Maxime},
  title = {Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2502.20914},
  doi = {10.48550/arXiv.2502.20914},
  arxivId = {2502.20914},
  url = {https://arxiv.org/abs/2502.20914},
  note = {
  CORE ARGUMENT: Drawing on identifiability from statistics, the paper examines whether MI explanations are unique. Testing two MI strategies—"where-then-what" (isolate circuit, then interpret) and "what-then-where" (find algorithm, then locate implementation)—on Boolean functions and small MLPs reveals systematic non-identifiability: multiple circuits replicate behavior, circuits have multiple interpretations, algorithms align with different subspaces. The paper questions whether uniqueness is necessary for understanding or whether predictive/manipulability standards suffice, referencing inner interpretability framework's multiple validation criteria.

  RELEVANCE: This paper directly challenges MI's epistemic foundations by demonstrating that mechanistic explanations are underdetermined by behavioral evidence. For AI safety, non-identifiability is problematic: if multiple incompatible mechanistic explanations fit the same model, which should guide safety interventions? The paper's connection to identifiability in statistics provides formal framework for understanding MI's epistemic limits. The discussion of whether uniqueness is necessary for understanding addresses fundamental questions about MI's explanatory standards. If non-unique explanations can still support safety, what additional criteria distinguish good from bad non-unique explanations?

  POSITION: Identifiability critique of MI—demonstrates systematic non-uniqueness of mechanistic explanations, raising questions about MI's explanatory standards and epistemic reliability.
  },
  keywords = {mechanistic-interpretability, identifiability, epistemology, Medium}
}

@article{vilas2024position,
  author = {Vilas, Martín and Adolfi, Federico and Poeppel, David and Roig, Gemma},
  title = {Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience},
  booktitle = {International Conference on Machine Learning},
  year = {2024},
  doi = {10.48550/arXiv.2406.01352},
  arxivId = {2406.01352},
  url = {https://arxiv.org/abs/2406.01352},
  note = {
  CORE ARGUMENT: Inner interpretability faces criticisms (lack of ground truth, unclear standards) that resemble challenges faced in cognitive neuroscience. By drawing parallels, the paper proposes a conceptual framework for building mechanistic explanations in AI interpretability research: distinguish phenomena from mechanisms, use multiple complementary methods, validate through convergent evidence, and iterate between data and theory. This framework helps position inner interpretability on a productive path by learning from cognitive neuroscience's methodological maturity in studying opaque systems (brains).

  RELEVANCE: The neuroscience-AI analogy is directly relevant because both Craver and Bechtel developed mechanistic frameworks primarily for neuroscience. This paper tests whether those frameworks transfer to AI by examining whether neuroscience's methodological lessons apply. For the research project, it provides concrete methodological guidance grounded in successful mechanistic explanation practice. The paper's emphasis on convergent evidence addresses the identifiability problem: non-unique explanations can be constrained through multiple independent validation methods. The framework offers pragmatic standards for evaluating MI work beyond pure theoretical considerations.

  POSITION: Neuroscience-inspired MI methodology—argues MI should adopt cognitive neuroscience's multi-method convergent approach to mechanistic explanation, providing practical framework for MI research.
  },
  keywords = {mechanistic-interpretability, cognitive-neuroscience, methodology, Medium}
}

@article{baron2025trust,
  author = {Baron, Sam},
  title = {Trust, Explainability and AI},
  journal = {Philosophy {\&} Technology},
  year = {2025},
  volume = {38},
  doi = {10.1007/s13347-024-00837-6},
  url = {https://doi.org/10.1007/s13347-024-00837-6},
  note = {
  CORE ARGUMENT: The common claim that explainability is necessary for trust in AI is examined through different notions of trust. For some trust concepts (interpersonal trust requiring shared values), explainability may be necessary, but these trust forms aren't appropriate for AI. For trust notions appropriate for AI (reliability-based trust), explainability isn't necessary—performance evidence can establish trust without understanding how systems work. Thus explainability is not necessary for trust in AI that matters.

  RELEVANCE: This paper challenges instrumental justifications for MI based on trust. If MI is pursued to establish trust, but trust doesn't require explainability, this undermines one major motivation. However, the argument cuts both ways: if AI safety requires more than trust (requires understanding for intervention), then MI may be necessary even if not for trust. The paper's analysis of different trust concepts helps clarify what epistemic relationship we want with AI systems. For the research project, it raises the question: what is MI for? If not trust, what epistemic or practical goal justifies the effort?

  POSITION: Trust-skeptical position on XAI—argues explainability isn't necessary for appropriate trust in AI, challenging common instrumental justifications for interpretability research.
  },
  keywords = {trust, XAI, instrumental-value, Low}
}

@article{paez2024axe,
  author = {Páez, Andrés},
  title = {Axe the X in XAI: A Plea for Understandable AI},
  journal = {ArXiv},
  year = {2024},
  volume = {abs/2403.00315},
  doi = {10.48550/arXiv.2403.00315},
  arxivId = {2403.00315},
  url = {https://arxiv.org/abs/2403.00315},
  note = {
  CORE ARGUMENT: The term "explanation" in XAI bears little resemblance to traditional scientific explanation (DN, causal-mechanical, mechanistic models). Attempts to apply philosophy of science explanation models to deep neural networks fail because DNNs don't fit the structural requirements. Instead of "explainable AI," we should pursue "understandable AI," evaluating systems by understanding they provide for using them and drawing correct inferences—a pragmatic conception following Kuorikoski & Ylikoski. This reframes evaluation from "does it explain?" to "does it support understanding?"

  RELEVANCE: Páez's critique targets the conceptual foundations of applying mechanistic explanation frameworks to AI. If DNNs don't meet criteria for genuine mechanisms, then "mechanistic interpretability" may be misnomer. However, the shift from explanation to understanding (similar to Fleisher) might accommodate MI by loosening standards: MI provides understanding through mechanism-like models even if not literal mechanisms. For the research project, this raises definitional questions: should MI be held to strict mechanistic explanation standards, or is "mechanistic-style understanding" sufficient? The distinction matters for evaluating success criteria.

  POSITION: Terminological critique—argues "explanation" is inappropriate term for XAI given mismatch with philosophy of science standards; proposes "understanding" as alternative evaluative framework.
  },
  keywords = {XAI, understanding, philosophy-of-science, Medium}
}

@article{boge2021two,
  author = {Boge, Florian J.},
  title = {Two Dimensions of Opacity and the Deep Learning Predicament},
  journal = {Minds and Machines},
  year = {2021},
  volume = {32},
  pages = {43--75},
  doi = {10.1007/s11023-021-09569-4},
  url = {https://doi.org/10.1007/s11023-021-09569-4},
  note = {
  CORE ARGUMENT: Deep neural networks exhibit two dimensions of opacity: (1) Instrumental opacity—trained DNNs are non-explanatory models useful only for prediction, and (2) Automated generation opacity—their unsupervised learning process is uniquely opaque. This combination creates unprecedented gap between discovery and explanation: when DNNs successfully identify patterns in exploratory contexts, scientists face new challenges in forming concepts required for understanding underlying mechanisms. The opacity isn't merely computational complexity but fundamental epistemic challenge.

  RELEVANCE: Boge's two-dimensional opacity framework helps diagnose why MI is difficult and what it must overcome. Instrumental opacity explains why DNNs are "black boxes"—not just complex but structured for prediction rather than explanation. Automated generation opacity explains why post-hoc interpretability is challenging—the learning process doesn't track explanatory structures. For the research project, this framework clarifies what mechanistic interpretability must achieve: overcome both opacity dimensions by recovering explanatory structure from prediction-optimized systems. The paper suggests MI's difficulty isn't contingent but reflects deep epistemic challenges.

  POSITION: Epistemology of opacity—analyzes why DNNs are opaque in ways requiring new approaches to understanding, contextualizing MI's challenges within broader epistemic framework.
  },
  keywords = {opacity, deep-learning, epistemology, Medium}
}

@incollection{felline2021mechanistic,
  author = {Felline, Laura},
  title = {Mechanistic Explanation in Physics},
  booktitle = {The Routledge Companion to Philosophy of Physics},
  year = {2021},
  publisher = {Routledge},
  doi = {10.4324/9781315623818-44},
  url = {https://doi.org/10.4324/9781315623818-44},
  note = {
  CORE ARGUMENT: Mechanistic explanation (ME) can apply to physics, but with important limitations. Some physics domains are incompatible with mechanistic reasoning (quantum mechanics at fundamental level), while others benefit from ME (classical mechanics, thermodynamics). ME in physics shows "how things work" through processes and entities, departing from law-based covering-law models. The paper argues comprehensive account of explanation in physics requires ME but cannot dispense with other explanation types—mechanistic and nomological explanations are complementary, not competing.

  RELEVANCE: This paper addresses whether mechanistic explanation applies outside its original biological domain. For AI, which involves computational/mathematical structures more similar to physics than biology, Felline's analysis is relevant: if ME has limited scope even in physics, what limits might apply to computational systems? The argument that ME and nomological explanation are complementary suggests MI might need supplementation with other explanation types (computational, algorithmic). The paper's domain-specificity thesis cautions against assuming mechanistic frameworks universally apply—they may fit some AI architectures better than others.

  POSITION: Scope analysis of mechanistic explanation—argues ME applies to physics but with domain limitations, suggesting mechanistic frameworks aren't universal and may require complementary explanation types.
  },
  keywords = {mechanistic-explanation, physics, scope, Low}
}
