# Research Proposal for Literature Review

## Research Question
Is Mechanistic Interpretability necessary or sufficient for AI Safety?

## Topic Area
Explainable AI (XAI), AI Interpretability, and AI Safety

## Target Audience
Analytic philosophers and journal editors in philosophy of science

## Purpose and Context

This literature review supports a research paper in analytical philosophy, ethics, and philosophy of science. The paper aims to clear up apparent confusion in the existing literature regarding:

1. **Definitional confusion:** The meaning of "mechanistic interpretability" varies significantly across sources
2. **Role confusion:** The relationship between Mechanistic Interpretability (MI) and AI Safety is disputed

## Key Papers Illustrating the Confusion

### Hendrycks and Hiscott (2025)
- **Citation:** Hendrycks, Dan, and Laura Hiscott. "The Misguided Quest for Mechanistic AI Interpretability." AI Frontiers, May 15, 2025. https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability
- **Definition of MI:** Activations of individual nodes or clusters of nodes in a neural network model (narrow, low-level definition)

### Kästner and Crook (2024)
- **Citation:** Kästner, Lena, and Barnaby Crook. "Explaining AI through Mechanistic Interpretability." European Journal for Philosophy of Science 14, no. 4 (2024): 52. https://doi.org/10.1007/s13194-024-00614-4
- **Definition of MI:** Includes functional and higher-level explanations as part of mechanistic interpretability (broader definition, contradicts Hendrycks and Hiscott)
- **Claims about MI and Safety:** 
  - Suggests MI is **necessary** for AI safety (in abstract)
  - Suggests MI is **sufficient** for AI safety (writing "MI enables us to meet desirable social desiderata including safety")

## Two Core Objectives for the Literature Review

1. **Clarify the definition of "mechanistic interpretability"** — Survey how different authors and communities define MI, identify the sources of definitional disagreement, and propose conceptual distinctions

2. **Assess how mechanistic interpretability relates to AI Safety** — Evaluate claims that MI is necessary and/or sufficient for AI safety, identify what other factors may be relevant, and assess the strength of arguments on various sides

## Scope and Sources

- **Temporal scope:** Focus on papers published between 2023 and today (late 2025)
- **Disciplinary scope:** Interdisciplinary philosophy literature, including:
  - Philosophy of science
  - Philosophy of mind
  - Ethics and AI ethics
  - Technical AI/ML papers on interpretability (especially arXiv)
  - AI safety research

## Working Directory

/Users/johannes/github_repos/philo-sota/reviews/mechanistic-interpretability-ai-safety/

## Expected Outputs

1. Domain-specific BibTeX files with verified citations
2. A structured synthesis outline emphasizing key debates and research gaps
3. A complete literature review (3000-4000 words) suitable for an analytical philosophy audience

## Planning Instructions

Please create a comprehensive literature review plan that:
- Decomposes this research into 5-7 distinct domains
- Provides concrete search strategies for each domain
- Uses philosophy-research skill scripts extensively (SEP, PhilPapers, Semantic Scholar, OpenAlex, arXiv)
- Balances philosophical foundations with recent technical work
- Prioritizes papers from 2023-2025 but includes foundational earlier works where essential
- Addresses both core objectives: definitional clarity and the necessity/sufficiency question

Output the plan to: lit-review-plan.md
