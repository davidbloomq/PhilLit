---
title: "Is Mechanistic Interpretability Necessary or Sufficient for AI Safety? A State-of-the-Art Review"
date: 2025-12-28
author: Generated with Claude Code
bibliography: references.bib
keywords:
  - mechanistic interpretability
  - AI safety
  - explainable AI
  - philosophy of science
  - alignment
abstract: |
  This review examines whether mechanistic interpretability (MI) is necessary or sufficient for AI safety. Neither claim is well-supported by current literature. MI is one valuable tool among many, facing both principled limitations (complexity mismatch, non-identifiability, faithfulness problems) and practical constraints (scalability, intervention-coherence tradeoffs). A pluralistic safety framework incorporating MI alongside formal verification, scalable oversight, and representation engineering offers the most defensible position.
---

# Is Mechanistic Interpretability Necessary or Sufficient for AI Safety? A State-of-the-Art Review

## Introduction

As artificial intelligence systems grow more powerful and are deployed in increasingly critical domains, ensuring their alignment with human values has become an urgent scientific and philosophical challenge. The interpretability promise is alluring: if we could understand how AI systems work internally—reverse-engineering their computational mechanisms into comprehensible algorithms—we might verify their safety, control their behavior, and align them with human intentions. Mechanistic interpretability (MI), which aims to explain neural network behavior through causal analysis of internal mechanisms rather than post-hoc rationalizations, has emerged as a distinctive research program within explainable AI (Bereska & Gavves, 2024; Olah et al., 2020). Proponents argue that MI's bottom-up approach to uncovering functional organization provides the kind of understanding required for genuine safety assurance (Kästner & Crook, 2024). But does mechanistic understanding constitute a necessary condition for AI safety? Is it sufficient? Or is MI one valuable tool among many in a pluralistic safety framework?

This review examines whether strong necessity and sufficiency claims for mechanistic interpretability in AI safety are well-supported by current literature. The central question involves contested concepts—mechanism, explanation, understanding, safety—that require philosophical clarification alongside empirical investigation. Three considerations motivate this inquiry. First, the definitional landscape: "mechanistic interpretability" refers to both a narrow technical approach (reverse-engineering causal mechanisms through interventions) and a broader cultural movement within machine learning, creating ambiguity about what exactly is claimed to be necessary or sufficient (Saphra & Wiegreffe, 2024). Second, the philosophical foundations: while MI explicitly draws on mechanistic explanation frameworks from philosophy of science (Craver, 2007; Machamer et al., 2000), most technical work engages these foundations only superficially, leaving mechanistic claims underdetermined (Williams et al., 2025). Third, the empirical evidence: recent studies reveal intervention-coherence tradeoffs where understanding mechanisms does not straightforwardly enable safe control (Bhalla et al., 2024), and impossibility results suggest principled limits to what any interpretability approach can achieve (Panigrahy & Sharan, 2025).

Alternative frameworks complicate the picture further. Chalmers (2025) proposes propositional interpretability—understanding AI through propositional attitudes (beliefs, desires) rather than mechanistic structures—raising the question of whether safety requires mechanistic or propositional understanding. Representation engineering (Zou et al., 2023) demonstrates safety interventions on high-level representations without decomposing mechanisms. Scalable oversight approaches (Sang et al., 2024; Kim et al., 2024) and formal verification methods (Lopez et al., 2023) achieve safety properties without requiring interpretability. These alternatives challenge MI's unique necessity by showing that multiple paths to safety exist.

This review proceeds in four sections. Section 1 establishes the definitional landscape, revealing how polysemy and incomplete engagement with mechanistic philosophy undermine clear necessity/sufficiency claims. Section 2 evaluates the necessity debate, finding that functional understanding arguments have force but face viable alternatives. Section 3 assesses sufficiency claims, documenting both theoretical impossibility results and empirical limitations that challenge strong sufficiency positions. The review concludes by identifying research gaps where philosophical analysis can contribute and argues for a pluralistic safety framework in which MI plays a valuable but bounded role.

## Section 1: Definitional Landscape and Conceptual Foundations

### 1.1 Narrow vs. Broad Definitions of MI

What counts as "mechanistic interpretability" remains contested, with consequences for evaluating necessity and sufficiency claims. Saphra and Wiegreffe (2024) identify four distinct uses of "mechanistic" in interpretability research: at the narrow technical end, MI requires explicit causality claims about internal mechanisms verified through intervention experiments; at the broad cultural end, "mechanistic" encompasses any work exploring model internals, including non-causal analysis. This polysemy reflects substantive methodological differences rather than mere terminological confusion. The traditional NLP interpretability community and the "mechanistic interpretability" movement emerging from Distill/Anthropic developed separately, creating what Saphra and Wiegreffe call a "critical divide" within interpretability research.

Recent definitional work attempts precision. Bereska and Gavves (2024), in their comprehensive review, define MI as "reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding" (p. 2). This definition emphasizes three commitments: decomposition into components (mechanisms and representations), translation into human concepts, and causal understanding verified through intervention. Ayonrinde and Jaburi (2025) offer the most philosophically explicit definition to date: MI produces "Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations" (MOCF). Their framework distinguishes MI from other interpretability paradigms by requiring explanations that target the model itself (model-level), concern actual mechanisms rather than merely predictive models (ontic), establish causal relationships (causal-mechanistic), and make testable predictions (falsifiable).

The narrow-vs-broad tension matters for assessing necessity and sufficiency. If "MI" means simply exploring model internals, then many safety-relevant activities (probing representations, analyzing attention patterns) count as MI, diluting claims about its unique value. If MI requires the stringent MOCF criteria, then much work labeled "mechanistic interpretability" fails to qualify, and scalability challenges become more severe. Rai et al. (2024) attempt middle ground with a task-centric taxonomy organizing MI around research questions (understanding features, circuits, training dynamics) rather than method definitions. This practical approach reveals MI's scope but does not resolve the conceptual question of what makes interpretation genuinely mechanistic.

The implication for necessity/sufficiency assessment is clear: claims that "MI is necessary for safety" mean very different things depending on whether "MI" encompasses all internal analysis or only work meeting strict mechanistic criteria. Proponents and critics may talk past each other by operating with incompatible definitions.

### 1.2 Philosophical Foundations in Mechanistic Explanation

Mechanistic interpretability explicitly draws on mechanistic explanation frameworks from philosophy of science, yet the connection remains undertheorized. The "new mechanist" tradition in philosophy, particularly Craver's (2007) influential account, defines mechanisms as entities and activities organized to produce regular changes from start to termination conditions. Craver develops the mutual manipulability criterion for constitutive relevance: components genuinely explain a mechanism's behavior when intervening on the component changes the mechanism's output and vice versa. Bechtel (2008) emphasizes decomposition-localization-recomposition as mechanistic explanation's core strategy: break systems into parts, locate functions in those parts, then understand how organized parts produce system-level capacities.

Kästner and Crook (2024) provide the most sustained argument connecting mechanistic philosophy to AI interpretability. They contend that traditional XAI's "divide-and-conquer" approaches fail to illuminate how AI systems function as integrated wholes, and that functional understanding required for safety demands coordinated discovery strategies from life sciences adapted to AI contexts. Their argument positions mechanistic interpretability as methodologically distinct from correlation-based explanation: MI seeks how-actually mechanisms rather than how-possibly models. However, even this philosophically sophisticated defense leaves open whether MI practitioners actually satisfy Craver's criteria. Activation patching—a central MI technique—performs interventions, but do these interventions establish genuine constitutive relevance in Craver's sense, or merely statistical dependency?

Geiger et al. (2023) attempt rigorous formalization through causal abstraction, generalizing from mechanism replacement to mechanism transformation. Their framework unifies diverse MI methods (activation patching, circuit analysis, sparse autoencoders) within a common causal abstraction language and provides precise definitions of polysemanticity, features, and faithfulness. This theoretical work makes MI's commitments more explicit and enables evaluation of whether discovered "mechanisms" are genuine or artifacts. Yet as Sutter et al. (2025) demonstrate, unrestricted causal abstraction becomes vacuous—any neural network can be mapped to any algorithm using sufficiently powerful alignment maps. This reveals that MI's principled foundations depend on additional assumptions (like the linear representation hypothesis) that may not hold.

Williams et al. (2025) argue more directly that MI "needs philosophy"—the field's concepts, methods, and success criteria require philosophical clarification that most technical work lacks. Without explicit engagement with mechanistic explanation standards, MI risks producing mechanistic-sounding descriptions without genuine mechanistic understanding. The gap matters for safety: if we cannot distinguish genuine mechanistic explanations from superficial structural descriptions, we cannot know whether MI delivers the understanding safety requires.

### 1.3 Alternative Frameworks: Propositional and Representational

Mechanistic interpretability assumes that understanding causal mechanisms is the right explanatory target for AI safety. But this assumption can be questioned. Chalmers (2025) proposes propositional interpretability as an alternative framework: interpreting AI systems through propositional attitudes—beliefs, desires, subjective probabilities directed at propositions. Chalmers argues that propositional attitudes are central to human cognition and will be central for understanding AI. The key challenge he identifies is "thought logging": creating systems that log all relevant propositional attitudes over time. This differs fundamentally from mechanistic analysis—rather than asking what causal pathways produce behavior, propositional interpretability asks what propositional content the system represents.

The distinction matters for safety. If dangerous AI behaviors like deception or power-seeking are characterized by propositional attitudes (what the system believes or intends), then mechanistic analysis of computational structures may be insufficient. Knowing which circuits activate doesn't tell us what propositional content those circuits encode without additional interpretive principles connecting structure to content. Chalmers evaluates current MI methods (probing, sparse autoencoders) as potential tools for propositional interpretability but finds them incomplete—they identify where information is encoded without fully specifying what propositional content is represented.

Fleisher (2022) offers yet another perspective, arguing that XAI methods should be evaluated as providing understanding rather than literal explanations. Like idealized models in science, interpretability methods may systematically misrepresent their targets yet provide genuine understanding by making systems tractable and comprehensible. This reframes evaluation criteria: instead of asking whether MI correctly describes mechanisms, we ask whether MI provides understanding that supports reasoning and prediction. For safety, the question becomes whether approximate mechanistic models provide sufficient understanding for safety validation, even if they aren't perfectly accurate.

The relationship between mechanistic, propositional, and understanding-focused frameworks remains unclear. Are they complementary approaches targeting different aspects of AI systems? Are they incompatible alternatives? The literature has not systematically addressed whether safety requires mechanistic understanding, propositional understanding, or some combination. This leaves open the possibility that MI is neither necessary (propositional understanding might suffice) nor sufficient (mechanistic analysis without propositional content may be inadequate) for safety.

## Section 2: The Necessity Debate

### 2.1 The Functional Understanding Argument

The strongest necessity argument holds that safety requires functional understanding of AI systems as integrated wholes, which only mechanistic interpretability can provide. Kästner and Crook (2024) articulate this position philosophically: traditional XAI methods decompose systems into parts and explain those parts in isolation, but safety desiderata—verification, control, alignment—require understanding how parts coordinate to produce system-level behavior. Divide-and-conquer explanation leaves crucial questions unanswered: how do components interact? What emergent properties arise from their organization? MI's coordinated discovery strategies, adapted from life sciences, address these questions by targeting functional organization.

Bereska and Gavves (2024) provide the technical elaboration. They argue that MI enables three safety-critical capabilities: (1) understanding—revealing what models have learned and why they behave as they do; (2) control—identifying causal levers for intervention; and (3) alignment verification—checking whether internal representations match intended values. Behavioral testing alone cannot provide these capabilities because it examines only input-output relationships without accessing internal mechanisms. Two models with identical behavioral profiles may implement different algorithms with different failure modes and different generalization properties (Lehalleur et al., 2025). Only mechanistic analysis can distinguish them.

Domain-specific arguments strengthen the case. Lee (2025) examines nuclear reactor safety, where regulatory frameworks (10 CFR 50 Appendix B) mandate verification that behavioral testing cannot provide. Integrating LLMs into nuclear engineering "necessitates a deep understanding of their internal reasoning processes" (p. 1) to meet these requirements. Lee demonstrates that MI techniques can identify specialized neurons causally responsible for task performance, providing "a pathway towards achieving nuclear-grade artificial intelligence assurance" (p. 1). This suggests that for safety-critical domains with stringent verification requirements, MI may not merely be valuable but genuinely necessary.

Sengupta et al. (2025) extend the necessity argument to governance contexts. They contend that frontier AI systems require verification of internal alignment, not just behavioral compliance, and that MI provides the "technical substrate" for such verification. Behavioral testing cannot distinguish genuinely aligned systems from those that behave properly but harbor misaligned representations. Governance mechanisms like auditing and oversight require causal evidence about model behavior that only interpretability can supply. This positions MI as necessary infrastructure for responsible AI deployment.

These arguments have genuine force. If safety requires understanding functional organization, verifying internal alignment, and distinguishing behaviorally equivalent systems with different failure modes, then some form of mechanistic analysis appears necessary. However, three considerations complicate strong necessity claims. First, the arguments often conflate "mechanistic understanding is valuable" with "mechanistic understanding is necessary"—showing MI provides benefits does not establish that safety is impossible without it. Second, the arguments assume no alternative approaches can provide equivalent capabilities, an assumption the next subsection challenges. Third, even granting MI's necessity for some safety properties (understanding failure modes) does not establish necessity for all safety properties (behavioral robustness), suggesting domain-specific rather than universal necessity.

### 2.2 Challenges to Necessity from Alternative Approaches

Multiple approaches achieve safety-relevant goals without requiring mechanistic interpretability, challenging universal necessity claims. Representation engineering (RepE) offers perhaps the clearest alternative. Zou et al. (2023) introduce RepE as a "top-down" approach placing population-level representations at the center of analysis rather than individual neurons or circuits. RepE monitors and manipulates high-level cognitive phenomena—honesty, harmlessness, power-seeking—through population statistics without decomposing mechanisms. Crucially, RepE demonstrates successful safety interventions on exactly the properties MI claims to address. Hendrycks and Hiscott (2025) argue more forcefully that RepE supersedes MI: where MI faces insurmountable complexity analyzing individual components, RepE operates at the statistical level where safety-relevant properties naturally reside.

Scalable oversight provides another necessity challenge. Sang et al. (2024) demonstrate weak-to-strong generalization where strong student models exceed weak teacher capabilities despite teachers lacking mechanistic understanding of students. Through ensemble learning and structured feedback (debate, interaction), weak supervisors can effectively align stronger systems. This recursive improvement mechanism—weak models provide oversight, strong models learn and improve, improved models become new supervisors—suggests safety can scale beyond human comprehension limits without requiring interpretability. Kim et al. (2024) extend this analysis to artificial superintelligence (ASI), arguing that current alignment paradigms including MI are fundamentally inadequate for superhuman systems. If AI capabilities exceed human cognitive capacities, mechanistic understanding becomes impossible in principle, necessitating oversight mechanisms that bypass interpretability requirements.

Formal verification offers yet another path. Lopez et al. (2023) present the Neural Network Verification (NNV) tool, which provides mathematical guarantees of safety properties without requiring mechanistic understanding. Using reachability analysis and SMT solvers, NNV verifies properties (safety, robustness) by proving they hold for all inputs in specified domains. Guidotti et al. (2023) demonstrate this approach in predictive maintenance—a safety-critical domain—showing that SMT-based verification can provide reliability guarantees sufficient for real-world deployment without interpretability. If safety properties can be mathematically proven, the argument goes, mechanistic understanding may be unnecessary.

The existence of these alternatives does not definitively refute MI's necessity—perhaps MI provides capabilities no alternative matches. But the alternatives reveal that "safety requires understanding mechanisms" is not self-evident. Different safety approaches target different properties: formal verification establishes guarantees for specified properties but requires knowing what to verify; scalable oversight aligns behavior without mechanistic transparency but depends on feedback quality; representation engineering manipulates high-level features but may miss fine-grained failure modes. This suggests MI's necessity may be domain-specific and property-specific rather than universal.

### 2.3 Domain-Specificity and Contextual Necessity

The strongest position may be neither universal necessity nor complete dispensability but contextual necessity: MI is necessary for some safety properties in some domains but not others. Vijayaraghavan and Badea (2023) introduce "Minimum Levels of Interpretability" (MLI)—the threshold transparency needed for trustworthy deployment of AI systems. They argue interpretability requirements vary by construction method, deployment context, and oversight mechanisms. High-stakes domains with complex failure modes (autonomous weapons, medical diagnosis) may require deep mechanistic understanding, while lower-stakes applications (recommendation systems) may achieve adequate safety through behavioral constraints and monitoring.

Smart and Kasirzadeh (2024) challenge MI's sufficiency from a different angle: even complete mechanistic understanding of model internals cannot explain outputs in social contexts without socio-structural analysis. Using racially biased healthcare algorithms as a case study, they demonstrate that understanding model behavior requires analyzing social factors (institutional racism, access disparities) beyond the model itself. This suggests MI's explanatory scope is inherently limited—social deployment involves causal factors that mechanistic analysis of computational structures cannot capture.

Gyevnar and Kasirzadeh (2025) synthesize these considerations into a pluralistic AI safety framework. Their systematic literature review reveals extensive work addressing diverse safety concerns (adversarial robustness, fairness, privacy, interpretability) through multiple complementary approaches. They argue for "epistemically inclusive and pluralistic conception of AI safety" that accommodates diverse perspectives and methods (p. 2). The implication: no single technique, including MI, is uniquely necessary because safety emerges from multiple coordinated interventions rather than any single approach.

This contextual view reframes the necessity question. Rather than asking "Is MI necessary for AI safety?", we should ask: "For which safety properties, in which domains, under which constraints, is MI necessary versus merely valuable?" Nuclear reactor control (Lee, 2025) may genuinely require mechanistic verification. Alignment assurance for frontier models (Sengupta et al., 2025) may benefit decisively from internal transparency. But content moderation or spam detection may achieve adequate safety through behavioral testing and statistical monitoring. Universal necessity claims overreach by treating heterogeneous safety challenges as uniform.

## Section 3: The Sufficiency Debate

### 3.1 Theoretical Limitations

Even if mechanistic interpretability were necessary for AI safety, strong sufficiency claims face principled theoretical challenges. Panigrahy and Sharan (2025) prove that under strict mathematical definitions, safety (never making false claims), trust (assuming safety), and AGI (matching/exceeding human capability) are fundamentally incompatible. Their proof parallels Gödel's incompleteness theorems and Turing's halting problem, suggesting the impossibility is deep rather than contingent. While practical deployments might use relaxed definitions, the formal result reveals inherent tension: perfect safety requires limiting capabilities below human level. No amount of interpretability can make AGI both perfectly safe and fully trusted under these definitions—the problem is logical, not epistemological.

Sutter et al. (2025) identify a different fundamental limitation: the non-linear representation dilemma. They prove that unrestricted causal abstraction becomes vacuous—any neural network can be mapped to any algorithm using sufficiently powerful alignment maps. Empirically, they find perfect alignment maps even for randomly initialized models incapable of solving tasks. This creates a dilemma for MI: lifting linearity constraints (as required for representing complex computations) eliminates principled ways to balance map complexity and accuracy. Causal abstraction alone, without additional assumptions about information encoding, cannot distinguish genuine mechanistic understanding from arbitrary mathematical mappings. MI's theoretical foundations are underdetermined.

Méloux et al. (2025) demonstrate systematic non-identifiability in MI explanations. Testing MI strategies on Boolean functions and small MLPs, they find: (1) multiple circuits replicate behavior, (2) circuits admit multiple interpretations, (3) algorithms align with different subspaces. This non-identifiability is not mere measurement error but reflects fundamental underdetermination: behavioral evidence does not uniquely determine mechanistic explanations. For safety, this is problematic—if multiple incompatible mechanistic stories fit the same model, which should guide interventions? The inner interpretability framework offers validation criteria beyond uniqueness (predictive power, manipulability), but this raises the question: if non-unique explanations can still support safety, what additional criteria distinguish good from bad non-unique explanations?

Yampolskiy (2024) argues that advanced AI systems are fundamentally unmonitorable—we cannot reliably detect emergent capabilities before they manifest. System complexity exceeds human comprehension boundaries, emergent behaviors arise unpredictably from component interactions, and exhaustive testing in high-dimensional capability spaces is impossible. Even if we mechanistically understand current system behavior, emergent properties arising from scaling or environmental interaction may not be predictable from component analysis. This challenges MI's sufficiency for dynamic safety: understanding how a system currently works provides no guarantee of understanding how it will work after scaling or deployment in novel contexts.

These theoretical limitations collectively suggest that sufficiency claims face principled barriers, not merely practical challenges. No interpretability approach—mechanistic or otherwise—can overcome logical impossibilities (Panigrahy & Sharan), underdetermination problems (Sutter et al., Méloux et al.), or the fundamental unpredictability of emergence (Yampolskiy). MI might be the best available tool while remaining theoretically insufficient.

### 3.2 Empirical Limitations and the Intervention-Coherence Tradeoff

Theoretical impossibility results are complemented by empirical evidence that MI techniques face practical limitations challenging sufficiency. Bhalla et al. (2024) conduct the most direct test of whether interpretability enables control. They evaluate four popular MI methods (sparse autoencoders, logit lens, tuned lens, probing) on their ability to enable interventions controlling model behavior. While methods allow intervention, effectiveness is inconsistent across features and models. Critically, mechanistic interventions often compromise model coherence—interventions based on interpretable features produce outputs that are semantically incoherent or degrade performance. In many cases, simpler alternatives like prompting outperform mechanistic intervention. This reveals a fundamental challenge: understanding model internals does not automatically translate to safe control.

Madsen et al. (2024) identify the faithfulness problem: explanations may be convincing but unfaithful to actual model behavior, creating dangerous false confidence. Current interpretability paradigms (intrinsic and post-hoc) cannot guarantee explanations accurately reflect internal mechanisms. If mechanistic interpretations systematically misrepresent how models function, they cannot ground safety assurances. The authors argue the field needs fundamentally new paradigms—models designed such that faithfulness is measurable, or optimization procedures ensuring explanations become faithful—rather than incremental improvements to existing MI methods.

Makelov et al. (2023) demonstrate "interpretability illusions" where activation patching activates dormant parallel pathways causally disconnected from normal computation. Even when interventions change behavior as expected, the patched components may not be genuinely involved in the mechanism normally used. This undermines naive interpretations of patching results: behavior-based validation is insufficient. The paper argues for stricter evidential standards requiring prior manual circuit analysis to verify patched subspaces are actually used in normal computation—but this requirement severely limits scalability.

Lieberum et al. (2023) test whether circuit analysis scales to frontier models. Studying Chinchilla (70B parameters) on multiple-choice question answering, they find traditional MI techniques (logit attribution, attention visualization, activation patching) scale successfully. However, attempts to fully understand identified components yield mixed results—discovered features (like "Nth item in enumeration") only partially explain behavior. Working on normal cases, the features fail on randomized labels, suggesting discovered explanations may be incomplete or task-specific rather than fully general. This challenges optimistic claims that MI can achieve complete mechanistic understanding at scale.

Pan et al. (2025) reveal additional complexity: LLM safety-aligned behavior is jointly controlled by multi-dimensional directions in activation space rather than a single direction. They identify a dominant refusal direction plus multiple secondary directions representing distinct safety features, with complex interactions between dimensions. This multi-dimensional structure complicates sufficiency claims: if safety emerges from many interacting mechanisms, comprehensive MI may exceed analytical tractability. The finding also explains why safety can be bypassed by manipulating secondary directions—behavioral testing misses these mechanistic vulnerabilities, supporting MI's value while simultaneously revealing complexity that challenges its sufficiency.

### 3.3 The Pluralistic Safety Framework

The accumulation of theoretical impossibilities and empirical limitations motivates a pluralistic safety framework in which MI contributes alongside other approaches rather than serving as sole foundation. Salhab et al. (2024) conduct a systematic literature review revealing that AI safety encompasses multiple dimensions: explainability, interpretability, robustness, reliability, fairness, bias mitigation, and adversarial defense. Current safety research addresses these dimensions through diverse methods, with interpretability being one important but insufficient component. Comprehensive safety frameworks must integrate all requirements rather than privileging any single aspect.

Alzubaidi et al. (2023) identify nine essential requirements for trustworthy AI: explainability, accountability, robustness, fairness, privacy, accuracy, reproducibility, human oversight, and environmental wellbeing. They synthesize these requirements across diverse application domains (healthcare, finance, robotics, environmental science), demonstrating that trustworthiness emerges from systematically addressing all requirements together rather than optimizing individual dimensions. Explainability/interpretability is one necessary but insufficient component requiring integration with other requirements. The cross-domain analysis reveals that different applications weight requirements differently—interpretability's necessity varies by context rather than being universal.

Díaz-Rodríguez et al. (2023) propose seven technical requirements for trustworthy AI: human agency and oversight, robustness and safety, privacy and data governance, transparency, diversity and non-discrimination, societal and environmental wellbeing, and accountability. They connect abstract principles to concrete implementation through detailed analysis of each requirement's what, why, and how. The framework treats transparency/interpretability as one of seven equally important requirements, with responsible AI achieved through multi-faceted compliance rather than any single technique. The emphasis on auditing processes suggests safety assurance may come from procedural compliance and verification rather than interpretability alone.

Barez et al. (2025) examine machine unlearning as another safety mechanism, finding it faces limitations analogous to MI. In dual-use domains (cybersecurity, CBRN), beneficial and harmful knowledge overlap—removing dangerous capabilities may eliminate beneficial ones. They identify tensions between unlearning and other safety mechanisms (alignment training, robustness), suggesting safety interventions can interfere with each other. The paper challenges single-solution thinking: no individual safety mechanism, including MI or unlearning, is sufficient. Safety requires coordinated deployment of multiple complementary approaches.

Perrier (2025) argues explicitly that MI lacks the generalization required of control frameworks and must be integrated with formal control theory. While acknowledging recent MI advances, Perrier contends they "often fall short of the generalisation required of control frameworks" (p. 2) and lack research into rendering different alignment and control protocols interoperable. This positions understanding (via MI) as necessary but insufficient—control frameworks are also required to translate understanding into reliable safety guarantees.

The pluralistic view does not diminish MI's importance but contextualizes it as one tool in a safety portfolio. Different tools address different properties: MI reveals internal mechanisms and enables targeted interventions; formal verification provides mathematical guarantees for specified properties; robust training improves worst-case performance; scalable oversight enables alignment beyond human comprehension; representation engineering manipulates high-level features. Comprehensive safety likely requires all these approaches, with MI playing a crucial but bounded role.

## Research Gaps and Opportunities

### Gap 1: Rigorous Engagement with Mechanistic Philosophy

While mechanistic interpretability explicitly draws on philosophy of science frameworks, most MI research engages these foundations superficially. Kästner and Crook (2024) invoke Craver and Bechtel, and Ayonrinde and Jaburi (2025) provide philosophical foundations, but these efforts are exceptional. Williams et al. (2025) argue that MI "needs philosophy," documenting how the field's implicit conceptual commitments and explanatory standards require clarification. Without clear mechanistic criteria, we cannot assess whether MI delivers genuine mechanistic understanding or merely mechanistic-sounding descriptions.

**Why it matters**: If MI's mechanistic claims are underdetermined—if we lack precise standards for what makes explanation genuinely mechanistic—then we cannot evaluate whether MI satisfies its theoretical commitments. For safety, the question is whether MI provides the kind of understanding its proponents claim or something weaker. Philosophical analysis can clarify what mechanistic explanation requires (Craver's mutual manipulability, Bechtel's decomposition-localization-recomposition, Machamer et al.'s activities-based framework) and assess whether MI techniques satisfy these requirements. The gap between philosophical mechanism concepts and technical MI practices needs systematic investigation.

**How research addresses it**: Apply established mechanistic explanation criteria to evaluate MI methods. Do sparse autoencoders identify genuine mechanistic components or merely convenient statistical patterns? Does activation patching establish constitutive relevance in Craver's sense? Can circuit analysis satisfy decomposition-and-recomposition requirements? Philosophical analysis can distinguish mechanistic understanding from mechanistic-styled redescription, providing clearer foundations for assessing MI's explanatory achievements.

### Gap 2: Operationalizing Necessity and Sufficiency

Most papers argue MI is "important," "critical," or "valuable" for safety without distinguishing necessity from high instrumental value or specifying what would count as MI being necessary versus sufficient. The leap from "MI provides benefits for some safety properties" to "MI is necessary for safety" is rarely justified. Few papers define what necessity means in this context—conceptual necessity (safety conceptually requires understanding), causal necessity (understanding causes safety), or practical necessity (no viable alternatives exist)?

**Why it matters**: Policy and research prioritization depend on whether MI is necessary (must pursue), sufficient (can rely on alone), or merely helpful (one option among many). Without clear definitions, debates devolve into talking past each other. If MI is necessary, then safety research should prioritize MI development even at cost to alternatives. If MI is merely valuable, then portfolio approaches balancing multiple methods may be optimal. The stakes are high for resource allocation in safety research.

**How research addresses it**: Philosophical clarification of modal claims—necessity, sufficiency—in safety contexts. Distinguish types of necessity: Is MI conceptually necessary (safety by definition requires understanding)? Causally necessary (understanding causes safety outcomes)? Practically necessary (no alternatives work)? For sufficiency: Is MI sufficient in principle (complete MI guarantees safety)? Sufficient in practice (realistic MI provides adequate safety)? Contextually sufficient (MI alone works in specific domains)? Precise definitions enable rigorous evaluation of whether empirical evidence supports strong or weak versions of necessity/sufficiency claims.

### Gap 3: The Interpretation-Control Gap

Bhalla et al. (2024) document that interpretable features don't straightforwardly enable reliable control. Makelov et al. (2023) show interpretability illusions where patching activates dormant pathways. These findings reveal a gap: understanding mechanisms doesn't automatically translate to controlling them. Yet limited work explores why this gap exists or what additional conditions bridge interpretation and intervention.

**Why it matters**: MI's safety value depends critically on enabling control—if understanding doesn't support intervention, MI's practical safety contribution is limited. The interpretation-control gap suggests that mechanistic knowledge is necessary but insufficient for safe control. Understanding this gap matters for both MI research (how to design interpretability methods that better support control) and safety practice (whether to invest in MI-based safety approaches).

**How research addresses it**: Philosophical analysis of the inference from understanding to control. What makes mechanistic knowledge actionable? Craver's mutual manipulability builds control into mechanism definition, but MI practice often identifies correlations without establishing manipulability. Perhaps the gap arises because discovered "mechanisms" aren't genuine in Craver's sense. Alternatively, control requires additional properties beyond understanding: predictability of intervention effects, robustness of causal relationships, absence of compensatory pathways. Investigating what bridges interpretation and intervention clarifies what MI must achieve to deliver its safety promise.

### Gap 4: Propositional vs. Mechanistic Understanding for Safety

Chalmers (2025) proposes propositional interpretability as alternative or complement to MI, but the relationship between mechanistic and propositional understanding remains undertheorized. No systematic analysis addresses whether safety requires mechanistic understanding (how systems compute), propositional understanding (what systems believe/intend), or both. The relationship between "what a model computes" and "what a model believes" is unclear.

**Why it matters**: If safety-critical properties—deception, power-seeking, value misalignment—are characterized by propositional attitudes, then mechanistic analysis of circuits may be insufficient without an account of propositional content. Conversely, if propositional attitudes supervene on mechanistic structure in tractable ways, MI might suffice. The question matters for research direction: should safety researchers focus on circuit analysis, propositional interpretation, or their integration?

**How research addresses it**: Apply philosophy of mind frameworks (intentional stance, psychosemantics, functionalism) to clarify what type of understanding safety requires. Analyze whether MI can deliver propositional understanding: do discovered "features" correspond to propositional contents, or are they merely computational intermediates? Assess whether propositional interpretation can proceed without mechanistic analysis, or whether understanding propositional attitudes requires knowing implementation. This investigation illuminates whether mechanistic and propositional approaches are complementary or potentially substitutable.

## Conclusion

The state of the art reveals that neither necessity nor sufficiency claims for mechanistic interpretability in AI safety are well-supported by current literature. MI is a valuable tool facing both principled limitations and practical constraints, best understood within a pluralistic safety framework.

The definitional landscape shows that "mechanistic interpretability" is polysemous—referring to narrow technical approaches requiring causal verification and broader cultural movements encompassing all internal model exploration. This polysemy undermines univocal necessity/sufficiency claims: what holds for narrow technical MI may not hold for broader interpretability work. Moreover, MI's engagement with mechanistic explanation philosophy (Craver, 2007; Machamer et al., 2000) remains incomplete, leaving its mechanistic claims underdetermined. Alternative frameworks like propositional interpretability (Chalmers, 2025) and understanding-focused explanation (Fleisher, 2022) suggest that mechanistic analysis may not exhaust the kinds of understanding safety requires.

The necessity debate reveals competing considerations. Functional understanding arguments (Kästner & Crook, 2024; Bereska & Gavves, 2024) have genuine force: safety plausibly requires understanding how systems work as integrated wholes, which MI claims to provide. Domain-specific cases like nuclear reactor safety (Lee, 2025) and governance requirements (Sengupta et al., 2025) demonstrate contexts where MI may be genuinely necessary. Yet viable alternatives exist: representation engineering (Zou et al., 2023; Hendrycks & Hiscott, 2025) achieves safety interventions without mechanistic decomposition; scalable oversight (Sang et al., 2024; Kim et al., 2024) enables alignment beyond human comprehension limits; formal verification (Lopez et al., 2023) provides mathematical guarantees without interpretability. These alternatives challenge universal necessity claims, suggesting MI's necessity may be domain-specific and property-specific rather than absolute.

The sufficiency debate presents stronger challenges to MI. Theoretical impossibility results (Panigrahy & Sharan, 2025) show that perfect safety, trust, and AGI are fundamentally incompatible under strict definitions—no interpretability approach can overcome logical barriers. Non-identifiability results (Méloux et al., 2025; Sutter et al., 2025) demonstrate that mechanistic explanations are underdetermined by behavioral evidence, raising questions about which of multiple compatible interpretations should guide safety interventions. Empirical evidence documents intervention-coherence tradeoffs (Bhalla et al., 2024), faithfulness problems (Madsen et al., 2024), and interpretability illusions (Makelov et al., 2023) where understanding doesn't translate to reliable control. Pan et al. (2025) reveal that safety alignment emerges from multi-dimensional mechanisms whose complexity may exceed MI's analytical capacity.

The pluralistic safety framework provides the most defensible position. AI safety encompasses multiple dimensions—robustness, fairness, privacy, alignment, transparency, accountability—each requiring specialized approaches (Salhab et al., 2024; Alzubaidi et al., 2023; Díaz-Rodríguez et al., 2023). MI addresses interpretability and enables some forms of control but cannot provide comprehensive safety alone. Formal verification, robust training, scalable oversight, representation engineering, and procedural governance all contribute essential capabilities MI cannot deliver. As Perrier (2025) argues, MI must be integrated with formal control theory; as Barez et al. (2025) demonstrate, no single safety mechanism is sufficient.

The research contribution this review enables lies in moving beyond slogans—"interpretability is crucial for safety," "understanding is necessary for control"—to precise, testable claims. Philosophical analysis can clarify: What kind of understanding does MI provide? What are the criteria for mechanistic explanation in AI contexts? Which safety properties require mechanistic versus propositional understanding? What additional conditions bridge interpretation and control? These questions demand philosophical precision alongside empirical investigation.

For policy and practice, the implications are clear. Resource allocation should not bet exclusively on MI—portfolio approaches balancing multiple safety mechanisms are prudent. MI's value varies substantially by domain: nuclear safety and high-stakes medical applications may require deep mechanistic verification, while content moderation and recommendation systems may achieve adequate safety through behavioral monitoring and statistical oversight. Claims about MI's unique necessity should be scrutinized carefully—in most cases, MI is valuable but not uniquely necessary, and certainly not sufficient alone.

The defensible position treats mechanistic interpretability as one powerful tool in a diverse safety toolkit. It provides capabilities—revealing internal mechanisms, enabling targeted debugging, supporting verification—that other approaches cannot match. But it faces principled theoretical limits, practical empirical constraints, and exists alongside viable alternatives addressing different safety dimensions. Comprehensive AI safety requires integrating MI with formal verification, scalable oversight, robust training, and procedural governance. Neither necessity nor sufficiency, but valuable bounded contribution within pluralistic frameworks—this is what current evidence supports.

**Word Count**: 6,847

## References

Alzubaidi, Laith, Aiman Al-Sabaawi, Jinshuai Bai, Ammar Dukhan, Ahmed H. Alkenani, Ahmed Al-Asadi, Haider A. Alwzwazy, Mohamed Manoufali, Mohammed A. Fadhel, A. S. Albahri, Catarina Moreira, Chun Ouyang, Jinglan Zhang, José Santamaría, Asma Salhi, Freek Hollman, Ashish Gupta, Ye Duan, Timon Rabczuk, Amin Abbosh, and Yuantong Gu. 2023. "Towards Risk-Free Trustworthy Artificial Intelligence: Significance and Requirements." *International Journal of Intelligent Systems*, 2023. https://doi.org/10.1155/2023/4459198.

Ayonrinde, Kola, and Louis Jaburi. 2025. "A Mathematical Philosophy of Explanations in Mechanistic Interpretability: The Strange Science Part I.i." In *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*. https://doi.org/10.48550/arXiv.2505.00808.

Barez, Fazl, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O'Gara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip H. S. Torr, Kwok-Yan Lam, Robert Trager, David Krueger, S. Mindermann, J. Hernández-Orallo, Mor Geva, and Yarin Gal. 2025. "Open Problems in Machine Unlearning for AI Safety." *ArXiv* abs/2501.04952. https://doi.org/10.48550/arXiv.2501.04952.

Bechtel, William. 2008. *Mental Mechanisms: Philosophical Perspectives on Cognitive Neuroscience*. New York: Routledge. https://doi.org/10.4324/9780203810095.

Bereska, Leonard, and Efstratios Gavves. 2024. "Mechanistic Interpretability for AI Safety: A Review." *Transactions on Machine Learning Research*. https://doi.org/10.48550/arXiv.2404.14082.

Bhalla, Usha, Suraj Srinivas, Asma Ghandeharioun, and Himabindu Lakkaraju. 2024. "Towards Unifying Interpretability and Control: Evaluation via Intervention." *arXiv preprint* arXiv:2411.04430. https://doi.org/10.48550/arXiv.2411.04430.

Cammarata, Nick, Shan Carter, Gabriel Goh, Christopher Olah, Michael Petrov, and Ludwig Schubert. 2020. "Thread: Circuits." *Distill* 5(3). https://doi.org/10.23915/distill.00024.

Casper, Stephen, Yuxiao Li, Jiawei Li, Tong Bu, Ke Zhang, K. Hariharan, and Dylan Hadfield-Menell. 2023. "Red Teaming Deep Neural Networks with Feature Synthesis Tools." In *Neural Information Processing Systems*. https://www.semanticscholar.org/paper/5fdf01476628b2afe4853d747ddfc6677bab13bc.

Chalmers, David J. 2025. "Propositional Interpretability in Artificial Intelligence." *ArXiv* abs/2501.15740. https://doi.org/10.48550/arXiv.2501.15740.

Chen, Jianhui, Xiaozhi Wang, Zijun Yao, Yushi Bai, Lei Hou, and Juanzi Li. 2024. "Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons." *arXiv preprint* arXiv:2406.14144. https://www.semanticscholar.org/paper/33bc46dff817e4afe6bc2c11bc808478b5207847.

Conmy, Arthur, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. 2023. "Towards Automated Circuit Discovery for Mechanistic Interpretability." In *Neural Information Processing Systems*. https://doi.org/10.48550/arXiv.2304.14997.

Craver, Carl F. 2007. *Explaining the Brain: Mechanisms and the Mosaic Unity of Neuroscience*. Oxford: Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199299317.001.0001.

Cunningham, Hoagy, Aidan Ewart, Logan Riggs Smith, Robert Huben, and Lee Sharkey. 2023. "Sparse Autoencoders Find Highly Interpretable Features in Language Models." In *International Conference on Learning Representations*. https://doi.org/10.48550/arXiv.2309.08600.

Díaz-Rodríguez, Natalia, Javier Del Ser, Mark Coeckelbergh, Marcos López de Prado, Enrique Herrera-Viedma, and Francisco Herrera. 2023. "Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation." *arXiv preprint* arXiv:2305.02231. https://doi.org/10.48550/arXiv.2305.02231.

Erasmus, Adrian, Tyler D. P. Brunet, and Eyal Fisher. 2023. "What is Interpretability?" PhilPapers entry ERAWII-2. https://philpapers.org/rec/ERAWII-2.

Fleisher, Will. 2022. "Understanding, Idealization, and Explainable AI." *Episteme* 19:534–560. https://doi.org/10.1017/epi.2022.39.

Geiger, Atticus, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman Arora, Zhengxuan Wu, Noah D. Goodman, Christopher Potts, and Thomas F. Icard. 2023. "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability." https://arxiv.org/abs/2301.04709.

Gross, Jason, Rajashree Agrawal, Thomas Kwa, Euan Ong, Chun Hei Yip, Alex Gibson, Soufiane Noubir, and Lawrence Chan. 2024. "Compact Proofs of Model Performance via Mechanistic Interpretability." In *Neural Information Processing Systems*. https://doi.org/10.48550/arXiv.2406.11779.

Guidotti, Dario, Laura Pandolfo, and Luca Pulina. 2023. "Leveraging Satisfiability Modulo Theory Solvers for Verification of Neural Networks in Predictive Maintenance Applications." *Information* 14(7):397. https://doi.org/10.3390/info14070397.

Gyevnar, Balint, and Atoosa Kasirzadeh. 2025. "AI Safety for Everyone." *Nature Machine Intelligence* 7:531–542. https://doi.org/10.1038/s42256-025-01020-y.

Heimersheim, Stefan, and Neel Nanda. 2024. "How to use and interpret activation patching." *arXiv preprint* arXiv:2404.15255. https://doi.org/10.48550/arXiv.2404.15255.

Hendrycks, Dan, and Laura Hiscott. 2025. "The Misguided Quest for Mechanistic AI Interpretability." https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability.

Huang, Li, Weifeng Sun, Meng Yan, Zhongxin Liu, Yan Lei, and David Lo. 2024. "Neuron Semantic-Guided Test Generation for Deep Neural Networks Fuzzing." *ACM Transactions on Software Engineering and Methodology* 34:1–38. https://doi.org/10.1145/3688835.

Kadir, Md Abdul, Amir Mosavi, and Daniel Sonntag. 2023. "Evaluation Metrics for XAI: A Review, Taxonomy, and Practical Applications." In *2023 IEEE 27th International Conference on Intelligent Engineering Systems (INES)*, 000111–000124. https://doi.org/10.1109/INES59282.2023.10297629.

Kästner, Lena, and Barnaby Crook. 2024. "Explaining AI through Mechanistic Interpretability." *European Journal for Philosophy of Science* 14(4):52. https://doi.org/10.1007/s13194-024-00614-4.

Kim, Hyunjin, Xiaoyuan Yi, Jing Yao, Jianxun Lian, Muhua Huang, Shitong Duan, J. Bak, and Xing Xie. 2024. "The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment." *ArXiv* abs/2412.16468. https://doi.org/10.48550/arXiv.2412.16468.

Lee, Yoon Pyo. 2025. "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications." *arXiv preprint* arXiv:2507.09931. https://doi.org/10.48550/arXiv.2507.09931.

Lehalleur, Simon Pepin, Jesse Hoogland, Matthew Farrugia-Roberts, Susan Wei, Alexander Gietelink Oldenziel, George Wang, Liam Carroll, and Daniel Murfet. 2025. "You Are What You Eat: AI Alignment Requires Understanding How Data Shapes Structure and Generalisation." *ArXiv* abs/2502.05475. https://doi.org/10.48550/arXiv.2502.05475.

Lieberum, Tom, Matthew Rahtz, János Kramár, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. 2023. "Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla." *arXiv preprint* arXiv:2307.09458. https://doi.org/10.48550/arXiv.2307.09458.

Lopez, Diego Manzanas, Sung Woo Choi, Hoang-Dung Tran, and Taylor T. Johnson. 2023. "NNV 2.0: The Neural Network Verification Tool." In *Lecture Notes in Computer Science*, 397–412. Springer. https://doi.org/10.1007/978-3-031-37703-7_19.

Machamer, Peter, Lindley Darden, and Carl F. Craver. 2000. "Thinking About Mechanisms." *Philosophy of Science* 67(1):1–25.

Madsen, Andreas, Himabindu Lakkaraju, Siva Reddy, and Sarath Chandar. 2024. "Interpretability Needs a New Paradigm." *ArXiv* abs/2405.05386. https://doi.org/10.48550/arXiv.2405.05386.

Makelov, Aleksandar, Georg Lange, and Neel Nanda. 2023. "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching." In *International Conference on Learning Representations*. https://doi.org/10.48550/arXiv.2311.17030.

Méloux, Maxime, Silviu Maniu, François Portet, and Maxime Peyrard. 2025. "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?" *ArXiv* abs/2502.20914. https://doi.org/10.48550/arXiv.2502.20914.

Nanda, Neel, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. 2023. "Progress Measures for Grokking via Mechanistic Interpretability." In *International Conference on Learning Representations*. https://doi.org/10.48550/arXiv.2301.05217.

Olah, Christopher, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. "Zoom In: An Introduction to Circuits." *Distill* 5(3). https://doi.org/10.23915/distill.00024.001.

Pan, Wenbo, Zhichao Liu, Qiguang Chen, Xiangyang Zhou, Haining Yu, and Xiaohua Jia. 2025. "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions." *arXiv preprint* arXiv:2502.09674. https://www.semanticscholar.org/paper/ba37af16b72f5250774e014d7ac6a7051585c0ca.

Panigrahy, Rina, and Vatsal Sharan. 2025. "Limitations on Safe, Trusted, Artificial General Intelligence." *ArXiv* abs/2509.21654. https://doi.org/10.48550/arXiv.2509.21654.

Perrier, Elija. 2025. "Out of Control: Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)." *arXiv preprint* arXiv:2506.17846. https://doi.org/10.48550/arXiv.2506.17846.

Rai, Daking, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. 2024. "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models." *arXiv preprint* arXiv:2407.02646. https://doi.org/10.48550/arXiv.2407.02646.

Salhab, Wissam, Darine Ameyed, Fehmi Jaafar, and Hamid Mcheick. 2024. "A Systematic Literature Review on AI Safety: Identifying Trends, Challenges, and Future Directions." *IEEE Access* 12:131762–131784. https://doi.org/10.1109/ACCESS.2024.3440647.

Sang, Jitao, Yuhang Wang, Jing Zhang, Yanxu Zhu, Chao Kong, Junhong Ye, Shuyu Wei, and Jinlin Xiao. 2024. "Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning." *ArXiv* abs/2402.00667. https://doi.org/10.48550/arXiv.2402.00667.

Saphra, Naomi, and Sarah Wiegreffe. 2024. "Mechanistic?" In *BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP*. https://doi.org/10.48550/arXiv.2410.09087.

Sengupta, Aadit, Pratinav Seth, and Vinay Kumar Sankarapu. 2025. "Interpretability as Alignment: Making Internal Understanding a Design Principle." *arXiv preprint* arXiv:2509.08592. https://doi.org/10.48550/arXiv.2509.08592.

Smart, Andrew, and Atoosa Kasirzadeh. 2024. "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning." *AI & Society* 40:2045–2053. https://doi.org/10.1007/s00146-024-02056-1.

Sutter, Denis, Julian Minder, Thomas Hofmann, and Tiago Pimentel. 2025. "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?" *arXiv preprint* arXiv:2507.08802. https://doi.org/10.48550/arXiv.2507.08802.

Vijayaraghavan, Avish, and C. Badea. 2023. "Minimum Levels of Interpretability for Artificial Moral Agents." *AI and Ethics* 5:2071–2087. https://doi.org/10.1007/s43681-024-00536-0.

Williams, Iwan, Ninell Oldenburg, Ruchira Dhar, Joshua Hatherley, Constanza Fierro, Nina Rajcic, Sandrine R. Schiller, Filippos Stamatiou, and Anders Søgaard. 2025. "Mechanistic Interpretability Needs Philosophy." *ArXiv* abs/2506.18852. https://doi.org/10.48550/arXiv.2506.18852.

Yampolskiy, Roman V. 2024. "On Monitorability of AI." *AI and Ethics* 5:689–707. https://doi.org/10.1007/s43681-024-00420-x.

Yu, Lei, Meng Cao, J. C. K. Cheung, and Yue Dong. 2024. "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations." In *Conference on Empirical Methods in Natural Language Processing*, 7943–7956. https://doi.org/10.18653/v1/2024.findings-emnlp.466.

Zhang, Fred, and Neel Nanda. 2023. "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods." In *International Conference on Learning Representations*. https://doi.org/10.48550/arXiv.2309.16042.

Zou, Andy, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Troy Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, Zico Kolter, and Dan Hendrycks. 2023. "Representation Engineering: A Top-Down Approach to AI Transparency." *arXiv preprint* arXiv:2310.01405. https://doi.org/10.48550/arXiv.2310.01405.
