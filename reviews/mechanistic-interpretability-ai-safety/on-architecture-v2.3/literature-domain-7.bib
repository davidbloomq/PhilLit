@comment{
====================================================================
DOMAIN: Case Studies and Empirical Evidence
SEARCH_DATE: 2025-12-28
PAPERS_FOUND: 12 total (High: 5, Medium: 5, Low: 2)
SEARCH_SOURCES: Semantic Scholar, arXiv, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain encompasses empirical studies that evaluate whether mechanistic
interpretability (MI) actually improves safety outcomes, including case studies
where MI successfully identified safety issues, benchmarks for measuring
interpretability effectiveness, and debugging tools that leverage interpretability
for finding failures. The literature reveals a critical gap between theoretical
interpretability and practical safety improvements. While evaluation metrics and
benchmarks have proliferated (e.g., XAI metrics, interpretability toolkits), most
studies measure interpretability quality rather than its causal impact on safety.
Key empirical findings include: (1) interventions based on interpretability often
compromise model coherence (Bhalla et al. 2024), (2) current metrics show weak
correlation with actual utility in safety tasks (Kadir et al. 2023), (3) debugging
tools reveal that interpretability helps identify failure modes but requires human
expertise to translate insights into fixes (Casper et al. 2023, Dietz et al. 2024).
Recent work has begun developing benchmarks specifically for measuring safety-relevant
properties like online safety analysis (Xie et al. 2024) and red teaming effectiveness
(Casper et al. 2023).

RELEVANCE_TO_PROJECT:
This domain directly addresses the research project's central question: does MI
actually improve AI safety? The empirical evidence is mixed but illuminating. Studies
show that while MI tools can identify bugs and failure modes, translating these
insights into safety improvements remains challenging. This creates a crucial gap
between "understanding" a model and making it safer. The case studies demonstrate
both MI's potential (successful bug discovery, improved debugging workflows) and its
limitations (intervention-coherence tradeoffs, metric-utility gaps).

NOTABLE_GAPS:
Few studies directly measure the causal impact of interpretability interventions on
safety outcomes in real-world deployments. Most work focuses on synthetic benchmarks
or controlled experiments. There is limited longitudinal evidence tracking whether
MI-guided interventions actually reduce safety incidents over time. The literature
also lacks rigorous comparison between MI-based safety approaches and alternative
methods (e.g., adversarial training, behavioral testing).

SYNTHESIS_GUIDANCE:
When synthesizing this domain, emphasize the distinction between interpretability
*quality* (how well we understand models) and interpretability *utility* (whether
understanding improves safety). Highlight the intervention-coherence dilemma as a
fundamental challenge. Use case studies to illustrate both successes (Casper's
trojan discovery) and limitations (Bhalla's intervention failures). Connect
evaluation metrics work to the broader question of what we're actually measuring
when we measure interpretability.

KEY_POSITIONS:
- Optimistic empiricism (5 papers): MI tools successfully identify bugs and improve debugging workflows when properly designed
- Skeptical empiricism (4 papers): Current MI methods show limited practical utility; metrics don't correlate with safety improvements
- Measurement-focused (3 papers): Need better benchmarks and evaluation frameworks before making claims about MI's safety value
====================================================================
}

@inproceedings{bhalla2024towards,
  author = {Bhalla, Usha and Srinivas, Suraj and Ghandeharioun, Asma and Lakkaraju, Himabindu},
  title = {Towards Unifying Interpretability and Control: Evaluation via Intervention},
  booktitle = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2411.04430},
  arxivId = {2411.04430},
  url = {https://www.semanticscholar.org/paper/7c7e1160c6fc55444378e1cc6205c68a29d13571},
  note = {
  CORE ARGUMENT: Evaluates four popular interpretability methods (SAEs, logit lens, tuned lens, probing) on their ability to enable interventions that control model behavior. Finds that while methods allow intervention, effectiveness is inconsistent across features/models; lens-based methods outperform SAEs/probes for simple interventions; mechanistic interventions often compromise model coherence, underperforming simpler alternatives like prompting. Introduces intervention success rate and coherence-intervention tradeoff metrics.

  RELEVANCE: Directly addresses whether interpretability enables control, finding a critical limitation: interpretable features can be identified but using them to steer behavior often breaks the model. This reveals a fundamental challenge for MI's safety promise—understanding model internals doesn't automatically translate to safe control. The finding that prompting outperforms mechanistic intervention undermines claims that MI is necessary for alignment.

  POSITION: Skeptical empiricism—demonstrates that current MI methods have limited practical utility for control despite theoretical appeal.
  },
  keywords = {mechanistic-interpretability, empirical-evaluation, intervention, High}
}

@article{kadir2023evaluation,
  author = {Kadir, Md Abdul and Mosavi, Amir and Sonntag, Daniel},
  title = {Evaluation Metrics for XAI: A Review, Taxonomy, and Practical Applications},
  booktitle = {2023 IEEE 27th International Conference on Intelligent Engineering Systems (INES)},
  year = {2023},
  pages = {000111--000124},
  doi = {10.1109/INES59282.2023.10297629},
  url = {https://www.semanticscholar.org/paper/c1aebb992a8a0640eb047486fa05e5c38fef95c3},
  note = {
  CORE ARGUMENT: Comprehensive review of XAI evaluation metrics, proposing two taxonomies: one based on applications, one based on evaluation metric types. Identifies lack of clear definitions for validity, reliability, and evaluation metrics of explainability. Reviews existing metrics for fidelity, faithfulness, and overall explainability across diverse data types and learning methodologies.

  RELEVANCE: Critical for the research project because it reveals the evaluation problem: we lack standardized ways to measure whether interpretability actually helps with safety. The diversity of metrics and their weak correlation suggests that "interpretability" is not a single coherent property but a collection of potentially orthogonal qualities. This complicates claims about MI improving safety—which metric should we optimize for safety outcomes?

  POSITION: Measurement-focused—emphasizes need for rigorous evaluation frameworks before making utility claims.
  },
  keywords = {XAI, evaluation-metrics, taxonomy, Medium}
}

@article{xie2024online,
  author = {Xie, Xuan and Song, Jiayang and Zhou, Zhehua and Huang, Yuheng and Song, Da and Ma, Lei},
  title = {Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward},
  journal = {IEEE Transactions on Artificial Intelligence},
  year = {2024},
  doi = {10.48550/arXiv.2404.08517},
  arxivId = {2404.08517},
  url = {https://www.semanticscholar.org/paper/aa3def6c0d6eef8183e9b78894a711ed9e092df6},
  note = {
  CORE ARGUMENT: First benchmark for online safety analysis during LLM generation (rather than post-hoc analysis). Validates feasibility of detecting unsafe outputs early in generation process. Tests state-of-the-art methods across models/tasks/datasets, revealing strengths/weaknesses and providing guidance for method selection. Explores hybrid methods combining multiple approaches for enhanced efficacy.

  RELEVANCE: Shifts focus from offline interpretability to real-time safety intervention—directly relevant to whether MI can enable practical safety improvements. The online detection framework represents a concrete use case where interpretability must provide actionable signals during generation. Findings on method limitations highlight gap between interpretability research and deployment requirements for safety-critical systems.

  POSITION: Optimistic empiricism—demonstrates practical safety applications while acknowledging current limitations.
  },
  keywords = {online-safety, LLM-safety, benchmark, High}
}

@inproceedings{casper2023red,
  author = {Casper, Stephen and Li, Yuxiao and Li, Jiawei and Bu, Tong and Zhang, Ke and Hariharan, K. and Hadfield-Menell, Dylan},
  title = {Red Teaming Deep Neural Networks with Feature Synthesis Tools},
  booktitle = {Neural Information Processing Systems},
  year = {2023},
  arxivId = {2302.10894},
  url = {https://www.semanticscholar.org/paper/5fdf01476628b2afe4853d747ddfc6677bab13bc},
  note = {
  CORE ARGUMENT: Benchmarks interpretability tools on debugging tasks by implanting human-interpretable trojans and evaluating whether tools help humans discover them. Finds that 16 feature attribution/saliency methods often fail even with direct access to trojan-triggered data. However, 7 feature-synthesis methods (which don't depend on datasets) show superior performance. Introduces trojan discovery as evaluation task for interpretability tools, analogous to finding OOD bugs with known ground truth.

  RELEVANCE: Provides rare empirical evidence of interpretability tools successfully identifying safety bugs (trojans). The feature-synthesis approach is particularly relevant to MI's safety promise—these methods can discover bugs that dataset-dependent approaches miss. However, even best-performing methods have limitations, revealing interpretability's incomplete coverage of potential safety issues. The benchmark design (known-ground-truth bugs) offers a model for rigorous MI evaluation.

  POSITION: Optimistic empiricism with methodological innovation—demonstrates MI utility while introducing rigorous evaluation approach.
  },
  keywords = {red-teaming, trojan-detection, debugging, High}
}

@article{dietz2024comgra,
  author = {Dietz, Florian and Fellenz, Sophie and Klakow, Dietrich and Kloft, Marius},
  title = {Comgra: A Tool for Analyzing and Debugging Neural Networks},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2407.21656},
  arxivId = {2407.21656},
  url = {https://www.semanticscholar.org/paper/6b71316bb3e4c9ff90340014e9e5055025b511b5},
  note = {
  CORE ARGUMENT: Introduces Comgra, an open-source PyTorch tool for extracting and visualizing internal activations via GUI. Shows summary statistics and individual data points, compares training stages, focuses on specific samples, visualizes gradient flow. Applications for debugging, neural architecture design, and mechanistic interpretability. Allows rapid hypothesis testing without model reruns.

  RELEVANCE: Represents practical tooling for MI-based debugging. The ability to rapidly test hypotheses and visualize internal dynamics addresses the workflow challenge—how do practitioners actually use interpretability insights for debugging? The focus on both summary statistics and individual samples connects to the broader question of what level of interpretability is sufficient for safety improvements. Tool's success depends on user expertise to translate visualizations into fixes.

  POSITION: Optimistic empiricism focused on practical tooling and workflow integration.
  },
  keywords = {debugging-tools, visualization, neural-networks, Medium}
}

@inproceedings{yu2024mechanistic,
  author = {Yu, Lei and Cao, Meng and Cheung, J. C. K. and Dong, Yue},
  title = {Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year = {2024},
  pages = {7943--7956},
  doi = {10.18653/v1/2024.findings-emnlp.466},
  arxivId = {2403.18167},
  url = {https://www.semanticscholar.org/paper/01b4977937966694c00f6c7b55e712eef50603a4},
  note = {
  CORE ARGUMENT: Traces hallucinations through internal model representations across LLMs (Llama-2, Pythia, GPT-J), discovering two mechanistic causes: (1) knowledge enrichment hallucinations from insufficient subject attribute knowledge in lower layer MLPs, and (2) answer extraction hallucinations from failure to select correct object attribute in upper layer attention heads. External manifestations reflect these internal causes. Proposes mitigation through targeted restoration of fact recall pipeline, outperforming baselines.

  RELEVANCE: Demonstrates successful application of MI to identify and fix a concrete safety problem (hallucinations). The mechanistic analysis reveals specific failure modes at specific layers, enabling targeted interventions. This represents one of the clearest cases where MI led to practical safety improvements. However, the fix is specific to hallucinations—unclear if approach generalizes to other safety issues. Shows MI's potential when failure mode is well-defined and mechanistically tractable.

  POSITION: Optimistic empiricism—successful case study of MI-guided safety improvement for specific failure mode.
  },
  keywords = {hallucinations, mechanistic-interpretability, mitigation, High}
}

@inproceedings{soltani2023spade,
  author = {Soltani Moakhar, Arshia and Iofinova, Eugenia and Alistarh, Dan},
  title = {SPADE: Sparsity-Guided Debugging for Deep Neural Networks},
  booktitle = {International Conference on Machine Learning},
  year = {2023},
  doi = {10.48550/arXiv.2310.04519},
  arxivId = {2310.04519},
  url = {https://www.semanticscholar.org/paper/5a4a8af9267537ea787420c0f04b1097d559e8cc},
  note = {
  CORE ARGUMENT: Incorporates sparsity into interpretation process itself via sample-specific preprocessing. Uses sample-targeted pruning to provide execution "trace" by reducing network to most important connections before computing interpretation. Significantly increases accuracy of image saliency maps across interpretability methods. Improves neuron visualizations, aiding human reasoning about network behavior. Does not constrain trained model or affect inference behavior.

  RELEVANCE: Novel approach that enhances interpretability quality by preprocessing samples rather than constraining models. The execution trace concept connects to MI's goal of understanding model computation paths. Improved saliency map accuracy could enhance debugging effectiveness. However, paper focuses on interpretability quality (better visualizations) rather than demonstrating safety improvements—represents the quality-utility gap in MI evaluation.

  POSITION: Measurement-focused—improves interpretability methods but doesn't measure impact on safety outcomes.
  },
  keywords = {sparsity, debugging, saliency-maps, Medium}
}

@article{sacha2024interpretability,
  author = {Sacha, Miko{\l}aj and Jura, Bartosz and Rymarczyk, Dawid and Struski, {\L}ukasz and Tabor, Jacek and Zieli{\'n}ski, Bartosz},
  title = {Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2024},
  volume = {38},
  number = {19},
  pages = {21494--21502},
  doi = {10.1609/aaai.v38i19.30154},
  url = {https://openalex.org/W4393161136},
  note = {
  CORE ARGUMENT: Introduces benchmark with dedicated metrics for quantifying "spatial explanation misalignment"—when prototype activation region's receptive field depends on image parts outside the region, leading to misleading interpretations. Proposes misalignment compensation method and applies to state-of-the-art models. Demonstrates through extensive empirical studies that compensation improves interpretability quality.

  RELEVANCE: Identifies and measures a specific failure mode in interpretability methods (spatial misalignment), showing that even "faithful" explanations can mislead. This is critical for MI safety claims—if our interpretability tools produce systematically misleading explanations, interventions based on them may harm rather than help. The benchmark enables rigorous evaluation of whether interpretability methods are trustworthy enough for safety-critical decisions.

  POSITION: Skeptical empiricism—reveals systematic flaws in existing interpretability methods.
  },
  keywords = {interpretability-benchmark, prototypical-parts, spatial-misalignment, Medium}
}

@article{huang2024neuron,
  author = {Huang, Li and Sun, Weifeng and Yan, Meng and Liu, Zhongxin and Lei, Yan and Lo, David},
  title = {Neuron Semantic-Guided Test Generation for Deep Neural Networks Fuzzing},
  journal = {ACM Transactions on Software Engineering and Methodology},
  year = {2024},
  volume = {34},
  pages = {1--38},
  doi = {10.1145/3688835},
  url = {https://www.semanticscholar.org/paper/c11e3b8d153a135375f595845b86d527880c168d},
  note = {
  CORE ARGUMENT: Proposes NSGen for DNN fuzzing using neuron semantics as guidance. Identifies critical neurons, translates their semantic features into natural language, assembles into human-readable DNN decision paths. Generates fault-revealing test inputs by quantifying similarity between original and mutated inputs. Outperforms 12 coverage-guided fuzzing criteria, increasing triggered faults by 21.4% to 61.2% compared to state-of-the-art. Demonstrates effectiveness in generating fault-revealing test inputs.

  RELEVANCE: Demonstrates that semantic interpretability (understanding what neurons represent) directly improves safety testing effectiveness. The natural language decision paths make fuzzing more targeted and interpretable. This is evidence that MI can enhance traditional safety approaches (fuzzing) rather than replace them. However, effectiveness depends on semantic extraction quality—shows both MI's potential and its dependence on foundational interpretability assumptions.

  POSITION: Optimistic empiricism—shows MI enhancing established safety methods (fuzzing).
  },
  keywords = {fuzzing, semantic-interpretability, testing, High}
}

@article{bareeva2024quanda,
  author = {Bareeva, Dilyara and Yolcu, Galip Umit and Hedstr{\"o}m, Anna and Schmolenski, Niklas and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
  title = {Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2410.07158},
  arxivId = {2410.07158},
  url = {https://www.semanticscholar.org/paper/a6972cb3c805a68926fc1f94068502e4196d1b05},
  note = {
  CORE ARGUMENT: Introduces Quanda toolkit for evaluating Training Data Attribution (TDA) methods—interpretability approaches that identify which training samples influenced model predictions. Provides comprehensive set of evaluation metrics and uniform interface for systematic benchmarking across TDA implementations. Addresses lack of unified framework that limits trust in TDA methods and stunts widespread adoption. Enables systematic comparison of TDA quality.

  RELEVANCE: TDA represents a specific interpretability approach relevant to safety—understanding training data influence helps audit model behavior and identify problematic training samples. Quanda addresses the evaluation gap: we need standardized ways to assess whether interpretability methods are reliable enough for safety applications. The toolkit enables researchers to rigorously compare methods rather than making unfounded claims about interpretability utility.

  POSITION: Measurement-focused—provides infrastructure for rigorous TDA evaluation rather than making claims about utility.
  },
  keywords = {training-data-attribution, evaluation-toolkit, benchmarking, Low}
}

@article{pach2025sparse,
  author = {Pach, Mateusz and Karthik, Shyamgopal and Bouniot, Quentin and Belongie, Serge and Akata, Zeynep},
  title = {Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models},
  journal = {arXiv preprint},
  year = {2025},
  doi = {10.48550/arXiv.2504.02821},
  arxivId = {2504.02821},
  url = {https://www.semanticscholar.org/paper/68510f18285aa3453f7d68542eaacfeb4bad0a0b},
  note = {
  CORE ARGUMENT: Extends SAEs to Vision-Language Models (VLMs) like CLIP, introducing comprehensive framework for evaluating monosemanticity at neuron-level in visual representations. Proposes benchmark derived from large-scale user study to ensure alignment with human perception. Shows SAEs significantly enhance neuron monosemanticity, with sparsity and wide latents being most influential factors. Demonstrates that SAE interventions on CLIP's vision encoder directly steer multimodal LLM outputs without modifying language model.

  RELEVANCE: Demonstrates SAEs can enhance interpretability in multimodal models and enable steering of downstream systems (LLaVA). This is direct evidence that MI can provide control in complex systems. The human-aligned benchmark addresses trustworthiness—interpretations should match human understanding. However, steering effectiveness depends on carefully calibrated interventions—reinforces Bhalla et al.'s finding about intervention-coherence tradeoffs.

  POSITION: Optimistic empiricism—shows SAEs enable both interpretability and practical control in multimodal setting.
  },
  keywords = {sparse-autoencoders, vision-language-models, steering, Medium}
}

@article{benbouzid2024pragmatic,
  author = {Benbouzid, Djalel and Plociennik, Christiane and Lucaj, Laura and Maftei, Mihai and Merget, Iris and Burchardt, Aljoscha and Hauer, Marc P. and Naceri, Abdeldjallil and van der Smagt, Patrick},
  title = {Pragmatic auditing: a pilot-driven approach for auditing Machine Learning systems},
  journal = {arXiv preprint},
  year = {2024},
  doi = {10.48550/arXiv.2405.13191},
  arxivId = {2405.13191},
  url = {https://www.semanticscholar.org/paper/8cade98278b0b5f05da2e870392d8588bf364246},
  note = {
  CORE ARGUMENT: Presents pragmatic ML auditing procedure extending EU AI-HLEG guidelines. Based on ML lifecycle model explicitly focused on documentation, accountability, and quality assurance. Uses principled risk assessment for audit scoping. Describes two real-world pilots and discusses shortcomings of ML algorithmic auditing. Aims to make auditing standard practice.

  RELEVANCE: Represents attempt to operationalize MI and interpretability principles in real-world auditing context. The lifecycle model emphasizing documentation and accountability connects to MI's transparency goals. However, pilots reveal practical challenges in applying interpretability to auditing—reinforces gap between interpretability research and deployment. Shows that even with interpretability tools, auditing requires domain expertise and contextual judgment.

  POSITION: Skeptical empiricism—reveals practical difficulties in translating interpretability into actionable auditing.
  },
  keywords = {ML-auditing, lifecycle-model, real-world-deployment, Low}
}

