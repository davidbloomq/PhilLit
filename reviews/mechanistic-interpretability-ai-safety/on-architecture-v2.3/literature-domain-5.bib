@comment{
====================================================================
DOMAIN: Arguments FOR MI as Necessary/Sufficient for Safety
SEARCH_DATE: 2025-12-28
PAPERS_FOUND: 12 total (High: 4, Medium: 6, Low: 2)
SEARCH_SOURCES: Semantic Scholar, PhilPapers, CrossRef, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain covers literature that explicitly argues mechanistic interpretability
is necessary and/or sufficient for AI safety. The foundational philosophical
argument comes from Kästner & Crook (2024), who claim that traditional XAI's
"divide-and-conquer" approach fails to illuminate how AI systems work as a whole,
and that functional understanding through mechanistic interpretability is needed
to satisfy safety desiderata. Bereska & Gavves (2024) provide the most
comprehensive technical review, arguing MI could "help prevent catastrophic
outcomes as AI systems become more powerful and inscrutable" by enabling
understanding, control, and alignment. Arguments typically emphasize: (1) MI as
necessary for understanding internal model behavior before deployment in
safety-critical domains (Lee 2025, nuclear reactor safety), (2) MI as
infrastructure for alignment verification (Sengupta et al. 2025), (3) MI enabling
causal intervention and control (Chen et al. 2024, safety neurons), and (4) MI
providing formal proofs of model performance (Gross et al. 2024). Critics
(Williams et al. 2025, Rabiza 2024) argue the mechanistic approach needs clearer
philosophical foundations and may face scalability limits.

RELEVANCE_TO_PROJECT:
This domain is central to the research project's inquiry into whether MI is
necessary/sufficient for AI safety. Papers here provide the strongest explicit
arguments FOR the necessity/sufficiency claims, which must be evaluated against
skeptical and neutral positions in other domains. The arguments range from
philosophical (Kästner & Crook, Williams et al.) to technical demonstration
(Chen et al., Lee, Gross et al.) to governance frameworks (Sengupta et al.),
offering multiple angles on the central question.

NOTABLE_GAPS:
Few papers provide rigorous philosophical analysis of what "necessary" vs
"sufficient" means in this safety context. Most arguments for MI's importance
are pragmatic/empirical rather than conceptually grounded. The relationship
between MI success on toy models and safety assurance for frontier systems
remains under-theorized. Limited engagement with alternative safety approaches
(e.g., formal verification, empirical testing) that might substitute for or
complement MI.

SYNTHESIS_GUIDANCE:
Organize arguments by type: philosophical (mechanism-based explanation),
technical (causal interventions, formal proofs), governance (auditability,
alignment verification), and domain-specific (safety-critical applications).
Distinguish necessity claims (MI required for safety) from sufficiency claims
(MI enough for safety). Note that most papers argue for MI as valuable/important
rather than strictly necessary or sufficient - be precise about strength of
claims.

KEY_POSITIONS:
- Strong Necessity: 4 papers - MI needed for functional understanding required
  by safety (Kästner & Crook, Bereska & Gavves, Lee, Sengupta et al.)
- Methodological: 3 papers - MI as best scientific approach but needs
  philosophical grounding (Williams et al., Rabiza, Ayonrinde & Jaburi)
- Technical Demonstration: 3 papers - MI enables specific safety capabilities
  (Chen et al., Gross et al., Garcia-Carrasco et al.)
- Cautious Optimism: 2 papers - MI promising but faces challenges (Bereska &
  Gavves dual-use, Williams et al. scalability)
====================================================================
}

@article{kastner2024explaining,
  author = {K{\"a}stner, Lena and Crook, Barnaby},
  title = {Explaining AI through Mechanistic Interpretability},
  journal = {European Journal for Philosophy of Science},
  year = {2024},
  volume = {14},
  number = {4},
  pages = {52},
  doi = {10.1007/s13194-024-00614-4},
  url = {https://www.semanticscholar.org/paper/c76144130347dc9be9b2b02bbea157714d84391a},
  note = {
  CORE ARGUMENT: This paper argues that traditional explainable AI (XAI) employs
  a "divide-and-conquer" strategy that fails to illuminate how trained AI systems
  work as a whole. The authors claim that functional understanding of complete
  systems is needed to satisfy important societal desiderata such as safety. They
  advocate for mechanistic interpretability - applying coordinated discovery
  strategies from the life sciences to uncover the functional organization of
  complex AI systems - as the remedy for XAI's limitations. The argument is
  explicitly normative: AI researchers *should* seek mechanistic interpretability
  to achieve safety.

  RELEVANCE: This is the foundational philosophical paper for Domain 5,
  explicitly arguing that MI is necessary for AI safety. It directly addresses
  the research project's central question by claiming that safety requires
  functional understanding, which requires MI. The paper's argument that
  divide-and-conquer XAI is insufficient for safety provides the key premise for
  MI necessity claims. However, the paper argues for necessity rather than
  sufficiency - it claims MI is needed for safety but does not claim it alone is
  enough.

  POSITION: Strong MI necessity argument grounded in philosophy of science.
  Contrasts MI with traditional XAI and argues coordinated mechanistic discovery
  is required for safety assurance. Cited by 23 subsequent papers examining MI's
  philosophical foundations and safety applications.
  },
  keywords = {mechanistic-interpretability, AI-safety, necessity-argument, philosophy-of-XAI, High}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety: A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  volume = {},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2404.14082},
  arxivId = {2404.14082},
  url = {https://www.semanticscholar.org/paper/8b750488d139f9beba0815ff8f46ebe15ebb3e58},
  note = {
  CORE ARGUMENT: This comprehensive review argues that understanding AI systems'
  inner workings is "critical for ensuring value alignment and safety." The
  authors claim mechanistic interpretability - reverse engineering computational
  mechanisms into human-understandable algorithms - provides "granular, causal
  understanding" necessary for safety. They argue MI could "help prevent
  catastrophic outcomes as AI systems become more powerful and inscrutable" by
  enabling benefits in understanding, control, and alignment. The review also
  acknowledges dual-use risks: MI could enable capability gains that increase
  danger.

  RELEVANCE: This is the most comprehensive technical review arguing for MI's
  necessity for AI safety. It directly supports the research project by
  cataloging specific safety benefits MI provides (understanding model behaviors,
  enabling control interventions, verifying alignment) and explaining why these
  are critical as AI systems scale. The dual-use acknowledgment is important:
  even advocates recognize MI is not pure safety benefit. The paper's 288
  citations indicate its influence as a definitive technical statement of the
  MI-for-safety position.

  POSITION: Strong necessity argument from technical perspective. Advocates for
  MI as critical safety infrastructure while acknowledging risks and challenges
  (scalability, automation, dual-use concerns). Most cited paper in this domain,
  establishing technical case for MI's safety importance.
  },
  keywords = {mechanistic-interpretability, AI-safety, technical-review, necessity-argument, High}
}

@article{sengupta2025interpretability,
  author = {Sengupta, Aadit and Seth, Pratinav and Sankarapu, Vinay Kumar},
  title = {Interpretability as Alignment: Making Internal Understanding a Design Principle},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2509.08592},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2509.08592},
  arxivId = {2509.08592},
  url = {https://www.semanticscholar.org/paper/ffdadb6c06013a7ad4265087bce7dbd15811ebdd},
  note = {
  CORE ARGUMENT: This paper argues that frontier AI systems require governance
  mechanisms that can verify internal alignment, not just behavioral compliance,
  and that mechanistic interpretability provides the necessary "technical
  substrate" for such verification. The authors frame interpretability not as
  post-hoc explanation but as a design constraint that embeds auditability,
  provenance, and bounded transparency within model architectures. They claim MI
  provides the infrastructure for private AI governance by bridging "the gap
  between technical reliability and institutional accountability."

  RELEVANCE: This paper makes a governance-focused necessity argument: MI is
  necessary because existing behavioral testing cannot verify internal alignment,
  and governance/audit mechanisms require causal evidence about model behavior
  that only MI can provide. This connects the MI necessity claim to regulatory
  and institutional contexts, complementing the technical and philosophical
  arguments. The "interpretability-first" design principle suggests MI is
  necessary not just for understanding but for building safe-by-design systems.

  POSITION: MI necessity for governance and assurance. Argues interpretability
  must be built into systems from the start rather than added post-hoc.
  Represents the alignment/governance perspective on why MI is necessary.
  },
  keywords = {mechanistic-interpretability, AI-safety, governance, alignment-verification, High}
}

@article{lee2025mechanistic,
  author = {Lee, Yoon Pyo},
  title = {Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2507.09931},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2507.09931},
  arxivId = {2507.09931},
  url = {https://www.semanticscholar.org/paper/4e2bafb0a851afa00bc4a6a635acb175ee16fc2d},
  note = {
  CORE ARGUMENT: This paper argues that integrating LLMs into safety-critical
  domains like nuclear engineering "necessitates a deep understanding of their
  internal reasoning processes." Through a case study on nuclear reactor safety,
  the author demonstrates that MI techniques can identify specialized neurons
  whose activation patterns causally determine task performance. The paper claims
  this provides "a pathway towards achieving nuclear-grade artificial
  intelligence (AI) assurance" by addressing verification and validation
  challenges that have "limited AI deployment in safety-critical nuclear
  operations."

  RELEVANCE: This provides domain-specific evidence for MI necessity in
  safety-critical applications. The nuclear context is particularly strong
  because regulatory frameworks (10 CFR 50 Appendix B) mandate verification that
  traditional black-box testing cannot provide. The paper demonstrates that MI
  enables tracing domain expertise to verifiable neural circuits, supporting the
  claim that MI is necessary for deploying AI in contexts with strict safety
  requirements. However, it demonstrates feasibility on a small model, leaving
  open whether MI scales to frontier systems.

  POSITION: MI necessity for safety-critical domain deployment. Demonstrates
  concrete MI application meeting regulatory verification requirements that
  behavioral testing cannot satisfy.
  },
  keywords = {mechanistic-interpretability, safety-critical-systems, nuclear-safety, verification, High}
}

@article{williams2025mechanistic,
  author = {Williams, Iwan and Oldenburg, Ninell and Dhar, Ruchira and Hatherley, Joshua and Fierro, Constanza and Rajcic, Nina and Schiller, Sandrine R. and Stamatiou, Filippos and S{\o}gaard, Anders},
  title = {Mechanistic Interpretability Needs Philosophy},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2506.18852},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2506.18852},
  arxivId = {2506.18852},
  url = {https://www.semanticscholar.org/paper/a5cbe230af78780bd14c5472d8e089c16f832b28},
  note = {
  CORE ARGUMENT: This position paper argues that mechanistic interpretability
  aims to explain neural networks through underlying causal mechanisms, but the
  field needs philosophical engagement to clarify concepts, refine methods, and
  assess epistemic and ethical stakes. The authors do not argue MI is necessary
  or sufficient for safety, but rather that MI research requires philosophical
  partnership to establish what mechanistic explanations are, when they succeed,
  and what they can achieve for safety. They illustrate this through three open
  problems requiring philosophical analysis.

  RELEVANCE: While not explicitly arguing for MI's necessity, this paper supports
  the research project by identifying conceptual gaps in MI research that affect
  its safety applications. The call for philosophical clarity on what counts as a
  mechanism, what explanatory success requires, and how MI relates to safety
  goals directly addresses weaknesses in strong necessity/sufficiency claims. The
  paper's interdisciplinary authorship (philosophers and ML researchers) models
  the kind of analysis needed to evaluate MI's role in safety rigorously.

  POSITION: Methodological position: MI is a promising approach but needs
  philosophical grounding to fulfill its potential for safety. Neither endorses
  nor rejects necessity claims but argues current MI research lacks conceptual
  foundations to support strong claims.
  },
  keywords = {mechanistic-interpretability, philosophy-of-AI, methodology, Medium}
}

@article{rabiza2024mechanistic,
  author = {Rabiza, Marcin},
  title = {A Mechanistic Explanatory Strategy for XAI},
  journal = {arXiv preprint},
  year = {2024},
  volume = {abs/2411.01332},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2411.01332},
  arxivId = {2411.01332},
  url = {https://www.semanticscholar.org/paper/fb05ae3cadeb7c7b47af80a845eda6297511fe3d},
  note = {
  CORE ARGUMENT: This paper outlines a mechanistic explanatory strategy for XAI
  grounded in philosophy of science, arguing that explaining opaque AI systems
  requires identifying underlying mechanisms through decomposition, localization,
  and recomposition. The author claims mechanistic principles provide "a
  foundation for a coordinated research strategy" beyond isolated XAI techniques,
  enabling "more holistic and systematic understanding" of AI models. The paper
  connects K{\"a}stner & Crook's philosophical framework to concrete MI case
  studies (circuits in GPT-2, vision models).

  RELEVANCE: This paper bridges philosophical arguments for MI (K{\"a}stner &
  Crook) and technical demonstrations, providing theoretical scaffolding for the
  claim that MI enables superior understanding. It argues mechanistic approaches
  are epistemically advantageous for understanding functional organization, but
  also acknowledges limitations (scalability, identifiability). The philosophical
  grounding helps clarify what mechanistic understanding provides and whether it
  is necessary for safety - though the paper focuses more on understanding than
  safety applications.

  POSITION: Methodological/philosophical support for MI as explanatory strategy.
  Argues mechanistic approach is epistemically superior to correlation-based XAI
  but acknowledges challenges. Cited heavily by K{\"a}stner & Crook, establishing
  philosophical foundations for MI necessity claims.
  },
  keywords = {mechanistic-interpretability, philosophy-of-science, XAI-methodology, Medium}
}

@inproceedings{ayonrinde2025mathematical,
  author = {Ayonrinde, Kola and Jaburi, Louis},
  title = {A Mathematical Philosophy of Explanations in Mechanistic Interpretability: The Strange Science Part I.i},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2025},
  volume = {},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2505.00808},
  arxivId = {2505.00808},
  url = {https://www.semanticscholar.org/paper/e7596e53f54d9a90be8b960771b1265013b2e864},
  note = {
  CORE ARGUMENT: This paper argues for the "Explanatory View Hypothesis": that
  mechanistic interpretability is a principled approach to understanding models
  because neural networks contain implicit explanations which can be extracted
  and understood. The authors propose a definition of MI as producing
  "Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations" and
  formulate the "Principle of Explanatory Optimism" - a necessary precondition
  for MI's success. They argue that Explanatory Faithfulness (how well
  explanations fit models) is well-defined, making MI scientifically rigorous.

  RELEVANCE: This paper provides philosophical foundations for MI's viability as
  a safety approach by arguing that mechanistic explanations of neural networks
  are coherent and achievable in principle. The "Explanatory Optimism" principle
  is critical: if neural networks don't contain extractable causal mechanisms, MI
  cannot deliver on safety promises. The paper's mathematical formalism addresses
  skepticism about whether MI explanations are well-defined, supporting the claim
  that MI can provide the understanding safety requires.

  POSITION: Philosophical defense of MI's coherence as explanatory paradigm.
  Argues MI is well-defined and viable but requires accepting Explanatory
  Optimism. Provides foundations for evaluating whether MI can deliver safety
  benefits it promises.
  },
  keywords = {mechanistic-interpretability, philosophy-of-explanation, foundations, Medium}
}

@article{chen2024towards,
  author = {Chen, Jianhui and Wang, Xiaozhi and Yao, Zijun and Bai, Yushi and Hou, Lei and Li, Juanzi},
  title = {Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons},
  journal = {arXiv preprint},
  year = {2024},
  volume = {abs/2406.14144},
  number = {},
  pages = {},
  doi = {},
  arxivId = {2406.14144},
  url = {https://www.semanticscholar.org/paper/33bc46dff817e4afe6bc2c11bc808478b5207847},
  note = {
  CORE ARGUMENT: This paper demonstrates that mechanistic interpretability can
  identify "safety neurons" - a sparse set (~5%) of neurons causally responsible
  for safety behaviors in LLMs. The authors show that patching activations of
  these neurons can restore over 90% of safety performance across red-teaming
  benchmarks without affecting general capabilities. They argue this mechanistic
  understanding explains the "alignment tax" phenomenon and enables practical
  safety interventions like detecting unsafe outputs before generation.

  RELEVANCE: This provides empirical evidence that MI enables specific safety
  capabilities (understanding, control, detection) that behavioral methods cannot
  achieve. The causal intervention experiments (activation patching) demonstrate
  MI is not just descriptive but enables control. However, the paper demonstrates
  this on specific models/tasks rather than proving MI is necessary in general.
  The dual nature of safety neurons (different activation patterns for safety vs
  helpfulness) suggests MI understanding is necessary to balance competing
  objectives.

  POSITION: Technical demonstration that MI enables safety understanding and
  control. Provides evidence for MI's utility but does not argue for strict
  necessity. Shows MI can achieve specific safety goals (detection, restoration)
  that justify its development.
  },
  keywords = {mechanistic-interpretability, safety-alignment, activation-steering, causal-intervention, Medium}
}

@inproceedings{gross2024compact,
  author = {Gross, Jason and Agrawal, Rajashree and Kwa, Thomas and Ong, Euan and Yip, Chun Hei and Gibson, Alex and Noubir, Soufiane and Chan, Lawrence},
  title = {Compact Proofs of Model Performance via Mechanistic Interpretability},
  booktitle = {Neural Information Processing Systems},
  year = {2024},
  volume = {},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2406.11779},
  arxivId = {2406.11779},
  url = {https://www.semanticscholar.org/paper/8487f133fed81e18caf17ecb0d2917a84d5fd218},
  note = {
  CORE ARGUMENT: This paper proposes using mechanistic interpretability to derive
  and compactly prove formal guarantees on model performance. The authors
  demonstrate this by formally proving accuracy lower bounds for small
  transformers, finding that shorter proofs require more mechanistic
  understanding and that more faithful understanding leads to tighter performance
  bounds. They argue MI can provide performance guarantees that behavioral
  testing alone cannot achieve.

  RELEVANCE: This demonstrates a novel safety capability MI uniquely enables:
  formal performance proofs. If safety assurance requires formal guarantees
  rather than empirical testing, this supports MI's necessity. The finding that
  mechanistic understanding enables tighter bounds suggests MI provides epistemic
  advantages for safety. However, the demonstration is on small toy models; the
  key question is whether this scales to frontier systems. The "compounding
  structureless errors" challenge they identify may limit MI's sufficiency.

  POSITION: MI enables formal verification capabilities behavioral testing
  cannot provide. Argues for MI's unique value but identifies scalability
  challenges that may limit sufficiency for frontier model safety.
  },
  keywords = {mechanistic-interpretability, formal-verification, model-performance, Medium}
}

@inproceedings{garciacarrasco2024detecting,
  author = {Garc{\'i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan},
  title = {Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability},
  booktitle = {International Joint Conference on Artificial Intelligence},
  year = {2024},
  volume = {},
  number = {},
  pages = {385--393},
  doi = {10.24963/ijcai.2024/43},
  arxivId = {2407.19842},
  url = {https://www.semanticscholar.org/paper/fa0cbfba4e41b9f2487df251fcc3b93c21381167},
  note = {
  CORE ARGUMENT: This paper proposes using mechanistic interpretability to detect
  and understand vulnerabilities in LLMs that make them prone to adversarial
  attacks. The authors demonstrate that MI techniques can locate circuits
  responsible for specific tasks, generate adversarial samples, and discover
  concrete vulnerabilities in model components. They argue this enables proactive
  security that behavioral testing cannot achieve - identifying where and how
  vulnerabilities exist rather than just detecting failures.

  RELEVANCE: This demonstrates another safety capability MI uniquely enables:
  understanding vulnerabilities at the mechanistic level to guide robust
  interventions. The adversarial robustness angle complements alignment-focused
  MI safety arguments. However, the paper demonstrates feasibility on small
  models (GPT-2 Small) with simple tasks (acronym prediction), leaving open
  whether MI can detect complex vulnerabilities in frontier systems. The approach
  requires knowing what task/vulnerability to look for.

  POSITION: MI enables vulnerability detection and understanding beyond
  behavioral testing. Demonstrates specific safety benefit but on limited scope.
  Supports MI's value for security alongside alignment.
  },
  keywords = {mechanistic-interpretability, adversarial-robustness, vulnerability-detection, Medium}
}

@article{pan2025hidden,
  author = {Pan, Wenbo and Liu, Zhichao and Chen, Qiguang and Zhou, Xiangyang and Yu, Haining and Jia, Xiaohua},
  title = {The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2502.09674},
  number = {},
  pages = {},
  doi = {},
  arxivId = {2502.09674},
  url = {https://www.semanticscholar.org/paper/ba37af16b72f5250774e014d7ac6a7051585c0ca},
  note = {
  CORE ARGUMENT: This paper discovers that LLM safety-aligned behavior is jointly
  controlled by multi-dimensional directions in activation space rather than a
  single direction. Using mechanistic analysis, the authors identify a dominant
  refusal direction plus multiple secondary directions representing distinct
  safety features (hypothetical narratives, role-playing). They show how
  different directions promote or suppress the dominant direction and demonstrate
  that removing certain trigger tokens can bypass learned safety by mitigating
  these directions.

  RELEVANCE: This provides mechanistic understanding of how safety alignment
  works and fails, demonstrating that MI reveals multi-dimensional safety
  mechanisms behavioral analysis misses. The finding that safety can be bypassed
  by manipulating secondary directions suggests behavioral alignment testing is
  insufficient - mechanistic understanding is needed to identify vulnerabilities.
  However, the paper reveals complexity that may challenge sufficiency claims: if
  safety emerges from many interacting directions, comprehensive MI may be
  intractable.

  POSITION: MI reveals multi-dimensional nature of safety alignment that
  behavioral methods cannot detect. Supports necessity argument (behavioral
  testing misses mechanisms) but complicates sufficiency (complexity of safety
  mechanisms may exceed MI capacity to analyze).
  },
  keywords = {mechanistic-interpretability, safety-alignment, multi-dimensional-analysis, Medium}
}

@article{perrier2025control,
  author = {Perrier, Elija},
  title = {Out of Control: Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)},
  journal = {arXiv preprint},
  year = {2025},
  volume = {abs/2506.17846},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2506.17846},
  arxivId = {2506.17846},
  url = {https://www.semanticscholar.org/paper/0041659f2dec5fe14037578764a8018909561b24},
  note = {
  CORE ARGUMENT: This position paper argues that formal optimal control theory
  should be central to AI alignment, offering distinct advantages over prevailing
  AI safety approaches. The author introduces an "Alignment Control Stack" - a
  hierarchical framework from physical to socio-technical layers for applying
  controls. While acknowledging recent advances in mechanistic interpretability,
  the paper argues they "often fall short of the generalisation required of
  control frameworks" and lack research into rendering different alignment/control
  protocols interoperable.

  RELEVANCE: This provides critical perspective on MI's sufficiency for safety by
  arguing that interpretation alone is insufficient - safety requires control
  frameworks. The claim that MI lacks generalization and interoperability
  challenges strong sufficiency claims. However, the paper does not reject MI but
  rather argues it must be integrated with formal control methods. This suggests
  MI may be necessary (for understanding systems to control) but not sufficient
  (control theory also required) for safety.

  POSITION: Critique of MI sufficiency from control theory perspective. Argues MI
  must be complemented with formal control frameworks for safety. Represents view
  that understanding (MI) alone insufficient - control also required.
  },
  keywords = {AI-safety, control-theory, alignment, MI-limitations, Low}
}

@article{delaney2024mapping,
  author = {Delaney, Oscar and Guest, Oliver and Williams, Zoe},
  title = {Mapping Technical Safety Research at AI Companies: A Literature Review and Incentives Analysis},
  journal = {arXiv preprint},
  year = {2024},
  volume = {abs/2409.07878},
  number = {},
  pages = {},
  doi = {10.48550/arXiv.2409.07878},
  arxivId = {2409.07878},
  url = {https://www.semanticscholar.org/paper/35b4ea799b27efb465a547e26aaa4bfacd21bbff},
  note = {
  CORE ARGUMENT: This literature review maps technical safety research at AI
  companies, categorizing approaches and analyzing incentive structures. The
  paper documents that mechanistic interpretability is one of several technical
  safety research directions being pursued, alongside scalable oversight,
  adversarial robustness, and other approaches. The incentives analysis examines
  what drives companies to invest in different safety methods.

  RELEVANCE: This provides context for evaluating MI necessity/sufficiency claims
  by showing MI is one approach among many in the technical safety landscape. The
  paper does not argue MI is necessary or sufficient but documents it as part of
  a portfolio of methods companies pursue. This portfolio approach suggests
  practitioners view multiple complementary methods as needed rather than MI
  alone being sufficient. The incentives analysis can inform questions about why
  MI receives attention relative to other approaches.

  POSITION: Neutral/descriptive mapping of safety research landscape including
  MI. Does not argue for or against necessity/sufficiency but provides context
  showing MI as one component of multi-pronged safety efforts.
  },
  keywords = {AI-safety, technical-safety-research, landscape-analysis, Low}
}
