@comment{
====================================================================
DOMAIN: Explainable AI (XAI) Taxonomies and Definitions
SEARCH_DATE: 2026-01-01
PAPERS_FOUND: 18 total (High: 9, Medium: 7, Low: 2)
SEARCH_SOURCES: PhilPapers, arXiv, Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
The Explainable AI (XAI) literature provides the conceptual architecture within which mechanistic interpretability (MI) must be situated. This domain is characterized by multiple taxonomic efforts that distinguish between post-hoc vs. inherent interpretability, local vs. global explanations, and model-agnostic vs. model-specific methods. The foundational Barredo Arrieta et al. (2019) survey establishes a comprehensive taxonomy distinguishing transparent models (inherently interpretable) from black-box models requiring post-hoc explanation. Philosophical analyses (Erasmus et al. 2020, Zednik 2021) clarify terminological confusion between "explainability," "interpretability," and "understandability," arguing these concepts serve different epistemic purposes. Recent work has identified a fundamental tension: Rudin (2019) argues for abandoning post-hoc explanations in favor of inherently interpretable models, while LIME (Ribeiro et al. 2016) and SHAP (Lundberg & Lee 2017) exemplify influential post-hoc methods. The landscape reveals competing frameworks: some organize XAI by method type (model-agnostic vs. specific), others by scope (local vs. global), and still others by explanatory purpose (diagnostic, expectation, role). Mechanistic interpretability represents a distinctive position within this taxonomy—it aims for inherent interpretability but through reverse engineering rather than by design, seeking global understanding through local circuit analysis.

RELEVANCE_TO_PROJECT:
This domain is essential for the MI necessity/sufficiency project because it provides the conceptual landscape against which MI's distinctiveness can be assessed. Understanding XAI taxonomies reveals whether MI is genuinely novel or merely rebrands existing approaches. The tension between post-hoc and inherent interpretability directly illuminates whether MI offers something categorically different from methods like LIME/SHAP (post-hoc, local) or decision trees (inherent by design). The philosophical clarifications of "interpretability" vs. "explainability" help assess what epistemic goals MI actually serves compared to other XAI approaches.

NOTABLE_GAPS:
While taxonomies effectively categorize existing methods, few systematically analyze how different XAI approaches serve different epistemic purposes beyond superficial classification. The relationship between mechanistic interpretability and traditional XAI categories remains underexplored—most surveys predate MI's emergence as a distinct research program. There is limited philosophical analysis of whether different XAI methods provide genuinely different kinds of understanding or merely different presentation formats of the same underlying information.

SYNTHESIS_GUIDANCE:
When synthesizing, emphasize the taxonomic dimensions that most clearly differentiate MI from other XAI approaches: (1) inherent vs. post-hoc, (2) by-design vs. reverse-engineered interpretability, (3) local-to-global vs. global-by-aggregation explanatory strategies, and (4) feature-based vs. mechanism-based understanding. Highlight philosophical analyses that clarify what different forms of "interpretability" actually provide epistemically. The Rudin critique of post-hoc explanations provides a useful foil for assessing MI's position.

KEY_POSITIONS:
- Taxonomists (9 papers): Classify XAI methods by technical properties (model-agnostic, local/global, post-hoc/inherent)
- Philosophers (4 papers): Clarify conceptual distinctions and epistemic purposes of different explanation types
- Method developers (5 papers): LIME, SHAP, and mechanistic approaches exemplifying different taxonomic categories
====================================================================
}

@article{barredoArrieta2019explainable,
  author = {Barredo Arrieta, Alejandro and D{\'i}az-Rodr{\'i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  title = {Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
  journal = {Information Fusion},
  year = {2019},
  volume = {58},
  pages = {82--115},
  doi = {10.1016/j.inffus.2019.12.012},
  note = {
  CORE ARGUMENT: Presents the most comprehensive and widely-cited taxonomy of XAI methods, distinguishing transparent models (simulatable, decomposable, algorithmically transparent) from post-hoc explanation techniques (text, visual, local, global, model-specific, model-agnostic). Argues XAI is essential for trustworthy, responsible AI deployment and identifies key challenges including the accuracy-interpretability trade-off.

  RELEVANCE: This foundational taxonomy provides the standard framework for situating mechanistic interpretability within the broader XAI landscape. MI's attempt to reverse-engineer neural network algorithms sits ambiguously in this taxonomy—seeking the transparency of inherent interpretability but through post-hoc reverse engineering rather than by-design transparency. The paper's distinction between "transparency" (understanding the model itself) and "post-hoc interpretability" (explaining model behavior) directly illuminates MI's hybrid status.

  POSITION: Represents the dominant taxonomic framework in XAI research, emphasizing method categorization over epistemic purpose. Highly cited (7400+) as the field's standard reference.
  },
  keywords = {XAI-taxonomy, foundational-survey, transparency-vs-posthoc, High}
}

@article{erasmus2020what,
  author = {Erasmus, Adrian and Brunet, Tyler D. P. and Fisher, Eyal},
  title = {What is Interpretability?},
  journal = {Philosophy \& Technology},
  year = {2020},
  volume = {34},
  pages = {833--862},
  doi = {10.1007/s13347-020-00435-2},
  note = {
  CORE ARGUMENT: Argues the XAI literature conflates "explainability," "understandability," and "interpretability" as distinct concepts. Proposes interpretability is an operation performed ON an explanation to make it more understandable, not a property of models themselves. Develops a typology: Total vs. Partial, Global vs. Local, Approximative vs. Isomorphic interpretation. Applies standard philosophy of science frameworks (D-N, causal-mechanical) to neural networks rather than inventing XAI-specific explanation theories.

  RELEVANCE: This conceptual clarification is crucial for the MI project because it questions whether "interpretability" is a model property or an epistemic activity. If Erasmus et al. are correct that interpretation is something we DO to explanations, then MI's claim to provide "inherently interpretable" models becomes problematic—it may instead provide explanations that still require interpretation. The typology helps distinguish MI's goals (isomorphic, global understanding via local circuits) from approximative methods like LIME.

  POSITION: Philosophical corrective to XAI terminology, arguing the field reinvented philosophical wheels. Challenges whether XAI methods provide genuinely novel forms of explanation.
  },
  keywords = {interpretability-definition, conceptual-clarification, philosophy-of-explanation, High}
}

@article{zednik2021solving,
  author = {Zednik, Carlos},
  title = {Solving the Black Box Problem: A Normative Framework for Explainable Artificial Intelligence},
  journal = {Philosophy \& Technology},
  year = {2021},
  volume = {34},
  pages = {265--288},
  doi = {10.1007/s13347-019-00382-7},
  note = {
  CORE ARGUMENT: Proposes a normative framework for evaluating XAI methods based on Marr's levels of analysis (computational, algorithmic, implementational). Argues successful XAI must answer different questions for different stakeholders at different levels. Distinguishes "opacity" (lack of understanding) from "inaccessibility" (technical barriers to inspection). Evaluates LIME, feature detection, and diagnostic classification against this framework.

  RELEVANCE: Zednik's multi-level framework provides analytical tools for assessing what MI actually explains. Does MI operate at Marr's algorithmic level (what algorithm the network implements) or implementational level (how neurons physically realize computation)? This matters for the necessity/sufficiency question: if MI targets the algorithmic level, it may be necessary for certain explanatory goals where implementation-level or computational-level methods suffice for others. The framework reveals MI might conflate levels—claiming algorithmic understanding while providing implementation-level detail.

  POSITION: Normative philosophical framework for evaluating XAI methods. Influential in clarifying what successful XAI requires and at what level of abstraction.
  },
  keywords = {XAI-framework, Marr-levels, opacity-analysis, High}
}

@article{rudin2019stop,
  author = {Rudin, Cynthia},
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  journal = {Nature Machine Intelligence},
  year = {2019},
  volume = {1},
  pages = {206--215},
  doi = {10.1038/s42256-019-0048-x},
  note = {
  CORE ARGUMENT: Argues post-hoc explanations of black-box models are fundamentally flawed for high-stakes decisions: they are unreliable approximations, can be gamed, and miss the opportunity to use inherently interpretable models with comparable accuracy. Advocates abandoning complex models in favor of interpretable-by-design alternatives (sparse linear models, decision trees, rule lists) except where black boxes demonstrably achieve substantially higher accuracy. Claims the accuracy-interpretability trade-off is often a myth.

  RELEVANCE: Rudin's critique creates a crucial tension for evaluating MI. If post-hoc explanations are unreliable, does MI count as "post-hoc" (reverse engineering trained models) or "inherent" (revealing the actual algorithm)? MI advocates claim to extract the true mechanisms, not approximations—but Rudin's argument applies if MI's circuit descriptions are themselves lossy or selective representations. The paper raises stakes for the necessity question: if inherently interpretable models suffice, why reverse-engineer opaque ones? MI must either demonstrate that neural networks outperform interpretable alternatives in ways that justify reverse engineering, or show that MI provides a fundamentally different kind of understanding than either black boxes with post-hoc explanations or inherently interpretable models.

  POSITION: Radical critique of post-hoc XAI paradigm. Highly influential (7400+ citations) and controversial argument for inherent interpretability.
  },
  keywords = {inherent-vs-posthoc, Rudin-critique, interpretable-models, High}
}

@inproceedings{lundberg2017unified,
  author = {Lundberg, Scott M. and Lee, Su-In},
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  year = {2017},
  pages = {4765--4774},
  note = {
  CORE ARGUMENT: Introduces SHAP (SHapley Additive exPlanations), a unified framework for feature importance that assigns each feature a value for a particular prediction based on game-theoretic Shapley values. Proves SHAP is the unique solution satisfying three desirable properties: local accuracy, missingness, and consistency. Shows SHAP unifies six existing feature attribution methods (LIME, DeepLIFT, Layer-Wise Relevance Propagation, etc.) as special cases.

  RELEVANCE: SHAP exemplifies the dominant paradigm in XAI that MI positions itself against: post-hoc, model-agnostic, local feature attribution. Understanding SHAP's approach—assigning importance scores to input features for individual predictions—highlights what MI does differently. MI seeks to identify intermediate computational mechanisms (circuits, features in hidden layers) rather than attributing importance to inputs. However, both approaches face similar challenges: SHAP's "feature importance" scores don't necessarily reveal HOW features are processed, just their marginal contribution; MI's circuits don't necessarily reveal WHY certain computations are useful. The comparison illuminates whether MI and SHAP answer fundamentally different questions or approach the same question differently.

  POSITION: Dominant post-hoc, model-agnostic explanation method. Massively influential (28,000+ citations) technical contribution unifying feature attribution approaches.
  },
  keywords = {SHAP, post-hoc-explanation, feature-attribution, High}
}

@inproceedings{ribeiro2016why,
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2016},
  pages = {1135--1144},
  doi = {10.1145/2939672.2939778},
  note = {
  CORE ARGUMENT: Introduces LIME (Local Interpretable Model-agnostic Explanations), which explains any classifier's predictions by approximating the model locally with an interpretable model (e.g., linear regression, decision tree). Argues explanation requires both interpretability (presentation in terms humans can understand) and local fidelity (accurate representation of model behavior in the vicinity of the instance). Demonstrates LIME helps users identify when NOT to trust a model.

  RELEVANCE: LIME represents the archetype of post-hoc, local, model-agnostic explanation that MI implicitly critiques. LIME approximates a black box locally with simpler models; MI attempts to reveal the actual mechanisms. This contrast illuminates a key question: is the "true" explanation the actual circuit (MI) or a simplified approximation that's easier to understand (LIME)? The trade-off between fidelity and simplicity appears differently: LIME sacrifices global fidelity for local interpretability; MI claims global fidelity but may sacrifice understandability due to circuit complexity. Understanding LIME's approach helps assess whether MI's mechanistic explanations are necessary or whether local approximations suffice for practical understanding.

  POSITION: Second most cited XAI paper (19,000+ citations). Paradigmatic model-agnostic, post-hoc, local explanation method.
  },
  keywords = {LIME, post-hoc-explanation, local-explanation, model-agnostic, High}
}

@article{das2020opportunities,
  author = {Das, Arun and Rad, Paul},
  title = {Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey},
  journal = {arXiv preprint arXiv:2006.11371},
  year = {2020},
  note = {
  CORE ARGUMENT: Comprehensive survey categorizing XAI techniques by scope (local vs. global), methodology (backpropagation, perturbation, surrogate models), and usage (pre-model, in-model, post-model interpretability). Proposes a taxonomy organizing methods by when explanation occurs in the ML pipeline. Reviews evaluation metrics for explanations and identifies key challenges: trade-off between accuracy and interpretability, lack of ground truth for evaluating explanations, and computational costs of explanation generation.

  RELEVANCE: This survey's pipeline-based taxonomy (pre-model, in-model, post-model) provides another lens for situating MI. Traditional MI appears "post-model" (reverse engineering trained networks) but aims for "in-model" understanding (what the model actually computes). This ambiguity matters for assessing necessity: if MI is post-model, it faces Rudin's critique of post-hoc methods; if in-model, it should be compared to inherently interpretable architectures. The survey's discussion of evaluation challenges applies directly to MI—how can we verify that identified circuits are the "true" explanation rather than one of many possible descriptions?

  POSITION: Comprehensive technical survey organizing XAI by pipeline stage and methodology. Useful for its systematic coverage of evaluation challenges.
  },
  keywords = {XAI-survey, taxonomy-pipeline, evaluation-metrics, Medium}
}

@article{li2020survey,
  author = {Li, Xiao-hui and Cao, Caleb Chen and Shi, Yuhan and Bai, Wei and Gao, Han and Qiu, Luyu and Wang, Cong and Gao, Yuanyuan and Zhang, Shenjia and Xue, Xun and Chen, Lei},
  title = {A Survey of Data-Driven and Knowledge-Aware eXplainable AI},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  year = {2020},
  volume = {34},
  pages = {29--49},
  doi = {10.1109/tkde.2020.2983930},
  note = {
  CORE ARGUMENT: Distinguishes between data-driven XAI methods (operating purely on data patterns) and knowledge-aware methods (incorporating external knowledge, ontologies, causal graphs). Argues knowledge-aware approaches produce more meaningful and trustworthy explanations by grounding them in domain expertise rather than purely statistical patterns. Provides taxonomy differentiating methods by their use of prior knowledge vs. learned representations.

  RELEVANCE: The data-driven vs. knowledge-aware distinction illuminates an important dimension for situating MI. Pure MI (reverse engineering without external constraints) is data-driven—circuits are identified from activation patterns without domain knowledge. However, some MI practitioners incorporate task knowledge (e.g., searching for "induction circuits" based on theories of in-context learning). This hybrid status matters for assessing MI's epistemic value: purely data-driven circuit discovery may find arbitrary correlations; knowledge-guided MI may import assumptions that bias findings. The tension parallels debates in philosophy of science about theory-laden observation.

  POSITION: Novel taxonomic dimension distinguishing data-driven from knowledge-aware XAI. Relatively influential (200+ citations) for emphasizing role of prior knowledge.
  },
  keywords = {data-driven-vs-knowledge, XAI-taxonomy, knowledge-integration, Medium}
}

@article{schneider2024explainable,
  author = {Schneider, Johannes},
  title = {Explainable Generative AI (GenXAI): A Survey, Conceptualization, and Research Agenda},
  journal = {Artificial Intelligence Review},
  year = {2024},
  volume = {57},
  doi = {10.1007/s10462-024-10916-x},
  note = {
  CORE ARGUMENT: Argues generative AI (GenAI) creates new explainability challenges and opportunities compared to discriminative models. Traditional XAI focuses on explaining predictions; GenAI requires explaining generation processes, training data influence, and failure modes like hallucination. Proposes new explainability criteria: verifiability (can outputs be checked?), interactivity (can users query explanations?), and security (can explanations reveal training data?). Develops taxonomy distinguishing explanation methods by what they target: training data, prompts, generation process, or outputs.

  RELEVANCE: While focused on generative models, this survey's emphasis on explaining generation PROCESSES rather than just predictions resonates with MI's mechanistic focus. The distinction between explaining "what" was generated vs. "how" it was generated parallels MI's emphasis on mechanisms over input-output mappings. The paper's discussion of verifiability applies to MI: how can we verify that identified circuits actually implement the claimed algorithms? GenXAI's interactive dimension suggests a criterion for evaluating MI—can mechanistic explanations support meaningful user interaction (e.g., editing circuits to change behavior)?

  POSITION: Recent survey extending XAI concepts to generative models. Emerging influence (70+ citations) for identifying new explainability challenges.
  },
  keywords = {generative-AI, XAI-extensions, process-explanation, Medium}
}

@article{yao2021explanatory,
  author = {Yao, Yiheng},
  title = {Explanatory Pluralism in Explainable AI},
  journal = {Lecture Notes in Computer Science},
  year = {2021},
  pages = {275--292},
  doi = {10.1007/978-3-030-84060-0_18},
  note = {
  CORE ARGUMENT: Proposes an explanatory pluralism framework for XAI based on viewing causation as manipulable relationships. Distinguishes four types of explanations serving different purposes: Diagnostic (exposing inner mechanisms), Explication (making outputs understandable), Expectation (forming stable generalizations), and Role (justifying model usage in social context). Argues these are genuinely different explanation types, not just different presentations of the same information, because they identify different intervention points in AI systems.

  RELEVANCE: Yao's pluralistic framework provides sophisticated tools for analyzing whether MI offers categorically different explanations than other XAI methods. MI clearly provides "Diagnostic" explanations (exposing mechanisms), but whether it provides better "Explication" (understandability) is questionable—circuits may be mechanistically accurate but not intuitive. The framework suggests MI and methods like LIME may serve different explanatory purposes: MI for diagnosis and scientific understanding, LIME for user-facing explication. This illuminates the necessity question: MI may be necessary for some explanatory goals (understanding mechanisms) but not others (trusting predictions).

  POSITION: Philosophical framework applying explanatory pluralism to XAI. Conceptually sophisticated typology based on intervention points.
  },
  keywords = {explanatory-pluralism, XAI-philosophy, explanation-types, High}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety: A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  note = {
  CORE ARGUMENT: Comprehensive review of mechanistic interpretability methods, arguing MI is essential for AI safety because it provides causal, mechanistic understanding rather than correlational explanations. Surveys techniques: feature visualization, activation patching, causal tracing, circuit discovery, and dictionary learning. Identifies key challenges: scalability to large models, automation of circuit discovery, and comprehensive interpretation beyond toy examples. Argues MI's focus on features and circuits provides unique safety benefits: detecting deceptive alignment, monitoring for dangerous capabilities, and enabling precise interventions.

  RELEVANCE: This is the definitive MI review, providing the standard characterization of the field. Its explicit positioning of MI within XAI landscape is crucial for the necessity/sufficiency project. The paper argues MI is categorically different from other XAI approaches because it seeks causal mechanisms rather than associations, but this claim requires scrutiny—do identified circuits represent true causal mechanisms or just useful descriptions? The safety focus reveals MI's distinctive motivations (AI alignment, capability detection) compared to traditional XAI goals (trust, fairness, debugging). Understanding MI's self-conception is essential before assessing whether it delivers on its promises.

  POSITION: Authoritative MI review from within the field. Highly influential (290+ citations) for characterizing MI's methods and safety applications.
  },
  keywords = {mechanistic-interpretability, MI-review, AI-safety, circuits, High}
}

@article{kowalska2025unboxing,
  author = {Kowalska, Bianka and Kwa\'snicka, Halina},
  title = {Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks},
  journal = {arXiv preprint arXiv:2511.19265},
  year = {2025},
  note = {
  CORE ARGUMENT: Proposes a unified taxonomy of MI approaches and provides detailed analysis of key techniques with concrete examples. Contextualizes MI within the broader interpretability landscape by comparing its goals, methods, and insights to other XAI strands. Traces MI's conceptual roots in cognitive science and neuroscience. Argues MI offers "scientific understanding" of ML systems—treating models as systems to study, not just tools to deploy. Distinguishes MI from other XAI by its focus on reverse engineering internal algorithms rather than explaining input-output behavior.

  RELEVANCE: This recent paper provides the clearest articulation of what makes MI distinctive within XAI: the goal is algorithmic understanding (what algorithm the network implements) rather than behavioral explanation (why it made this prediction). This distinction is crucial for the necessity/sufficiency question. If MI and traditional XAI target fundamentally different questions, both may be necessary for different purposes. However, the paper also reveals MI's conceptual ambiguity—is the "algorithm" implemented by a network a property of the network itself (waiting to be discovered) or a useful description imposed by researchers (one of many possible interpretations)? This ambiguity affects whether MI provides uniquely necessary insights.

  POSITION: Recent comprehensive MI tutorial and taxonomy. Emerging influence for situating MI historically and conceptually within XAI.
  },
  keywords = {mechanistic-interpretability, MI-taxonomy, algorithmic-understanding, Medium}
}

@article{facchini2022towards,
  author = {Facchini, Alessandro and Termine, Alberto},
  title = {Towards a Taxonomy for the Opacity of AI Systems},
  journal = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2022},
  note = {
  CORE ARGUMENT: Develops a contextual taxonomy of AI opacity distinguishing between technical opacity (implementation details inaccessible), epistemic opacity (cannot understand how system works even with access), and contextual opacity (understanding varies by stakeholder). Argues opacity is not binary but exists on multiple dimensions depending on what aspect of the system is opaque to whom for what purpose. Challenges the assumption that "opening the black box" is always possible or desirable—some forms of opacity may be ineliminable.

  RELEVANCE: This taxonomy of opacity types provides analytical tools for evaluating MI's scope and limits. MI primarily addresses technical opacity (accessing implementation details through reverse engineering) and claims to reduce epistemic opacity (making mechanisms understandable). However, Facchini and Termine's framework suggests MI may reduce some forms of opacity while increasing others—exposing circuit details may create information overload that increases contextual opacity for non-expert users. This matters for the necessity question: if MI trades technical transparency for contextual opacity, simpler XAI methods may be more appropriate for some stakeholders even if MI provides deeper mechanistic insight.

  POSITION: Philosophical analysis of opacity as multidimensional and contextual. Challenges assumptions underlying much XAI research.
  },
  keywords = {opacity-taxonomy, AI-transparency, contextual-explanation, Medium}
}

@article{rawal2021recent,
  author = {Rawal, Adarsh and McCoy, James and Rawat, Danda B. and Sadler, Brian M. and Amant, Robert St.},
  title = {Recent Advances in Trustworthy Explainable Artificial Intelligence: Status, Challenges and Perspectives},
  journal = {IEEE Transactions on Artificial Intelligence},
  year = {2021},
  volume = {PP},
  pages = {1--1},
  doi = {10.1109/TAI.2021.3133846},
  note = {
  CORE ARGUMENT: Comprehensive survey of XAI methods with emphasis on trustworthiness—arguing explanations must themselves be trustworthy (robust, reliable, secure) not just informative. Provides detailed taxonomy of XAI goals (transparency, causality, informativeness, confidence, fairness, accessibility), methods (model-agnostic, model-specific, example-based, visualization), and evaluation approaches. Identifies security vulnerabilities: adversarial attacks can manipulate explanations without changing predictions, creating false sense of trustworthiness.

  RELEVANCE: The focus on trustworthy explanations applies directly to MI. Are mechanistic explanations trustworthy? The paper's discussion of adversarial attacks on explanations suggests MI faces unique challenges: if circuits can be adversarially crafted to appear meaningful while implementing different algorithms, MI's claims of revealing "true" mechanisms are undermined. The security perspective reveals a gap in MI evaluation—most work validates circuits on normal inputs but doesn't test robustness to adversarial circuits or explanations. The trustworthiness framework suggests criteria for assessing MI necessity: does MI produce more robust explanations than other XAI methods?

  POSITION: Security-focused XAI survey emphasizing explanation robustness and trustworthiness. Influential (160+ citations) for identifying vulnerabilities in explanation methods.
  },
  keywords = {trustworthy-XAI, XAI-security, explanation-robustness, Medium}
}

@article{buchholz2023means,
  author = {Buchholz, Oliver},
  title = {A Means-End Account of Explainable Artificial Intelligence},
  journal = {AI \& Society},
  year = {2023},
  note = {
  CORE ARGUMENT: Proposes analyzing XAI through means-end rationality: the suitability of XAI methods depends on the specific goals, stakeholders, and context of deployment. Develops a taxonomy where XAI methods are classified by the ends they serve (debugging, trust-building, fairness auditing, scientific understanding) and the means they employ (visualization, feature importance, counterfactuals, examples). Argues no single XAI method is universally appropriate—selection should be driven by matching means to ends.

  RELEVANCE: The means-end framework provides a pragmatic lens for evaluating MI's necessity. Instead of asking "Is MI necessary?" in the abstract, ask "For which ends is MI a suitable means?" MI may be well-suited for scientific understanding of networks and debugging subtle failures, but poorly suited for building user trust or satisfying legal requirements for explanation. This framework shifts the necessity question from categorical to conditional: MI is necessary for certain purposes but not others. The analysis helps identify where MI's mechanistic approach provides distinctive value versus where simpler methods suffice.

  POSITION: Pragmatic philosophical framework for XAI method selection. Emphasizes context-dependency and purpose-relativity of explanation quality.
  },
  keywords = {means-end-analysis, XAI-philosophy, context-dependent, Medium}
}

@inproceedings{adadi2018peeking,
  author = {Adadi, Amina and Berrada, Mohammed},
  title = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
  journal = {IEEE Access},
  year = {2018},
  volume = {6},
  pages = {52138--52160},
  note = {
  CORE ARGUMENT: Early influential XAI survey organizing methods into three categories: pre-model interpretability (using inherently interpretable models), in-model interpretability (designing interpretable architectures), and post-model interpretability (explaining existing black boxes). Emphasizes the fundamental accuracy-interpretability trade-off and argues post-model techniques are necessary for high-performance deep learning while pre-model approaches sacrifice accuracy. Reviews both model-agnostic (LIME, SHAP) and model-specific (attention visualization, saliency maps) techniques.

  RELEVANCE: This survey's three-way categorization (pre, in, post) highlights MI's ambiguous status. MI appears to be post-model (reverse engineering trained networks) but aims for in-model understanding (revealing internal algorithms). This categorization ambiguity matters because different categories face different challenges: post-model methods risk unreliable approximations (Rudin's critique), in-model methods may sacrifice performance. If MI is genuinely post-model, it inherits all the reliability concerns about approximation and gaming; if it achieves in-model understanding, it should be compared to inherently interpretable architectures on performance-interpretability trade-offs.

  POSITION: Influential early XAI survey establishing common taxonomic frameworks. Widely cited for its comprehensive coverage of methods.
  },
  keywords = {XAI-survey, pre-in-post-model, accuracy-interpretability-tradeoff, Low}
}

@article{paez2021pragmatic,
  author = {P{\'a}ez, Andr{\'e}s},
  title = {The Pragmatic Turn in Explainable Artificial Intelligence (XAI)},
  journal = {Minds and Machines},
  year = {2021},
  volume = {31},
  pages = {433--459},
  note = {
  CORE ARGUMENT: Argues XAI should shift from seeking "true" or "faithful" explanations to providing pragmatically useful understanding. Draws on philosophical pragmatism to argue explanation quality depends on context, audience, and purpose rather than objective correctness. Proposes evaluating XAI methods by whether they enable objectual understanding (grasping how components relate) rather than propositional knowledge (knowing that X caused Y). Suggests interpretative models that approximate black boxes can provide better understanding than mechanistically accurate but overwhelming detail.

  RELEVANCE: Paez's pragmatic framework challenges MI's implicit assumption that mechanistic accuracy is the primary explanatory virtue. If explanation quality is pragmatic rather than objective, MI's detailed circuits may provide worse explanations than simplified approximations for many purposes. This creates tension: MI advocates claim to reveal "true" mechanisms, but Paez argues no explanation is objectively true—only more or less useful for particular goals. The pragmatic framework suggests MI may be necessary for some purposes (detailed debugging, scientific understanding) while being inferior to simpler methods for others (user trust, quick diagnosis). The emphasis on objectual understanding over propositional knowledge aligns with MI's goal but questions whether circuit-level detail actually enhances understanding.

  POSITION: Pragmatist philosophical critique of XAI's search for objective explanations. Influential for reframing XAI evaluation criteria.
  },
  keywords = {pragmatic-XAI, understanding-vs-explanation, philosophy-of-science, Medium}
}

@article{london2019artificial,
  author = {London, Alex John},
  title = {Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability},
  journal = {Hastings Center Report},
  year = {2019},
  volume = {49},
  pages = {15--21},
  note = {
  CORE ARGUMENT: Challenges the assumption that medical AI must be explainable, arguing that in domains where causal knowledge is incomplete and precarious, empirical accuracy may matter more than mechanistic explanation. Draws on Aristotelian distinction between episteme (theoretical knowledge) and techne (practical knowledge) to argue medicine often relies on techne where explaining HOW treatments work is less important than verifying THAT they work reliably. Suggests focusing on validating AI performance rather than demanding explanations that provide false confidence.

  RELEVANCE: London's argument creates a provocative challenge to MI's value proposition in medical AI. If mechanistic explanations don't actually improve decision quality when our causal theories are incomplete, MI's detailed circuit descriptions may provide false sense of understanding without improving outcomes. This matters for necessity: London suggests explanations are necessary primarily for trust and accountability, not for actual decision quality—and simpler summary statistics might serve these purposes better than complex mechanistic details. However, MI advocates might respond that revealing mechanisms enables us to improve causal theories rather than just validating black boxes. The debate illuminates whether MI's value is primarily epistemic (advancing understanding) or practical (improving decisions).

  POSITION: Contrarian argument questioning whether explainability is necessary for medical AI. Challenges XAI assumptions about explanation's value.
  },
  keywords = {medical-AI, accuracy-vs-explainability, episteme-vs-techne, Low}
}
