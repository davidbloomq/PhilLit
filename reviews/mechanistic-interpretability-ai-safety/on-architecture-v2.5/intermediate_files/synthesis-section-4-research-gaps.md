## Research Gaps and Opportunities

The preceding analysis reveals that the debate over mechanistic interpretability's role in AI safety suffers from four interconnected gaps: conceptual imprecision, underspecified claims, limited empirical grounding, and insufficient engagement with fundamental epistemic constraints. Addressing these gaps is essential for productive research on whether MI is necessary or sufficient for safe AI systems.

### Gap 1: No Systematic Application of Philosophical Criteria to MI

Despite the new mechanism literature providing sophisticated criteria for evaluating mechanistic explanations, these resources remain largely untapped in MI research. Craver's (2007) mutual manipulability criterion specifies that components are constitutively relevant to a mechanism only if interventions on components change the mechanism's behavior and vice versa. Povich and Craver (2017) clarify that mechanistic levels are constitutive part-whole hierarchies, not merely spatial scales or disciplinary divisions, enabling non-competitive coexistence of multiple mechanistic levels. Piccinini and Craver (2011) demonstrate that functional analyses qualify as mechanism sketches that complement rather than compete with implementational detail.

Yet MI practitioners operate with implicit operational definitions rather than explicit philosophical criteria. Ayonrinde and Jaburi (2025) represent the only sustained attempt to bridge philosophy of mechanism and MI, arguing that MI produces "model-level, ontic, causal-mechanistic, and falsifiable explanations." This isolation matters: without agreed criteria for what counts as mechanistic explanation, practitioners cannot evaluate whether circuit discovery provides genuine mechanistic understanding or merely useful descriptions that happen to employ mechanistic vocabulary. Applying Craver's mutual manipulability criterion systematically to current MI methods---assessing whether activation patching interventions satisfy the conditions for constitutive relevance---would clarify MI's explanatory status and reveal which techniques deliver genuine mechanistic insight.

### Gap 2: Underspecified Necessity and Sufficiency Claims

Current arguments for and against MI's necessity remain at the level of competing intuitions rather than testable claims. Kastner and Crook (2024) argue MI is necessary for satisfying "safety desiderata" but do not specify which desiderata require mechanistic understanding versus which might be satisfied through behavioral testing or alternative approaches. Hendrycks and Hiscott (2025) critique MI's tractability without specifying which safety properties might still benefit from interpretability even if complete mechanistic understanding proves impossible.

This imprecision reflects deeper conceptual confusion. Baum (2025) demonstrates that "alignment" itself is multidimensional, varying across aim (safety, ethicality, legality), scope (outcome versus execution), and constituency (individual versus collective). Zednik (2019) shows that different stakeholders require different forms of transparency at different Marr levels. Amodei et al. (2016) identify five concrete safety problems---side effects, reward hacking, scalable supervision, safe exploration, distributional shift---each potentially requiring different interventions. Without mapping specific safety properties to specific interpretability requirements, debates about MI's necessity conflate distinct questions: Is MI necessary for detecting deceptive alignment? For preventing reward hacking? For ensuring robustness to distribution shift? Bereska and Gavves (2024) gesture toward this differentiation by distinguishing MI's potential contributions to understanding, control, and alignment, but systematic mapping remains absent.

### Gap 3: Limited Empirical Evidence on MI's Safety Impact

The strongest MI results demonstrate circuit discovery and feature extraction, not safety-relevant outcomes. Nanda et al. (2023) fully reverse-engineered the algorithm learned by small transformers on modular addition; Olsson et al. (2022) identified induction heads as the mechanism underlying in-context learning; Templeton et al. (2024) extracted millions of interpretable features from Claude 3 Sonnet, including safety-relevant concepts like deception and bias. These achievements establish MI's technical viability but leave open whether mechanistic understanding improves safety outcomes.

More troublingly, Makelov et al. (2024) demonstrate a significant gap between interpretability and control: sparse autoencoders capture interpretable features but fail at reliable intervention due to feature occlusion and over-splitting. Even when features are interpretable, they may not enable the behavioral steering that safety applications require. Conmy et al.'s (2023) ACDC algorithm successfully rediscovered manually-identified circuits, yet scaling these methods to frontier models and diverse behaviors remains undemonstrated. Necessity and sufficiency are ultimately empirical questions requiring evidence that MI improves specific safety metrics---reduces deceptive behavior detection time, enables more reliable capability elicitation, supports more effective red-teaming---rather than merely advancing scientific understanding of neural networks. Identifying tractable test cases where MI's safety contribution can be measured against alternatives (behavioral testing, formal verification, debate) would ground the normative debate in empirical reality.

### Gap 4: Neglected Relationship Between Epistemic Opacity and MI

The philosophy of computational science has developed sophisticated accounts of epistemic opacity that MI literature largely ignores. Humphreys (2009) introduces "essential epistemic opacity": a computational process is essentially opaque if its core method cannot be reproduced by unaided human calculation in reasonable time. This opacity is not merely practical but stems from the computational nature of the method itself. Burrell (2016) distinguishes three forms of ML opacity---intentional secrecy, technical illiteracy, and inherent algorithmic complexity arising from high-dimensional optimization---arguing the third form is categorically distinct and not addressable by the same means as the others.

MI aims to address Burrell's third form of opacity, yet the relationship between circuit-level decomposition and human-scale understanding remains undertheorized. Does identifying that a transformer uses discrete Fourier transforms for modular addition (Nanda et al. 2023) overcome essential epistemic opacity, or merely relocate it to understanding how thousands of such circuits interact? Beisbart (2021) argues opacity involves multiple dimensions of knowledge and understanding; MI might address some (component mechanisms) while leaving others untouched (emergent behavior, failure prediction). Duran and Formanek (2018) propose computational reliabilism as an alternative to transparency-based trust, suggesting that validation processes (verification, robustness testing) may ground justified use without requiring MI-level mechanistic understanding. Whether MI techniques address inherent algorithmic opacity or create new forms of interpretive opacity---where the explanations themselves require specialized expertise to comprehend---remains an open question with significant implications for MI's practical value.

### Synthesis

These gaps collectively reveal that the MI-safety debate lacks the conceptual precision, empirical grounding, and engagement with epistemic constraints necessary for productive resolution. Advocates and critics alike operate with underspecified conceptions of what MI claims to provide, which safety properties are at stake, and how mechanistic understanding relates to trustworthy deployment. The research project addresses this by: (1) applying philosophical criteria from mechanism literature to evaluate whether current MI methods deliver genuine mechanistic explanation; (2) developing a systematic mapping from specific safety properties to interpretability requirements, enabling precise conditional claims about necessity and sufficiency; (3) identifying tractable empirical tests for MI's safety contribution; and (4) analyzing whether MI overcomes or merely relocates the epistemic opacity that motivates transparency demands. Without such clarification, the field risks either overinvesting in MI approaches that cannot deliver on their safety promises or prematurely abandoning a research program with genuine but limited utility.
