@comment{
====================================================================
DOMAIN: Epistemic Standards and Normative Dimensions of AI Explanation
SEARCH_DATE: 2026-01-01
PAPERS_FOUND: 15 total (High: 6, Medium: 7, Low: 2)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain addresses fundamental epistemological and normative questions about AI systems' explainability and transparency. The literature encompasses three interconnected debates: (1) the nature and extent of epistemic opacity in machine learning systems, drawing on Humphreys' foundational work on computational science; (2) the normative demands for AI explanation arising from legal frameworks (particularly GDPR's right to explanation), ethical principles of accountability and transparency, and social requirements for algorithmic governance; and (3) the relationship between epistemic access to AI internals and practical requirements for contestability, oversight, and trust.

Key developments include Burrell's influential taxonomy distinguishing intentional secrecy, technical illiteracy, and inherent algorithmic opacity; the Wachter-Mittelstadt-Floridi debate on whether GDPR establishes meaningful right-to-explanation requirements; and recent work (Alvarado, Dur\u00e1n, Sullivan) examining AI as epistemic technology and the limits of transparency-based approaches. The field increasingly recognizes that epistemic opacity is not merely a technical challenge but shapes what kinds of normative demands can be reasonably imposed on AI systems. Recent work on epistemic injustice (Pozzi, Mollema, Hull) extends these debates by examining how ML opacity can systematically disadvantage marginalized epistemic communities.

RELEVANCE_TO_PROJECT:
This domain provides the epistemological and normative context essential for evaluating mechanistic interpretability's necessity and sufficiency claims. If epistemic opacity is inherent to deep learning systems (Humphreys, Burrell), MI faces fundamental limits in what it can reveal. The normative literature on rights to explanation and algorithmic accountability establishes the practical demands MI must meet to be deemed sufficient for governance purposes. Understanding these epistemic constraints and normative requirements is crucial for assessing whether MI can deliver the kind of understanding and control that regulatory frameworks presuppose.

NOTABLE_GAPS:
Limited engagement between the epistemic opacity literature (which emphasizes fundamental limits) and the MI technical literature (which emphasizes new capabilities). Few papers directly address whether MI techniques overcome or merely relocate epistemic opacity. The relationship between different forms of opacity (Burrell's taxonomy) and different MI methods remains underexplored.

SYNTHESIS_GUIDANCE:
Connect epistemic opacity debates to MI's technical capabilities: does circuit-level analysis overcome or exemplify essential epistemic opacity? Examine whether normative demands (right to explanation, contestability) require the kind of understanding MI claims to provide, or whether they can be met through other means (e.g., counterfactual explanations). Consider epistemic injustice literature as framework for evaluating whose understanding MI prioritizes.

KEY_POSITIONS:
- Epistemic Opacity: 6 papers - Fundamental limits to understanding complex computational systems (Humphreys, Burrell, Dur\u00e1n)
- Legal/Regulatory: 4 papers - GDPR and rights to explanation (Wachter et al., Zerilli et al.)
- Algorithmic Accountability: 3 papers - Normative demands for transparency and contestability (Binns, Diakopoulos)
- Epistemic Injustice: 2 papers - AI opacity as source of systematic epistemic harms (Pozzi, Mollema)
====================================================================
}

@article{burrell2016how,
  author = {Burrell, Jenna},
  title = {How the machine 'thinks': Understanding opacity in machine learning algorithms},
  journal = {Big Data \& Society},
  year = {2016},
  volume = {3},
  number = {1},
  pages = {1--12},
  doi = {10.1177/2053951715622512},
  note = {
  CORE ARGUMENT: Burrell distinguishes three forms of opacity in machine learning: (1) intentional corporate/state secrecy, (2) technical illiteracy among users, and (3) opacity inherent to ML algorithms arising from the mismatch between high-dimensional mathematical optimization and human-scale reasoning. She argues that recognizing these distinct forms is essential for determining appropriate technical and non-technical solutions to algorithmic harms.

  RELEVANCE: This foundational taxonomy is critical for the MI project because it challenges whether interpretability techniques can address the third form of opacity, which stems from the scale and complexity of ML models rather than lack of access or expertise. If MI methods reveal circuit-level mechanisms but do not bridge the gap to human-scale understanding, they may not overcome the most fundamental form of opacity.

  POSITION: Establishes the standard taxonomy of ML opacity; argues that inherent algorithmic opacity is distinct from and not solvable by the same means as secrecy or illiteracy.
  },
  keywords = {epistemic-opacity, machine-learning, taxonomy, High}
}

@incollection{humphreys2009computational,
  author = {Humphreys, Paul},
  title = {The philosophical novelty of computer simulation methods},
  booktitle = {Synthese},
  year = {2009},
  volume = {169},
  pages = {615--626},
  doi = {10.1007/s11229-008-9435-2},
  note = {
  CORE ARGUMENT: Humphreys introduces the concept of "essential epistemic opacity" for computational processes: a process is epistemically opaque if its core method of deriving results cannot be reproduced by unaided human calculation in a reasonable time period. This opacity is essential (not merely practical) because it stems from the computational nature of the method itself.

  RELEVANCE: Humphreys' concept of essential epistemic opacity poses a fundamental challenge to MI's sufficiency claims. If deep learning models are essentially opaque in Humphreys' sense, then MI techniques that decompose circuits into components may still leave the overall computational process epistemically inaccessible. The question becomes whether MI relocates or eliminates essential opacity.

  POSITION: Foundational argument that computational methods introduce genuine epistemic novelty through essential opacity; this is a feature, not a bug, of computational science.
  },
  keywords = {epistemic-opacity, computational-science, philosophy-of-science, High}
}

@article{wachter2017why,
  author = {Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
  title = {Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation},
  journal = {International Data Privacy Law},
  year = {2017},
  volume = {7},
  number = {2},
  pages = {76--99},
  doi = {10.1093/idpl/ipx005},
  note = {
  CORE ARGUMENT: Wachter et al. argue that GDPR does not establish a general right to explanation of automated decisions, contrary to widespread interpretation. They contend that Articles 13-15 require only limited information about the logic involved, not meaningful explanations of specific decisions, and that Article 22's provisions apply narrowly to fully automated decisions with legal/significant effects.

  RELEVANCE: This analysis is crucial for assessing MI's normative necessity. If legal frameworks do not actually mandate the kind of mechanistic understanding MI aims to provide, then MI's value must be justified on other grounds (ethical, epistemic, practical). However, the debate also reveals normative gaps that MI might help address.

  POSITION: Legal minimalism regarding explanation rights; argues GDPR's requirements are less demanding than commonly assumed, focusing on contestability rather than understanding.
  },
  keywords = {right-to-explanation, GDPR, legal-frameworks, normative-demands, High}
}

@article{wachter2017counterfactual,
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  title = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  journal = {Harvard Journal of Law \& Technology},
  year = {2018},
  volume = {31},
  number = {2},
  pages = {841--887},
  doi = {10.2139/ssrn.3063289},
  note = {
  CORE ARGUMENT: The authors propose counterfactual explanations as an alternative to transparency-based approaches that require "opening the black box." Counterfactuals specify the minimal changes needed to achieve a different outcome, supporting contestability and actionability without requiring users to understand the model's internal logic. They argue this approach better serves GDPR's goals than mechanistic explanations.

  RELEVANCE: This paper directly challenges whether MI-style mechanistic explanations are necessary for meeting normative demands. If counterfactual explanations can provide meaningful contestability and recourse without internal transparency, then MI's necessity claim weakens considerably. The debate becomes whether understanding mechanisms provides additional normative value beyond what counterfactuals deliver.

  POSITION: Pragmatic alternative to transparency; argues that explanations should be evaluated by their practical utility (enabling action) rather than epistemic completeness (enabling understanding).
  },
  keywords = {counterfactual-explanations, GDPR, explainability, alternative-approaches, High}
}

@article{alvarado2023ai,
  author = {Alvarado, Ram\u00f3n},
  title = {AI as an Epistemic Technology},
  journal = {Science and Engineering Ethics},
  year = {2023},
  volume = {29},
  number = {5},
  pages = {1--30},
  doi = {10.1007/s11948-023-00451-3},
  note = {
  CORE ARGUMENT: Alvarado develops a framework for understanding AI systems as epistemic technologies that mediate our knowledge production and shape epistemic practices. He argues that AI's opacity, explainability, and trustworthiness should be analyzed through this epistemic lens, examining how AI transforms our relationship to knowledge rather than treating these as purely technical problems requiring technical fixes.

  RELEVANCE: This framework provides important context for evaluating MI's epistemic significance. Rather than asking whether MI makes models transparent, we should ask how MI as an epistemic technology reshapes scientific and practical understanding of AI systems. This reframes the necessity/sufficiency question: MI may be necessary not for eliminating opacity but for establishing new forms of epistemic access appropriate to AI's role as knowledge-producing technology.

  POSITION: Epistemic technology framework; emphasizes that AI fundamentally changes knowledge production practices, requiring new epistemic categories rather than forcing AI into traditional transparency frameworks.
  },
  keywords = {epistemic-technology, AI-epistemology, explainability, High}
}

@article{duran2018grounds,
  author = {Dur\u00e1n, Juan M. and Formanek, Nico},
  title = {Grounds for Trust: Essential Epistemic Opacity and Computational Reliabilism},
  journal = {Minds and Machines},
  year = {2018},
  volume = {28},
  number = {4},
  pages = {645--666},
  doi = {10.1007/s11023-018-9481-6},
  note = {
  CORE ARGUMENT: Dur\u00e1n and Formanek argue that transparency-based approaches to trustworthiness are insufficient because computational processes exhibit "essential epistemic opacity" (following Humphreys). They propose "computational reliabilism" as an alternative: trustworthiness should be grounded in reliable processes (verification, validation, robustness, expert knowledge) rather than full transparency about internal mechanisms.

  RELEVANCE: This paper directly challenges MI's necessity by arguing that trustworthiness can be established through reliabilist means without requiring transparency into mechanisms. However, it also raises questions about whether MI techniques themselves constitute a form of validation that supports computational reliabilism. The debate becomes whether MI is necessary for establishing reliability or whether statistical validation suffices.

  POSITION: Computational reliabilism; argues against transparency as epistemic foundation for trust, proposing process reliability as alternative justification.
  },
  keywords = {computational-reliabilism, epistemic-opacity, trust, alternative-approaches, High}
}

@article{zerilli2019transparency,
  author = {Zerilli, John and Knott, Alistair and Maclaurin, James and Gavaghan, Colin},
  title = {Transparency in Algorithmic and Human Decision-Making: Is There a Double Standard?},
  journal = {Philosophy \& Technology},
  year = {2019},
  volume = {32},
  number = {4},
  pages = {661--683},
  doi = {10.1007/s13347-018-0330-6},
  note = {
  CORE ARGUMENT: Zerilli et al. examine whether we apply a "double standard" by demanding greater transparency from algorithmic systems than from human decision-makers. They argue that transparency demands should be calibrated to decision-making contexts and stakes rather than uniformly requiring more transparency from algorithms. The paper questions whether the push for algorithmic transparency reflects genuine epistemic or normative requirements or unjustified algorithmic exceptionalism.

  RELEVANCE: This analysis is crucial for assessing MI's normative necessity. If transparency demands on AI systems exceed those we apply to human experts (who are epistemically opaque in their own ways), we need to justify why MI-level mechanistic understanding is required. Alternatively, if AI transparency standards should match human standards, MI may be more demanding than normatively necessary.

  POSITION: Questions double standards in transparency requirements; argues for context-sensitive rather than categorical demands for algorithmic transparency.
  },
  keywords = {transparency, double-standards, normative-demands, algorithmic-accountability, Medium}
}

@article{binns2018algorithmic,
  author = {Binns, Reuben},
  title = {Algorithmic Accountability and Public Reason},
  journal = {Philosophy \& Technology},
  year = {2018},
  volume = {31},
  number = {4},
  pages = {543--556},
  doi = {10.1007/s13347-017-0263-5},
  note = {
  CORE ARGUMENT: Binns argues that algorithmic accountability should be understood through Rawlsian public reason: decision-makers must provide justifications that affected parties can reasonably accept, using terms and concepts accessible to them. This requires more than technical transparency; it demands that explanations connect to shared normative frameworks and values that stakeholders recognize.

  RELEVANCE: Binns' public reason framework provides a demanding normative standard for evaluating MI's sufficiency. Even if MI reveals complete mechanistic details, this may not constitute adequate accountability if the explanations are not accessible to affected parties or don't connect to shared normative concepts. MI's sufficiency depends not just on epistemic completeness but on whether its explanations support public reasoning about algorithmic governance.

  POSITION: Public reason framework for algorithmic accountability; emphasizes accessibility and normative grounding of explanations, not just technical detail.
  },
  keywords = {algorithmic-accountability, public-reason, normative-demands, explainability, Medium}
}

@article{sullivan2022understanding,
  author = {Sullivan, Emily},
  title = {Understanding from Machine Learning Models},
  journal = {The British Journal for the Philosophy of Science},
  year = {2022},
  volume = {73},
  number = {1},
  pages = {109--133},
  doi = {10.1093/bjps/axz035},
  note = {
  CORE ARGUMENT: Sullivan develops an account of how machine learning models can provide scientific understanding despite their opacity. She argues that ML models can support "link-making" understanding by revealing dependence relations between variables, even when the model's internal mechanisms remain opaque. This understanding is genuine but differs from traditional mechanistic or causal understanding.

  RELEVANCE: Sullivan's framework offers a nuanced middle position for evaluating MI. Rather than assuming transparency is necessary for understanding, she shows how opaque models can provide one kind of understanding (dependence relations) while lacking another (mechanistic detail). MI might be necessary for mechanistic understanding but not for the link-making understanding ML models already provide. This helps clarify what specific kind of understanding MI adds.

  POSITION: Defends possibility of understanding from opaque ML models through link-making; distinguishes multiple forms of understanding rather than treating it as all-or-nothing.
  },
  keywords = {understanding, machine-learning, scientific-explanation, epistemology, Medium}
}

@article{pozzi2023automated,
  author = {Pozzi, Giorgia},
  title = {Automated opioid risk scores: a case for machine learning-induced epistemic injustice in healthcare},
  journal = {Ethics and Information Technology},
  year = {2023},
  volume = {25},
  number = {2},
  pages = {1--14},
  doi = {10.1007/s10676-023-09676-z},
  note = {
  CORE ARGUMENT: Pozzi analyzes automated opioid risk assessment systems through Miranda Fricker's framework of hermeneutical injustice, arguing that ML opacity can inflict epistemic injustice by appropriating hermeneutical resources and meanings without human oversight. She introduces "automated hermeneutical appropriation" to describe how ML systems establish meanings that impair understanding and communication among stakeholders, particularly harming physicians' ability to safeguard patients.

  RELEVANCE: This epistemic injustice framework provides a normative lens for evaluating MI's necessity beyond traditional transparency arguments. If ML opacity systematically disadvantages certain epistemic communities (patients, frontline clinicians) by appropriating interpretive resources, MI might be necessary to prevent these harms even if other forms of explanation (e.g., counterfactuals) provide contestability. The question becomes whether MI can restore epistemic agency to marginalized knowers.

  POSITION: Extends epistemic injustice framework to ML opacity; argues opacity inflicts distinctive harm through automated hermeneutical appropriation, particularly in high-stakes domains like healthcare.
  },
  keywords = {epistemic-injustice, healthcare-AI, hermeneutical-injustice, opacity, Medium}
}

@article{mollema2025taxonomy,
  author = {Mollema, Warmhold Jan Thomas},
  title = {A taxonomy of epistemic injustice in the context of AI and the case for generative hermeneutical erasure},
  journal = {AI and Ethics},
  year = {2025},
  volume = {5},
  pages = {5535--5555},
  doi = {10.1007/s43681-025-00801-w},
  note = {
  CORE ARGUMENT: Mollema develops a systematic taxonomy of epistemic injustices arising from AI systems, including testimonial injustice automation, hermeneutical marginalization through algorithmic systems, and a novel category of "generative hermeneutical erasure." He argues that large language models impose a "view from nowhere" epistemology that systematically inferiorizes non-Western epistemologies and erodes epistemic particulars, contributing to "epistemicide."

  RELEVANCE: This taxonomy extends epistemic injustice analysis beyond individual harms to structural and global impacts of AI opacity. For the MI project, it raises the question of whose epistemic access MI prioritizes: Does MI democratize understanding of AI systems or reinforce existing epistemic hierarchies by requiring highly technical expertise? The generative hermeneutical erasure concept also challenges whether making models more interpretable addresses or exacerbates global epistemic injustices.

  POSITION: Systematic taxonomy of AI-related epistemic injustice; introduces "generative hermeneutical erasure" as distinctive harm from AI systems' epistemic homogenization.
  },
  keywords = {epistemic-injustice, AI-ethics, hermeneutical-erasure, global-justice, Medium}
}

@article{hull2023dirty,
  author = {Hull, Gordon},
  title = {Dirty data labeled dirt cheap: epistemic injustice in machine learning systems},
  journal = {Ethics and Information Technology},
  year = {2023},
  volume = {25},
  number = {3},
  pages = {1--14},
  doi = {10.1007/s10676-023-09712-y},
  note = {
  CORE ARGUMENT: Hull examines how ML training data practices, particularly reliance on precarious labor for data labeling, instantiate epistemic injustice. He argues that the "dirty data" problem—where training data reflects and amplifies social biases—is exacerbated by exploitative labeling practices that systematically exclude marginalized perspectives from shaping the epistemic resources encoded in ML systems.

  RELEVANCE: Hull's analysis connects epistemic opacity to the political economy of ML development, suggesting that transparency about model mechanisms (via MI) may be insufficient if the underlying data practices remain opaque and unjust. This challenges MI's sufficiency: even complete circuit-level understanding may not reveal or address epistemic injustices embedded in training data. MI might need to be supplemented with data provenance and labor practice transparency.

  POSITION: Connects epistemic injustice to ML labor practices; argues that data-level opacity and injustice may be more fundamental than model-level opacity.
  },
  keywords = {epistemic-injustice, training-data, labor, bias, Medium}
}

@article{heinrichs2019your,
  author = {Heinrichs, Bert and Eickhoff, Simon B.},
  title = {Your evidence? Machine learning algorithms for medical diagnosis and prediction},
  journal = {Human Brain Mapping},
  year = {2020},
  volume = {41},
  number = {6},
  pages = {1435--1444},
  doi = {10.1002/hbm.24886},
  note = {
  CORE ARGUMENT: Heinrichs and Eickhoff argue that ML-based medical diagnosis systems face two interconnected ethical challenges: epistemic opacity undermines understanding and information rights, while the absence of transparent reasoning undermines responsibility attribution. They propose that solutions require integrating discursive elements—the practice of giving and asking for reasons—into ML systems, pointing toward explainable AI initiatives.

  RELEVANCE: This paper connects epistemic opacity directly to normative demands for responsibility and informed consent in high-stakes medical contexts. It suggests MI might be necessary not primarily for technical validation but for supporting the discursive practices essential to medical ethics. The question becomes whether MI can provide the kind of reason-giving required for genuine informed consent and responsibility attribution.

  POSITION: Links opacity to breakdown of discursive practices (giving/asking for reasons); argues explainability must restore capacity for reason-giving in medical contexts.
  },
  keywords = {medical-AI, epistemic-opacity, informed-consent, responsibility, Medium}
}

@article{walmsley2021artificial,
  author = {Walmsley, Joel},
  title = {Artificial intelligence and the value of transparency},
  journal = {AI \& Society},
  year = {2021},
  volume = {36},
  number = {2},
  pages = {585--595},
  doi = {10.1007/s00146-020-01066-z},
  note = {
  CORE ARGUMENT: Walmsley surveys and taxonomizes the variety of ways "transparency" is invoked in AI ethics discourse, exploring both epistemological and ethical dimensions. He distinguishes transparency of process, transparency of outcomes, transparency of rationale, and transparency of values, arguing that different transparency demands serve different purposes and may conflict with each other.

  RELEVANCE: Walmsley's taxonomy helps clarify what specific form of transparency MI provides and whether it serves the normative purposes often attributed to transparency in general. MI offers transparency of process/mechanism but may not provide transparency of rationale (why this decision was normatively appropriate) or values (what goals the system serves). This suggests MI may be necessary for some transparency goals but insufficient for others.

  POSITION: Pluralist taxonomy of transparency; warns against treating transparency as univocal concept, emphasizes different forms serve different purposes.
  },
  keywords = {transparency, AI-ethics, taxonomy, normative-analysis, Medium}
}

@article{diakopoulos2015algorithmic,
  author = {Diakopoulos, Nicholas},
  title = {Algorithmic Accountability: Journalistic investigation of computational power structures},
  journal = {Digital Journalism},
  year = {2015},
  volume = {3},
  number = {3},
  pages = {398--415},
  doi = {10.1080/21670811.2014.976411},
  note = {
  CORE ARGUMENT: Diakopoulos develops the concept of "algorithmic accountability" in the context of journalism, arguing that algorithms function as power structures requiring investigation and oversight. He outlines methods for journalistic investigation of algorithms including reverse engineering, auditing, crowdsourcing, and leak-based reporting. Accountability requires understanding not just how algorithms work but who they serve and what power relations they instantiate.

  RELEVANCE: Diakopoulos' framework shifts focus from transparency-for-understanding to transparency-for-accountability, emphasizing power analysis over technical detail. For MI, this raises questions about whether circuit-level mechanistic understanding serves accountability goals: Does knowing how a model works reveal who it benefits and harms? MI might provide insufficient accountability if it reveals mechanisms without illuminating power relations, values, and distributional consequences.

  POSITION: Accountability-focused approach to algorithmic transparency; emphasizes investigative methods and power analysis rather than universal transparency requirements.
  },
  keywords = {algorithmic-accountability, journalism, power-analysis, investigation, Low}
}

@article{beisbart2021opacity,
  author = {Beisbart, Claus},
  title = {Opacity thought through: on the intransparency of computer simulations},
  journal = {Synthese},
  year = {2021},
  volume = {198},
  pages = {10449--10470},
  doi = {10.1007/s11229-021-03305-2},
  note = {
  CORE ARGUMENT: Beisbart critiques and extends Humphreys' concept of epistemic opacity, arguing that existing definitions are too narrow. He proposes that opacity should be understood as a method's disposition to resist knowledge and understanding, examining what types of knowledge and understanding are required and why they're difficult to achieve. Opacity is not binary but admits of degrees and types.

  RELEVANCE: Beisbart's refined concept of opacity provides a framework for evaluating whether MI reduces or merely relocates opacity. If opacity involves multiple dimensions of knowledge and understanding, MI might address some forms (knowledge of mechanism components) while leaving others untouched (understanding of emergent behavior, prediction of failure modes). This supports a nuanced assessment of MI's contribution rather than treating it as fully solving the opacity problem.

  POSITION: Refined multidimensional account of opacity; argues opacity involves disposition to resist multiple types of knowledge and understanding, not just computational reproducibility.
  },
  keywords = {epistemic-opacity, computer-simulation, philosophy-of-science, understanding, Low}
}

