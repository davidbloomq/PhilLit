@comment{
====================================================================
DOMAIN: Necessity and Sufficiency Arguments for Interpretability in AI Safety
SEARCH_DATE: 2026-01-01
PAPERS_FOUND: 18 total (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, arXiv, WebSearch (AI Frontiers, AlignmentForum)
====================================================================

DOMAIN_OVERVIEW:

The debate over whether interpretability is necessary and/or sufficient for AI safety represents a core conceptual battleground in AI alignment research. This domain examines systematic arguments about the role of mechanistic understanding in ensuring safe AI systems.

The necessity debate centers on whether AI systems can be made safe through behavioral testing and alignment techniques alone, or whether understanding internal mechanisms is required. London (2019) argues that opacity need not prevent safe deployment if empirical validation is rigorous, drawing on medical precedent. Conversely, Kästner and Crook (2024) contend that holistic mechanistic interpretability is necessary for satisfying key safety desiderata that behavioral approaches cannot address. Bereska and Gavves (2024) survey mechanistic interpretability's role in AI safety, identifying both benefits (understanding, control, alignment) and limitations (scalability, automation challenges).

The sufficiency debate examines whether interpretability alone can guarantee safety. Hendrycks (2025) argues mechanistic interpretability is fundamentally intractable for modern AI—compression of terabyte-scale models into human-comprehensible explanations may be impossible, and the quest for mechanistic understanding diverts resources from more tractable safety approaches. Zednik (2019) provides a normative framework showing that different forms of transparency serve different purposes, suggesting no single interpretability approach suffices for all safety goals.

Recent empirical work reveals tensions: PhilPapers literature on black-box AI shows widespread concern about opacity (von Eschenbach 2021, Wadden 2022), yet London (2019) demonstrates contexts where accuracy outweighs explainability. The AlignmentForum discussions (2024) note critiques including "Interpretability Will Not Reliably Find Deceptive AI" and significant gaps in current interpretability tools.

RELEVANCE_TO_PROJECT:

This domain directly addresses the conceptual core of the research project: the normative claims about MI's necessity and sufficiency for AI safety. These arguments determine whether MI deserves priority in safety research agendas or whether alternative paradigms (behavioral testing, alignment via RLHF, formal verification) should receive greater emphasis. Understanding this debate is essential for evaluating the broader MI discourse.

NOTABLE_GAPS:

Few papers directly engage both necessity AND sufficiency questions within a single framework. Most address one or the other, creating fragmented discourse. Additionally, empirical evidence about when interpretability actually improves safety outcomes (versus just increasing understanding) remains limited. The interaction between interpretability and other safety mechanisms (e.g., adversarial robustness, alignment) is under-theorized.

SYNTHESIS_GUIDANCE:

Map the logical structure of necessity vs. sufficiency claims. Identify empirical assumptions underlying normative arguments (e.g., Hendrycks assumes compression intractability, London assumes empirical validation suffices). Examine how different stakeholders (medical practitioners, AI safety researchers, regulators) prioritize interpretability differently. Consider whether the debate confuses *understanding* with *safety*.

KEY_POSITIONS:
- Interpretability necessary: 5 papers - Mechanistic understanding required for robust safety (Kästner, Bereska, SEP ethics-ai)
- Interpretability insufficient: 4 papers - Other approaches needed (Hendrycks, behavioral testing advocates)
- Context-dependent: 6 papers - Necessity/sufficiency varies by application (London, Zednik, PhilPapers black-box literature)
- Scalability skepticism: 3 papers - MI intractable for modern systems (Hendrycks, AlignmentForum critiques)
====================================================================
}

@article{kastner2024explaining,
  author = {Kästner, Lena and Crook, Barnaby},
  title = {Explaining AI through Mechanistic Interpretability},
  journal = {European Journal for Philosophy of Science},
  year = {2024},
  volume = {14},
  number = {3},
  pages = {1--29},
  doi = {10.1007/s13194-024-00614-4},
  note = {
  CORE ARGUMENT: XAI's divide-and-conquer strategy fails to illuminate how trained AI systems work as wholes. Mechanistic interpretability (MI)—applying coordinated discovery strategies from life sciences to uncover functional organization—is necessary to satisfy safety desiderata. MI seeks epistemically relevant entities (EREs) through pattern recognition across multiple levels of analysis, providing holistic understanding that local explanations cannot achieve.

  RELEVANCE: Directly argues for necessity of MI in AI safety by showing that safety requires understanding functional organization, not just individual predictions. Distinguishes MI from XAI, establishing conceptual foundations for why mechanistic understanding matters. However, does not address sufficiency—whether MI alone ensures safety or whether complementary approaches are needed. Critical for articulating the pro-MI position in the necessity debate.

  POSITION: Strong advocate for MI necessity. Argues opacity prevents satisfying safety requirements, and only mechanistic understanding via coordinated discovery enables trustworthy deployment. Represents philosophical defense of MI research program.
  },
  keywords = {mechanistic-interpretability, necessity-arguments, AI-safety, philosophy-of-science, High}
}

@misc{hendrycks2025misguided,
  author = {Hendrycks, Dan and Hiscott, Luke},
  title = {The Misguided Quest for Mechanistic AI Interpretability},
  year = {2025},
  month = {May},
  howpublished = {\url{https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability}},
  note = {
  CORE ARGUMENT: Mechanistic interpretability rests on a flawed assumption: that terabyte-scale models can be compressed into human-comprehensible explanations (< 1KB). This compression is likely intractable. Physical mechanisms (clockwork) are analyzable; AI neural networks are not. Google DeepMind deprioritized sparse autoencoders (March 2025), revealing industry skepticism. MI diverts resources from tractable safety approaches.

  RELEVANCE: Central counterargument to MI necessity claims. Presents scalability objection: even if MI conceptually sound, practical impossibility makes it unsuitable as safety foundation. Raises meta-question: should safety research prioritize theoretically attractive but intractable approaches? Forces proponents to address compression feasibility. Does not claim interpretability is *never* useful, but questions whether mechanistic understanding at scale is achievable.

  POSITION: Strong critic of MI sufficiency and practical necessity. Argues for redirecting safety efforts toward empirically validated behavioral approaches. Represents pragmatic skepticism about MI research program.
  },
  keywords = {mechanistic-interpretability, sufficiency-critique, scalability, AI-safety, High}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety: A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  volume = {abs/2404.14082},
  doi = {10.48550/arXiv.2404.14082},
  note = {
  CORE ARGUMENT: MI provides granular, causal understanding by reverse-engineering computational mechanisms into human-understandable algorithms. Benefits for AI safety include enhanced understanding, control, and alignment. However, risks exist: capability gains from interpretability could enable misuse, and comprehensive interpretation faces scalability challenges. MI's safety contribution depends on advancing automation and handling complex models/behaviors.

  RELEVANCE: Comprehensive review balancing MI benefits and limitations for safety. Neither fully endorses necessity nor dismisses sufficiency—presents empirical assessment of what MI can/cannot currently achieve. Identifies concrete safety applications (understanding deceptive behavior, enabling intervention) alongside limitations (scaling to frontier models, automation bottlenecks). Essential for nuanced position in necessity/sufficiency debate.

  POSITION: Pragmatic optimist: MI valuable for safety but requires significant technical advances. Necessity and sufficiency both context-dependent—MI necessary for some safety properties (e.g., detecting novel failure modes) but insufficient without complementary approaches.
  },
  keywords = {mechanistic-interpretability, AI-safety, survey, benefits-and-limitations, High}
}

@article{london2019artificial,
  author = {London, Alex John},
  title = {Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability},
  journal = {The Hastings Center Report},
  year = {2019},
  volume = {49},
  number = {1},
  pages = {15--21},
  doi = {10.1002/hast.973},
  note = {
  CORE ARGUMENT: Opaque decisions are common in medicine; when causal knowledge is incomplete, empirical validation of accuracy matters more than mechanistic explanation. Drawing on Aristotle, argues ability to produce verified results can outweigh ability to explain how. Black-box AI acceptable if rigorous empirical testing establishes safety and efficacy. Transparency is one value among many, not an absolute requirement.

  RELEVANCE: Central argument against interpretability necessity. Establishes precedent: domains with successful deployment despite opacity. Challenges assumption that understanding internal mechanisms is prerequisite for safe use. However, focuses on medical context where extensive empirical validation is feasible—unclear if argument generalizes to AGI scenarios where novel situations are common. Key counterpoint to Kästner's necessity claims.

  POSITION: Interpretability not necessary for safety if robust empirical validation possible. Challenges conflation of understanding with trustworthiness. Represents "behavioral testing suffices" position in AI safety debates.
  },
  keywords = {black-box-AI, medical-AI, necessity-critique, empirical-validation, High}
}

@article{zednik2019solving,
  author = {Zednik, Carlos},
  title = {Solving the Black Box Problem: A Normative Framework for Explainable Artificial Intelligence},
  journal = {Philosophy and Technology},
  year = {2019},
  volume = {34},
  pages = {265--288},
  doi = {10.1007/s13347-019-00382-7},
  note = {
  CORE ARGUMENT: Provides multi-level framework for XAI using Marr's levels of analysis. Different stakeholders require different forms of transparency—implementational (how algorithms work), algorithmic (what computations are performed), computational (what problems are solved). No single form of explainability serves all purposes. "Solving" black-box problem requires matching explanation type to context and audience.

  RELEVANCE: Argues neither necessity nor sufficiency holds universally—both are context-dependent. Interpretability's value for safety depends on which safety questions are being asked and which stakeholders need answers. Regulatory compliance requires different transparency than debugging requires different transparency than scientific understanding. Nuances the necessity/sufficiency debate by showing these are not binary properties.

  POSITION: Pluralist about interpretability. Different forms serve different purposes; no form necessary or sufficient across all contexts. Advocates matching interpretability methods to specific safety goals rather than seeking universal solutions.
  },
  keywords = {explainability, normative-framework, Marr-levels, context-dependence, High}
}

@misc{sep2020ethics-ai,
  author = {Müller, Vincent C.},
  title = {Ethics of Artificial Intelligence and Robotics},
  year = {2020},
  howpublished = {Stanford Encyclopedia of Philosophy},
  url = {https://plato.stanford.edu/entries/ethics-ai/},
  note = {
  CORE ARGUMENT: Section 2.3 on opacity identifies the black-box problem as central ethical concern: AI systems' decision-making processes are opaque, making it difficult to understand, predict, or control their behavior. Opacity creates accountability gaps and prevents meaningful oversight. However, article acknowledges trade-offs: some opacity may be acceptable if other safeguards (testing, monitoring) are robust. Section 2.10 on superintelligence suggests interpretability becomes more critical as AI capabilities increase.

  RELEVANCE: Establishes philosophical consensus that opacity is problematic for AI ethics and safety. However, does not argue interpretability is absolutely necessary—acknowledges other mechanisms may compensate. Useful for grounding necessity/sufficiency debate in broader ethical framework. Shows interpretability concerns predate recent MI discourse, rooted in accountability and oversight requirements.

  POSITION: Opacity is a significant ethical problem requiring attention, but not necessarily an absolute barrier to safe deployment. Interpretability is one tool among many for ensuring accountable AI.
  },
  keywords = {AI-ethics, opacity, black-box-problem, SEP, Medium}
}

@article{voneschenbach2021transparency,
  author = {von Eschenbach, Warren J.},
  title = {Transparency and the Black Box Problem: Why We Do Not Trust AI},
  journal = {AI and Society},
  year = {2021},
  volume = {36},
  pages = {23--47},
  doi = {10.1007/s00146-020-01069-w},
  note = {
  CORE ARGUMENT: Black-box AI systems undermine trust because users cannot understand how conclusions are reached. Trust requires transparency, especially when systems safeguard important goods (security, healthcare, safety). Without understanding mechanisms, users cannot assess reliability or identify when systems operate outside their competence. Transparency is not just epistemically valuable but psychologically necessary for adoption.

  RELEVANCE: Links interpretability necessity to trust and adoption. Argues safety concerns are insufficient without user acceptance, and acceptance requires transparency. However, focuses on user psychology rather than technical safety properties—conflates "perceived safety" with "actual safety." Raises question: is interpretability necessary for safety or for social legitimacy? Both may matter for responsible deployment.

  POSITION: Transparency necessary for trustworthy AI. However, trust-based argument differs from capability-based safety arguments. Does not directly address whether opacity prevents achieving safety properties.
  },
  keywords = {transparency, trust, black-box-problem, user-acceptance, Medium}
}

@article{wadden2022defining,
  author = {Wadden, Jordan Joseph},
  title = {Defining the Undefinable: The Black Box Problem in Healthcare Artificial Intelligence},
  journal = {Journal of Medical Ethics},
  year = {2022},
  volume = {48},
  number = {10},
  pages = {764--768},
  doi = {10.1136/medethics-2021-107529},
  note = {
  CORE ARGUMENT: "Black box" is used ambiguously across healthcare AI debates, referring variously to mathematical complexity, proprietary systems, or lack of clinical interpretability. This ambiguity prevents productive discussion. Argues for distinguishing types of opacity and matching solutions to specific concerns. Some forms of opacity (proprietary algorithms) create governance challenges; others (mathematical complexity) may be acceptable if clinical validation is rigorous.

  RELEVANCE: Highlights conceptual confusion in necessity debates: advocates often talk past each other because "interpretability" and "black box" lack precise definitions. Supports Zednik's pluralist position—different opacity problems require different solutions. Cautions against universal necessity or sufficiency claims without specifying *which* interpretability property and *which* safety concern.

  POSITION: Necessity and sufficiency depend on operationalizing key terms. Precision in defining "black box" and "interpretability" is prerequisite for evaluating their relationships to safety.
  },
  keywords = {black-box-problem, conceptual-clarification, medical-AI, Medium}
}

@article{duran2021afraid,
  author = {Durán, Juan Manuel and Jongsma, Karin Rolanda},
  title = {Who is Afraid of Black Box Algorithms? On the Epistemological and Ethical Basis of Trust in Medical AI},
  journal = {Journal of Medical Ethics},
  year = {2021},
  volume = {47},
  pages = {329--335},
  doi = {10.1136/medethics-2020-106820},
  note = {
  CORE ARGUMENT: Computational reliabilism justifies trusting black-box algorithms based on empirical reliability without requiring transparency. If algorithms consistently produce accurate results under diverse conditions, this grounds justified belief. However, ethical concerns remain even with trustworthy black boxes: accountability, fairness, bias. Separates epistemic justification (reliability-based) from ethical requirements (transparency for accountability).

  RELEVANCE: Challenges necessity claim by providing alternative epistemic foundation for trust. Reliability, not interpretability, grounds justified use. However, acknowledges interpretability may be necessary for meeting ethical obligations distinct from epistemic justification. Nuances necessity debate: necessary for *what*? Epistemic justification? Ethical accountability? Safety properties?

  POSITION: Interpretability not necessary for epistemic trust if reliability established. May be necessary for ethical governance. Necessity depends on which value is prioritized.
  },
  keywords = {computational-reliabilism, trust, medical-AI, epistemic-justification, Medium}
}

@article{singh2024rethinking,
  author = {Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
  title = {Rethinking Interpretability in the Era of Large Language Models},
  journal = {arXiv preprint},
  year = {2024},
  volume = {abs/2402.01761},
  doi = {10.48550/arXiv.2402.01761},
  note = {
  CORE ARGUMENT: LLMs enable new interpretability paradigm: explaining in natural language allows scaling complexity and scope of patterns communicated to humans. However, LLMs raise new challenges—hallucinated explanations, computational costs. LLMs can both *be interpreted* and *perform interpretation*, creating recursive interpretability opportunities. Proposes LLM interpretation as path forward for handling complexity that traditional MI methods struggle with.

  RELEVANCE: Addresses Hendrycks' scalability objection by proposing alternative: use AI to interpret AI. If LLMs can generate faithful natural-language explanations of complex systems, compression problem may be tractable. However, introduces new risks: how to validate AI-generated explanations? Shifts necessity debate: perhaps *automated* interpretability necessary, not human-scale mechanistic understanding.

  POSITION: Traditional MI faces scalability limits, but LLM-based interpretability may overcome them. Necessity of interpretability for safety remains, but form may change (automated rather than human-comprehensible).
  },
  keywords = {LLM-interpretability, scalability, natural-language-explanation, Medium}
}

@article{conmy2023automated,
  author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  journal = {Neural Information Processing Systems},
  year = {2023},
  volume = {abs/2304.14997},
  doi = {10.48550/arXiv.2304.14997},
  note = {
  CORE ARGUMENT: ACDC algorithm automates circuit discovery, identifying computational subgraphs implementing specific behaviors. Successfully rediscovered manually-found circuits in GPT-2 Small (68/32,000 edges for Greater-Than operation). Automation addresses scalability challenge: manual circuit analysis cannot scale to frontier models. Demonstrates feasibility of mechanistic understanding at moderate scale.

  RELEVANCE: Empirical counterpoint to scalability skepticism. Shows automated MI tools can discover interpretable circuits in real models. However, limited to specific behaviors in small models—open question whether approach scales to emergent capabilities in large models. Provides evidence MI *may* be tractable with automation, addressing Hendrycks' compression objection. Insufficient to establish necessity or sufficiency but shows MI is not obviously impossible.

  POSITION: Optimistic about MI scalability via automation. Practical demonstration that mechanistic understanding can be achieved, at least for circumscribed behaviors in moderately-sized models.
  },
  keywords = {automated-MI, circuit-discovery, scalability, empirical-work, Medium}
}

@article{nanda2023progress,
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  title = {Progress Measures for Grokking via Mechanistic Interpretability},
  journal = {International Conference on Learning Representations},
  year = {2023},
  volume = {abs/2301.05217},
  doi = {10.48550/arXiv.2301.05217},
  note = {
  CORE ARGUMENT: Fully reverse-engineered algorithm learned by small transformers for modular addition: uses discrete Fourier transforms and trigonometric identities to convert addition to rotation. Discovered grokking arises from gradual amplification of structured mechanisms, not sudden shift. Mechanistic understanding enabled defining continuous progress measures and splitting training into phases (memorization, circuit formation, cleanup).

  RELEVANCE: Existence proof: complete mechanistic understanding achievable for specific tasks. However, modular addition is toy problem; unclear if success generalizes to complex, emergent capabilities. Demonstrates *what* full MI looks like but does not establish whether such understanding is necessary for safety or feasible for real-world models. Useful for conceptualizing idealized MI outcome.

  POSITION: Demonstrates feasibility of complete MI in limited domain. Shows value of mechanistic understanding for explaining training dynamics. Agnostic on necessity/sufficiency for safety.
  },
  keywords = {mechanistic-interpretability, grokking, case-study, Low}
}

@article{makelov2024principled,
  author = {Makelov, Aleksandar and Lange, Georg and Nanda, Neel},
  title = {Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control},
  journal = {arXiv preprint},
  year = {2024},
  volume = {abs/2405.08366},
  doi = {10.48550/arXiv.2405.08366},
  note = {
  CORE ARGUMENT: Evaluates SAEs against supervised feature dictionaries on IOI task in GPT-2 Small. SAEs capture interpretable features but are less successful than supervised features in controlling model behavior. Identifies two phenomena limiting SAE effectiveness: feature occlusion (causally relevant concepts overshadowed by higher-magnitude features) and feature over-splitting (binary features split into many smaller, less interpretable features). Control requires more than interpretability.

  RELEVANCE: Reveals gap between interpretability and control: SAEs achieve reasonable interpretation but fail at intervention. Challenges sufficiency claims—even when features are interpretable, they may not enable reliable control. Suggests interpretability is necessary but not sufficient for safety-relevant capabilities like steering model behavior. Empirical evidence that current MI techniques have limitations for control applications.

  POSITION: Current MI methods achieve partial interpretability but fall short on control. Sufficiency is not established even in favorable conditions (small model, well-defined task).
  },
  keywords = {sparse-autoencoders, control, interpretability-vs-control, Low}
}

@misc{alignmentforum2024blindspots,
  author = {{AI Alignment Forum}},
  title = {EIS V: Blind Spots in AI Safety Interpretability Research},
  year = {2024},
  howpublished = {\url{https://www.alignmentforum.org/posts/7TFJAvjYfMKxKQ4XS/eis-v-blind-spots-in-ai-safety-interpretability-research}},
  note = {
  CORE ARGUMENT: AI safety interpretability research concentrated in small community with shared assumptions. Focus limited to circuits-style MI, mechanistic anomaly detection, causal scrubbing, and probing. Community might benefit from broader engagement. Internal steering methods (SAEs, logit lens) underperform simple prompting, raising concerns about current tool effectiveness. Critiques include "Interpretability Will Not Reliably Find Deceptive AI."

  RELEVANCE: Community self-assessment identifying limitations in current MI research. Suggests necessity claims may rest on narrow conception of interpretability, and sufficiency is undermined by empirical underperformance. Highlights need for evaluating MI against alternative safety approaches rather than assuming necessity. Represents internal skepticism within AI safety community about MI's current trajectory.

  POSITION: Critical of current MI research program's scope and effectiveness. Questions whether existing MI approaches are necessary or sufficient for safety goals they claim to address.
  },
  keywords = {AI-safety, community-critique, interpretability-limitations, web-source, Medium}
}

@misc{alignmentforum2024critiques,
  author = {{AI Alignment Forum}},
  title = {EIS VI: Critiques of Mechanistic Interpretability Work in AI Safety},
  year = {2024},
  howpublished = {\url{https://www.alignmentforum.org/posts/wt7HXaCWzuKQipqz3/eis-vi-critiques-of-mechanistic-interpretability-work-in-ai}},
  note = {
  CORE ARGUMENT: Catalogues major critiques of MI in AI safety context: (1) MI will not reliably find deceptive AI, (2) mechanistic understanding requires solving hard problems MI does not address, (3) MoSSAIC framework suggests AI safety should move beyond mechanism focus, (4) Hendrycks' "Misguided Quest" argues MI is intractable. Community debate reveals uncertainty about MI's safety contributions.

  RELEVANCE: Comprehensive overview of arguments against MI necessity and sufficiency. Shows active contestation within AI safety community. Necessity challenged by alternative approaches (behavioral testing, formal verification); sufficiency challenged by deception detection limitations and mechanism-understanding gaps. Essential for understanding current state of debate and representing skeptical positions.

  POSITION: Presents skeptical positions without endorsing. Reveals that MI's necessity and sufficiency are contested within AI safety community, not settled assumptions.
  },
  keywords = {mechanistic-interpretability, critiques, AI-safety, deception-detection, web-source, High}
}

@article{gyevnar2025safety,
  author = {Gyevnar, Balint and Kasirzadeh, Atoosa},
  title = {AI Safety for Everyone},
  journal = {Nature Machine Intelligence},
  year = {2025},
  volume = {7},
  pages = {531--542},
  doi = {10.1038/s42256-025-01020-y},
  note = {
  CORE ARGUMENT: Systematic literature review reveals vast array of AI safety work addressing immediate, practical concerns (adversarial robustness, interpretability, fairness) beyond existential risk. Safety research naturally extends existing technological and systems safety practices. Advocates for epistemically inclusive and pluralistic conception of AI safety accommodating diverse motivations and perspectives. Interpretability is one safety concern among many.

  RELEVANCE: Contextualizes necessity/sufficiency debate within broader AI safety landscape. Interpretability contributes to safety but is neither uniquely necessary nor sufficient—other approaches (robustness, verification, alignment) also required. Pluralistic framework suggests false dichotomy in framing interpretability as "necessary or not"—safety requires portfolio of complementary approaches. Challenges MI-centric framing of safety research.

  POSITION: Pluralist about AI safety methods. Interpretability valuable but not privileged. Necessity and sufficiency both overstated—safety requires multiple approaches working together.
  },
  keywords = {AI-safety, pluralism, interpretability-as-one-tool, Medium}
}

@article{quinn2021three,
  author = {Quinn, Thomas P. and Jacobs, Stephan and Senadeera, Manisha and Le, Vuong and Coghlan, Simon},
  title = {The Three Ghosts of Medical AI: Can the Black-Box Present Deliver?},
  journal = {Artificial Intelligence in Medicine},
  year = {2021},
  volume = {124},
  pages = {102158},
  doi = {10.1016/j.artmed.2021.102158},
  note = {
  CORE ARGUMENT: Reviews medical AI black-box problem through three temporal perspectives: past (historical context), present (current capabilities and limitations), future (potential developments). Present black-box systems show promise but face trust and adoption barriers. Future may bring interpretable-by-design architectures, but trade-offs between accuracy and interpretability persist. Neither interpretability nor accuracy alone sufficient for successful deployment.

  RELEVANCE: Argues both interpretability and performance are necessary, neither sufficient. Challenges dichotomy between "accurate black boxes" and "interpretable but limited models"—real-world deployment requires both properties. Suggests necessity debate is misframed: question is not whether interpretability is necessary, but how to achieve both interpretability and performance. Trade-off framing may be empirical question, not conceptual necessity.

  POSITION: Interpretability and accuracy both necessary; neither sufficient alone. Future work should seek to overcome trade-offs rather than accepting them as fundamental.
  },
  keywords = {black-box-medical-AI, interpretability-accuracy-tradeoff, Low}
}

@article{rudin2019why,
  author = {Rudin, Cynthia and Radin, Joanna},
  title = {Why Are We Using Black Box Models in AI When We Don't Need To? A Lesson from an Explainable AI Competition},
  journal = {Harvard Data Science Review},
  year = {2019},
  volume = {1},
  number = {2},
  doi = {10.1162/99608f92.5a8a3a3d},
  note = {
  CORE ARGUMENT: 2018 Explainable Machine Learning Challenge revealed interpretable models can match black-box performance. One team achieved winning results with fully interpretable model, violating competition assumption that black boxes necessary for accuracy. Questions whether black-box models are used when interpretable alternatives would suffice. Advocates for interpretable-by-design models in high-stakes domains.

  RELEVANCE: Challenges necessity of opacity—if interpretable models can match black-box performance, why use black boxes? However, domain-specific (finance); unclear if generalizes to complex domains like language modeling. Raises meta-question: is MI necessary because opacity is fundamental to AI capabilities, or because researchers default to opaque architectures unnecessarily? Empirical question with implications for necessity debate.

  POSITION: Opacity often unnecessary; interpretable-by-design models should be prioritized. Challenges assumption that interpretability requires post-hoc analysis of opaque systems.
  },
  keywords = {interpretable-by-design, black-box-critique, Low}
}
