## Introduction

The AI safety community increasingly invokes "mechanistic interpretability" (MI) as essential for ensuring safe AI systems, yet fundamental disagreements persist about what MI means and what it can deliver. MI aims to reverse-engineer neural network computations into human-understandable algorithms and circuits (Bereska and Gavves 2024), promising granular causal understanding of how AI systems produce their outputs. Proponents argue this mechanistic knowledge is necessary for detecting deceptive alignment, monitoring dangerous capabilities, and enabling precise interventions on model behavior. Critics counter that the enterprise rests on intractable assumptions about compressing terabyte-scale models into human-comprehensible explanations.

Two recent papers crystallize this debate with opposing conclusions. Kastner and Crook (2024) argue that standard explainable AI (XAI) methods, which employ divide-and-conquer strategies to analyze individual components in isolation, fail to illuminate how trained systems work as integrated wholes. Drawing on philosophy of science, they contend that only holistic mechanistic interpretability---applying coordinated discovery strategies from the life sciences to uncover functional organization---can satisfy key safety desiderata. Hendrycks and Hiscott (2025) respond that MI is a "misguided quest": physical mechanisms like clockwork are analyzable, but neural networks are not. The compression required to render models comprehensible may be fundamentally intractable, and the field diverts resources from more tractable safety approaches.

This disagreement reflects deeper conceptual confusion operating at three levels. First, what counts as "mechanistic" interpretability? The philosophy of mechanistic explanation (Craver 2007) provides criteria for identifying genuine mechanisms---organized entities and activities producing regular changes---but these criteria have not been systematically applied to evaluate MI methods. Second, is MI necessary for AI safety? Necessity claims require specifying which safety properties are at stake: detecting deceptive alignment may demand internal inspection, while robustness to distribution shift may not. Third, is MI sufficient? Even complete mechanistic understanding may fail to guarantee safety if that understanding does not translate to control, or if safety failures stem from deployment contexts and organizational pressures rather than model internals (Raji and Dobbe 2023).

The opacity of machine learning systems has long been recognized as a central challenge. Burrell (2016) distinguishes three forms: intentional secrecy, technical illiteracy, and inherent algorithmic opacity arising from the mismatch between high-dimensional optimization and human-scale reasoning. MI claims to address this third form by decomposing complex computations into interpretable circuits and features. Whether it succeeds---and whether success is necessary or sufficient for safety---remains contested.

This review disambiguates these questions by drawing on five bodies of literature: philosophy of mechanistic explanation, XAI taxonomies and definitions, technical MI research, AI safety frameworks and threat models, and epistemic standards for AI explanation. The analysis proceeds as follows: Section 1 examines what counts as mechanistic interpretability by mapping definitional disputes onto philosophical debates about levels, decomposition, and the relationship between mechanism and function. Section 2 addresses whether MI is necessary or sufficient for AI safety by analyzing the logical structure of these claims, identifying their empirical assumptions, and specifying the conditions under which each holds or fails. The review concludes by identifying research gaps and articulating a pluralistic position: MI's necessity and sufficiency are not categorical properties but depend on specifying safety goals, stakeholders, and contexts.

The stakes are considerable. For analytic philosophers and journal editors in philosophy of science, the MI debate offers a live case study in applying philosophical frameworks to emerging technologies. For AI safety researchers, the legitimacy of the MI research program depends on clarifying these foundational questions. And for policymakers, determining whether to mandate interpretability requirements presupposes answers about what interpretability can and cannot deliver.
