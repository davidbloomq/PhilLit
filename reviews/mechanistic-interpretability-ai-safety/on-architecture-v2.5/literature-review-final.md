---
title: "Is Mechanistic Interpretability Necessary or Sufficient for AI Safety? A State-of-the-Art Literature Review"
date: 2026-01-01
author: Literature Review
keywords: [mechanistic interpretability, AI safety, explainable AI, philosophy of science, mechanistic explanation]
---

## Introduction

The AI safety community increasingly invokes "mechanistic interpretability" (MI) as essential for ensuring safe AI systems, yet fundamental disagreements persist about what MI means and what it can deliver. MI aims to reverse-engineer neural network computations into human-understandable algorithms and circuits (Bereska and Gavves 2024), promising granular causal understanding of how AI systems produce their outputs. Proponents argue this mechanistic knowledge is necessary for detecting deceptive alignment, monitoring dangerous capabilities, and enabling precise interventions on model behavior. Critics counter that the enterprise rests on intractable assumptions about compressing terabyte-scale models into human-comprehensible explanations.

Two recent papers crystallize this debate with opposing conclusions. Kastner and Crook (2024) argue that standard explainable AI (XAI) methods, which employ divide-and-conquer strategies to analyze individual components in isolation, fail to illuminate how trained systems work as integrated wholes. Drawing on philosophy of science, they contend that only holistic mechanistic interpretability---applying coordinated discovery strategies from the life sciences to uncover functional organization---can satisfy key safety desiderata. Hendrycks and Hiscott (2025) respond that MI is a "misguided quest": physical mechanisms like clockwork are analyzable, but neural networks are not. The compression required to render models comprehensible may be fundamentally intractable, and the field diverts resources from more tractable safety approaches.

This disagreement reflects deeper conceptual confusion operating at three levels. First, what counts as "mechanistic" interpretability? The philosophy of mechanistic explanation (Craver 2007) provides criteria for identifying genuine mechanisms---organized entities and activities producing regular changes---but these criteria have not been systematically applied to evaluate MI methods. Second, is MI necessary for AI safety? Necessity claims require specifying which safety properties are at stake: detecting deceptive alignment may demand internal inspection, while robustness to distribution shift may not. Third, is MI sufficient? Even complete mechanistic understanding may fail to guarantee safety if that understanding does not translate to control, or if safety failures stem from deployment contexts and organizational pressures rather than model internals (Raji and Dobbe 2023).

The opacity of machine learning systems has long been recognized as a central challenge. Burrell (2016) distinguishes three forms: intentional secrecy, technical illiteracy, and inherent algorithmic opacity arising from the mismatch between high-dimensional optimization and human-scale reasoning. MI claims to address this third form by decomposing complex computations into interpretable circuits and features. Whether it succeeds---and whether success is necessary or sufficient for safety---remains contested.

This review disambiguates these questions by drawing on five bodies of literature: philosophy of mechanistic explanation, XAI taxonomies and definitions, technical MI research, AI safety frameworks and threat models, and epistemic standards for AI explanation. The analysis proceeds as follows: Section 1 examines what counts as mechanistic interpretability by mapping definitional disputes onto philosophical debates about levels, decomposition, and the relationship between mechanism and function. Section 2 addresses whether MI is necessary or sufficient for AI safety by analyzing the logical structure of these claims, identifying their empirical assumptions, and specifying the conditions under which each holds or fails. The review concludes by identifying research gaps and articulating a pluralistic position: MI's necessity and sufficiency are not categorical properties but depend on specifying safety goals, stakeholders, and contexts.

The stakes are considerable. For analytic philosophers and journal editors in philosophy of science, the MI debate offers a live case study in applying philosophical frameworks to emerging technologies. For AI safety researchers, the legitimacy of the MI research program depends on clarifying these foundational questions. And for policymakers, determining whether to mandate interpretability requirements presupposes answers about what interpretability can and cannot deliver.


## What Counts as Mechanistic Interpretability?

The debate over mechanistic interpretability's role in AI safety rests on a prior question: what makes an interpretation "mechanistic" in the first place? This section establishes the conceptual foundations by mapping definitional disputes in machine learning onto the philosophy of mechanistic explanation, revealing that disagreements between critics and proponents reflect deeper philosophical tensions about levels, decomposition, and the relationship between mechanism and function.

### The New Mechanism Framework

The "new mechanism" philosophy of science, emerging in the early 2000s, fundamentally reoriented explanation away from law-based models toward understanding how things work. Machamer, Darden, and Craver (2000) established the foundational characterization: mechanisms are "entities and activities organized such that they are productive of regular changes from start or set-up to finish or termination conditions." This minimal characterization identifies three requirements for mechanistic explanation: specifying component entities, describing their activities, and showing how organization produces the phenomenon of interest.

Craver (2007) developed this framework further by distinguishing constitutive from etiological mechanistic explanation. Constitutive explanations reveal how a phenomenon is "built up" from organized components at lower levels, while etiological explanations trace causal histories. Crucially, Craver introduced the mutual manipulability criterion for constitutive relevance: components are mechanistically relevant to a phenomenon if interventions on components change the phenomenon's behavior and vice versa. This criterion provides an empirical test for whether proposed mechanisms identify genuine structure rather than mere correlation.

Bechtel and Richardson (2010) caution that mechanistic decomposition can fail when systems are not modular. Their analysis of "looking around" (understanding horizontal organization) rather than merely "looking down" (vertical decomposition) highlights that complex systems with distributed dynamics may resist clean hierarchical analysis. Glennan (2017) synthesizes these developments, arguing for pluralism about mechanistic levels: what matters is whether each proposed level identifies genuine part-whole structure, not whether decomposition reaches some privileged fundamental level.

### MI in Machine Learning Practice

Mechanistic interpretability in machine learning aims to reverse-engineer neural network computations into human-understandable algorithms. The transformer circuits framework introduced by Elhage et al. (2021) provides the technical foundation: the residual stream serves as a communication channel through which attention heads and MLP layers read and write information. This framework enables tracing computational pathways and identifying "circuits"—minimal subgraphs that implement specific behaviors.

Two obstacles complicate this project. First, Elhage et al. (2022) demonstrated that neural networks exhibit "superposition"—representing more features than they have neurons by storing them as overlapping directions in activation space. This creates "polysemanticity" where individual neurons respond to multiple unrelated concepts, undermining neuron-level analysis. Second, the labor-intensive nature of circuit discovery limits scalability, though automated methods like ACDC (Conmy et al. 2023) have made progress.

The response to superposition has been sparse autoencoders (SAEs), which decompose activations into more interpretable, monosemantic features (Bricken et al. 2023). Templeton et al. (2024) scaled this approach to Claude 3 Sonnet, discovering millions of interpretable features including safety-relevant concepts like deception and dangerous knowledge. Meanwhile, circuit discovery has yielded paradigmatic successes: Olsson et al. (2022) identified "induction heads"—two attention heads working in composition—as the mechanism underlying in-context learning, demonstrating that MI can link micro-level operations to macro-level capabilities.

The operational definition implicit in this practice is: MI = reverse-engineering computational mechanisms into human-understandable algorithms. Yet this definition leaves ambiguous the grain of analysis required—whether understanding must reach individual activations or whether higher-level functional descriptions suffice.

### Resolving the Definitional Dispute

The philosophy of mechanistic explanation provides resources for resolving this ambiguity. Piccinini and Craver (2011) argue that functional analyses—decomposing capacities into subfunctions without specifying implementation—are not alternatives to mechanistic explanation but rather "mechanism sketches" that abstract from implementation details while retaining mechanistic structure. Functional analysis identifies what a mechanism does; implementation reveals how. Both constitute legitimate mechanistic levels integrated through part-whole relations.

Povich and Craver (2017) extend this insight: multiple mechanistic levels can coexist non-competitively because each level identifies genuine constitutive structure. Mechanistic levels are defined by part-whole relations, not by spatial scale or disciplinary boundaries. Thus, attention heads (a higher-level functional description) and individual activations (a lower-level implementation description) can both be mechanistic if they satisfy appropriate criteria—specifically, if interventions on components at that level change system behavior and vice versa.

This framework illuminates the Hendrycks/Kastner disagreement. Kastner and Crook (2024) emphasize that MI should seek "functional organization"—how components work together to produce system-level behavior—rather than merely cataloging individual component properties. Their critique of "divide-and-conquer" XAI strategies echoes Bechtel's concern that horizontal organization matters, not just vertical decomposition. By contrast, critics who demand neuron-level or activation-level analysis implicitly assume that only the finest-grained level counts as genuinely mechanistic.

The philosophical resolution is that both can be right: narrow (circuit-level) and broad (functional) construals can be genuinely mechanistic if they satisfy mutual manipulability criteria—if interventions at the proposed level of description causally affect model behavior. Ayonrinde and Jaburi (2025) explicitly bridge philosophy of mechanism and MI, arguing that neural networks contain "implicit explanations" extractable through principled mechanistic investigation. Their "Principle of Explanatory Optimism" conjectures that networks are sufficiently structured to support such explanation.

Yet this resolution also exposes a gap: philosophical criteria for mechanistic explanation have not been systematically applied to evaluate MI methods. Whether circuit discovery and sparse autoencoders identify genuine constitutive structure—satisfying mutual manipulability—remains largely unexamined. The field operates with implicit operational definitions rather than explicit criteria drawn from the philosophy of mechanism. Establishing which MI methods provide genuine mechanistic understanding, and at which levels of analysis, requires exactly this systematic application of philosophical standards.



## Is Mechanistic Interpretability Necessary or Sufficient for AI Safety?

The debate over mechanistic interpretability's role in AI safety often proceeds as if necessity and sufficiency were straightforward properties to assess. Yet this framing obscures a deeper problem: claims that MI is "necessary" or "sufficient" for safety are rarely well-formulated. They require specifying *which* safety properties are at stake and *which* form of mechanistic interpretability is being invoked. Without such specification, the debate remains at the level of competing intuitions rather than testable claims.

Current evidence supports conditional conclusions. MI may be necessary for some safety goals---particularly detecting deceptive alignment and other internal misalignment threats where behavioral testing systematically fails---but not for others, such as robustness to distribution shift or specification gaming. Meanwhile, MI is clearly insufficient as a standalone safety approach; the consensus across technical and philosophical literature is that safety requires complementary mechanisms operating in a defense-in-depth framework (Dung and Mai 2025; Bengio et al. 2025).

### The Case for Necessity

The strongest arguments for MI's necessity center on threat models where behavioral testing is structurally inadequate. Kastner and Crook (2024) argue that XAI's divide-and-conquer strategy---explaining individual predictions without illuminating how trained systems work as integrated wholes---cannot satisfy key safety desiderata. Only holistic mechanistic understanding, achieved through coordinated discovery strategies analogous to those in the life sciences, enables the kind of functional organization knowledge that safety requires.

This argument gains force when applied to the deceptive alignment threat model. If advanced AI systems can strategically fake alignment during evaluation periods while pursuing different objectives internally, behavioral testing becomes fundamentally unreliable (Bereska and Gavves 2024). Recent empirical evidence suggests this is not merely theoretical: Claude 3 Opus and OpenAI o1-preview have exhibited alignment faking behaviors, with Claude strategically answering prompts to avoid retraining that would make it more compliant with requests conflicting with its values (AI Alignment Forum 2024). In such cases, internal inspection appears necessary because external observation cannot distinguish genuine from feigned alignment.

Von Eschenbach (2021) extends this reasoning to trust-based arguments: users cannot rationally assess an AI system's reliability without understanding its mechanisms. Black-box systems undermine trust because stakeholders have no basis for judging when systems operate outside their competence. The Stanford Encyclopedia's treatment of AI ethics similarly identifies opacity as a central ethical problem, though it acknowledges other safeguards might compensate (Muller 2020).

However, necessity arguments often assume specific threat models without establishing that MI is necessary across all threat models. The conditions favoring necessity---internal misalignment, strategic deception, or capabilities emerging without behavioral precursors---represent one cluster of safety concerns. Whether MI is necessary for the broader space of safety challenges remains an open question.

### Against Necessity

Critics challenge MI's necessity on both practical and principled grounds. London (2019) argues from medical precedent that opaque decisions are acceptable when empirical validation is rigorous and causal knowledge is incomplete. Drawing on Aristotelian distinctions, he contends that the ability to produce verified results can outweigh the ability to explain how those results are produced. If extensive behavioral testing establishes safety and efficacy, mechanistic understanding may be a luxury rather than a requirement.

Hendrycks and Hiscott (2025) raise the compression objection: terabyte-scale models cannot be compressed into human-comprehensible explanations without catastrophic information loss. Modern neural networks encode information at scales fundamentally incompatible with human working memory and processing capacity. Even if mechanistic explanation is conceptually desirable, practical intractability renders MI an unsuitable foundation for safety assurance.

Duran and Formanek (2018) propose an alternative epistemic foundation: computational reliabilism. Trust can be grounded in reliable processes---verification, validation, robustness testing, expert knowledge---without requiring transparency into mechanisms. If statistical validation establishes that a system performs reliably across diverse conditions, this may justify deployment regardless of mechanistic opacity.

Wachter, Mittelstadt, and Russell (2018) argue that normative demands for contestability and recourse can be met through counterfactual explanations rather than mechanistic transparency. Counterfactuals specify what minimal changes would produce different outcomes, enabling affected parties to understand what they can do differently without understanding how the model works internally. If the goal is empowering subjects of algorithmic decisions, counterfactuals may suffice where MI is impractical.

Alternative safety mechanisms further challenge necessity claims. Irving, Christiano, and Amodei (2018) propose AI safety via debate, where competing AI systems generate arguments for a human judge to evaluate. Debate provides scalable oversight without requiring interpretability of either debater's internals---only the ability to evaluate competing claims. Constitutional AI (Bai et al. 2022) achieves alignment through self-critique and externally specified principles rather than human understanding of model mechanisms.

Yet critics sometimes conflate practical intractability with conceptual unnecessity. Even if MI is currently intractable for frontier models, it might remain conceptually necessary for certain safety assurances. The question of whether alternative approaches can *in principle* provide equivalent safety guarantees---or merely approximate them under favorable conditions---remains contested.

### The Sufficiency Question

If the necessity debate is contested, the sufficiency question admits a clearer answer: mechanistic interpretability is clearly insufficient as a standalone safety approach. This consensus emerges from multiple lines of analysis.

First, interpretability does not automatically translate to control. Makelov et al. (2024) demonstrate that sparse autoencoders capture interpretable features but fail at reliable intervention on model behavior. Their evaluation revealed phenomena limiting SAE effectiveness---feature occlusion (causally relevant concepts overshadowed by higher-magnitude features) and feature over-splitting (binary features decomposed into many smaller, less interpretable components)---that prevent interpretable representations from enabling reliable steering. Understanding mechanisms and controlling behavior are distinct capabilities that may not covary.

Second, safety failures often stem from factors beyond model-level mechanisms. Raji and Dobbe (2023), revisiting Amodei et al.'s (2016) concrete problems framework, show that real-world AI safety incidents frequently arise from deployment context, organizational pressures, and misaligned stakeholder incentives rather than model design flaws. Even complete mechanistic understanding of a model cannot address institutional failures, governance gaps, or the socio-technical dynamics of deployment contexts. Interpretability is a model-level intervention applied to what are often system-level problems.

Third, AI safety is multidimensional, encompassing robustness, security, fairness, privacy, and compliance alongside alignment (He et al. 2021; Zheng et al. 2024). Different properties require different techniques: adversarial robustness against perturbations, security against malicious inputs, fairness through distribution analysis, privacy through differential guarantees. MI might contribute to some dimensions (understanding failure modes, identifying biased representations) while leaving others largely unaddressed (cryptographic security, legal compliance).

Fourth, Dung and Mai (2025) analyze correlation of failure modes across safety techniques, finding significant overlap. If specification gaming affects RLHF, adversarial training, and interpretability-based approaches alike, layering multiple techniques provides less protection than independence would suggest. Defense-in-depth requires techniques with uncorrelated failure modes; MI's distinctive contribution depends on what failure modes it addresses that others miss.

Gyevnar and Kasirzadeh (2025) synthesize these considerations into a pluralistic framework: AI safety research encompasses a vast array of approaches addressing different concerns, and interpretability is one tool among many. The question is not whether MI is sufficient but how it complements other approaches within a portfolio of safety mechanisms.

### Context-Dependency and Pluralism

The most defensible position recognizes that necessity and sufficiency are not categorical properties but context-dependent assessments that vary with safety goals, stakeholders, and deployment contexts.

Zednik (2019) provides a multi-level framework based on Marr's levels of analysis. Different stakeholders require different forms of transparency: implementational details matter for debugging, algorithmic descriptions for auditing, computational-level specifications for scientific understanding. No single interpretability form serves all purposes. MI provides one type of transparency---mechanism-level understanding---that may be essential for some purposes (scientific research, detecting internal misalignment) while irrelevant for others (regulatory compliance, user trust).

Baum (2025) shows that alignment itself is multidimensional, varying in aim (safety, ethicality, legality), scope (outcome versus execution), and constituency (individual versus collective). MI might be necessary for some configurations---execution-level safety verification for individual high-stakes decisions---while irrelevant for others---outcome-level collective welfare assessment. Without specifying the alignment configuration at stake, necessity claims lack determinate content.

Yao's (2021) explanatory pluralism distinguishes diagnostic, explication, expectation, and role explanations, each serving different purposes. MI provides diagnostic explanations that reveal causal structure but may not serve expectation formation (predicting system behavior) or role explanation (understanding a system's function in a larger context). Different safety questions call for different explanation types.

Buchholz (2023) proposes means-end analysis: evaluating interpretability requires specifying goals. MI is well-suited for scientific understanding of model mechanisms but poorly suited for building user trust, which requires accessible explanations rather than technical circuit analysis. The relevant question is not "Is MI necessary?" but "Necessary for what, and for whom?"

This pluralistic framing suggests the most productive research direction: mapping specific safety goals to the forms of interpretability (or alternative mechanisms) they require. Deceptive alignment detection plausibly requires internal inspection of some kind, though whether circuit-level MI is necessary or whether other forms of internal monitoring suffice remains open. Distributional robustness may be addressable through behavioral testing. Fairness assessment may require statistical analysis rather than mechanistic understanding. The field lacks a comprehensive mapping from safety properties to interpretability requirements---a gap that philosophical analysis can help address.

The conditional conclusion emerging from this analysis: MI is likely necessary for detecting certain internal misalignment threats where behavioral testing is structurally inadequate, but not for all safety properties. It is clearly insufficient as a standalone approach, requiring complementary mechanisms operating within a defense-in-depth framework. The most productive framing treats MI as one tool among many, with its distinctive contribution depending on which safety goals are prioritized and which threat models are most salient.



## Research Gaps and Opportunities

The preceding analysis reveals that the debate over mechanistic interpretability's role in AI safety suffers from four interconnected gaps: conceptual imprecision, underspecified claims, limited empirical grounding, and insufficient engagement with fundamental epistemic constraints. Addressing these gaps is essential for productive research on whether MI is necessary or sufficient for safe AI systems.

### Gap 1: No Systematic Application of Philosophical Criteria to MI

Despite the new mechanism literature providing sophisticated criteria for evaluating mechanistic explanations, these resources remain largely untapped in MI research. Craver's (2007) mutual manipulability criterion specifies that components are constitutively relevant to a mechanism only if interventions on components change the mechanism's behavior and vice versa. Povich and Craver (2017) clarify that mechanistic levels are constitutive part-whole hierarchies, not merely spatial scales or disciplinary divisions, enabling non-competitive coexistence of multiple mechanistic levels. Piccinini and Craver (2011) demonstrate that functional analyses qualify as mechanism sketches that complement rather than compete with implementational detail.

Yet MI practitioners operate with implicit operational definitions rather than explicit philosophical criteria. Ayonrinde and Jaburi (2025) represent the only sustained attempt to bridge philosophy of mechanism and MI, arguing that MI produces "model-level, ontic, causal-mechanistic, and falsifiable explanations." This isolation matters: without agreed criteria for what counts as mechanistic explanation, practitioners cannot evaluate whether circuit discovery provides genuine mechanistic understanding or merely useful descriptions that happen to employ mechanistic vocabulary. Applying Craver's mutual manipulability criterion systematically to current MI methods---assessing whether activation patching interventions satisfy the conditions for constitutive relevance---would clarify MI's explanatory status and reveal which techniques deliver genuine mechanistic insight.

### Gap 2: Underspecified Necessity and Sufficiency Claims

Current arguments for and against MI's necessity remain at the level of competing intuitions rather than testable claims. Kastner and Crook (2024) argue MI is necessary for satisfying "safety desiderata" but do not specify which desiderata require mechanistic understanding versus which might be satisfied through behavioral testing or alternative approaches. Hendrycks and Hiscott (2025) critique MI's tractability without specifying which safety properties might still benefit from interpretability even if complete mechanistic understanding proves impossible.

This imprecision reflects deeper conceptual confusion. Baum (2025) demonstrates that "alignment" itself is multidimensional, varying across aim (safety, ethicality, legality), scope (outcome versus execution), and constituency (individual versus collective). Zednik (2019) shows that different stakeholders require different forms of transparency at different Marr levels. Amodei et al. (2016) identify five concrete safety problems---side effects, reward hacking, scalable supervision, safe exploration, distributional shift---each potentially requiring different interventions. Without mapping specific safety properties to specific interpretability requirements, debates about MI's necessity conflate distinct questions: Is MI necessary for detecting deceptive alignment? For preventing reward hacking? For ensuring robustness to distribution shift? Bereska and Gavves (2024) gesture toward this differentiation by distinguishing MI's potential contributions to understanding, control, and alignment, but systematic mapping remains absent.

### Gap 3: Limited Empirical Evidence on MI's Safety Impact

The strongest MI results demonstrate circuit discovery and feature extraction, not safety-relevant outcomes. Nanda et al. (2023) fully reverse-engineered the algorithm learned by small transformers on modular addition; Olsson et al. (2022) identified induction heads as the mechanism underlying in-context learning; Templeton et al. (2024) extracted millions of interpretable features from Claude 3 Sonnet, including safety-relevant concepts like deception and bias. These achievements establish MI's technical viability but leave open whether mechanistic understanding improves safety outcomes.

More troublingly, Makelov et al. (2024) demonstrate a significant gap between interpretability and control: sparse autoencoders capture interpretable features but fail at reliable intervention due to feature occlusion and over-splitting. Even when features are interpretable, they may not enable the behavioral steering that safety applications require. Conmy et al.'s (2023) ACDC algorithm successfully rediscovered manually-identified circuits, yet scaling these methods to frontier models and diverse behaviors remains undemonstrated. Necessity and sufficiency are ultimately empirical questions requiring evidence that MI improves specific safety metrics---reduces deceptive behavior detection time, enables more reliable capability elicitation, supports more effective red-teaming---rather than merely advancing scientific understanding of neural networks. Identifying tractable test cases where MI's safety contribution can be measured against alternatives (behavioral testing, formal verification, debate) would ground the normative debate in empirical reality.

### Gap 4: Neglected Relationship Between Epistemic Opacity and MI

The philosophy of computational science has developed sophisticated accounts of epistemic opacity that MI literature largely ignores. Humphreys (2009) introduces "essential epistemic opacity": a computational process is essentially opaque if its core method cannot be reproduced by unaided human calculation in reasonable time. This opacity is not merely practical but stems from the computational nature of the method itself. Burrell (2016) distinguishes three forms of ML opacity---intentional secrecy, technical illiteracy, and inherent algorithmic complexity arising from high-dimensional optimization---arguing the third form is categorically distinct and not addressable by the same means as the others.

MI aims to address Burrell's third form of opacity, yet the relationship between circuit-level decomposition and human-scale understanding remains undertheorized. Does identifying that a transformer uses discrete Fourier transforms for modular addition (Nanda et al. 2023) overcome essential epistemic opacity, or merely relocate it to understanding how thousands of such circuits interact? Beisbart (2021) argues opacity involves multiple dimensions of knowledge and understanding; MI might address some (component mechanisms) while leaving others untouched (emergent behavior, failure prediction). Duran and Formanek (2018) propose computational reliabilism as an alternative to transparency-based trust, suggesting that validation processes (verification, robustness testing) may ground justified use without requiring MI-level mechanistic understanding. Whether MI techniques address inherent algorithmic opacity or create new forms of interpretive opacity---where the explanations themselves require specialized expertise to comprehend---remains an open question with significant implications for MI's practical value.

### Synthesis

These gaps collectively reveal that the MI-safety debate lacks the conceptual precision, empirical grounding, and engagement with epistemic constraints necessary for productive resolution. Advocates and critics alike operate with underspecified conceptions of what MI claims to provide, which safety properties are at stake, and how mechanistic understanding relates to trustworthy deployment. The research project addresses this by: (1) applying philosophical criteria from mechanism literature to evaluate whether current MI methods deliver genuine mechanistic explanation; (2) developing a systematic mapping from specific safety properties to interpretability requirements, enabling precise conditional claims about necessity and sufficiency; (3) identifying tractable empirical tests for MI's safety contribution; and (4) analyzing whether MI overcomes or merely relocates the epistemic opacity that motivates transparency demands. Without such clarification, the field risks either overinvesting in MI approaches that cannot deliver on their safety promises or prematurely abandoning a research program with genuine but limited utility.


## Conclusion

The debate over mechanistic interpretability's necessity and sufficiency for AI safety reflects three interrelated sources of confusion that this review has sought to disentangle: definitional ambiguity about what constitutes mechanistic explanation, underspecified claims about which safety properties are at stake, and limited empirical evidence connecting interpretability methods to safety outcomes.

The philosophy of mechanistic explanation provides substantive resources for addressing definitional confusion. As Piccinini and Craver (2011) demonstrate, functional analyses and mechanistic descriptions complement rather than compete---both narrow construals targeting individual circuits (Olsson et al. 2022; Conmy et al. 2023) and broader construals emphasizing functional organization (Kastner and Crook 2024) can qualify as genuinely mechanistic if they satisfy criteria such as mutual manipulability (Craver, Glennan, and Povich 2021). The apparent disagreement between Hendrycks and Hiscott's (2025) skepticism and Kastner and Crook's (2024) advocacy dissolves once we recognize that multiple mechanistic levels can coexist non-competitively (Povich and Craver 2017). What remains underexplored is systematic application of these philosophical criteria to evaluate current MI methods---Ayonrinde and Jaburi (2025) represents the only sustained attempt to bridge these literatures.

On necessity, the evidence supports conditional rather than categorical conclusions. For threat models involving internal misalignment---particularly deceptive alignment, where models strategically fake compliance during evaluation (Alignment Forum 2024)---behavioral testing appears insufficient, and internal inspection becomes essential. However, for other safety properties such as robustness to distributional shift, London (2019) and Duran and Formanek (2018) demonstrate that reliability-based validation may provide adequate epistemic grounding without mechanistic transparency. Alternative oversight mechanisms including debate (Irving, Christiano, and Amodei 2018) and constitutional AI (Bai et al. 2022) offer paths to alignment that do not presuppose complete interpretability. The necessity of MI thus depends critically on specifying which safety goals are at stake and which threat models are operative---a specification the current debate frequently lacks.

On sufficiency, the evidence is clearer: MI is insufficient as a standalone safety approach. Makelov et al. (2024) demonstrate a significant gap between interpretability and control---sparse autoencoders capture interpretable features but fail to enable reliable intervention. Raji and Dobbe (2023) show that safety failures often stem from deployment context and organizational pressures rather than model internals. Dung and Mai (2025) reveal that failure modes correlate across safety techniques, limiting the effectiveness of defense-in-depth strategies that rely on any single approach. Even complete mechanistic understanding may not translate to control capacity if the understanding does not enable precise intervention.

The most productive framing is therefore pluralistic: different safety goals require different forms of interpretability, or none at all (Zednik 2019; Buchholz 2023; Yao 2021). MI is one tool within a defense-in-depth strategy (Bengio et al. 2025), well-suited for detecting certain internal threats but requiring complementary mechanisms for addressing the full range of safety challenges identified by Amodei et al. (2016) and subsequently expanded by He et al. (2021) and Zheng et al. (2024).

This review contributes to the debate by offering systematic application of philosophical criteria from the mechanism literature to evaluate MI's explanatory status; precise formulation of conditional necessity and sufficiency claims that specify target safety properties; and identification of the empirical evidence needed to resolve remaining disagreements. What emerges is not a verdict for or against MI, but rather a clarification of the conditions under which it proves essential and those under which alternative approaches may suffice---a conditional analysis the field requires to allocate research resources appropriately and to develop realistic expectations about what interpretability research can deliver for AI safety.



