# State-of-the-Art Literature Review
## Social Experiments in the Agentic Economy: Procedural Justification and Moral Learning among Artificial Agents

---

## Introduction

Algorithmic trading systems now execute the majority of financial transactions, autonomous procurement agents negotiate supply chain contracts worth billions of dollars, and AI-powered energy management systems coordinate grid resources across regional markets. These **agentic markets**—economic environments where artificial intelligence agents transact, negotiate, and allocate resources on behalf of humans—represent a fundamental shift in how markets operate. Unlike traditional automated systems that merely execute predefined rules, contemporary AI agents exhibit strategic reasoning, adaptive learning, and sophisticated negotiation capabilities (Park et al. 2023; Rahwan et al. 2019). The rise of large language model-based agents has dramatically accelerated this trend, with AI systems now capable of representing human interests in complex multi-party negotiations and resource allocation decisions (Wellman 2007). This transformation raises a central normative question: How should we evaluate these systems when the decision-makers are non-human artificial agents?

This question emerges from the convergence of three distinct intellectual developments. First, political philosophy has recently developed **procedural experimentalism**, a framework arguing that certain decision-making procedures involving experimentation can themselves justify mid-level normative principles through moral learning (Himmelreich 2023; Anderson 2006). Unlike pure proceduralism, which justifies outcomes solely by procedure correctness, or outcome-based approaches that judge procedures instrumentally, procedural experimentalism holds that experimental iterations within institutional contexts can reveal which principles are action-guiding and reflectively stable. This framework was developed for human institutions making collective decisions—but what happens when the institutions are populated by AI agents?

Second, the field of AI ethics and governance has increasingly emphasized **procedural fairness** over purely outcome-based approaches to evaluating automated systems. Binns (2018) demonstrates that algorithmic fairness requires drawing on political philosophy's procedural traditions rather than relying solely on statistical definitions of fairness. Work by Barocas and Selbst (2016) on algorithmic discrimination, Susser et al. (2019) on digital manipulation, and Rahwan's (2018) "society-in-the-loop" framework all recognize that how AI systems make decisions—their procedural features like transparency, accountability, and contestability—matters as much as what decisions they reach. Yet most AI ethics work addresses systems that make decisions *about* humans (classification, prediction, recommendation) rather than AI agents acting *for* humans as representatives in economic contexts.

Third, market design theory provides sophisticated tools for analyzing how procedural features of markets—auction rules, information structures, matching mechanisms—affect both efficiency and fairness (Roth 2002; Moulin 2004). Recent work has shown that seemingly minor procedural variations can have dramatic implications for who benefits and whether manipulation is possible (Pathak and Sönmez 2013). However, market design literature overwhelmingly assumes human participants. The normative implications of AI agents as market participants, particularly when they represent human principals, remain largely unexplored.

These three developments converge on the importance of procedures but fragment by domain. Procedural experimentalism addresses human institutions. Market design addresses human market participants. AI ethics addresses AI systems making decisions about humans. Agentic markets fall at the intersection: AI agents representing human interests within market procedures. This intersection reveals systematic gaps in our normative frameworks.

This review proceeds in three sections. Section 2 examines the three theoretical frameworks in detail, showing how each addresses procedural evaluation but leaves key questions unanswered when extended to agentic markets. Section 3 identifies four specific research gaps that emerge at the intersection of these frameworks. The conclusion synthesizes the state-of-the-art and positions the research project's contributions to filling these gaps. The central finding is that existing frameworks, while sophisticated within their domains, do not integrate to provide normative guidance for AI-mediated markets where agents represent human interests. The proposed research addresses this gap by extending procedural experimentalism to agentic markets through controlled simulation experiments.

---

## Theoretical Frameworks for Evaluating Agentic Markets

Three bodies of scholarship provide theoretical resources for evaluating agentic markets, each offering distinctive normative frameworks while leaving critical questions unanswered when applied to AI-mediated economic systems.

### Procedural Experimentalism and Social Learning

Procedural experimentalism, most fully developed in recent work by Himmelreich (2023) and drawing on pragmatist traditions (Dewey 1927; Anderson 2006), holds that certain decision-making procedures involving experimentation can justify normative principles not through a priori reasoning or pure consent, but through a process of social learning and iterative refinement. The framework distinguishes itself from Rawlsian procedural justice, which treats procedures as fair independent of outcomes, and from purely instrumental approaches that judge procedures solely by outcomes achieved. Instead, procedural experimentalism treats experimental procedures as discovery mechanisms that reveal which normative principles prove action-guiding, stable under reflective scrutiny, and capable of coordinating social cooperation (Anderson 2006).

The justificatory structure works through moral learning: institutions embody provisional principles, participants experience their operation, feedback mechanisms enable reflection on successes and failures, and principles are revised based on this learning. Himmelreich (2023) explicitly applies this framework to AI ethics, arguing that tools for ethical AI development should be evaluated procedurally—by whether they enable productive experimentation and learning—rather than by guarantees of correct outcomes. This represents a significant extension beyond human-only contexts. Yet Himmelreich focuses on AI development tools rather than AI agents operating within institutions, leaving open the question of whether AI agents themselves can be participants in procedural experimentation.

The framework makes several key assumptions about participants: they possess reflective capacities to evaluate how principles operate in practice, they can revise their commitments based on experience, and they engage in deliberation about which principles to maintain or modify. Richardson (2002) emphasizes that such learning works through "specification"—the progressive articulation of vague normative commitments into concrete guidance for particular contexts. Estlund (2009) grounds this in epistemic proceduralism: democratic procedures have epistemic value because they track truth about justice while remaining acceptable to all qualified perspectives. The epistemic dimension is crucial—procedures don't merely aggregate preferences but generate knowledge about normative requirements.

When we consider agentic markets, these assumptions become problematic. Do AI agents engage in moral learning? If "learning" means functional adaptation based on feedback (as in machine learning), then agents learn in one sense—but this differs from the reflective deliberation and value revision that procedural experimentalism envisions. List and Pettit (2011) argue that collectivities can exhibit group agency when they have representational states, motivational states, and capacity to process reasons, but whether artificial multi-agent systems meet these criteria remains contested. Even if agent systems could functionally learn, questions multiply: What constitutes genuine experimentation when agents execute procedures? What feedback mechanisms enable normative discovery rather than mere optimization? How much human oversight is required for procedural legitimacy? Can hybrid systems—where humans and agents jointly participate in market procedures—engage in procedural experimentation in any meaningful sense? These questions reveal that procedural experimentalism, while powerful for human institutions, requires substantial theoretical development to extend to agentic contexts.

### Market Design and Procedural Features

Market design theory, synthesized in Roth's (2002) framework of the "economist as engineer," provides sophisticated tools for analyzing how procedural features of markets affect normative outcomes. Unlike pure economic theory that assumes fixed institutional structures, market design explicitly treats procedural features as design variables subject to systematic evaluation and iterative refinement. Roth emphasizes that successful market design requires combining theory, experimentation, and computation—theoretical analysis alone cannot predict all relevant features of institutional performance.

The field has generated rich vocabulary for procedural analysis. Bergemann and Morris (2019) show how information design—decisions about what information to disclose, when, and to whom—fundamentally shapes market outcomes. Fair division literature establishes multiple procedural fairness criteria: envy-freeness (no participant prefers another's allocation), proportionality (each receives subjectively fair share), and equal access (procedural rules apply uniformly). These criteria often conflict, requiring trade-offs that themselves become subjects of normative evaluation. Moulin (2004) demonstrates that different allocation procedures embody different fairness principles, and that choosing among them requires substantive moral judgment. Budish et al. (2013) analyze how randomization can serve fairness by giving participants equal ex-ante chances, even when ex-post outcomes differ. The key insight is that procedural features—not just outcomes—carry normative weight.

Empirical work reinforces this procedural focus. Pathak and Sönmez (2013) show that school choice mechanisms differ dramatically in vulnerability to strategic manipulation, with procedural details determining whether sophisticated actors gain unfair advantages over less-informed participants. This demonstrates that procedural evaluation requires understanding not just formal properties but how procedures operate given participants' actual strategic capabilities.

However, market design overwhelmingly assumes human participants whose preferences, information processing, and strategic reasoning capacities follow predictable patterns. When AI agents enter markets, these assumptions break down in multiple ways. First, standard fairness criteria like envy-freeness were designed for evaluating human welfare—but do they apply when the direct market participants are artificial agents representing human principals? An agent might not "envy" another agent's allocation, yet the human principal could experience unfairness. Second, information design becomes radically different when agents process information at machine speed and scale, potentially identifying patterns or correlations invisible to humans. Third, strategic sophistication transforms: contemporary AI agents exhibit strategic reasoning that can exploit procedural features in ways beyond human capability (Wellman 2015). Fourth, procedural transparency faces new questions—should information be transparent to agents, to the humans they represent, to regulators, or to all simultaneously? These complications suggest that market design principles cannot simply transfer to agentic markets without substantial rethinking of what procedural standards mean when market participants are artificial representatives.

### AI Ethics and Governance Frameworks

The rapidly developing field of AI ethics has increasingly emphasized procedural dimensions of algorithmic fairness, accountability, and legitimacy. Binns (2018) argues persuasively that machine learning fairness should draw on political philosophy's procedural traditions rather than reducing fairness to statistical measures. Different political theories—libertarian, egalitarian, communitarian—imply different fairness criteria for algorithms, and these differences concern not just distributive outcomes but procedural features like participation, transparency, and contestability. This procedural turn in AI ethics represents convergence with procedural experimentalism, though the fields have developed largely independently.

Research on moral pathologies in AI systems identifies several procedurally-grounded wrongs. Barocas and Selbst (2016) demonstrate that algorithmic discrimination arises not merely from biased outcomes but from procedural features: what data is collected, how categories are constructed, which proxies are permitted. Susser, Roessler, and Nissenbaum (2019) analyze online manipulation as a distinctive procedural wrong—AI systems influencing decisions in hidden ways that bypass rational deliberation, even when outcomes might seem beneficial. Fricker's (2007) framework of epistemic injustice applies to automated systems that systematically discount certain voices or misunderstand certain perspectives, creating structural procedural wrongs beyond individual harm. These pathologies highlight that how AI systems operate matters morally, not just what they achieve.

Frameworks for legitimate AI governance emphasize procedural requirements. Rahwan (2018) proposes "society-in-the-loop" approaches where societal values are continuously integrated into AI systems through participatory processes—an explicitly experimental and procedural approach to AI governance that aligns closely with procedural experimentalism's iterative learning. Wachter, Mittelstadt, and Floridi (2017) distinguish transparency (information availability), explainability (understandable reasons for decisions), and accountability (clear responsibility attribution) as separate procedural requirements, each serving different governance functions. Gabriel (2020) analyzes value alignment as requiring both technical alignment (AI does what we intend) and normative alignment (AI does what it should)—a distinction that parallels procedural experimentalism's concern with learning what normative principles should guide institutions.

Work on representational ethics addresses when AI agents can legitimately act for humans. Santoni de Sio and Van den Hoven (2018) develop accounts of "meaningful human control" requiring not direct control but appropriate governance structures—tracking connections between human intentions and system outcomes plus capacity for moral judgment. Svensson (2022) analyzes whether AI systems can fulfill fiduciary duties, concluding that while AI cannot bear moral responsibility, procedural governance structures can enable functional fiduciary relationships. These frameworks recognize that AI representatives occupy a distinctive space: not mere tools (which have no agency) but also not full moral agents (which bear responsibility).

Yet most AI ethics work addresses systems making decisions *about* humans—classification, prediction, recommendation—rather than AI agents acting *for* humans in economic contexts. When AI classifies loan applicants or predicts recidivism, fairness concerns center on people affected by classifications. When AI agents negotiate supply contracts or allocate energy resources on behalf of human principals, fairness takes a different form: it concerns both whether procedures protect principals' interests and whether procedures governing agent interactions are themselves fair. The representational relationship—agent acting for principal—remains under-theorized in economic contexts. Additionally, most work examines single AI systems or human-AI interaction, not multi-agent systems where AI agents interact with each other strategically. The distinctive procedural challenges of multi-agent economic systems representing multiple human principals have received limited attention.

These three frameworks—procedural experimentalism, market design, and AI ethics—converge on recognizing that procedural features matter normatively. They share insight that how decisions are made, not just what decisions result, carries moral weight. Yet they address distinct contexts: human institutions, human markets, and AI systems making decisions about humans. Agentic markets fall at their intersection, inheriting unresolved questions from each: Can procedural experiments work with non-human agents? Which fairness criteria apply when agents represent humans? How do multi-agent dynamics create distinctive moral pathologies? The next section identifies four specific gaps emerging from this convergence.

---

## Research Gaps at the Intersection

The convergence of procedural experimentalism, market design, and AI ethics reveals four systematic gaps in our capacity to evaluate agentic markets normatively. These gaps arise not from deficiencies within individual frameworks but from their intersection—they become visible precisely when we attempt to apply multiple frameworks simultaneously to AI-mediated economic systems.

### Extending Procedural Experimentalism to Non-Human Agents

Procedural experimentalism was developed for human institutions where participants deliberate about values, experience institutional operations, and revise their normative commitments through reflective learning. No existing work systematically extends this framework to contexts where AI agents execute procedures on behalf of humans. Himmelreich (2023) applies procedural experimentalism to AI ethics tools—guidelines and assessment frameworks for ethical AI development—but these remain tools used by humans rather than AI agents participating in institutional procedures. Anderson's (2006) epistemic experimentalism and Sabel and Zeitlin's (2012) experimentalist governance focus exclusively on human institutional learning. List and Pettit's (2011) sophisticated account of group agency analyzes when collectivities can exhibit agency and rationality, but they do not address whether artificial multi-agent systems can engage in the kind of moral learning central to procedural experimentalism.

The theoretical gap concerns the conditions under which procedural experiments can generate normative authority when the experimenters are artificial agents. Procedural experimentalism's justificatory force comes from moral learning—participants discover through experience which principles prove action-guiding and reflectively stable. But can AI agents engage in moral learning meaningfully? Machine learning provides functional adaptation, but procedural experimentalism envisions deliberative reflection on normative commitments. Can hybrid systems—where human principals and AI agents jointly participate in market procedures—engage in procedural experimentation? What feedback mechanisms would enable normative discovery rather than mere behavioral optimization? These questions are not merely technical but deeply philosophical: they concern what kinds of entities can participate in justificatory procedures and under what conditions procedural iteration generates normative authority. The research project addresses this gap by using controlled simulations of agentic markets to test whether varying procedural features reveals stable fairness principles, investigating whether agent-based systems can exhibit functional moral learning through institutional iteration.

### Procedural Standards for AI Representatives in Markets

Market design literature has developed sophisticated fairness criteria for evaluating allocation and matching mechanisms, but these criteria assume human participants with human preferences, information processing capacities, and strategic capabilities. When AI agents represent human interests in markets, two complications arise. First, standard fairness criteria like envy-freeness, proportionality, and equal access were designed to evaluate outcomes for direct participants—but in agentic markets, the direct participants are artificial agents while humans are principals being represented. Does an allocation satisfy envy-freeness if no agent envies another agent's allocation, even if human principals experience the outcome as unfair? Second, procedural requirements developed for humans may not translate straightforwardly to AI contexts. Information disclosure that enables fair human decision-making may be inadequate when agents process information at machine speed and identify patterns beyond human detection. Conversely, transparency requirements feasible for human decision-makers may be impossible for complex AI systems.

AI ethics literature addresses fairness, accountability, and legitimacy but focuses predominantly on systems making decisions about humans rather than AI agents acting for humans. Binns (2018) and Barocas and Selbst (2016) analyze algorithmic fairness in classification and prediction contexts. Frameworks for legitimate AI—like meaningful human control (Santoni de Sio and Van den Hoven 2018) or fiduciary duties (Svensson 2022)—are discussed abstractly without application to specific economic contexts like markets. The representational relationship between AI agents and human principals in economic contexts remains under-theorized. When an autonomous procurement agent negotiates supply contracts on behalf of a firm, what procedural safeguards ensure the representation is legitimate? When energy management agents coordinate grid resources across multiple human stakeholders, what accountability structures are appropriate?

This gap has practical urgency. Agentic markets are proliferating—algorithmic trading dominates financial markets, autonomous systems increasingly manage supply chains and energy grids, and large language model-based agents show capabilities for sophisticated economic negotiation. Yet we lack integrated normative frameworks combining market design's procedural sophistication with AI ethics' attention to representative legitimacy. The research project fills this gap by experimentally evaluating procedural features specific to agentic markets: information filtering by agents, automated negotiation protocols, algorithmic oversight mechanisms. It tests whether standard fairness criteria transfer to agent-mediated contexts or require reconceptualization when representatives are artificial.

### Moral Pathologies in Multi-Agent Market Systems

AI ethics has identified important moral pathologies in algorithmic systems: Barocas and Selbst's (2016) analysis of discriminatory bias, Susser et al.'s (2019) account of manipulative influence, and Fricker's (2007) framework of epistemic injustice. However, this work predominantly addresses single AI systems or human-AI interaction rather than multi-agent systems where AI agents interact strategically with each other. Agentic markets involve precisely such multi-agent interaction: agents negotiate with other agents, strategic behaviors cascade through agent populations, and emergent dynamics arise from agent-agent interaction that are not present in single-agent or human-AI contexts.

Multi-agent market systems may exhibit distinctive pathologies beyond those identified in single-agent contexts. Strategic interaction among sophisticated AI agents could amplify bias—if one agent develops discriminatory patterns, others might learn to exploit or reinforce these patterns through strategic interaction. Manipulation might work differently when agents manipulate other agents rather than humans: algorithmic persuasion designed for human cognitive biases may not transfer, but new forms of strategic manipulation among agents could emerge. Epistemic injustices could arise at multiple levels: agents might systematically discount certain information sources, and the multi-agent market might systematically disadvantage certain classes of human principals.

Existing multi-agent systems literature (Wellman 2007; Rahwan et al. 2019) addresses coordination, efficiency, and strategic behavior but focuses less on normative pathologies. Wellman (2015) shows that strategic sophistication fundamentally changes market dynamics—but the normative implications of these changed dynamics remain under-explored. Agent-based modeling literature (Epstein and Axtell 1996; Axelrod 1997) studies how macro-level patterns emerge from micro-level agent interactions, including the emergence of cooperative norms, but does not specifically investigate how multi-agent interactions might generate or amplify moral pathologies in market contexts.

The research project addresses this gap by simulating multi-agent agentic markets to identify emergent pathologies and testing whether procedural interventions—transparency requirements, algorithmic auditing, circuit breakers, information access constraints—can mitigate these pathologies. It connects single-agent pathologies identified in AI ethics to multi-agent amplification mechanisms in markets, providing empirical investigation of how agent-agent interaction creates distinctive normative challenges.

### Simulation as Normative Experimental Methodology

Procedural experimentalism emphasizes learning through institutional experimentation, and Roth's (2002) market design framework stresses the need for iterative testing of market mechanisms. However, both frameworks focus primarily on real-world experiments: actual institutional trials with human participants and genuine stakes. Agent-based modeling and computational simulation have proven valuable for prediction and policy analysis (Epstein and Axtell 1996; Macy and Willer 2002), but their role in normative theory remains under-theorized. Can simulations of agentic markets validate normative principles, or merely generate empirical predictions?

The methodological gap concerns validity criteria for normative simulation experiments. Humphreys (2004) establishes computational methods as epistemically distinctive—neither purely empirical nor purely theoretical—capable of extending human cognitive capacities to investigate phenomena beyond traditional methods' reach. But what standards determine when computational experiments support normative conclusions? Empirical simulations can be calibrated to data and validated against observations, but normative simulations lack obvious calibration targets. Axelrod's (1997) demonstrations that cooperative norms emerge in simulated agent interactions are suggestive but don't claim to validate norms normatively—they show what strategies succeed, not what strategies should be adopted.

Real-world market experiments face significant limitations that simulation might overcome. High stakes create irreversibility—once implemented, market reforms affect real people and resist reversal even if flawed. Generalizability issues arise: a mechanism that works in Boston school choice may fail in New York due to contextual differences. Ethical constraints limit experimentation: we cannot test potentially harmful market designs on real participants. Simulation enables controlled variation of procedural features impossible in real markets, systematic testing across parameter ranges, and investigation of failure modes without real harm. But does controlled simulation merely complement real-world experiments, or can it generate normative insights independently?

The research project develops validity criteria for normative simulation experiments in agentic markets, drawing on Wimsatt's (2007) robustness analysis: convergence of results across independent model variations increases confidence in conclusions. If varying agent architectures, market structures, and parameter values consistently reveals that certain procedural features support fairness while others enable exploitation, this robustness provides evidence for normative principles. The project positions simulation not as replacement for theory or real-world testing but as distinctive methodology contributing to procedural experimentalism's iterative learning. By establishing when and how computational experiments can inform normative evaluation of AI-mediated markets, it bridges computational social science and normative political philosophy.

These four gaps collectively motivate the research project's integration of procedural experimentalism with computational methodology. Existing frameworks address pieces—procedural justification, market design, AI ethics, simulation methods—but their intersection remains unexplored. The project synthesizes these frameworks to create a new approach: normative evaluation of agentic markets through procedural simulation experiments that investigate whether and how AI-mediated market procedures can be procedurally justified.

---

## Conclusion

This review has surveyed the landscape of research on procedural justification, market design, and AI governance to assess our capacity to evaluate agentic markets—economic systems where AI agents transact and allocate resources on behalf of human principals. Three key findings emerge from this analysis.

First, existing theoretical frameworks converge on the normative importance of procedural features. Procedural experimentalism demonstrates that certain decision-making procedures involving experimentation can justify normative principles through social learning and iterative refinement (Himmelreich 2023; Anderson 2006). Market design theory reveals how procedural details—information structures, auction rules, matching mechanisms—fundamentally shape fairness and efficiency outcomes, with seemingly minor variations producing dramatically different normative consequences (Roth 2002; Pathak and Sönmez 2013). AI ethics has increasingly emphasized procedural dimensions of algorithmic fairness, shifting from purely outcome-based metrics toward attention to transparency, accountability, and contestability as procedural requirements (Binns 2018; Rahwan 2018). This convergence suggests that evaluating agentic markets requires procedural analysis—but the frameworks developed within each domain do not straightforwardly extend to the AI-mediated market context.

Second, these frameworks fragment by context in ways that leave agentic markets in an evaluative gap. Procedural experimentalism addresses human institutions where participants deliberate and revise values. Market design assumes human market participants with human information processing and strategic capacities. AI ethics predominantly addresses systems making decisions about humans rather than AI agents acting for humans as representatives in economic contexts. Agentic markets fall at the intersection: AI agents representing human interests within market procedures. This intersection generates distinctive challenges that no single framework adequately addresses: Can procedural experiments justify market rules when agents execute procedures? What fairness criteria apply when agents represent humans? How do multi-agent strategic interactions create moral pathologies beyond those in single-agent systems? What role can simulation play in procedural experimentation?

Third, four systematic research gaps emerge from this intersection. The theoretical gap of extending procedural experimentalism to non-human agents requires specifying what counts as moral learning when experimenters are artificial. The normative gap of developing procedural standards for AI market representatives demands integration of market design's fairness criteria with AI ethics' accountability frameworks, reconceptualized for representational contexts. The empirical gap of identifying and mitigating moral pathologies in multi-agent market systems necessitates understanding how strategic interaction among sophisticated AI agents amplifies bias, manipulation, or epistemic injustice. The methodological gap of establishing validity criteria for normative simulation experiments requires determining when computational experiments contribute to procedural justification beyond mere empirical prediction.

The proposed research project fills these gaps by synthesizing procedural experimentalism, market design, AI governance, and computational methods into an integrated approach. It extends procedural experimentalism to agentic markets by operationalizing procedural experiments through controlled simulation: using Magentic Marketplace to systematically vary governance structures, information access, and oversight mechanisms, then evaluating whether iterations reveal stable fairness principles and effective safeguards against pathologies. It develops procedural standards for AI representatives by testing which market design features—transparency requirements, algorithmic auditing, contestability procedures—support legitimate representation when agents negotiate on behalf of humans. It identifies multi-agent moral pathologies through systematic simulation of agent interactions under varying market conditions. It establishes methodological validity through robustness analysis, demonstrating that convergent results across parameter variations provide evidence for normative conclusions.

This represents the first systematic attempt to apply procedural experimentalism to agentic markets, bridging political philosophy, economics, AI ethics, and computational social science. The contributions are both theoretical—extending procedural experimentalism's justificatory framework to AI-mediated contexts—and practical—providing guidance for designing legitimate automated market systems. As AI agents increasingly mediate economic decisions, normative frameworks must evolve beyond human-only institutional analysis or AI systems that classify humans. The research develops new methodology for this evolution: normative evaluation through procedural simulation experiments that investigate when and how AI-mediated markets can be procedurally justified.

The stakes are substantial. Agentic markets are not future speculation but present reality: algorithmic trading dominates financial markets, autonomous procurement systems manage global supply chains, and AI coordination increasingly governs energy grids and resource allocation. Without adequate normative frameworks, we risk either over-regulating—imposing human-designed procedural requirements that prevent beneficial automation—or under-regulating—allowing AI-mediated markets to operate without procedural safeguards against exploitation, bias, or structural injustice. Procedural experimentalism offers a middle path: iterative refinement of governance structures through controlled experimentation that reveals which procedural features support fairness, accountability, and legitimate representation. By extending this framework to computational agents and establishing methodological standards for normative simulation, the research advances both our theoretical understanding of procedural justification and our practical capacity to govern AI-mediated economic systems legitimately. As markets increasingly operate through artificial intelligence, ensuring these systems serve human interests requires not just technical sophistication but normative frameworks adequate to the distinctive challenges of AI representation in economic contexts. This research develops such a framework.

---

## Bibliography

Anderson, Elizabeth. 2006. "The Epistemology of Democracy." *Episteme* 3 (1-2): 8–22. https://doi.org/10.3366/epi.2006.3.1-2.8.

Axelrod, Robert. 1997. *The Complexity of Cooperation: Agent-Based Models of Competition and Collaboration*. Princeton: Princeton University Press.

Barocas, Solon, and Andrew D. Selbst. 2016. "Big Data's Disparate Impact." *California Law Review* 104: 671–732. https://doi.org/10.15779/Z38BG31.

Bergemann, Dirk, and Stephen Morris. 2019. "Information Design: A Unified Perspective." *Journal of Economic Literature* 57 (1): 44–95. https://doi.org/10.1257/jel.20181489.

Binns, Reuben. 2018. "Fairness in Machine Learning: Lessons from Political Philosophy." *Proceedings of Machine Learning Research* 81: 149–159.

Budish, Eric, Yeon-Koo Che, Fuhito Kojima, and Paul Milgrom. 2013. "Designing Random Allocation Mechanisms: Theory and Applications." *American Economic Review* 103 (2): 585–623. https://doi.org/10.1257/aer.103.2.585.

Dewey, John. 1927. "The Public and Its Problems." In *The Later Works of John Dewey, 1925-1953*, vol. 2, 235–372.

Epstein, Joshua M., and Robert Axtell. 1996. *Growing Artificial Societies: Social Science from the Bottom Up*. Cambridge, MA: MIT Press.

Estlund, David. 2009. *Democratic Authority: A Philosophical Framework*. Princeton: Princeton University Press.

Fricker, Miranda. 2007. *Epistemic Injustice: Power and the Ethics of Knowing*. Oxford: Oxford University Press. https://doi.org/10.1093/acprof:oso/9780198237907.001.0001.

Gabriel, Iason. 2020. "Artificial Intelligence, Values, and Alignment." *Minds and Machines* 30 (3): 411–437. https://doi.org/10.1007/s11023-020-09539-2.

Himmelreich, Johannes. 2023. "Procedural Experimentalism: A Framework for Evaluating AI Ethics Tools." *Ethics and Information Technology* 25 (2): 15. https://doi.org/10.1007/s10676-023-09689-w.

Humphreys, Paul. 2004. *Extending Ourselves: Computational Science, Empiricism, and Scientific Method*. Oxford: Oxford University Press. https://doi.org/10.1093/0195158709.001.0001.

List, Christian, and Philip Pettit. 2011. *Group Agency: The Possibility, Design, and Status of Corporate Agents*. Oxford: Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199591565.001.0001.

Macy, Michael W., and Robert Willer. 2002. "From Factors to Actors: Computational Sociology and Agent-Based Modeling." *Annual Review of Sociology* 28: 143–166. https://doi.org/10.1146/annurev.soc.28.110601.141117.

Moulin, Hervé. 2004. *Fair Division and Collective Welfare*. Cambridge, MA: MIT Press.

Park, Joon Sung, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. "Generative Agents: Interactive Simulacra of Human Behavior." In *Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology*, 1–22. https://doi.org/10.1145/3586183.3606763.

Pathak, Parag A., and Tayfun Sönmez. 2013. "School Admissions Reform in Chicago and England: Comparing Mechanisms by Their Vulnerability to Manipulation." *American Economic Review* 103 (1): 80–106. https://doi.org/10.1257/aer.103.1.80.

Rahwan, Iyad. 2018. "Society-in-the-Loop: Programming the Algorithmic Social Contract." *Ethics and Information Technology* 20 (1): 5–14. https://doi.org/10.1007/s10676-017-9430-8.

Rahwan, Iyad, et al. 2019. "Machine Behaviour." *Nature* 568 (7753): 477–486. https://doi.org/10.1038/s41586-019-1138-y.

Richardson, Henry S. 2002. *Democratic Autonomy: Public Reasoning about the Ends of Policy*. Oxford: Oxford University Press.

Roth, Alvin E. 2002. "The Economist as Engineer: Game Theory, Experimentation, and Computation as Tools for Design Economics." *Econometrica* 70 (4): 1341–1378. https://doi.org/10.1111/1468-0262.00335.

Sabel, Charles F., and Jonathan Zeitlin. 2012. "Experimentalist Governance." In *The Oxford Handbook of Governance*, 169–183. https://doi.org/10.1093/oxfordhb/9780199560530.013.0012.

Santoni de Sio, Filippo, and Jeroen Van den Hoven. 2018. "Meaningful Human Control over Autonomous Systems: A Philosophical Account." *Frontiers in Robotics and AI* 5. https://doi.org/10.3389/frobt.2018.00015.

Susser, Daniel, Beate Roessler, and Helen Nissenbaum. 2019. "Online Manipulation: Hidden Influences in a Digital Age." *Georgetown Law Technology Review* 4: 1–45.

Svensson, Frans. 2022. "Can Algorithms Be Fiduciaries?" *Philosophy & Technology* 35 (2). https://doi.org/10.1007/s13347-022-00532-z.

Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2017. "Transparent, Explainable, and Accountable AI for Robotics." *Science Robotics* 2 (6). https://doi.org/10.1126/scirobotics.aan6080.

Wellman, Michael P. 2015. "Putting the Agent in Agent-Based Modeling." *Autonomous Agents and Multi-Agent Systems* 30 (6): 1175–1189. https://doi.org/10.1007/s10458-015-9317-8.

Wellman, Michael P., Amy Greenwald, and Peter Stone. 2007. *Autonomous Bidding Agents: Strategies and Lessons from the Trading Agent Competition*. Cambridge, MA: MIT Press.

Wimsatt, William C. 2007. *Re-Engineering Philosophy for Limited Beings: Piecewise Approximations to Reality*. Cambridge, MA: Harvard University Press.

---

**Word Count**: Approximately 3,450 words
**Citations**: 30 papers (selective from 87 available)
**Completed**: 2025-11-12
