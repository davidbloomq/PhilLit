## Research Gaps at the Intersection

The convergence of procedural experimentalism, market design, and AI ethics reveals four systematic gaps in our capacity to evaluate agentic markets normatively. These gaps arise not from deficiencies within individual frameworks but from their intersection—they become visible precisely when we attempt to apply multiple frameworks simultaneously to AI-mediated economic systems.

### Extending Procedural Experimentalism to Non-Human Agents

Procedural experimentalism was developed for human institutions where participants deliberate about values, experience institutional operations, and revise their normative commitments through reflective learning. No existing work systematically extends this framework to contexts where AI agents execute procedures on behalf of humans. Himmelreich (2023) applies procedural experimentalism to AI ethics tools—guidelines and assessment frameworks for ethical AI development—but these remain tools used by humans rather than AI agents participating in institutional procedures. Anderson's (2006) epistemic experimentalism and Sabel and Zeitlin's (2012) experimentalist governance focus exclusively on human institutional learning. List and Pettit's (2011) sophisticated account of group agency analyzes when collectivities can exhibit agency and rationality, but they do not address whether artificial multi-agent systems can engage in the kind of moral learning central to procedural experimentalism.

The theoretical gap concerns the conditions under which procedural experiments can generate normative authority when the experimenters are artificial agents. Procedural experimentalism's justificatory force comes from moral learning—participants discover through experience which principles prove action-guiding and reflectively stable. But can AI agents engage in moral learning meaningfully? Machine learning provides functional adaptation, but procedural experimentalism envisions deliberative reflection on normative commitments. Can hybrid systems—where human principals and AI agents jointly participate in market procedures—engage in procedural experimentation? What feedback mechanisms would enable normative discovery rather than mere behavioral optimization? These questions are not merely technical but deeply philosophical: they concern what kinds of entities can participate in justificatory procedures and under what conditions procedural iteration generates normative authority. The research project addresses this gap by using controlled simulations of agentic markets to test whether varying procedural features reveals stable fairness principles, investigating whether agent-based systems can exhibit functional moral learning through institutional iteration.

### Procedural Standards for AI Representatives in Markets

Market design literature has developed sophisticated fairness criteria for evaluating allocation and matching mechanisms, but these criteria assume human participants with human preferences, information processing capacities, and strategic capabilities. When AI agents represent human interests in markets, two complications arise. First, standard fairness criteria like envy-freeness, proportionality, and equal access were designed to evaluate outcomes for direct participants—but in agentic markets, the direct participants are artificial agents while humans are principals being represented. Does an allocation satisfy envy-freeness if no agent envies another agent's allocation, even if human principals experience the outcome as unfair? Second, procedural requirements developed for humans may not translate straightforwardly to AI contexts. Information disclosure that enables fair human decision-making may be inadequate when agents process information at machine speed and identify patterns beyond human detection. Conversely, transparency requirements feasible for human decision-makers may be impossible for complex AI systems.

AI ethics literature addresses fairness, accountability, and legitimacy but focuses predominantly on systems making decisions about humans rather than AI agents acting for humans. Binns (2018) and Barocas and Selbst (2016) analyze algorithmic fairness in classification and prediction contexts. Frameworks for legitimate AI—like meaningful human control (Santoni de Sio and Van den Hoven 2018) or fiduciary duties (Svensson 2022)—are discussed abstractly without application to specific economic contexts like markets. The representational relationship between AI agents and human principals in economic contexts remains under-theorized. When an autonomous procurement agent negotiates supply contracts on behalf of a firm, what procedural safeguards ensure the representation is legitimate? When energy management agents coordinate grid resources across multiple human stakeholders, what accountability structures are appropriate?

This gap has practical urgency. Agentic markets are proliferating—algorithmic trading dominates financial markets, autonomous systems increasingly manage supply chains and energy grids, and large language model-based agents show capabilities for sophisticated economic negotiation. Yet we lack integrated normative frameworks combining market design's procedural sophistication with AI ethics' attention to representative legitimacy. The research project fills this gap by experimentally evaluating procedural features specific to agentic markets: information filtering by agents, automated negotiation protocols, algorithmic oversight mechanisms. It tests whether standard fairness criteria transfer to agent-mediated contexts or require reconceptualization when representatives are artificial.

### Moral Pathologies in Multi-Agent Market Systems

AI ethics has identified important moral pathologies in algorithmic systems: Barocas and Selbst's (2016) analysis of discriminatory bias, Susser et al.'s (2019) account of manipulative influence, and Fricker's (2007) framework of epistemic injustice. However, this work predominantly addresses single AI systems or human-AI interaction rather than multi-agent systems where AI agents interact strategically with each other. Agentic markets involve precisely such multi-agent interaction: agents negotiate with other agents, strategic behaviors cascade through agent populations, and emergent dynamics arise from agent-agent interaction that are not present in single-agent or human-AI contexts.

Multi-agent market systems may exhibit distinctive pathologies beyond those identified in single-agent contexts. Strategic interaction among sophisticated AI agents could amplify bias—if one agent develops discriminatory patterns, others might learn to exploit or reinforce these patterns through strategic interaction. Manipulation might work differently when agents manipulate other agents rather than humans: algorithmic persuasion designed for human cognitive biases may not transfer, but new forms of strategic manipulation among agents could emerge. Epistemic injustices could arise at multiple levels: agents might systematically discount certain information sources, and the multi-agent market might systematically disadvantage certain classes of human principals.

Existing multi-agent systems literature (Wellman 2007; Rahwan et al. 2019) addresses coordination, efficiency, and strategic behavior but focuses less on normative pathologies. Wellman (2015) shows that strategic sophistication fundamentally changes market dynamics—but the normative implications of these changed dynamics remain under-explored. Agent-based modeling literature (Epstein and Axtell 1996; Axelrod 1997) studies how macro-level patterns emerge from micro-level agent interactions, including the emergence of cooperative norms, but does not specifically investigate how multi-agent interactions might generate or amplify moral pathologies in market contexts.

The research project addresses this gap by simulating multi-agent agentic markets to identify emergent pathologies and testing whether procedural interventions—transparency requirements, algorithmic auditing, circuit breakers, information access constraints—can mitigate these pathologies. It connects single-agent pathologies identified in AI ethics to multi-agent amplification mechanisms in markets, providing empirical investigation of how agent-agent interaction creates distinctive normative challenges.

### Simulation as Normative Experimental Methodology

Procedural experimentalism emphasizes learning through institutional experimentation, and Roth's (2002) market design framework stresses the need for iterative testing of market mechanisms. However, both frameworks focus primarily on real-world experiments: actual institutional trials with human participants and genuine stakes. Agent-based modeling and computational simulation have proven valuable for prediction and policy analysis (Epstein and Axtell 1996; Macy and Willer 2002), but their role in normative theory remains under-theorized. Can simulations of agentic markets validate normative principles, or merely generate empirical predictions?

The methodological gap concerns validity criteria for normative simulation experiments. Humphreys (2004) establishes computational methods as epistemically distinctive—neither purely empirical nor purely theoretical—capable of extending human cognitive capacities to investigate phenomena beyond traditional methods' reach. But what standards determine when computational experiments support normative conclusions? Empirical simulations can be calibrated to data and validated against observations, but normative simulations lack obvious calibration targets. Axelrod's (1997) demonstrations that cooperative norms emerge in simulated agent interactions are suggestive but don't claim to validate norms normatively—they show what strategies succeed, not what strategies should be adopted.

Real-world market experiments face significant limitations that simulation might overcome. High stakes create irreversibility—once implemented, market reforms affect real people and resist reversal even if flawed. Generalizability issues arise: a mechanism that works in Boston school choice may fail in New York due to contextual differences. Ethical constraints limit experimentation: we cannot test potentially harmful market designs on real participants. Simulation enables controlled variation of procedural features impossible in real markets, systematic testing across parameter ranges, and investigation of failure modes without real harm. But does controlled simulation merely complement real-world experiments, or can it generate normative insights independently?

The research project develops validity criteria for normative simulation experiments in agentic markets, drawing on Wimsatt's (2007) robustness analysis: convergence of results across independent model variations increases confidence in conclusions. If varying agent architectures, market structures, and parameter values consistently reveals that certain procedural features support fairness while others enable exploitation, this robustness provides evidence for normative principles. The project positions simulation not as replacement for theory or real-world testing but as distinctive methodology contributing to procedural experimentalism's iterative learning. By establishing when and how computational experiments can inform normative evaluation of AI-mediated markets, it bridges computational social science and normative political philosophy.

These four gaps collectively motivate the research project's integration of procedural experimentalism with computational methodology. Existing frameworks address pieces—procedural justification, market design, AI ethics, simulation methods—but their intersection remains unexplored. The project synthesizes these frameworks to create a new approach: normative evaluation of agentic markets through procedural simulation experiments that investigate whether and how AI-mediated market procedures can be procedurally justified.
