## Theoretical Frameworks for Evaluating Agentic Markets

Three bodies of scholarship provide theoretical resources for evaluating agentic markets, each offering distinctive normative frameworks while leaving critical questions unanswered when applied to AI-mediated economic systems.

### Procedural Experimentalism and Social Learning

Procedural experimentalism, most fully developed in recent work by Himmelreich (2023) and drawing on pragmatist traditions (Dewey 1927; Anderson 2006), holds that certain decision-making procedures involving experimentation can justify normative principles not through a priori reasoning or pure consent, but through a process of social learning and iterative refinement. The framework distinguishes itself from Rawlsian procedural justice, which treats procedures as fair independent of outcomes, and from purely instrumental approaches that judge procedures solely by outcomes achieved. Instead, procedural experimentalism treats experimental procedures as discovery mechanisms that reveal which normative principles prove action-guiding, stable under reflective scrutiny, and capable of coordinating social cooperation (Anderson 2006).

The justificatory structure works through moral learning: institutions embody provisional principles, participants experience their operation, feedback mechanisms enable reflection on successes and failures, and principles are revised based on this learning. Himmelreich (2023) explicitly applies this framework to AI ethics, arguing that tools for ethical AI development should be evaluated procedurally—by whether they enable productive experimentation and learning—rather than by guarantees of correct outcomes. This represents a significant extension beyond human-only contexts. Yet Himmelreich focuses on AI development tools rather than AI agents operating within institutions, leaving open the question of whether AI agents themselves can be participants in procedural experimentation.

The framework makes several key assumptions about participants: they possess reflective capacities to evaluate how principles operate in practice, they can revise their commitments based on experience, and they engage in deliberation about which principles to maintain or modify. Richardson (2002) emphasizes that such learning works through "specification"—the progressive articulation of vague normative commitments into concrete guidance for particular contexts. Estlund (2009) grounds this in epistemic proceduralism: democratic procedures have epistemic value because they track truth about justice while remaining acceptable to all qualified perspectives. The epistemic dimension is crucial—procedures don't merely aggregate preferences but generate knowledge about normative requirements.

When we consider agentic markets, these assumptions become problematic. Do AI agents engage in moral learning? If "learning" means functional adaptation based on feedback (as in machine learning), then agents learn in one sense—but this differs from the reflective deliberation and value revision that procedural experimentalism envisions. List and Pettit (2011) argue that collectivities can exhibit group agency when they have representational states, motivational states, and capacity to process reasons, but whether artificial multi-agent systems meet these criteria remains contested. Even if agent systems could functionally learn, questions multiply: What constitutes genuine experimentation when agents execute procedures? What feedback mechanisms enable normative discovery rather than mere optimization? How much human oversight is required for procedural legitimacy? Can hybrid systems—where humans and agents jointly participate in market procedures—engage in procedural experimentation in any meaningful sense? These questions reveal that procedural experimentalism, while powerful for human institutions, requires substantial theoretical development to extend to agentic contexts.

### Market Design and Procedural Features

Market design theory, synthesized in Roth's (2002) framework of the "economist as engineer," provides sophisticated tools for analyzing how procedural features of markets affect normative outcomes. Unlike pure economic theory that assumes fixed institutional structures, market design explicitly treats procedural features as design variables subject to systematic evaluation and iterative refinement. Roth emphasizes that successful market design requires combining theory, experimentation, and computation—theoretical analysis alone cannot predict all relevant features of institutional performance.

The field has generated rich vocabulary for procedural analysis. Bergemann and Morris (2019) show how information design—decisions about what information to disclose, when, and to whom—fundamentally shapes market outcomes. Fair division literature establishes multiple procedural fairness criteria: envy-freeness (no participant prefers another's allocation), proportionality (each receives subjectively fair share), and equal access (procedural rules apply uniformly). These criteria often conflict, requiring trade-offs that themselves become subjects of normative evaluation. Moulin (2004) demonstrates that different allocation procedures embody different fairness principles, and that choosing among them requires substantive moral judgment. Budish et al. (2013) analyze how randomization can serve fairness by giving participants equal ex-ante chances, even when ex-post outcomes differ. The key insight is that procedural features—not just outcomes—carry normative weight.

Empirical work reinforces this procedural focus. Pathak and Sönmez (2013) show that school choice mechanisms differ dramatically in vulnerability to strategic manipulation, with procedural details determining whether sophisticated actors gain unfair advantages over less-informed participants. This demonstrates that procedural evaluation requires understanding not just formal properties but how procedures operate given participants' actual strategic capabilities.

However, market design overwhelmingly assumes human participants whose preferences, information processing, and strategic reasoning capacities follow predictable patterns. When AI agents enter markets, these assumptions break down in multiple ways. First, standard fairness criteria like envy-freeness were designed for evaluating human welfare—but do they apply when the direct market participants are artificial agents representing human principals? An agent might not "envy" another agent's allocation, yet the human principal could experience unfairness. Second, information design becomes radically different when agents process information at machine speed and scale, potentially identifying patterns or correlations invisible to humans. Third, strategic sophistication transforms: contemporary AI agents exhibit strategic reasoning that can exploit procedural features in ways beyond human capability (Wellman 2015). Fourth, procedural transparency faces new questions—should information be transparent to agents, to the humans they represent, to regulators, or to all simultaneously? These complications suggest that market design principles cannot simply transfer to agentic markets without substantial rethinking of what procedural standards mean when market participants are artificial representatives.

### AI Ethics and Governance Frameworks

The rapidly developing field of AI ethics has increasingly emphasized procedural dimensions of algorithmic fairness, accountability, and legitimacy. Binns (2018) argues persuasively that machine learning fairness should draw on political philosophy's procedural traditions rather than reducing fairness to statistical measures. Different political theories—libertarian, egalitarian, communitarian—imply different fairness criteria for algorithms, and these differences concern not just distributive outcomes but procedural features like participation, transparency, and contestability. This procedural turn in AI ethics represents convergence with procedural experimentalism, though the fields have developed largely independently.

Research on moral pathologies in AI systems identifies several procedurally-grounded wrongs. Barocas and Selbst (2016) demonstrate that algorithmic discrimination arises not merely from biased outcomes but from procedural features: what data is collected, how categories are constructed, which proxies are permitted. Susser, Roessler, and Nissenbaum (2019) analyze online manipulation as a distinctive procedural wrong—AI systems influencing decisions in hidden ways that bypass rational deliberation, even when outcomes might seem beneficial. Fricker's (2007) framework of epistemic injustice applies to automated systems that systematically discount certain voices or misunderstand certain perspectives, creating structural procedural wrongs beyond individual harm. These pathologies highlight that how AI systems operate matters morally, not just what they achieve.

Frameworks for legitimate AI governance emphasize procedural requirements. Rahwan (2018) proposes "society-in-the-loop" approaches where societal values are continuously integrated into AI systems through participatory processes—an explicitly experimental and procedural approach to AI governance that aligns closely with procedural experimentalism's iterative learning. Wachter, Mittelstadt, and Floridi (2017) distinguish transparency (information availability), explainability (understandable reasons for decisions), and accountability (clear responsibility attribution) as separate procedural requirements, each serving different governance functions. Gabriel (2020) analyzes value alignment as requiring both technical alignment (AI does what we intend) and normative alignment (AI does what it should)—a distinction that parallels procedural experimentalism's concern with learning what normative principles should guide institutions.

Work on representational ethics addresses when AI agents can legitimately act for humans. Santoni de Sio and Van den Hoven (2018) develop accounts of "meaningful human control" requiring not direct control but appropriate governance structures—tracking connections between human intentions and system outcomes plus capacity for moral judgment. Svensson (2022) analyzes whether AI systems can fulfill fiduciary duties, concluding that while AI cannot bear moral responsibility, procedural governance structures can enable functional fiduciary relationships. These frameworks recognize that AI representatives occupy a distinctive space: not mere tools (which have no agency) but also not full moral agents (which bear responsibility).

Yet most AI ethics work addresses systems making decisions *about* humans—classification, prediction, recommendation—rather than AI agents acting *for* humans in economic contexts. When AI classifies loan applicants or predicts recidivism, fairness concerns center on people affected by classifications. When AI agents negotiate supply contracts or allocate energy resources on behalf of human principals, fairness takes a different form: it concerns both whether procedures protect principals' interests and whether procedures governing agent interactions are themselves fair. The representational relationship—agent acting for principal—remains under-theorized in economic contexts. Additionally, most work examines single AI systems or human-AI interaction, not multi-agent systems where AI agents interact with each other strategically. The distinctive procedural challenges of multi-agent economic systems representing multiple human principals have received limited attention.

These three frameworks—procedural experimentalism, market design, and AI ethics—converge on recognizing that procedural features matter normatively. They share insight that how decisions are made, not just what decisions result, carries moral weight. Yet they address distinct contexts: human institutions, human markets, and AI systems making decisions about humans. Agentic markets fall at their intersection, inheriting unresolved questions from each: Can procedural experiments work with non-human agents? Which fairness criteria apply when agents represent humans? How do multi-agent dynamics create distinctive moral pathologies? The next section identifies four specific gaps emerging from this convergence.
