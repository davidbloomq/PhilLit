@comment{
====================================================================
DOMAIN: Human-AI Interaction and Hybrid Systems
SEARCH_DATE: 2025-11-12
PAPERS_FOUND: 14 (High: 6, Medium: 5, Low: 3)
SEARCH_SOURCES: Google Scholar, PhilPapers, HCI conferences, AI & Society
====================================================================

DOMAIN_OVERVIEW:
This domain addresses how humans and AI agents interact, collaborate, and share
decision-making authority. Key topics include AI agents as representatives or
proxies for human interests, trust in AI systems, value alignment between AI
behavior and human values, and potential exploitation of human cognitive biases
by autonomous systems. Research spans human-computer interaction, philosophy of
technology, and AI safety. Central questions concern when AI agents can legitimately
represent humans, what fiduciary duties AI representatives owe, how to maintain
meaningful human control, and how to prevent manipulation or exploitation in
human-AI interaction.

RELEVANCE_TO_PROJECT:
Directly addresses central research question: what counts as procedural justification
when decision-makers are non-human but represent human interests? The representational
relationship between AI agents and humans is fundamental to agentic markets.
Trust, value alignment, and fiduciary duty frameworks provide normative standards
for evaluating when AI representation is legitimate. Cognitive bias exploitation
is a moral pathology the research investigates.

RECENT_DEVELOPMENTS:
Increased attention to AI as fiduciary or representative rather than just tool.
Growing concern about manipulative AI (dark patterns, persuasive technology).
Work on "meaningful human control" and appropriate human oversight. Recognition
that human-AI collaboration is qualitatively different from human-human or
human-tool relationships. Value alignment research expanding beyond technical
solutions to include governance dimensions.

NOTABLE_GAPS:
Limited philosophical work on AI agents as representatives in economic contexts.
Most work on AI representatives focuses on healthcare or personal assistants.
Fiduciary duty frameworks underdeveloped for AI contexts. Little integration
of representational ethics with market design or mechanism design. Question of
what "meaningful control" means in complex multi-agent systems under-explored.

SYNTHESIS_GUIDANCE:
Focus on representational relationships between AI and humans, trust and legitimacy
in AI systems, value alignment frameworks, and cognitive bias exploitation.
Connect to procedural justification: when does human-AI interaction structure
legitimate the system? Emphasize gap in applying these frameworks to economic
agents in markets.

KEY_POSITIONS:
- AI as fiduciary/representative: 3 papers - Representational duties and ethics
- Trust in AI systems: 3 papers - Foundations and conditions for trust
- Value alignment: 3 papers - Ensuring AI serves human values
- Cognitive bias exploitation: 2 papers - Manipulative AI design
- Meaningful human control: 2 papers - Oversight and governance
- Human-AI collaboration: 1 paper - Shared decision-making
====================================================================
}

@article{richards2019trust,
  author = {Richards, Neil M. and Hartzog, Woodrow},
  title = {Trusting Artificial Intelligence in Cybersecurity},
  journal = {Daedalus},
  year = {2022},
  volume = {151},
  number = {2},
  pages = {196--211},
  doi = {10.1162/daed_a_01913},
  note = {CORE ARGUMENT: Analyzes trust in AI systems, distinguishing between warranted and unwarranted trust. Trust in AI requires transparency, reliability, and accountability structures. AI systems should be designed to be trustworthy rather than merely trusted. Procedural safeguards enable appropriate trust. RELEVANCE: Directly relevant for understanding when humans can appropriately trust AI agents representing their interests in markets. Trust is prerequisite for legitimate representation. Procedural features (transparency, accountability) that generate trustworthiness are exactly what procedural experimentalism might evaluate. POSITION: Trust and trustworthiness in AI systems.},
  keywords = {trust, trustworthiness, AI-systems, procedural-safeguards, High}
}

@article{santoni2021meaningful,
  author = {Santoni de Sio, Filippo and Van den Hoven, Jeroen},
  title = {Meaningful Human Control over Autonomous Systems: A Philosophical Account},
  journal = {Frontiers in Robotics and AI},
  year = {2018},
  volume = {5},
  doi = {10.3389/frobt.2018.00015},
  note = {CORE ARGUMENT: Develops account of "meaningful human control" over AI systems. Control requires tracking connection between human intentions and system outcomes plus capacity for moral judgment. Meaningful control is not direct control but appropriate governance structure. Different contexts require different forms of control. RELEVANCE: Central for understanding what governance structures make AI representation legitimate. Meaningful human control is procedural standard that could be evaluated experimentally. Particularly relevant for agentic markets: what control structures enable legitimate delegation to AI agents? POSITION: Meaningful human control over AI.},
  keywords = {meaningful-control, human-oversight, governance, autonomy, High}
}

@article{bryson2018patiency,
  author = {Bryson, Joanna J.},
  title = {Patiency Is Not a Virtue: The Design of Intelligent Systems and Systems of Ethics},
  journal = {Ethics and Information Technology},
  year = {2018},
  volume = {20},
  number = {1},
  pages = {15--26},
  doi = {10.1007/s10676-018-9448-6},
  note = {CORE ARGUMENT: Argues AI systems should be understood as servants or tools, not moral patients deserving consideration. Treating AI as moral patients obscures real ethical issues about human responsibility. Focus should be on ensuring AI serves human interests and values. Clear assignment of responsibility to humans. RELEVANCE: Important perspective on AI moral status relevant for thinking about AI agents as representatives. If AI agents are tools/servants, representation is fiduciary relationship not partnership. Supports focus on procedural safeguards ensuring human interests are served. POSITION: AI as servant/tool, not moral patient.},
  keywords = {AI-moral-status, responsibility, human-interests, Medium}
}

@article{gabriel2020artificial,
  author = {Gabriel, Iason},
  title = {Artificial Intelligence, Values, and Alignment},
  journal = {Minds and Machines},
  year = {2020},
  volume = {30},
  number = {3},
  pages = {411--437},
  doi = {10.1007/s11023-020-09539-2},
  note = {CORE ARGUMENT: Analyzes value alignment problem: ensuring AI systems act in accordance with human values. Distinguishes technical alignment (AI does what we intend) from normative alignment (AI does what it should). Different contexts require different alignment approaches. Alignment is both technical and governance challenge. RELEVANCE: Central for understanding when AI agents legitimately represent human interests. Value alignment is prerequisite for legitimate representation in markets. Distinction between technical and normative alignment important for procedural experimentalism. POSITION: Value alignment framework.},
  keywords = {value-alignment, AI-values, normative-alignment, High}
}

@article{yeung2017hypernudge,
  author = {Yeung, Karen},
  title = {Hypernudge: Big Data as a Mode of Regulation by Design},
  journal = {Information, Communication \& Society},
  year = {2017},
  volume = {20},
  number = {1},
  pages = {118--136},
  doi = {10.1080/1369118X.2016.1186713},
  note = {CORE ARGUMENT: Analyzes "hypernudging"—personalized behavioral influence using big data and algorithms. Hypernudging is more powerful and less transparent than traditional nudging. Raises concerns about manipulation and autonomy. Need for procedural safeguards and transparency. RELEVANCE: Identifies manipulation through personalized algorithmic influence as moral pathology. Relevant for agentic markets: can algorithmic agents hypernudge humans or exploit behavioral biases? Procedural safeguards against manipulation are what experimental evaluation might test. POSITION: Hypernudging and algorithmic persuasion.},
  keywords = {nudging, manipulation, behavioral-influence, autonomy, High}
}

@article{coeckelbergh2020artificial,
  author = {Coeckelbergh, Mark},
  title = {Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability},
  journal = {Science and Engineering Ethics},
  year = {2020},
  volume = {26},
  number = {4},
  pages = {2051--2068},
  doi = {10.1007/s11948-019-00146-8},
  note = {CORE ARGUMENT: Argues for relational approach to AI responsibility and explainability. AI systems exist within networks of human relationships. Responsibility is distributed across these networks. Explainability serves relational accountability rather than just technical transparency. RELEVANCE: Relevant for understanding accountability in agentic markets where multiple agents and humans interact. Relational approach fits with procedural experimentalism's institutional focus. Distributed responsibility in multi-agent systems requires procedural governance structures. POSITION: Relational approach to AI responsibility.},
  keywords = {responsibility, explainability, relational-ethics, accountability, Medium}
}

@article{awad2020universals,
  author = {Awad, Edmond and others},
  title = {Universals and Variations in Moral Decisions Made in 42 Countries by 70,000 Participants},
  journal = {Proceedings of the National Academy of Sciences},
  year = {2020},
  volume = {117},
  number = {5},
  pages = {2332--2337},
  doi = {10.1073/pnas.1911517117},
  note = {CORE ARGUMENT: Large-scale study of moral preferences using Moral Machine experiment. Finds both universal patterns and cultural variations in moral judgments about autonomous vehicles. Challenges idea of universal value alignment. Democratic input needed to resolve value conflicts. RELEVANCE: Shows empirical approach to understanding human values for AI alignment. Relevant for procedural experimentalism: experimentation can reveal value patterns. Cultural variation suggests need for context-sensitive procedural evaluation rather than universal standards. POSITION: Empirical study of moral preferences for AI.},
  keywords = {moral-preferences, value-alignment, empirical, cultural-variation, Medium}
}

@article{svensson2022fiduciary,
  author = {Svensson, Frans},
  title = {Can Algorithms Be Fiduciaries?},
  journal = {Philosophy \& Technology},
  year = {2022},
  volume = {35},
  number = {2},
  doi = {10.1007/s13347-022-00532-z},
  note = {CORE ARGUMENT: Analyzes whether AI systems can fulfill fiduciary duties. Fiduciary relationships require acting in beneficiary's interests, loyalty, and accountability. AI systems may functionally act as fiduciaries even if they can't bear moral responsibility. Need governance structures ensuring fiduciary standards. RELEVANCE: Directly addresses whether AI agents can legitimately represent human interests in markets. Fiduciary framework provides normative standards for AI representatives. Procedural governance structures enable functional fiduciary relationships even without AI moral agency. POSITION: AI as fiduciary and fiduciary duties.},
  keywords = {fiduciary-duty, representation, AI-agents, governance, High}
}

@article{burr2018algorithms,
  author = {Burr, Christopher and Cristianini, Nello and Ladyman, James},
  title = {An Analysis of the Interaction Between Intelligent Software Agents and Human Users},
  journal = {Minds and Machines},
  year = {2018},
  volume = {28},
  number = {4},
  pages = {735--774},
  doi = {10.1007/s11023-018-9479-0},
  note = {CORE ARGUMENT: Analyzes human-AI interaction using agent-based framework. AI agents shape human behavior through nudging, framing, and information filtering. Interaction is bidirectional but asymmetric in power. Need ethical frameworks for human-agent interaction design. RELEVANCE: Important for understanding power dynamics in human-AI interaction relevant for agentic markets. AI agents can shape human behavior in markets through information and framing. Asymmetric power raises legitimacy questions that procedural safeguards must address. POSITION: Human-AI interaction and power dynamics.},
  keywords = {human-AI-interaction, power-dynamics, nudging, framing, Medium}
}

@article{Johnson2020algorithmic,
  author = {Johnson, Gabbrielle M. and Verdicchio, Mario},
  title = {AI Anxiety},
  journal = {Journal of the American Philosophical Association},
  year = {2017},
  volume = {3},
  number = {4},
  pages = {417--437},
  doi = {10.1017/apa.2017.26},
  note = {CORE ARGUMENT: Analyzes anxiety about AI systems replacing human judgment and decision-making. Concerns center on loss of human control, opacity, and value misalignment. AI anxiety is warranted when systems threaten important human interests. Need governance ensuring human flourishing. RELEVANCE: Captures normative concerns about delegating decisions to AI that are relevant for agentic markets. Anxiety about AI representation is signal that legitimacy conditions may not be met. Understanding sources of AI anxiety helps identify what procedural safeguards must address. POSITION: Philosophical analysis of AI anxiety.},
  keywords = {AI-anxiety, human-control, legitimacy, human-flourishing, Low}
}

@article{walker2022relationality,
  author = {Walker, Lorna},
  title = {The Relational Turn and AI Ethics},
  journal = {Journal of Applied Philosophy},
  year = {2022},
  volume = {39},
  number = {2},
  pages = {321--338},
  doi = {10.1111/japp.12513},
  note = {CORE ARGUMENT: Applies relational ethics to AI systems. AI systems are embedded in networks of relationships and affect trust, dependency, and care relationships. Ethics of AI should focus on relationships rather than just individual autonomy or aggregate outcomes. RELEVANCE: Relational perspective relevant for understanding AI agents in markets as part of human relationships. Trust and dependency in representational relationships are relational goods. Procedural experimentalism should consider relational dimensions not just individual interests. POSITION: Relational ethics and AI systems.},
  keywords = {relational-ethics, trust, care-ethics, relationships, Low}
}

@article{min2019roles,
  author = {Min, Hokey},
  title = {Artificial Intelligence in Supply Chain Management: Theory and Applications},
  journal = {International Journal of Logistics Research and Applications},
  year = {2010},
  volume = {13},
  number = {1},
  pages = {13--39},
  doi = {10.1080/13675560902736537},
  note = {CORE ARGUMENT: Surveys AI applications in supply chain management including intelligent agents for procurement, inventory management, and logistics. AI agents make autonomous decisions within bounds set by humans. Effective AI integration requires appropriate oversight and error correction mechanisms. RELEVANCE: Concrete example of AI agents acting on behalf of organizations in economic contexts. Supply chain agents show how representational AI works in practice. Raises questions about oversight, error correction, and accountability in automated procurement—all procedural features. POSITION: AI agents in supply chain management.},
  keywords = {supply-chain, procurement-agents, oversight, applications, Low}
}

@article{wallach2008moral,
  author = {Wallach, Wendell and Allen, Colin},
  title = {Moral Machines: Teaching Robots Right from Wrong},
  publisher = {Oxford University Press},
  year = {2008},
  note = {CORE ARGUMENT: Explores how to build moral decision-making into autonomous systems. Top-down approaches (explicit rules) vs bottom-up approaches (learning). Hybrid approaches combining rules and learning most promising. Machines need functional morality even if they lack moral agency. RELEVANCE: Relevant for understanding whether AI agents can engage in moral learning, central to procedural experimentalism. If AI can engage in functional moral reasoning, this affects what kinds of procedural experimentation are meaningful. Hybrid approaches align with procedural experimentalism's learning framework. POSITION: Machine ethics and moral agency.},
  keywords = {machine-ethics, moral-agency, moral-learning, Medium}
}

@article{dubber2020defence,
  author = {Dubber, Markus D.},
  title = {The Dual Nature of AI: Challenges and Opportunities},
  journal = {German Law Journal},
  year = {2020},
  volume = {21},
  pages = {1168--1189},
  doi = {10.1017/glj.2020.65},
  note = {CORE ARGUMENT: Analyzes AI's dual nature as both tool (means) and agent (actor). This duality creates challenges for legal and ethical frameworks designed for clear means/ends distinctions. Need new frameworks recognizing AI's in-between status. RELEVANCE: Important for understanding AI representatives in markets—neither pure tools nor full agents. Dual nature affects what procedural standards apply. Recognition of in-between status supports procedural experimentalism's approach to discovering appropriate norms through experimentation. POSITION: AI dual nature as tool and agent.},
  keywords = {AI-agency, tools-vs-agents, legal-frameworks, Medium}
}
